 1/1:
# this is python
print('Hello, buddy!')
 1/2:
# guessing game
from random import randint
from IPython.display import clear_output
guessed = False
number = randint(0, 100)
guesses = 0
while not guessed:
    ans = input('Try to guess the number i am thinking about')
    quesses += 1
    clear_output()
    if int(ans) == number:
        print('Congrates! you guessed it correctly.')
        print('It took you {} guesses'.format(guesses))
        break
    elif int(ans) > number:
        print('The number is lower than what you guessed.')
    elif int(ans) < number:
        print('The number is greater then what you guessed.')
 1/3:
# guessing game
from random import randint
from IPython.display import clear_output
guessed = False
number = randint(0, 100)
guesses = 0
while not guessed:
    ans = input('Try to guess the number i am thinking about')
    quesses += 1
    clear_output()
    if int(ans) == number:
        print('Congrates! you guessed it correctly.')
        print('It took you {} guesses'.format(guesses))
        break
    elif int(ans) > number:
        print('The number is lower than what you guessed.')
    elif int(ans) < number:
        print('The number is greater then what you guessed.')
 1/4:
# guessing game
from random import randint
from IPython.display import clear_output
guessed = False
number = randint(0, 100)
guesses = 0
while not guessed:
    ans = input('Try to guess the number i am thinking about')
    guesses += 1
    clear_output()
    if int(ans) == number:
        print('Congrates! you guessed it correctly.')
        print('It took you {} guesses'.format(guesses))
        break
    elif int(ans) > number:
        print('The number is lower than what you guessed.')
    elif int(ans) < number:
        print('The number is greater then what you guessed.')
 3/1:
# this is a comment
print("Hello") # this is also a comment
 3/2:
""" This is a multi
line comment"""
 3/3:
"""

This is a multiline comment

"""
 3/4:
"""
This is a multiline comment
"""
 3/5: # the following are integers
 3/6:
print(1)
print(2)
print(50)
 3/7: print(type 8.0)
 3/8: print(type(8.0))
 3/9: # Variables
3/10:
num1 = 5
num2 = 8.4
print(num1, num2)
3/11:
switch = True
print(switch)
3/12:
# storing strings in a variable
name = 'John Smith'
fav_number = '9'
print(name, fav_number)
3/13:
# using two variables to create another variable
result = num1 + num2
print(result)
3/14:
# adding, deleting, multiplying, dividing from a variable
result += 1
print(result)
result *= num1
print(result)
3/15:
# defining a variable and overwriting it's value
name = 'John'
print(name)
name = 'Sam'
print(name)
3/16:
x = 3
y = 10
result = x * y
print(x + y)
3/17:
x = 3
y = 10
result = x * y
print('x + y = ', x + y)
3/18:
x = 3
y = 10
result = x * y
print('{} + {} = '.format(x, y), x + y)
3/19:
width = 245.54
height = 13.66
print(width * height)
3/20:
# using the addition operator without variables
name = 'John ' + 'Smith'
print(name)
3/21:
# using the addition operator without variables
name = 'John' + '' + 'Smith'
print(name)
3/22:
# using the addition operator without variables
name = 'John' + ' ' + 'Smith'
print(name)
3/23:
first_name = 'John'
last_name = 'Smith'
full_name = first_name + ' ' + last_name
print(full_name)
3/24:
name = 'John'
print('Hello {}'.format(name))
print('Hello {}, you are {} years old!'.format(name, 28))
3/25:
# using the new f string
name = 'John'
print(f'Hello{name}')
3/26:
# using the new f string
name = 'John'
print(f'Hello {name}')
3/27:
# using indexes to print each element
word = 'hello'
print(word[0])
print(word[1])
print(word[2])
3/28:
# using indexes to print each element
word = 'hello'
print(word[0])
print(word[1])
print(word[-1])
3/29:
# using indexes to print each element
word = 'hello'
print(word[0])
print(word[1])
print(word[-1])
print(word[0:2])
3/30:
# using indexes to print each element
word = 'hello'
print(word[0])
print(word[1])
print(word[-1])
print(word[0:2])
print(word[0: 5: 2])
3/31:
# using indexes to print each element
word = 'hello'
print(word[0])
print(word[1])
print(word[-1])
print(word[0:2])
print(word[0: 5: 1])
3/32:
# using indexes to print each element
word = 'hello'
print(word[0])
print(word[1])
print(word[-1])
print(word[0:2])
print(word[0: 5: 1.5])
3/33:
# using indexes to print each element
word = 'hello'
print(word[0])
print(word[1])
print(word[-1])
print(word[0:2])
print(word[0: 5: 2])
3/34: print(24, 4.5, False, 'John')
3/35: print('{} favourite sport is {}'.format('John', 'Swimming'))
3/36: print('{} is working on {} programming'.format('John Doe', 'python'))
3/37:
# using the title method to capitalize strings
name = 'John'
print(name.title())
3/38:
# using the title method to capitalize strings
name = 'john'
print(name.title())
3/39:
# using the title method to capitalize strings
first_name = 'john'
last_name = 'Smith'
print(first_name.title())
print(last_name.upper())
print(last_name.lower())
3/40:
# replacing an exclamation mark with a period
words = 'Hello there!'
print( words.replace( '!', '.'))
3/41:
# removing white spaces with strip
name = '  john  '
print(name.strip())
3/42:
# removing white spaces with strip
name = '  john  '
print(name)
3/43:
# removing white spaces with strip
name = '  john  '
print(name.strip())
3/44:
# removing white spaces with strip
name = '  john  smith'
print(name.strip())
3/45:
# removing white spaces with strip
name = '  john  smith       '
print(name)
3/46:
# removing white spaces with strip
name = '  john  smith.       '
print(name)
3/47:
# removing white spaces with strip
name = '  john  smith       '
print(name)
3/48:
# removing white spaces with strip
name = '  john  smith       '
print(name.strip())
3/49:
# removing white spaces with strip
name = '  john  smith       '
print(name.strip())

# remove white spaces from the left
name = ' jane'
print(name.lstrip())
3/50:
# removing white spaces with strip
name = '  john  smith       '
print(name.strip())

# remove white spaces from the left
name = ' jane'
print(name.lstrip())

# remove white spaces from the right
name = 'doe   '
print(name.rstrip())
3/51:
# finding the search index of our searched term
s = 'Look over that way'
print(s.find('over'))
3/52:
# converting a string into a list of words
s = 'These words are seperated by spaces'
print(s.split(''))
3/53:
# converting a string into a list of words
s = 'These words are seperated by spaces'
print(s.split(' '))
3/54:
# converting a string into a list of words
s = 'These words are seperated by spaces'
print(s.split('|'))
3/55:
# converting a string into a list of words
s = 'These words are seperated by spaces'
print(s.split(' '))
3/56:
# thursday challenge
s = 'uppercase'
print(s.uppercase())
3/57:
# thursday challenge
s = 'uppercase'
print(s.upper())
3/58:
# thursday challenge
s = 'uppercase'
print(s.upper())

s = '$$John Smith'
print(s.lstrip('$'))
3/59:
# thursday challenge
s = 'uppercase'
print(s.upper())

s = '$$John Smith'
print(s.strip('$'))
3/60:
# thursday challenge
s = 'uppercase'
print(s.upper())

s = '$$John Smith'
print(s.lstrip('$'))
3/61:
# thursday challenge
s = 'uppercase'
print(s.upper())

s = '$$John Smith$'
print(s.lstrip('$'))
3/62:
# thursday challenge
s = 'uppercase'
print(s.upper())

s = '$$John Smith$'
print(s.strip('$'))
3/63:
# thursday challenge
s = 'uppercase'
print(s.upper())

s = '$$John Smith'
print(s.lstrip('$'))
3/64: help('', strip)
3/65: help('', .strip)
3/66: help(' '.strip)
3/67: >>>help(' '.strip)
3/68: help(' '.strip)
3/69:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))
3/70:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)
3/71:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')
3/72:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))
3/73:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))
3/74:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))

# print a line between sections
print('=' * 50)
3/75:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))

# print a line between sections
print('=' * 50)

# printout the header for the section of totals
print(\t\t\tTotal)

# calculate total price and print it out
total = p1_price + p2_price + p3_price
print('\t\t\t${}'.format(total))
3/76:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))

# print a line between sections
print('=' * 50)

# printout the header for the section of totals
print(\t\t\tTotal)

# calculate total price and print it out
total = p1_price + p2_price + p3_price
print('\t\t\t${}'.format(total))
3/77:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))

# print a line between sections
print('=' * 50)

# printout the header for the section of totals
print('\t\t\tTotal')

# calculate total price and print it out
total = p1_price + p2_price + p3_price
print('\t\t\t${}'.format(total))
3/78:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))

# print a line between sections
print('=' * 50)

# printout the header for the section of totals
print('\t\t\tTotal')

# calculate total price and print it out
total = p1_price + p2_price + p3_price
print('\t\t\t${}'.format(total))

# print a line between sections
print('=' * 50)
3/79:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))

# print a line between sections
print('=' * 50)

# printout the header for the section of totals
print('\t\t\tTotal')

# calculate total price and print it out
total = p1_price + p2_price + p3_price
print('\t\t\t${}'.format(total))

# print a line between sections
print('=' * 50)


# output thank you message
print('\n\t{}\n'.format(message))
3/80:
# create product and price for three items
p1_name, p1_price = 'Books', 49.95
p2_name, p2_price = 'Computer', 579.99
p3_name, p3_price = 'Monitor', 124.89

# create a company name and information
company_name = 'coding temple, inc.'
company_address = '283 Franklin St.'
company_city = 'Boston, MA'

# declare ending message
message = 'Thanks for shopping with us today!'

# create top border
print('*' * 50)

# print company information first using format
print('\t\t{}'.format(company_name.title()))
print('\t\t{}'.format(company_address))
print('\t\t{}'.format(company_city))

# print a line between sections
print('=' * 50)

# print out headers for sections of items
print('\tProduct Name\tProduct Price')

# create a print statement for each product
print('\t{}\t\t${}'.format(p1_name.title(), p1_price))
print('\t{}\t\t${}'.format(p2_name.title(), p2_price))
print('\t{}\t\t${}'.format(p3_name.title(), p3_price))

# print a line between sections
print('=' * 50)

# printout the header for the section of totals
print('\t\t\tTotal')

# calculate total price and print it out
total = p1_price + p2_price + p3_price
print('\t\t\t${}'.format(total))

# print a line between sections
print('=' * 50)


# output thank you message
print('\n\t{}\n'.format(message))

# create a bottom border
print('*' * 50)
 4/1:
# accepting and outputting user input
print(input('What is your name?'))
 4/2:
# saving what the user input
ans = input('What is your name?')
print(ans)
 4/3:
# saving what the user input
ans = input('What is your name?')
print('Hello {}'.format(ans))
 4/4:
# how to check the data type of any given variable
num = 5
print(type(num))
 4/5:
# converting a variable from one data type to another
num = '9'
num = int(num)
print(type(num))
 4/6: print(str(True))
 4/7:
ans = input('Type a number to add:')
print(type(ans))
result = 100 + int(ans)
print('100 + {} = {}'.format(ans, result))
 4/8:
ans = input('Type a number to add:')
print(type(ans))
result = 100 + int(ans)
print('100 + {} = {}'.format(ans, result))
 4/9:
ans = input('Type a number to add:')
print(type(ans))
result = 100 + int(ans)
print('100 + {} = {}'.format(ans, result))
4/10:
# using the try and except block
try:
    ans = float(input('Type a number to add:'))
    print('100 + {} = {}'.format(ans, 100 + ans))
except:
    print('You did not put in a valid number!')
print('The program did not break')
4/11:
# using the try and except block
try:
    ans = float(input('Type a number to add:'))
    print('100 + {} = {}'.format(ans, 100 + ans))
except:
    print('You did not put in a valid number!')
print('The program did not break')
4/12:
# using the try and except block
try:
    ans = float(input('Type a number to add:'))
    print('100 + {} = {}'.format(ans, 100 + ans))
except:
    print('You did not put in a valid number!')
print('The program did not break')
4/13:
s = 'True'
print(bool(s))
4/14:
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))
4/15:
# converting
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))

# sum of inputs
num1 = input('Enter first number:')
num2 = input('Enter second number:')
print('The sum of {} and {} is: {}'.format(num1, num2, (num1 + num2)))
4/16:
# converting
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))

# sum of inputs
num1 = input('Enter first number:')
num2 = input('Enter second number:')
result = num1 + num2
print('The sum of {} and {} is: {}'.format(num1, num2, result))
4/17:
# converting
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))

# sum of inputs
try:
    num1 = int(input('Enter first number:'))
    num2 = int(input('Enter second number:'))
    result = num1 + num2
    print('The sum of {} and {} is: {}'.format(num1, num2, result))
except:
    print('Your input is invalid')
4/18:
# converting
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))

# sum of inputs
try:
    num1 = int(input('Enter first number:'))
    num2 = int(input('Enter second number:'))
    result = num1 + num2
    print('The sum of {} and {} is: {}'.format(num1, num2, result))
except:
    print('Your input is invalid')
4/19:
# converting
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))

# sum of inputs
try:
    num1 = int(input('Enter first number:'))
    num2 = int(input('Enter second number:'))
    result = num1 + num2
    print('The sum of {} and {} is: {}'.format(num1, num2, result))
except:
    print('Your input is invalid')
4/20:
# converting
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))

# sum of inputs
try:
    num1 = float(input('Enter first number:'))
    num2 = float(input('Enter second number:'))
    result = num1 + num2
    print('The sum of {} and {} is: {}'.format(num1, num2, result))
except:
    print('Your input is invalid')
4/21:
# converting
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))

# sum of inputs
try:
    num1 = float(input('Enter first number:'))
    num2 = float(input('Enter second number:'))
    result = num1 + num2
    print('The sum of {} and {} is: {}'.format(num1, num2, result))
except:
    print('Your input is invalid')
4/22:
# converting
s = 'True'
bs = bool(s)
print(bs)
print(type(bs))

# sum of inputs
try:
    num1 = float(input('Enter first number:'))
    num2 = float(input('Enter second number:'))
    result = num1 + num2
    print('The sum of {} and {} is: {}'.format(num1, num2, result))
except:
    print('Your input is invalid')
4/23:
# car information
try:
    year = int(input('Enter the year make of your car'))
    make = str(input('Enter the make of your car'))
    model = str(input('Enter the model of your car'))
    color = str(input('Enter the color of your car'))
    print('{} {} {} {}.'.format(year, color, model, make))
except:
    print('Your input is invalid')
4/24:
# car information
try:
    year = int(input('Enter the year make of your car:'))
    make = str(input('Enter the make of your car:'))
    model = str(input('Enter the model of your car:'))
    color = str(input('Enter the color of your car:'))
    print('{} {} {} {}.'.format(year, color, model, make))
except:
    print('Your input is invalid')
4/25:
# car information
try:
    year = int(input('Enter the year make of your car:')).strip()
    make = str(input('Enter the make of your car:'))
    model = str(input('Enter the model of your car:'))
    color = str(input('Enter the color of your car:'))
    print('{} {} {} {}.'.format(year, color, model, make))
except:
    print('Your input is invalid')
4/26:
# car information
try:
    year = int(input('Enter the year make of your car:').strip())
    make = str(input('Enter the make of your car:'))
    model = str(input('Enter the model of your car:'))
    color = str(input('Enter the color of your car:'))
    print('{} {} {} {}.'.format(year, color, model, make))
except:
    print('Your input is invalid')
3/81:
# this is a comment
print("Hello") # this is also a comment
4/27:
# car information
try:
    year = int(input('Enter the year make of your car:').strip())
    make = str(input('Enter the make of your car:').strip())
    model = str(input('Enter the model of your car:').strip())
    color = str(input('Enter the color of your car:').strip())
    print('{} {} {} {}.'.format(year, color, model, make))
except:
    print('Your input is invalid')
 5/1:
# using an if statement to only run code if the condition is met
x, y = 5, 10
if(x < y):
    print('x is less than y')
 5/2:
# car information
try:
    year = int(input('Enter the year make of your car:'))
    make = str(input('Enter the make of your car:'))
    model = str(input('Enter the model of your car:'))
    color = str(input('Enter the color of your car:'))
    print('{} {} {} {}.'.format(year, color, model, make))
except:
    print('Your input is invalid')
 5/3:
# using an if statement to only run code if the condition is met
x, y = 5, 10
if(x < y):
    print('x is less than y')
 5/4:
# checking user input
ans = int(input('What is 5 + 5?'))
if ans == 10:
    print('You got it right!')
 5/5:
# using the key word and in an if statement
x, y, z = 5, 10, 5
if x < y and x == z:
    print('Both statements were true')
 5/6:
# using key word or in an if statement
x, y, z = 5, 10, 5
if x < y or x = z:
    print('One or both statements were true')
 5/7:
# using key word or in an if statement
x, y, z = 5, 10, 5
if x < y or x == z:
    print('One or both statements were true')
 5/8:
# using the key word not in an if statement
flag = False
if not flag:
    print('flag is False')
 5/9:
# using the keyword in within an if statement
word = 'Baseball'
if 'b' in word:
    print('{} contains the character b'.format(word))
5/10:
# using the keyword not in within an if statement
word = 'Baseball'
if 'x' not in word:
    print('{} does not contain the character x'.format(word))
5/11:
# using the keyword not in within an if statement
word = 'Baseball'
if 'X' not in word:
    print('{} does not contain the character x'.format(word))
5/12:
# using the keyword not in within an if statement
word = 'Baseball'
if 'x' not in word:
    print('{} does not contain the character x'.format(word))
5/13:
user_input = input('Enter any word')
if 'es' in user_input:
    print('{} includes \'es\''.format(user_input))
5/14:
user_input = input('Enter any word')
if 'ing' in user_input:
    print('{} includes \'ing\''.format(user_input))
5/15:
user_input = input('Enter any word')
if 'ing' in user_input:
    print(len(user_input))
    print('{} includes \'ing\''.format(user_input))
5/16:
user_input = input('Enter any word')
if 'ing' in user_input:
    print(user_input[len(user_input) - 3, len(user_input)])
    print('{} includes \'ing\''.format(user_input))
5/17:
user_input = input('Enter any word')
if 'ing' in user_input:
    print(user_input[len(user_input) - 3: len(user_input)])
    print('{} includes \'ing\''.format(user_input))
5/18:
user_input = input('Enter any word')
if 'ing' in (user_input[len(user_input) - 3: len(user_input)]):
    print(user_input[len(user_input) - 3: len(user_input)])
    print('{} includes \'ing\' at the end.'.format(user_input))
5/19:
user_input = input('Enter any word')
if 'ing' in (user_input[len(user_input) - 3: len(user_input)]):
    print(user_input[len(user_input) - 3: len(user_input)])
    print('{} includes \'ing\' at the end.'.format(user_input))
5/20:
user_input = input('Enter any word')
if 'ing' in (user_input[len(user_input) - 3 : len(user_input)]):
    print('{} includes \'ing\' at the end.'.format(user_input))
5/21:
first_word = input('Enter any word')
second_word = input('Enter another word')
if first_word.lower() === second_word.lower():
    print('{} and {} are the same'.format(first_word, second_word))
5/22:
first_word = input('Enter any word')
second_word = input('Enter another word')
if first_word.lower() == second_word.lower():
    print('{} and {} are the same'.format(first_word, second_word))
5/23:
first_word = input('Enter any word')
second_word = input('Enter another word')
if first_word.lower() == second_word.lower():
    print('{} and {} are the same'.format(first_word, second_word))
5/24:
num = float(input('Enter any number: '))
if num < 10:
    print(num ** 2)
5/25:
num = float(input('Enter any number: '))
if num < 10:
    print(num * num)
5/26:
num = float(input('Enter any number: '))
if num < 10:
    print(num ** 2)
5/27:
# using elif conditional statements
x, y = 5, 10
if x > y:
    print('x is greater than y')
elif x < y:
    print('x is less than y')
5/28:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))
5/29:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))
5/30:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))
5/31:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/32:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/33:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))

reverse_option = str(input('Would you like to reverse the numbers? Yes or No'))

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/34:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))

try:
    reverse_option = str(input('Would you like to reverse the numbers? Yes or No').lower())
    if reverse_option == 'yes':
        num1 = num2
        num2 = num1
        
        print('First Number {}'.format(num1))
        print('Second Number {}'.format(num2))
except:
    print('Invalid input')
    

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/35:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))
reverse_option_number = num1

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    reverse_option = str(input('Would you like to reverse the numbers? Yes or No').lower())
    if reverse_option == 'yes':
        num1 = num2
        num2 = reverse_option_number
        
        print('First Number {}'.format(num1))
        print('Second Number {}'.format(num2))
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/36:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))
reverse_option_number = float(num1)

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    reverse_option = str(input('Would you like to reverse the numbers? Yes or No').lower())
    if reverse_option == 'yes':
        num1 = num2
        num2 = reverse_option_number
        
        print('First Number {}'.format(num1))
        print('Second Number {}'.format(num2))
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/37:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))
reverse_option_number = float(num1)

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    reverse_option = str(input('Would you like to reverse the numbers? Yes or No').lower())
    if reverse_option == 'yes':
        num1 = num2
        num2 = reverse_option_number
        
        print('First Number {}'.format(num1))
        print('Second Number {}'.format(num2))
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/38:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))
reverse_option_number = float(num1)

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    reverse_option = str(input('Would you like to reverse the numbers? Yes or No: ').lower())
    if reverse_option == 'yes':
        num1 = num2
        num2 = reverse_option_number
        
        print('First Number {}'.format(num1))
        print('Second Number {}'.format(num2))
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/39:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))
reverse_option_number = float(num1)

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    reverse_option = str(input('Would you like to reverse the numbers? Yes or No: ').lower())
    if reverse_option == 'yes':
        num1 = num2
        num2 = reverse_option_number
        
        print('First Number {}'.format(num1))
        print('Second Number {}'.format(num2))
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/40:
# step 1: ask user for which operation to be performed
operation = input('Would you like to add/subtract/multiply/divide?').lower()
print('You chose {}'.format(operation))

# step 2: ask for numbers, alert order matters for subtraction and division
if operation == 'subtract' or operation == 'divide':
    print('You chose {}'.format(operation))
    print('Please keep in mind that the order of your numbers matters')

num1 = input('What is the first number?')
num2 = input('What is the second number?')
print('First Number {}'.format(num1))
print('Second Number {}'.format(num2))
reverse_option_number = float(num1)

# step 3: setup try/except for mathematical operations
try:
    num1, num2 = float(num1), float(num2)
    reverse_option = str(input('Would you like to reverse the numbers? Yes or No: ').lower())
    if reverse_option == 'yes':
        num1 = num2
        num2 = reverse_option_number
        
        print('First Number {}'.format(num1))
        print('Second Number {}'.format(num2))
    if operation == 'add':
        result = num1 + num2
        print('{} + {} = {}'.format(num1, num2, result))
    elif operation == 'subtract':
        result = num1 - num2
        print('{} - {} = {}'.format(num1, num2, result))
    elif operation == 'multiply':
        result = num1 * num2
        print('{} * {} = {}'.format(num1, num2, result))
    elif operation == 'divide':
        result = num1 / num2
        print('{} / {} = {}'.format(num1, num2, result))
    else:
        print('Sorry, but \'{}\' is not an operation'.format(operation))
except:
    print('Error, improper numbers used. Please try again.')
5/41:
age = int(input('Enter your age: '))
if age > 0 and age < 13:
    print('Kid')
elif age > 12 and age < 20:
    print('Teenager')
elif age > 19 and age < 31:
    print('Young Adult')
elif age > 30 and age < 65:
    print('Adult')
elif age >= 65:
    print('Senior')
5/42:
age = int(input('Enter your age: '))
if age >= 0 and age <= 12:
    print('Kid')
elif age >= 13 and age <= 19:
    print('Teenager')
elif age >= 20 and age <= 30:
    print('Young Adult')
elif age >= 31 and age <= 64:
    print('Adult')
elif age >= 65:
    print('Senior')
5/43:
age = int(input('Enter your age: '))
if age >= 0 and age <= 12:
    print('Kid')
elif age >= 13 and age <= 19:
    print('Teenager')
elif age >= 20 and age <= 30:
    print('Young Adult')
elif age >= 31 and age <= 64:
    print('Adult')
elif age >= 65:
    print('Senior')
 7/1:
# declaring a list of numbers
nums = [5, 10, 15.2, 20]
print(nums)
 7/2:
# accessing elements within a list
print( nums[1])
num = nums[2]
print(num)
 7/3:
# declaring a list of mixed data types
num = 4.3
data = [num, 'word', True]
print(data)
 7/4:
# understanding lists within lists
data = [5, 'book', [34, 'hello'], True]
print(data)
print(data[2])
 7/5:
# using the double bracket anotation to access lists within lists
print(data[2])
 7/6:
# using the double bracket anotation to access lists within lists
print(data[2][0])
 7/7:
# using the double bracket anotation to access lists within lists
print(data[2][0])
inner_list = data[2]
print( inner_list[1])
 7/8:
# changing values in a list through index
data = [5, 10, 15, 20]
print(data)
data[0] = 100
print(data)
 7/9:
# understanding how lists are stored
a = [5, 10]
b = a
print( 'a: {}\t b: {}'.format(a, b) )
print( 'Location a[0]: {}\t Location b[0]: {}'.format(id(a[0]), id(b[0])))
a[0] = 20
print( 'a: {}\t b:{}'.format(a, b) )
7/10:
# using [:] to copy a list
data = [5, 10, 15, 20]
data_copy = data[:]
data[0] = 50
print( 'data: {}\t data_copy: {}'.format(data, data_copy) )
7/11:
# sports
sports = ['Football', 'Tennis', 'Rugby', 'Cricket', 'Swimming', 'Boxing']
print( 'I like to play {}.'.format(sports[0]) )
print( 'I like to play {}.'.format(sports[1]) )
print( 'I like to play {}.'.format(sports[2]) )
print( 'I like to play {}.'.format(sports[3]) )
print( 'I like to play {}.'.format(sports[4]) )
print( 'I like to play {}.'.format(sports[5]) )
7/12:
names = ['John', 'Abraham', 'Sam', 'Kelly']
print( names[0][0] )
print( names[1][0] )
print( names[2][0] )
print( names[3][0] )
7/13:
names = ['John', 'Abraham', 'Sam', 'Kelly']
print( '\'{}\', \'{}\', \'{}\', \'{}\''.format(names[0][0], names[1][0], names[2][0], names[3][0]) )
7/14:
# writing your first for loop using range
for num in range(5):
    print( 'Value: {}'.format(num) )
7/15:
# providing the start stop and step for the range function
for num in range(2, 10, 2):
    print( 'Value: {}.'.format(num) )
7/16:
# providing the start stop and step for the range function
for num in range(2, 10, 2):
    print( 'Value: {}'.format(num) )
7/17:
# printing all characters in a name using the 'in' operator
name = 'John Smith'
for letter in name:
    print( 'Value: {}'.format(letter) )
7/18:
# using the continue statement within the for loop
for num in range(5):
    if num == 3:
        continue
    print( num )
7/19:
# beaking out of the loop using the 'break' statement
for num in range(5):
    if num == 3:
        break
    print( num )
7/20:
# setting the place holder using the 'pass' keyword
for i in range(5):
    # TODO: add code to print number
    pass
7/21:
# loop that prints out numbers 1 to 100 that are divisible by three
for num in range(1, 100):
    if num % 3 == 0:
        print( num )
7/22:
# only vowels
vowels = ['a', 'e', 'i', 'o', 'u']
user_input = input('Enter any word: ')
for vowel in user_input:
    if vowel in vowels:
        print( vowel )
7/23:
# only vowels
vowels = ['a', 'e', 'i', 'o', 'u']
user_input = input('Enter any word: ')
for vowel in user_input:
    if vowel in vowels:
        print( '{}'.format(vowel) )
7/24:
# only vowels
vowels = ['a', 'e', 'i', 'o', 'u']
found_vowels = []
user_input = input('Enter any word: ')
for vowel in user_input:
    if vowel in vowels:
        found_vowels = vowel
    print( found_vowels )
7/25:
# only vowels
vowels = ['a', 'e', 'i', 'o', 'u']
found_vowels = []
user_input = input('Enter any word: ')
for vowel in user_input:
    if vowel in vowels:
        print( '{}'.format(vowel) )
7/26:
# writing your first while loop
health = 10
while healt > 0:
    print(healt)
    health -= 1
7/27:
# writing your first while loop
health = 10
while health > 0:
    print(healt)
    health -= 1
7/28:
# writing your first while loop
health = 10
while health > 0:
    print(healt)
    health -= 1
7/29:
# writing your first while loop
health = 10
while health > 0:
    print(health)
    health -= 1
7/30:
# using two or more loops together is called a nested loop
for i in range(2):
    for j in range(3):
        print( i, j )
7/31:
user_input = input('Enter any word: ')
while user_input is not 'quit':
    print( input('Enter any word: ') )
7/32:
user_input = input('Enter any word: ').lower()
while user_input != 'quit':
    user_input = input('Enter any word: ').lower()
7/33:
user_input = input('Enter any word: ').lower()
while user_input != 'quit':
    user_input = input('Enter any word: ').lower()
7/34:
game_over = False
while game_over:
    for num in range(0, 5):
        if num == 3:
            game_over = True
            break
        pass
7/35:
game_over = False
while game_over:
    for num in range(0, 5):
        if num == 3:
            game_over = True
            break
        print( num )
7/36:
game_over = False
while game_over:
    for num in range(0, 5):
        if num == 3:
            game_over = True
            break
        print( num )
7/37:
game_over = False
while not game_over:
    for num in range(0, 5):
        if num == 3:
            game_over = True
            break
        print( num )
7/38:
# checking the number of items within a list
nums = [5, 10, 15]
length = len(nums)
print( length )
7/39:
# accessing specific items of a list with slices
print(nums[1:3])
7/40:
# accessing specific items of a list with slices
print(nums[1:2])
7/41:
# accessing specific items of a list with slices
print(nums[1:3])
7/42:
# accessing specific items of a list with slices
print(nums[1:3])
print(nums[: 2])
7/43:
# accessing specific items of a list with slices
print(nums[1 : 3])
print(nums[: 2])
7/44:
# accessing specific items of a list with slices
print(nums[1 : 3])
print(nums[: 2])
print(nums[: : 2])
7/45:
# accessing specific items of a list with slices
print(nums[1 : 3])
print(nums[: 2])
print(nums[: : 2])
print(numa[ -2 : ])
7/46:
# accessing specific items of a list with slices
print(nums[1 : 3])
print(nums[: 2])
print(nums[: : 2])
print(nums[ -2 : ])
7/47:
# accessing specific items of a list with slices
print(nums[1 : 3])
print(nums[: 2])
print(nums[: : 2])
print(nums[ -1 : ])
7/48:
# accessing specific items of a list with slices
print(nums[1 : 3])
print(nums[: 2])
print(nums[: : 2])
print(nums[ -2 : ])
7/49:
# adding an item to the back of the list using append
nums = [10, 20]
nums.append(5)
print( nums )
7/50:
# adding a value to the beginning of the list
words = [ 'ball', 'base' ]
nums.insert(0, 'glove')
7/51:
# adding a value to the beginning of the list
words = [ 'ball', 'base' ]
nums.insert(0, 'glove')
7/52:
# adding a value to the beginning of the list
words = [ 'ball', 'base' ]
nums.insert(0, 'glove')
print( nums )
7/53:
# adding a value to the beginning of the list
words = [ 'ball', 'base' ]
nums.insert(0, 'glove')
print( nums )
7/54:
# adding a value to the beginning of the list
words = [ 'ball', 'base' ]
nums.insert(0, 'glove')
print( nums )
7/55:
# using pop to remove items and saving to a variable for use later
items = [5, 'ball', True]
items.pop()
removed_item = items.pop(0)
print(removed_item, '\n', items)
7/56:
# using the remove method with try and except
sports = ['baseball', 'soccer', 'football', 'hockey']
try:
    sports.remove('soccer')
except:
    print('That item does not exist in the list')
print( sports )
7/57:
#using min, max, sum
nums = [5, 3, 9]
print( min(nums) )
print( max(nums) )
print( sum(nums) )
7/58:
# using lists on numerical and alphanumerical data
nums = [5, 8, 0, 2]
sorted_nums = sorted(nums)
print( nums, sorted_nums )
7/59:
# sorting a list with .sort() in-replace
nums = [5, 8, 0, 2]
nums.sort()
print(nums)
7/60:
# using conditional statements on a list
names = [ 'Jack', 'Robert', 'Mary' ]
if 'Mary' in names:
    print('found')
if 'Jimmy' not in names:
    print('not found')
7/61:
# using conditionals to check if a list is empty
nums = []
if not nums:
    print( 'empty' )
7/62:
# using a for loop to print all items in a list
sports = [ 'Baseball', 'Hockey', 'Football', 'Basketball' ]
for sport in sports:
    print( sport )
7/63:
# using while loop to remove a certain value
names = [ 'Bob', 'Jack', 'Rob', 'Bob', 'Robert' ]
while 'Bob' in names:
    names.remove('Bob')
print( names )
7/64: # Thursday Exercises
7/65:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
for name in names:
    if names.count(name) > 0:
        names.pop(name)
print( names )
7/66:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
for name in names:
    if names.count(name) > 0:
        names.remove(name)
print( names )
7/67:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
if names.count(name) > 0:
    names.remove(name)    
print( names )
7/68:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
while name in names:
    if names.count(name) > 0:
        names.remove(name)
print( names )
7/69:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
for name in names:
    if names.count(name) > 0:
        names.remove(name)
print( names )
7/70:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
for name in names:
    if names.count(name) > 0:
        names.remove(name)
    else:
        names.append(name)
print( names )
7/71:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
for name in names:
    if names.count(name) > 0:
        names.remove(name)
        unique_names.append(name)
    else:
        unique_names.append(name)
print( unique_names )
7/72:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
for name in names:
    if names.count(name) > 0:
        unique_names.append(name)
    else:
        unique_names.append(name)
print( unique_names )
7/73:
names = ['Bob', 'Kenny', 'Amanda', 'Bob', 'Kenny']
unique_names = []
for name in names:
    if names.count(name) > 0:
        names.remove(name)
        unique_names.append(name)
    else:
        unique_names.append(name)
print( unique_names )
7/74:
# user input
user_input = input('Enter any word: ').lower()
user_input_list = []
while user_input != 'quit':
    user_input = input('Enter any word: ').lower()
    user_input_list.append(user_input)
for word in user_input_list:
    print( word )
7/75:
# user input
#user_input = input('Enter any word: ').lower()
user_input_list = []

while user_input != 'quit':
    user_input = input('Enter any word: ').lower()
    user_input_list.append(user_input)

for word in user_input_list:
    print( word )
7/76:
# user input
user_input = input('Enter any word: ').lower()
user_input_list = []

while user_input != 'quit':
    user_input = input('Enter any word: ').lower()
    user_input_list.append(user_input)

for word in user_input_list:
    print( word )
7/77:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = [ '_' ] * len(word)

# create main game loop
while not game_over:
    ans = input('Type quit or guess a letter: ').lower()
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
7/78:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = [ '_' ] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input()
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
 8/1:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = [ '_' ] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input()
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
 8/2:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = [ '_' ] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input(clear_output())
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
 9/1:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = [ '_' ] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input(clear_output())
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
 9/2:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = [ '_' ] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input(clear_output())
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
        # create a loop to change underscore to a proper lettter
        for i in range( len(word) ):
            if word[i] == ans:
                guesses[i] = ansf
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
 9/3:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = ['_'] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input(clear_output())
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
        # create a loop to change underscore to a proper lettter
        for i in range( len(word) ):
            if word[i] == ans:
                guesses[i] = ans
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
10/1:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = ['_'] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input(clear_output())
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
        # create a loop to change underscore to a proper lettter
        for i in range( len(word) ):
            if word[i] == ans:
                guesses[i] = ans
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
10/2:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = ['_'] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input(clear_output())
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
        # create a loop to change underscore to a proper lettter
        for i in range( len(word) ):
            if word[i] == ans:
                guesses[i] = ans
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
        
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
    elif word == ''.join(guesses):
        print('Congratulations, you guessed it correctly!')
        game_over = True
10/3:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = ['_'] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('You guessed letters: {}'.format(guessed))
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    ans = input(clear_output())
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word:
        print('You guessed correctly!')
        # create a loop to change underscore to a proper lettter
        for i in range( len(word) ):
            if word[i] == ans:
                guesses[i] = ans
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
        if ans not in guessed:
            guessed.append(ans)
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
    elif word == ''.join(guesses):
        print('Congratulations, you guessed it correctly!')
        game_over = True
10/4:
# pyramids
for i in range(4):
    print(i)
10/5:
# pyramids
for i in range(4):
    print(i * x)
10/6:
# pyramids
for i in range(4):
    print(i * 'x')
10/7:
# pyramids
for i in range(5):
    print(i * 'x')
10/8:
# pyramids
for i in range(50):
    print(i * 'x')
10/9:
# pyramids
for i in range(10, 50, 4):
    print(i * 'x')
10/10:
# pyramids
for i in range(0, 50, 4):
    print(i * 'x')
10/11:
# pyramids
for i in range(0, 50):
    print(i * 'x')
10/12:
# pyramids
for i in range(50):
    print(i * 'x')
10/13:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str:
        print(name)
10/14:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name != '':
        print(name)
10/15:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name != '':
        print(name)
10/16:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name != ' ':
        print(name)
10/17:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name != ' ':
        print(name)
10/18:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name[0]:
        print(name)
10/19:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name[0] != '':
        print(name)
10/20:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name[0] != '':
        print(name)
10/21:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name[0] != ' ':
        print(name)
10/22:
# output names
names = ['John', '  ', 'Amanda', 5]
for name in names:
    if type(name) == str and name[0] != ' ':
        print(name)
10/23:
# convert celsius
temps = [32, 12, 44, 29]
result_temps = []
for temp in temps:
    F = (9/5) * temp + 32
    result_temps.append(F)
print(F)
10/24:
# convert celsius
temps = [32, 12, 44, 29]
result_temps = []
for temp in temps:
    result_temps.append((9/5) * temp + 32)
print(F)
10/25:
# convert celsius
temps = [32, 12, 44, 29]
result_temps = []
for temp in temps:
    result_temps.append((9/5) * temp + 32)
print(result_temps)
11/1:
# writing your first function
def printInfo():
    print('Name:John Smith')
    print('Age 45')

printInfo()
printInfo()
11/2:
# performing a calculation in a function
def calc():
    x, y = 5, 10
    print( x + y )
calc()
11/3:
# print name
def myName():
    print('John Doe')
    
myName()
11/4:
# pizza toppings
def pizzaToppings():
    print('Meat')
    print('Vegitables')
    peint('Chicken')
11/5:
# pizza toppings
def pizzaToppings():
    print('Meat')
    print('Vegitables')
    peint('Chicken')

pizzaToppings()
11/6:
# pizza toppings
def pizzaToppings():
    print('Meat')
    print('Vegitables')
    print('Chicken')

pizzaToppings()
11/7:
# passing a single parameter into a function
def printName(full_name):
    print( 'Your name is: {}'.format(full_name) )

printName('John Doe')
11/8:
# passing a single parameter into a function
def printName(full_name):
    print( 'Your name is: {}'.format(full_name) )

printName('John Doe')
printName('Amanda')
11/9:
# passing multiple parameters into a function
def addNums(num1, num2):
    result = num1 + num2
    print( '{} + {} = {}'.format(num1, num2, result) )

addNums( 5, 8 )
addNums( 3.5, 5.5 )
11/10:
# using a function to square all information
numbers1 = [ 2, 4, 5, 10 ]
numbers2 = [ 1, 3, 6 ]
def squares(nums):
    for num in nums:
        print( num ** 2 )

squares(numbers1)
squares(numbers2)
11/11:
# setting default parameter values
def calcArea(r, pi=3.14):
    area = pi * (r ** 2)
    print( 'Area: {}'.format(area) )

calcArea(2)
11/12:
# setting default parameter values
def printName(first, last, middle = ''):
    if middle:
        print( '{} {} {}'.format(first, middle, last) )
    else:
        print( '{} {}'.format(first, last) )

printName('John', 'Smith')
printName('John', 'Smith', 'Paul')
11/13:
# explicitly assigning values to parametersby referencing the name
def addNums(num1, num2):
    print(num1)
    print(num2)
    
addNums(5, num2 = 2.5)
11/14:
# using args parameter to take in a turple of arbituary values
def outputData(name, *args):
    print( type(args) )
    for arg in args:
        print( arg )
    
outputData('John Smith', 5, True, 'Jess')
11/15:
# using kwargs to take in a dictionary of arbituary values
def outputData(**kwargs):
    print( type(kwargs) )
    print( kwargs['name'] )
    print( kwargs['num'] )
    
outputData(name = 'John Smith', num = 5, b = True)
12/1:
# user input
user_input = input('Enter a word: ')
def checkUppercase(user_input):
    if user_input[0].upper():
        print( True )
    else:
        print( False )
checkUppercase(user_input)
12/2:
# user input
user_input = input('Enter a word: ')
def checkUppercase(user_input):
    if user_input[0].upper():
        print( True )
    else:
        print( False )
checkUppercase(user_input)
12/3:
# user input
user_input = input('Enter a word: ')

def checkUppercase(user_input):
    first_letter = user_input[0]
    print( first_letter )
#     if user_input[0].upper():
#         print( True )
#     else:
#         print( False )
checkUppercase(user_input)
12/4:
# user input
user_input = input('Enter a word: ')

def checkUppercase(user_input):
    first_letter = user_input[0]
    print( first_letter )
#     if user_input[0].upper():
#         print( True )
#     else:
#         print( False )
checkUppercase(user_input)
12/5:
# user input
user_input = str(input('Enter a word: '))

def checkUppercase(user_input):
    first_letter = user_input[0]
    if first_letter.upper():
        print( True )
    else:
        print( False )
checkUppercase(user_input)
12/6:
# user input
user_input = str(input('Enter a word: '))

def checkUppercase(user_input):
    first_letter = user_input[0]
    if first_letter.upper():
        print( True )
    else:
        print( False )
checkUppercase(user_input)
12/7:
# user input
user_input = str(input('Enter a word: '))

def checkUppercase(user_input):
    first_letter = user_input[0]
    if first_letter == first_letter.upper():
        print( True )
    else:
        print( False )
checkUppercase(user_input)
12/8:
# user input
user_input = str(input('Enter a word: '))

def checkUppercase(user_input):
    first_letter = user_input[0]
    if first_letter == first_letter.upper():
        print( True )
    else:
        print( False )
checkUppercase(user_input)
12/9:
# no name
def name(first_name = '', last_name = ''):
    if first_name and last_name:
        print( 'First name is: {}, last name is: {}'.format(first_name, last_name) )
    else:
        print( 'No name is passed' )
12/10:
# no name
def name(first_name = '', last_name = ''):
    if first_name and last_name:
        print( 'First name is: {}, last name is: {}'.format(first_name, last_name) )
    else:
        print( 'No name is passed' )
name('John', 'Doe')
name('Jane')
12/11:
# no name
def name(first_name = '', last_name = ''):
    if first_name or last_name:
        print( 'First name is: {}, last name is: {}'.format(first_name, last_name) )
    else:
        print( 'No name is passed' )
name('John', 'Doe')
name('Jane')
12/12:
# no name
def name(first_name = '', last_name = ''):
    if first_name and last_name:
        print( 'First name is: {}, last name is: {}'.format(first_name, last_name) )
    else:
        print( 'No name is passed' )
name('John', 'Doe')
name('Jane')
12/13:
# no name
def name(first_name = '', last_name = ''):
    if first_name:
        print( 'First name is: {}'.format(first_name) )
    elif last_name:
        print( 'Last name is: {}'.format(last_name) )
    else:
        print( 'No name is passed' )
name('John', 'Doe')
name('Jane')
12/14:
# no name
def name(first_name = '', last_name = ''):
    if first_name:
        print( 'First name is: {}'.format(first_name) )
    elif last_name:
        print( 'Last name is: {}'.format(last_name) )
    else:
        print( 'No name is passed' )
name('John', 'Doe')
name('Jane')
name()
12/15:
# using the return keyword to return the sum of two numbers
def addNums(num1, num2):
    return num1 + num2
num = addNums(5.5, 4.5)
print(num)
print(addNums(10, 10))
12/16:
# shorthand syntax using a ternary operator
def searchList(aList, el):
    return True if el in aList else False
result = searchList([ 'One', 2, 'Three' ], 2)
print(result)
12/17:
# full name
def fullName(first_name, last_name):
    return '{}, {}'.format(first_name, lastname)

fullName('John', 'Doe')
12/18:
# full name
def fullName(first_name, last_name):
    return '{}, {}'.format(first_name, last_name)

fullName('John', 'Doe')
12/19:
# full name
def fullName(first_name, last_name):
    return '{}, {}'.format(first_name, last_name)

fullName('John', 'Doe')
12/20:
# full name
def fullName(first_name, last_name):
    return '{}, {}'.format(first_name, last_name)

fullName('John', 'Doe')
12/21:
# full name
def fullName(first_name, last_name):
    return '{} {}'.format(first_name, last_name)

fullName('John', 'Doe')
12/22:
# user input
def userInput():
    user_input = input('Enter a word: ')
    return user_input

word = userInput()
print(word)
12/23:
# where global variables can be accessed
number = 5
def scopeTest():
    number += 1
scopeTest()
12/24:
# accessing variables declared in a function
def scopeTest():
    word = 'function'
    return word
value = scopeTest()
print(value)
12/25:
# changing list item values by index
sports = [ 'baseball', 'football', 'hockey', 'basketball' ]
def change(aList):
    aList[0] = 'soccer'
print('Before Altering: {}'.format(sports))
change(sports)
print('After Altering: {}'.format(sports))
12/26:
# names
names = ['Bob', 'Rich', 'Amanda']
def changeValue(aList, name, index):
    name[index] = name
    
changeValue(names, 'Bill', 1)
12/27:
# names
names = ['Bob', 'Rich', 'Amanda']
def changeValue(aList, name, index):
    names[index] = name
    
changeValue(names, 'Bill', 1)
12/28:
# names
names = ['Bob', 'Rich', 'Amanda']
def changeValue(aList, name, index):
    names[index] = name
    
changeValue(names, 'Bill', 1)
12/29:
# names
names = ['Bob', 'Rich', 'Amanda']
def changeValue(aList, name, index):
    names[index] = name
    return names
    
newList = changeValue(names, 'Bill', 1)
print(names)
12/30:
# import necessary functions
from IPython.display import clear_output
# global list variable
cart = []

# create a function to add items to cart
def addItem(item):
    clear_output()
    cart.append(item)
    print( '{} has been added.'.format(item) )
    
# create function to remove items from cart
def removeItem(item):
    clear_output()
    try:
        cart.remove(item)
        print( '{} has been removed.'.format(item) )
    except:
        print( 'Sorry we could not remove that item.' )

# create a function to show items in a cart
def showCart():
    clear_output()
    if cart:
        print( 'Here is your cart:' )
        for item in cart:
            print( '- {}'.format(item) )
    else:
        print( 'Your cart is empty.' )

# create function to clear items from the cart
def clearCart():
    clear_output()
    cart.clear()
    print( 'Your cart is empty.' )

# create the main function that loops until the user quits
def main():
    done = False
    while not done:
        ans = input('quit/add/remove/show/clear: ').lower()
        # base case
        if ans == 'quit':
            print( 'Thanks for using our program.' )
            showCart()
            done = True
main()
12/31:
# import necessary functions
from IPython.display import clear_output
# global list variable
cart = []

# create a function to add items to cart
def addItem(item):
    clear_output()
    cart.append(item)
    print( '{} has been added.'.format(item) )
    
# create function to remove items from cart
def removeItem(item):
    clear_output()
    try:
        cart.remove(item)
        print( '{} has been removed.'.format(item) )
    except:
        print( 'Sorry we could not remove that item.' )

# create a function to show items in a cart
def showCart():
    clear_output()
    if cart:
        print( 'Here is your cart:' )
        for item in cart:
            print( '- {}'.format(item) )
    else:
        print( 'Your cart is empty.' )

# create function to clear items from the cart
def clearCart():
    clear_output()
    cart.clear()
    print( 'Your cart is empty.' )

# create the main function that loops until the user quits
def main():
    done = False
    while not done:
        ans = input('quit/add/remove/show/clear: ').lower()
        # base case
        if ans == 'quit':
            print( 'Thanks for using our program.' )
            showCart()
            done = True
main()
12/32:
# import necessary functions
from IPython.display import clear_output
# global list variable
cart = []

# create a function to add items to cart
def addItem(item):
    clear_output()
    cart.append(item)
    print( '{} has been added.'.format(item) )
    
# create function to remove items from cart
def removeItem(item):
    clear_output()
    try:
        cart.remove(item)
        print( '{} has been removed.'.format(item) )
    except:
        print( 'Sorry we could not remove that item.' )

# create a function to show items in a cart
def showCart():
    clear_output()
    if cart:
        print( 'Here is your cart:' )
        for item in cart:
            print( '- {}'.format(item) )
    else:
        print( 'Your cart is empty.' )

# create function to clear items from the cart
def clearCart():
    clear_output()
    cart.clear()
    print( 'Your cart is empty.' )

# create the main function that loops until the user quits
def main():
    done = False
    while not done:
        ans = input('quit/add/remove/show/clear: ').lower()
        # base case
        if ans == 'quit':
            print( 'Thanks for using our program.' )
            showCart()
            done = True
        elif ans == 'add':
            item = input('What would you like to add?').title()
            addItem(item)
        elif ans == 'remove':
            showCart()
            item = input('What item would you like to remove?').title()
            removeItem(item)
        elif ans == 'show':
            showCart()
        elif ans == 'clear':
            clearCart()
        else:
            print( 'Sorry that was not an option.' )
main()
12/33:
# import necessary functions
from IPython.display import clear_output
# global list variable
cart = []

# create a function to add items to cart
def addItem(item):
    clear_output()
    cart.append(item)
    print( '{} has been added.'.format(item) )
    
# create function to remove items from cart
def removeItem(item):
    clear_output()
    try:
        cart.remove(item)
        print( '{} has been removed.'.format(item) )
    except:
        print( 'Sorry we could not remove that item.' )

# create a function to show items in a cart
def showCart():
    clear_output()
    if cart:
        print( 'Here is your cart:' )
        for item in cart:
            print( '- {}'.format(item) )
    else:
        print( 'Your cart is empty.' )

# create function to clear items from the cart
def clearCart():
    clear_output()
    cart.clear()
    print( 'Your cart is empty.' )

# create the main function that loops until the user quits
def main():
    done = False
    while not done:
        ans = input('quit/add/remove/show/clear: ').lower()
        # base case
        if ans == 'quit':
            print( 'Thanks for using our program.' )
            showCart()
            done = True
        elif ans == 'add':
            item = input('What would you like to add?').title()
            addItem(item)
        elif ans == 'remove':
            showCart()
            item = input('What item would you like to remove?').title()
            removeItem(item)
        elif ans == 'show':
            showCart()
        elif ans == 'clear':
            clearCart()
        else:
            print( 'Sorry that was not an option.' )
main()
13/1:
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = ['_'] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('You guessed letters: {}'.format(guessed))
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    clear_output()
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word and ans not in guessed:
        print('You guessed correctly!')
        # create a loop to change underscore to a proper lettter
        for i in range( len(word) ):
            if word[i] == ans:
                guesses[i] = ans
    elif ans in guessed:
        print('You already guessed that. Try again.')
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
        if ans not in guessed:
            guessed.append(ans)
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
    elif word == ''.join(guesses):
        print('Congratulations, you guessed it correctly!')
        game_over = True
12/34:
# refactor hangman
# import additional functions
from random import choice
from IPython.display import clear_output

# declare game variables
words = [ 'tree', 'basket', 'chair', 'paper', 'python' ]
word = choice(words)
guessed, lives, game_over = [], 7, False

# create a list of underscores to the length of the word
guesses = ['_'] * len(word)

# create main game loop
while not game_over:
    # output game information
    hidden_word = ''.join(guesses)
    print('You guessed letters: {}'.format(guessed))
    print('Word to guess: {}'.format(hidden_word))
    print('Lives: {}'.format(lives))
    ans = input('Type quit or guess a letter: ').lower()
    clear_output()
    if ans == 'quit':
        print('Thanks for playing.')
        game_over = True
    elif ans in word and ans not in guessed:
        print('You guessed correctly!')
        # create a loop to change underscore to a proper lettter
        for i in range( len(word) ):
            if word[i] == ans:
                guesses[i] = ans
    elif ans in guessed:
        print('You already guessed that. Try again.')
    else:
        lives -= 1
        print('Incorrect, you lost a life.!')
        if ans not in guessed:
            guessed.append(ans)
    if lives <= 0:
        print('You lost all your lives, you lost!')
        game_over = True
    elif word == ''.join(guesses):
        print('Congratulations, you guessed it correctly!')
        game_over = True
12/35:
# palindrome
word = 'madam'
print( word[::-1] )
12/36:
# palindrome
word = 'abcdef'
print( word[::-1] )
12/37:
# palindrome
word = 'abcdef'
print( word[::-2] )
12/38:
# palindrome
word = 'abcdef'
print( word[::-1] )
12/39:
# palindrome
word = input('Enter a palindrome word: ')
print( word[::-1] )
def reverse(word):
    return word[::-1]

def checkPalindrome(word):
    reversed_word = reverse(word)
    if word = reversed_word:
        print( True )
    else:
        print( False )

checkPalindrome(word)
12/40:
# palindrome
word = input('Enter a palindrome word: ')
print( word[::-1] )
def reverse(word):
    return word[::-1]

def checkPalindrome(word):
    reversed_word = reverse(word)
    if word == reversed_word:
        print( True )
    else:
        print( False )

checkPalindrome(word)
12/41:
# palindrome
word = input('Enter a palindrome word: ')
def reverse(word):
    return word[::-1]

def checkPalindrome(word):
    reversed_word = reverse(word)
    if word == reversed_word:
        print( True )
    else:
        print( False )

checkPalindrome(word)
12/42:
# palindrome
word = input('Enter a palindrome word: ').strip()
def reverse(word):
    return word[::-1]

def checkPalindrome(word):
    reversed_word = reverse(word)
    if word == reversed_word:
        print( True )
    else:
        print( False )

checkPalindrome(word)
12/43:
# palindrome
word = input('Enter a palindrome word: ').strip()
word = ''.join(word)
def reverse(word):
    return word[::-1]

def checkPalindrome(word):
    reversed_word = reverse(word)
    if word == reversed_word:
        print( True )
    else:
        print( False )

checkPalindrome(word)
12/44:
# palindrome
word = input('Enter a palindrome word: ').strip()
word = word.replace(' ', '')
def reverse(word):
    return word[::-1]

def checkPalindrome(word):
    reversed_word = reverse(word)
    if word == reversed_word:
        print( True )
    else:
        print( False )

checkPalindrome(word)
14/1:
# declaring a dictionary variable
empty = {}
person = {
    'name': 'John Smith'
}
customer = {
    'name': 'Morty',
    'age': 26
}

print( customer )
14/2:
# accessing dictionary information through keys
person = {
    'name': 'John'
}
print( person['name'] )
14/3:
# using the get method to access dictionary information
person = { 'name': 'John' }
print( person.get('name') )
print( person.get('age', 'Age is not available.') )
14/4:
# storing a list within a dictionary and accessing it
data = { 'sports': [ 'baseball', 'football', 'hockey', 'soccer' ] }
print( data['sports'][0] )
14/5:
# improperly storing a list into a dictionary
sports = [ 'baseball', 'football', 'hockey', 'soccer' ]
sports_dict = dict(sports)
14/6:
# improperly storing a list into a dictionary
sports = [ 'baseball', 'football', 'hockey', 'soccer' ]
sports_dict = dict( sports )
14/7:
# improperly storing a list into a dictionary
sports = [ 'baseball', 'football', 'hockey', 'soccer' ]
sports_dict = dict( { 'sports': sports } )
14/8:
# storing a dictionary within a list and accessing it
data = [ 'John', 'Denis', { 'name': 'Kirsten' } ]
print( data[2] )
print( data[2]['name'] )
14/9:
# storing a dictionary within a dictionary and accessing it
data = {
    'team': 'Boston Red Sox',
    'wins': '{ '2018': 108, '2017': 93 }'
}
print( data['wins'] )
print( data['wins']['2018'] )
14/10:
# storing a dictionary within a dictionary and accessing it
data = {
    'team': 'Boston Red Sox',
    'wins': '{ '2018': 108, '2017': 93 }'
}
print( data['wins'] )
#print( data['wins']['2018'] )
14/11:
# storing a dictionary within a dictionary and accessing it
data = {
    'team': 'Boston Red Sox',
    'wins': { '2018': 108, '2017': 93 }
}
print( data['wins'] )
print( data['wins']['2018'] )
14/12:
# user input
user_name = input('Enter your name: ').title()
age = int(input('Enter your age'))

user_data = {
    'name': user_name,
    'age': age
}

print( user_data )
14/13:
pizza = {
    'ingredients': ['cheese', 'sausage', 'peppers']
}

for ingredient in pizza['ingredients']:
    print( ingredient )
14/14:
# adding new key/value pairs to a dictionary
car = { 'year': '2018' }
car['color'] = 'Blue'
print( 'Year: {} \t Color: {}'.format(car['year'], car['color']) )
14/15:
# updating a value for a key/value pair that already exists
car = { 'year': '2018', 'color': 'Blue' }
car['color'] = 'red'
print( 'Year: {} \t Color: {}'.format(car['year'], car['color']) )
14/16:
# deleting a key/value pair from a dictionary
car = { 'year': 2018 }
try:
    del car['year']
    print( car )
except:
    print( 'That key does not exist' )
14/17:
# looping over the dictionary via the keys
person = { 'name': 'John', 'age': 26 }
for key in person.keys():
    print( key )
    print( person[key] )
14/18:
# looping over the dictionary via the values
person = { 'name': 'John', 'age': 26 }
for value in person.values():
    print( value )
14/19:
# looping over the dictionary via the key/value pair
person = { 'name': 'John', 'age': 26 }
for key, value in person.items():
    print( '{}: {}'.format(key, value) )
14/20:
# user input
user_info = {
    'name': 'John Doe',
    'address': 'New York',
    'number': '(5053) 225 643'
}

for key, value in user_info.items():
    print( 'name: {}, address: {}, number: {}'.format(key, value) )
14/21:
# user input
user_info = {
    'name': 'John Doe',
    'address': 'New York',
    'number': '(5053) 225 643'
}

for key, value in user_info.items():
    print( 'name: {}'.format(key, value) )
14/22:
# user input
user_info = {
    'name': 'John Doe',
    'address': 'New York',
    'number': '(5053) 225 643'
}

for key, value in user_info.items():
    print( 'name: {}'.format(value.name) )
    print( 'address: {}'.format(value.address) )
    print( 'number: {}'.format(value.number) )
14/23:
# user input
user_info = {
    'name': 'John Doe',
    'address': 'New York',
    'number': '(5053) 225 643'
}

for key, value in user_info.items():
    print( '{}, {}, {}'.format(value) )
14/24:
# user input
user_info = {
    'name': 'John Doe',
    'address': 'New York',
    'number': '(5053) 225 643'
}

for key, value in user_info.items():
    print( '{}, {}, {}'.format(value) )
14/25:
# user input
user_info = {
    'name': 'John Doe',
    'address': 'New York',
    'number': '(5053) 225 643'
}

for key, value in user_info.items():
    print( '{}'.format(value) )
14/26:
# declaring a turple
t1 = ('hello', 2, 'hello')
t2 = True, 1
print( type(t1), type(t2) )
14/27:
# declaring a turple
t1 = ('hello', 2, 'hello')
t2 = True, 1
print( type(t1), type(t2) )
t1[0] = 1
14/28:
# declaring a set
s1 = set( [1, 2, 3, 1] )
s2 = { 4, 4, 5 }
print( type(s1), type(s2) )
14/29:
# declaring a set
s1 = set( [1, 2, 3, 1] )
s2 = { 4, 4, 5 }
print( type(s1), type(s2) )
s1.add(5)
s1.remove(1)
print( s1 )
14/30:
# declaring a frozen set
fset = ([ 1, 2, 3, 4 ])
print( type(fset) )
14/31:
# declaring a frozen set
fset = ( [1, 2, 3, 4] )
print( type(fset) )
14/32:
# user input
bank_number_list = []
while bank_number != 'quit':
    bank_numbers = input('Enter Bank Number')
    bank_number_list.append(bank_numbers)

fbank_number_list = ( bank_number_list )

print( type(fbank_number_list) )
14/33:
# user input
bank_numbers = input('Enter Bank Number')
bank_number_list = []
while bank_numbers != 'quit':
    bank_numbers = input('Enter Bank Number')
    bank_number_list.append(bank_numbers)

fbank_number_list = ( bank_number_list )

print( type(fbank_number_list) )
14/34:
# user input
bank_numbers = input('Enter Bank Number')
bank_number_list = []
while bank_numbers != 'quit':
    bank_numbers = input('Enter Bank Number')
    bank_number_list.append(bank_numbers)

fbank_number_list = ( bank_number_list )

for item in fbank_number_list:
    print( item )

print( type(fbank_number_list) )
14/35:
# conversion
nums = [3, 4, 3, 7, 10]
nums_set = ( nums )
print( type(nums_set) )
14/36:
# conversion
nums = [3, 4, 3, 7, 10]
nums_set = {}
for num in nums:
    nums_set.add(num)
print( type(nums_set) )
14/37:
# conversion
nums = [3, 4, 3, 7, 10]
nums_set = set()
for num in nums:
    nums_set.add(num)
print( type(nums_set) )
14/38:
# conversion
nums = [3, 4, 3, 7, 10]
nums_set = set()
for num in nums:
    if nums.count(num) == 1:
        nums_set.add(num)
print( type(nums_set) )
for item in nums_set:
    print(item)
14/39:
# conversion
nums = [3, 4, 3, 7, 10]
nums_set = set()
for num in nums:
    if nums.count(num) >= 1:
        nums_set.add(num)
print( type(nums_set) )
for item in nums_set:
    print(item)
14/40:
# conversion
nums = [3, 4, 3, 7, 10]
nums_set = set()
for num in nums:
   # if nums.count(num) >= 1:
    nums_set.add(num)
print( type(nums_set) )
for item in nums_set:
    print(item)
14/41:
# conversion
nums = [3, 4, 3, 7, 10]
nums_set = set()

for num in nums:
    nums_set.add(num)
    
for item in nums_set:
    print(item)
14/42:
# opening/creating and writing to a text file
f = open('test.txt', 'w+')
f.write('this is a test')
f.close()

# reading from a text file
f = open('test.txt', 'r')
data = f.read()
f.close()
print(data)
14/43:
# opening/creating and writing to a csv file
import csv
with open('test.csv', mode='w', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    writer.writerow( ['Name', 'City'] )
    writer.writerow( ['Craig Lou', 'Taiwan'] )
14/44:
# reading from csv files
with open('test.csv', mode='r') as f:
    reader = csv.reader(f, delimiter=',')
    for row in reader:
        print( row )
14/45:
# user input
user_input = int(input('Enter any number'))
f = open('sample-file.txt', 'w+')
f.write(user_input)
f.close()

f.open('sample-file.txt', 'r')
data = f.read()
f.close()

print( data )
14/46:
# user input
user_input = input('Enter any number')
f = open('sample-file.txt', 'w+')
f.write(user_input)
f.close()

f.open('sample-file.txt', 'r')
data = f.read()
f.close()

print( data )
14/47:
# user input
user_input = str(input('Enter any number'))
f = open('sample-file.txt', 'w+')
f.write(user_input)
f.close()

f.open('sample-file.txt', 'r')
data = f.read()
f.close()

print( data )
14/48:
# user input
user_input = str(input('Enter any number'))
f = open('sample-file.txt', 'w+')
f.write(user_input)
f.close()

f = open('sample-file.txt', 'r')
data = f.read()
f.close()

print( data )
14/49:
# user input
user_input = input('Enter any number')
f = open('sample-file.txt', 'w+')
f.write(user_input)
f.close()

f = open('sample-file.txt', 'r')
data = f.read()
f.close()

print( data )
14/50:
# data dumping

import csv

data = {
'name' : ['Dave', 'Dennis', 'Peter', 'Jess'],
'language': ['Python', 'C', 'Java', 'Python']
}

with open('sample-csv', mode='w', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    writer.writerow( [ data['name'], data['language'] ] )
    for item in data:
        writer.writerow( [ data['name'], data['language'] ] )
14/51:
# data dumping

import csv

data = {
'name' : ['Dave', 'Dennis', 'Peter', 'Jess'],
'language': ['Python', 'C', 'Java', 'Python']
}

with open('sample-csv', mode='w', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    #writer.writerow( [ data['name'], data['language'] ] )
    for item in data['name']:
        writer.writerow( item )
    for item in data['language']:
        writer.writerow( item )
14/52:
# data dumping

import csv

data = {
'name' : ['Dave', 'Dennis', 'Peter', 'Jess'],
'language': ['Python', 'C', 'Java', 'Python']
}

with open('sample-csv', mode='w', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    writer.writerow( [ data['name'], data['language'] ] )
14/53:
# data dumping

import csv

data = {
'name' : ['Dave', 'Dennis', 'Peter', 'Jess'],
'language': ['Python', 'C', 'Java', 'Python']
}

with open('sample-csv', mode='w', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    writer.writerow( data )
    writer.writerow( [ data['name'], data['language'] ] )
14/54:
# data dumping

import csv

data = {
'name' : ['Dave', 'Dennis', 'Peter', 'Jess'],
'language': ['Python', 'C', 'Java', 'Python']
}

with open('sample-csv', mode='w', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    writer.writerow( [ data ] )
    writer.writerow( [ data['name'], data['language'] ] )
14/55:
# data dumping

import csv

data = {
'name' : ['Dave', 'Dennis', 'Peter', 'Jess'],
'language': ['Python', 'C', 'Java', 'Python']
}

with open('sample-csv', mode='w', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    writer.writerow( data )
    writer.writerow( [ data['name'], data['language'] ] )
14/56:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in users['email'] and password in users['password']:
                   isLoggedIn = True
                   main()
                   print( users['email'] )
    # else display error and restart
    else:
                   print('Wrong email or password')
                   main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/57:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in users['email']:
                   isLoggedIn = True
                   main()
                   print( users['email'] )
    # else display error and restart
    else:
                   print('Wrong email or password')
                   main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/58:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in users.get(email) and password in users.get(password):
                   isLoggedIn = True
                   main()
                   print( users['email'] )
    # else display error and restart
    else:
                   print('Wrong email or password')
                   main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/59:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in users.get('email') and password in users.get('password'):
                   isLoggedIn = True
                   main()
                   print( users['email'] )
    # else display error and restart
    else:
                   print('Wrong email or password')
                   main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/60:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in users.get('email') and password in users.get('password'):
       isLoggedIn = True
       main()
       print( users['email'] )
    # else display error and restart
    else:
                   print('Wrong email or password')
                   main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/61:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if users.get('email').includes(email) and users.get('password').includes():
       isLoggedIn = True
       main()
       print( users['email'] )
    # else display error and restart
    else:
                   print('Wrong email or password')
                   main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/62:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if users.get('email').includes(email) and users.get('password').includes(password):
       isLoggedIn = True
       main()
       print( users['email'] )
    # else display error and restart
    else:
                   print('Wrong email or password')
                   main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/63:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in users['email'] and password in users['password']:
       isLoggedIn = True
       main()
       print( users['email'] )
    # else display error and restart
    else:
                   print('Wrong email or password')
                   main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/64:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

emails = []
password = []

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in emails and password in passwords:
       isLoggedIn = True
       main()
       print( users['email'] )
    # else display error and restart
    else:
       print('Wrong email or password')
       main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/65:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

emails = []
passwords = []

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in emails and password in passwords:
       isLoggedIn = True
       main()
       print( users['email'] )
    # else display error and restart
    else:
       print('Wrong email or password')
       main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/66:
isLoggedIn = False

users = {
    'email': [],
    'password': []
}

emails = []
passwords = []

def main():
    # check if user is logged in
    while isLoggedIn is False:
        if isLoggedIn:
            user_input = str(input('Would you like to log out/quit'))
            if user_input == 'quit':
                logOut()
        else:
            user_feedback = str(input('Would you like to login/register/quit'))
            if user_feedback == 'login':
                login()
            elif user_feedback == 'register':
                registerUser()
            elif user_feedback == 'quit':
                print('Thank you')
        
def login():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: ')

    # if correct login and restart
    if email in emails:
       isLoggedIn = True
       main()
       print( users['email'] )
    # else display error and restart
    else:
       print('Wrong email or password')
       main()
        
def registerUser():
    email = str(input('Enter your email: '))
    password = str(input('Enter your password: '))
    password2 = str(input('Confirm your password: '))

    # if passwords match, save user and restart
    if password == password2:
        users['email'] = email
        users['password'] = passwor
        main()
    # else display error and restart
    else:
        print( 'The passwords do not match' )
        main()

def logOut():
    isLoggedIn = False;
                   
def addUserToFile():
    pass
14/67:
# import all necessary packages to be used

import csv
from IPython.display import clear_output

# variables for main loop
active = True
isLoggedIn = False

# main loop

while active:
        if isLoggedIn:
            print('1. Logout\n2. Quit')
        else:
            print('1. Login\n2. Register\n3. Quit')
        choice = input('What would you like to do?').lower()
        clear_output()
        
        if choice == 'register' and isLoggedIn == False:
            registerUser()
        elif choice == 'login' and isLoggedIn == False:
            isLoggedIn = loginUser()
        elif choice == 'quit':
            active == False
            print('Thank you for using our software!')
        elif choice == 'logout' and isLoggedIn == True:
            isLoggedIn = False
            print('You are now logged out')
        else:
            print('Sorry, please try again!')

# ask for user info and return true to login or false if incorrect info
def loginUser():
    print('To login, please enter your info:')
    email = input('E-mail: ')
    password = input('Password: ')
    clear_output()
    
    with open('users.csv', mode='r') as f:
        reader = csv.reader(f, delimiter=',')
        for row in reader:
            if row == [email, password]:
                print('You are logged in!')
                return True
    print('Something went wrong, try again.')
    return False
    

# handle user registration and write to csv
def registerUser():                   
    with open('users.csv', mode='a', newline='') as f:
        writer = csv.writer(f, delimiter=',')
        print('To register, please enter your info:')
        email = str(input('Enter your email: '))
        password = str(input('Enter your password: '))
        password2 = str(input('Confirm your password: '))
        clear_output()

        # if passwords match, save user and restart
        if password == password2:
            writer.writerow( [email, password] )
            print('You are now registered!')
            #main()
        # else display error and restart
        else:
            print( 'Something went wrong. Try again please' )
            #main()
14/68:
# import all necessary packages to be used

import csv
from IPython.display import clear_output

# variables for main loop
active = True
isLoggedIn = False

# main loop

while active:
        if isLoggedIn:
            print('1. Logout\n2. Quit')
        else:
            print('1. Login\n2. Register\n3. Quit')
        choice = input('What would you like to do?').lower()
        clear_output()
        
        if choice == 'register' and isLoggedIn == False:
            registerUser()
        elif choice == 'login' and isLoggedIn == False:
            isLoggedIn = loginUser()
        elif choice == 'quit':
            active == False
            print('Thank you for using our software!')
        elif choice == 'logout' and isLoggedIn == True:
            isLoggedIn = False
            print('You are now logged out')
        else:
            print('Sorry, please try again!')

# ask for user info and return true to login or false if incorrect info
def loginUser():
    print('To login, please enter your info:')
    email = input('E-mail: ')
    password = input('Password: ')
    clear_output()
    
    with open('users.csv', mode='r') as f:
        reader = csv.reader(f, delimiter=',')
        for row in reader:
            if row == [email, password]:
                print('You are logged in!')
                return True
    print('Something went wrong, try again.')
    return False
    

# handle user registration and write to csv
def registerUser():                   
    with open('users.csv', mode='a', newline='') as f:
        writer = csv.writer(f, delimiter=',')
        print('To register, please enter your info:')
        email = str(input('Enter your email: '))
        password = str(input('Enter your password: '))
        password2 = str(input('Confirm your password: '))
        clear_output()

        # if passwords match, save user and restart
        if password == password2:
            writer.writerow( [email, password] )
            print('You are now registered!')
            #main()
        # else display error and restart
        else:
            print( 'Something went wrong. Try again please' )
            #main()
14/69:
# import all necessary packages to be used

import csv
from IPython.display import clear_output

# variables for main loop
active = True
isLoggedIn = False

# main loop

while active:
        if isLoggedIn:
            print('1. Logout\n2. Quit')
        else:
            print('1. Login\n2. Register\n3. Quit')
        choice = input('What would you like to do?').lower()
        clear_output()
        
        if choice == 'register' and isLoggedIn == False:
            registerUser()
        elif choice == 'login' and isLoggedIn == False:
            isLoggedIn = loginUser()
        elif choice == 'quit':
            active == False
            print('Thank you for using our software!')
        elif choice == 'logout' and isLoggedIn == True:
            isLoggedIn = False
            print('You are now logged out')
        else:
            print('Sorry, please try again!')

# ask for user info and return true to login or false if incorrect info
def loginUser():
    print('To login, please enter your info:')
    email = input('E-mail: ')
    password = input('Password: ')
    clear_output()
    
    with open('users.csv', mode='r') as f:
        reader = csv.reader(f, delimiter=',')
        for row in reader:
            if row == [email, password]:
                print('You are logged in!')
                return True
    print('Something went wrong, try again.')
    return False
    

# handle user registration and write to csv
def registerUser():                   
    with open('users.csv', mode='a', newline='') as f:
        writer = csv.writer(f, delimiter=',')
        print('To register, please enter your info:')
        email = str(input('Enter your email: '))
        password = str(input('Enter your password: '))
        password2 = str(input('Confirm your password: '))
        clear_output()

        # if passwords match, save user and restart
        if password == password2:
            writer.writerow( [email, password] )
            print('You are now registered!')
            #main()
        # else display error and restart
        else:
            print( 'Something went wrong. Try again please' )
            #main()
14/70:
# import all necessary packages to be used

import csv
from IPython.display import clear_output

# ask for user info and return true to login or false if incorrect info
def loginUser():
    print('To login, please enter your info:')
    email = input('E-mail: ')
    password = input('Password: ')
    clear_output()
    
    with open('users.csv', mode='r') as f:
        reader = csv.reader(f, delimiter=',')
        for row in reader:
            if row == [email, password]:
                print('You are logged in!')
                return True
    print('Something went wrong, try again.')
    return False
    

# handle user registration and write to csv
def registerUser():                   
    with open('users.csv', mode='a', newline='') as f:
        writer = csv.writer(f, delimiter=',')
        print('To register, please enter your info:')
        email = str(input('Enter your email: '))
        password = str(input('Enter your password: '))
        password2 = str(input('Confirm your password: '))
        clear_output()

        # if passwords match, save user and restart
        if password == password2:
            writer.writerow( [email, password] )
            print('You are now registered!')
            #main()
        # else display error and restart
        else:
            print( 'Something went wrong. Try again please' )
            #main()


# variables for main loop
active = True
isLoggedIn = False

# main loop

while active:
        if isLoggedIn:
            print('1. Logout\n2. Quit')
        else:
            print('1. Login\n2. Register\n3. Quit')
        choice = input('What would you like to do?').lower()
        clear_output()
        
        if choice == 'register' and isLoggedIn == False:
            registerUser()
        elif choice == 'login' and isLoggedIn == False:
            isLoggedIn = loginUser()
        elif choice == 'quit':
            active == False
            print('Thank you for using our software!')
        elif choice == 'logout' and isLoggedIn == True:
            isLoggedIn = False
            print('You are now logged out')
        else:
            print('Sorry, please try again!')
14/71:
values = { 4:4, 8:8, "Q":10, "ACE":11 }
card = ("Q", "Hearts")
print("{ }".format(values[ card[ 0 ] ] ) )
14/72:
values = { 4:4, 8:8, "Q":10, "ACE":11 }
card = ("Q", "Hearts")
print("{}".format(values[ card[ 0 ] ] ) )
15/1:
# creating your first class
class Car():
    pass
15/2:
# creating your first class
class Car():
    pass

print( type(Car) )
15/3:
# creating your first class
class Car():
    pass
15/4:
# instantiating an object from a class
class Car():
    pass

ford = Car()
print(ford)
15/5:
# instantiating multiple objects from the same class
class Car():
    pass

ford = Car()
subaru = Car()
print( hash(ford) )
print( hash(subaru) )
15/6:
# Animals
class Animals():
    pass

lion = Animals()
tiger = Animals()
15/7:
# how to define a class attributes
class car():
    sound = 'beep'
    
    color = 'red'
    
ford = Car()
print( ford.color )
15/8:
# how to define a class attributes
class car():
    sound = 'beep'
    
    color = 'red'
    
ford = Car()
print( ford.color() )
15/9:
# how to define a class attributes
class car():
    sound = 'beep'
    
    color = 'red'
    
ford = Car()
print( ford )
15/10:
# how to define a class attributes
class car():
    sound = 'beep'
    
    color = 'red'
    
ford = Car()
print( ford.color )
15/11:
# how to define a class attributes
class car():
    sound = 'beep'
    
    color = 'red'
    
ford = Car()
print( ford.color )
15/12:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print( ford.color )
15/13:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print( ford.color )
15/14:
# how to define a class attributes
class car( ):
    sound = 'beep'
    color = 'red'
    
ford = Car( )
print( ford.color )
15/15:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print(ford.color)
15/16:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print(ford.color)
15/17:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print(ford.color)
15/18:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print(ford.color)
15/19:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print(ford.color)
15/20:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print(ford.color)
15/21:
# how to define a class attributes
class car():
    sound = 'beep'
    color = 'red'
    
ford = Car()
print(ford.color)
15/22:
# how to define a class attributes
class foo():
    sound = 'beep'

    color = 'red'
    
ford = foo()
print( ford.color )
15/23:
# how to define a class attributes
class Car():
    sound = 'beep'

    color = 'red'
    
ford = Car()
print( ford.color )
15/24:
# changing the value of an attribute
class Car():
    sound = 'beep'

    color = 'red'
    
ford = Car()
print( ford.sound )

ford.sound = 'honk'
print( ford.sound )
15/25:
# using the init method to give instances personalized attributes upon creation
class Car():
    def __init__(self, color):
        self.color = color
        
ford = Color('blue')
print( ford.color )
15/26:
# using the init method to give instances personalized attributes upon creation
class Car():
    def __init__(self, color):
        self.color = color
        
ford = Car('blue')
print( ford.color )
15/27:
# defining different values for multiple instances
class Car():
    def __init__(self, color, year):
        self.color = color
        self.year = year
        
ford = Car('blue', 2016)
subaru = Car('red', 2018)

print( ford.color, ford.year )
print( subaru.color, subaru.year )
15/28:
# using and accessing global class instances
class Car():
    sound = 'beep'
    def __init__(self, color):
        self.color = 'blue'
        
print( Car.sound )
15/29:
# using and accessing global class instances
class Car():
    sound = 'beep'
    def __init__(self, color):
        self.color = 'blue'
        
print( Car.color )
15/30:
# using and accessing global class instances
class Car():
    sound = 'beep'
    def __init__(self, color):
        self.color = 'blue'
        
print( Car.sound )
15/31:
# using and accessing global class instances
class Car():
    sound = 'beep'
    def __init__(self, color):
        self.color = 'blue'
        
print( Car.sound )

ford = Color('blue')
print( ford.color )
15/32:
# using and accessing global class instances
class Car():
    sound = 'beep'
    def __init__(self, color):
        self.color = 'blue'
        
print( Car.sound )

ford = Car('blue')
print( ford.color )
15/33:
# using and accessing global class instances
class Car():
    sound = 'beep'
    def __init__(self, color):
        self.color = 'blue'
        
print( Car.sound )

ford = Car('blue')
print( ford.sound, ford.color )
15/34:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = ''
        self.breed = ''

husky = Dog('Sammi')
print( husky )

casey = Dog('Chocolate Lab')
print( casey )
15/35:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = ''
        self.breed = ''

husky = Dog('Sammi', '')
print( husky )

casey = Dog('Chocolate Lab', '')
print( casey )
15/36:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = ''
        self.breed = ''

husky = Dog('Sammi', '')
print( husky.name )

casey = Dog('Chocolate Lab', '')
print( casey.name )
15/37:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = ''
        self.breed = ''

husky = Dog('Sammi', '')
print( husky.name )

casey = Dog('Chocolate Lab', '')
print( casey.name )
15/38:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = ''
        self.breed = ''

husky = Dog('Sammi', '')
print( husky.species )

casey = Dog('Chocolate Lab', '')
print( casey.name )
15/39:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = ''
        self.breed = ''

husky = Dog('Sammi', '')
print( husky.name )

casey = Dog('Chocolate Lab', '')
print( casey.name )
15/40:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = ''
        self.breed = ''

husky = Dog('Sammi', '')
print( husky.name, husky.breed )

casey = Dog('Chocolate Lab', '')
print( casey.name )
15/41:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = name
        self.breed = breed

husky = Dog('Sammi', '')
print( husky.name, husky.breed )

casey = Dog('Chocolate Lab', '')
print( casey.name )
15/42:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = name
        self.breed = breed

husky = Dog('Sammi', '')
print( husky.name, husky.breed )

casey = Dog('Chocolate Lab', '')
print( casey.name, casy.breed )
15/43:
# Dogs
class Dog():
    species = 'Canine'
    def __init__(self, name, breed):
        self.name = name
        self.breed = breed

husky = Dog('Sammi', '')
print( husky.name, husky.breed )

casey = Dog('Chocolate Lab', '')
print( casey.name, casey.breed )
15/44:
# user input
class Person():
    def __init__(self, name):
        self.name = name

john = Person('john doe')
print( john.name )
15/45:
# defining and calling our first class method
class Dog():
    def makeSound(self):
        print( 'bark' )
        
sam = Dog()
sam.makeSound()
15/46:
# using the self keyword to access attributes within class methods
class Dog():
    sound = 'bark'
    def makeSound(self):
        print( self.sound )

sam = Dog()
sam.makeSound()
15/47:
# understanding which methods are accessible via the class itself and class instances
class Dog():
    sound = 'bark'
    def makeSound(self):
        print(self.sound)
    def printInfo():
        print( 'I am a dog.' )

Dog.printInfo()

sam = Dog()
sam.makeSound()
15/48:
# understanding which methods are accessible via the class itself and class instances
class Dog():
    sound = 'bark'
    def makeSound(self):
        print(self.sound)
    def printInfo():
        print( 'I am a dog.' )

Dog.printInfo()

sam = Dog()
sam.makeSound()
sam.printInfo()
15/49:
# understanding which methods are accessible via the class itself and class instances
class Dog():
    sound = 'bark'
    def makeSound(self):
        print(self.sound)
    def printInfo():
        print( 'I am a dog.' )

Dog.printInfo()

sam = Dog()
sam.makeSound()
15/50:
# writing methods that accept parameters
class Dog():
    def showAge(self, age):
        print( age )

sam = Dog()
sam.showAge(6)
15/51:
# using methods to set or return attribute values, proper programming practice
class Dog():
    name = ''
    
    def setName(self, new_name):
        self.new_name = new_name
        
    def getName(self):
        return self.name
    
sam = Dog()
sam.setName('Sammi')

print( sam.getName() )
15/52:
# using methods to set or return attribute values, proper programming practice
class Dog():
    name = ''
    
    def setName(self, new_name):
        self.new_name = new_name
        
    def getName(self):
        return self.name
    
sam = Dog()
sam.setName('Sammi')

print( sam.getName() )
15/53:
# using methods to set or return attribute values, proper programming practice
class Dog():
    name = ''
    
    def setName(self, new_name):
        self.new_name = new_name
        
    def getName(self):
        print self.name
    
sam = Dog()
sam.setName('Sammi')

print( sam.getName() )
15/54:
# using methods to set or return attribute values, proper programming practice
class Dog():
    name = ''
    
    def setName(self, new_name):
        self.new_name = new_name
        
    def getName(self):
        print( self.name )
    
sam = Dog()
sam.setName('Sammi')

print( sam.getName() )
15/55:
# using methods to set or return attribute values, proper programming practice
class Dog():
    name = ''
    
    def setName(self, new_name):
        self.new_name = new_name
        
    def getName(self):
        return self.name
    
sam = Dog()
sam.setName('Sammi')

print( sam.getName() )
15/56:
# using methods to set or return attribute values, proper programming practice
class Dog():
    name = ''
    
    def setName(self, new_name):
        self.new_name = new_name
        
    def getName(self):
        return self.name
    
sam = Dog()
sam.setName('Sammi')

print( sam.getName() )
15/57:
# incrementing/decrementing attribute values with methods, best programming practice
class Dog():
    age = 5
    
    def happyBirthday(self):
        self.age += 1
        
sam = Dog()
sam.happyBirthday()
print( sam.age )
15/58:
# calling a method from another method
class Dog():
    age = 6
    
    def getAge(self):
        return self.age
    
    def printInfo(self):
        if self.getAge() < 10:
            print( 'Puppy' )

sam = Dog()
sam.printInfo()
15/59:
# using magic methods
class Dog():
    def __str__(self):
        return 'This is a dog'
    
sam = Dog()
print( sam )
15/60:
# animals
class Animals():
    species = ''
    
    getSpecies(self):
        return self.species
    
    setSpecies(self, species):
        self.species = species
        
lion = Animal()
lion.setSpecies('feline')
print( lion.getSpecies )
15/61:
# animals
class Animals():
    species = ''
    
    getSpecies(self):
        return self.species
    
    setSpecies(self, species):
        self.species = species
        
lion = Animal()
lion.setSpecies('feline')
print( lion.getSpecies )
15/62:
# animals
class Animals():
    species = ''
    
    getSpecies(self):
        return self.species
    
    setSpecies(self, species):
        self.species = species
        
lion = Animal()
lion.setSpecies('feline')
print( lion.getSpecies() )
15/63:
# animals
class Animals():
    species = ''
    
    def getSpecies(self):
        return self.species
    
    def setSpecies(self, species):
        self.species = species
        
lion = Animal()
lion.setSpecies('feline')
print( lion.getSpecies() )
15/64:
# animals
class Animals():
    species = ''
    
    def getSpecies(self):
        return self.species
    
    def setSpecies(self, species):
        self.species = species
        
lion = Animals()
lion.setSpecies('feline')
print( lion.getSpecies() )
15/65:
# user input
class Person():
    age = 0
    
    def __init__(self, name):
        self.name = name
    
    def getAge(self):
        age = int(input('Enter your age: '))
        return self.age
        
    def setAge(self, age):
        self.age = self.getAge()
        
person = Person('John Doe')

age = person.getAge()

print( age )
15/66:
# user input
class Person():
    age = 0
    
    def __init__(self, name):
        self.name = name
    
    def getAge(self):
        age = int(input('Enter your age: '))
        age = self.setAge(age)
        return age
        
    def setAge(self, age):
        self.age = age
        return age
        
person = Person('John Doe')

age = person.getAge()

print( age )
15/67:
# user input
class Person():
    age = 0
    
    def __init__(self, name):
        self.name = name
    
    def getAge(self):
        age = int(input('Enter your age: '))
        age = self.setAge(age)
        return age
        
    def setAge(self, age):
        self.age = age
        return age
        
person = Person('John Doe')

age = person.getAge()

print( 'You are {} years old.'.format(age) )
15/68: # inheriting a class and accessing inherited methods
15/69:
# inheriting a class and accessing inherited method
class Animal():

    def makeSound(self):
        print( 'roar' )

class Dog(Animal):
    species = 'Canine'

sam = Dog()
sam.makeSound()

lion = Animal()
15/70:
# inheriting a class and accessing inherited method
class Animal():

    def makeSound(self):
        print( 'roar' )

class Dog(Animal):
    species = 'Canine'

sam = Dog()
sam.makeSound()

lion = Animal()
lion.species
15/71:
# inheriting a class and accessing inherited method
class Animal():

    def makeSound(self):
        print( 'roar' )

class Dog(Animal):
    species = 'Canine'

sam = Dog()
sam.makeSound()

lion = Animal()
15/72:
# using the super() method to declare inherited attributes
class Animal():
    def __init__(self, species):
        self.species = species
    
class Dog(Animal):
    def __init__(self, species, name):
        self.name = name
        super().__init__(species)

sam = Dog('Canine', 'Sammi')
print( sam.species )
15/73:
# using the super() method to declare inherited attributes
class Animal():
    def __init__(self, species):
        self.species = species
    
class Dog(Animal):
    def __init__(self, species, name):
        self.name = name
        super().__init__(species)

sam = Dog('Canine', 'Sammi')
print( sam.name )
15/74:
# using the super() method to declare inherited attributes
class Animal():
    def __init__(self, species):
        self.species = species
    
class Dog(Animal):
    def __init__(self, species, name):
        self.name = name
        super().__init__(species)

sam = Dog('Canine', 'Sammi')
print( sam.species )
15/75:
# using the super() method to declare inherited attributes
class Animal():
    def __init__(self, species):
        self.species = species
    
class Dog(Animal):
    def __init__(self, species, name):
        self.name = name
        super().__init__(species)

sam = Dog('Canine', 'Sammi')
print( sam.species )

doe = Animal('Canine')
print( doe.species )
15/76:
# using the super() method to declare inherited attributes
class Animal():
    def __init__(self, species):
        self.species = species
    
class Dog(Animal):
    def __init__(self, species, name):
        self.name = name
        super().__init__(species)

sam = Dog('Canine', 'Sammi')
print( sam.species )

doe = Animal('Canine', 'dpe')
print( doe.name )
15/77:
# using the super() method to declare inherited attributes
class Animal():
    def __init__(self, species):
        self.species = species
    
class Dog(Animal):
    def __init__(self, species, name):
        self.name = name
        super().__init__(species)

sam = Dog('Canine', 'Sammi')
print( sam.species )
15/78:
# using the super() method to declare inherited attributes
class Animal():
    def __init__(self, species):
        self.species = species
    
class Dog(Animal):
    def __init__(self, species, name):
        self.name = name
        super().__init__(species)

sam = Dog('Canine', 'Sammi')
print( sam.species )
15/79:
# overriding methods defined in the superclass
class Animal():
    def makeSound(self):
        print( 'roar' )

class Dog(Animal):
    def makeSound(self):
        print( 'bark' )
        
sam, lion = Animal(), Dog()

sam.makeSound()
lion.makeSound()
15/80:
# how to inherit multiple classes
class  Physics():
    gravity = 9.8
    
class Automobile():
    def __init__(self, make, model, year):
        self.make, self.model, self.year = make, model, year
        
class Ford(Physics, Automobile):
    def __init__(self, model, year):
        Automobile.__init__(self, 'Ford', model, year)

truck = Ford('F-150', 2018)
print( truck.gravity, truck.make )
15/81:
# good guys / bad guys
class Characters():
    def __init__(self, name, team, height, weight):
        self.name, self.team, self.height, self.weight = name, team, height, weight
        
    def sayHello(self):
        print( 'My name is {} and I\'m the on the {} guys'.format(self.name, self.team) )

class GoodGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'good', height, weight)

class BadGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'bad', height, weight)
        
Max = GoodGuys()
Max.name = 'Max'
print( Max )
15/82:
# good guys / bad guys
class Characters():
    def __init__(self, name, team, height, weight):
        self.name, self.team, self.height, self.weight = name, team, height, weight
        
    def sayHello(self):
        print( 'My name is {} and I\'m the on the {} guys'.format(self.name, self.team) )

class GoodGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'good', height, weight)

class BadGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'bad', height, weight)
        
Max = GoodGuys()
Max.name = 'Max'
print( Max.sayHello() )
15/83:
# good guys / bad guys
class Characters():
    def __init__(self, name, team, height, weight):
        self.name, self.team, self.height, self.weight = name, team, height, weight
        
    def sayHello(self):
        print( 'My name is {} and I\'m the on the {} guys'.format(self.name, self.team) )

class GoodGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'good', height, weight)

class BadGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'bad', height, weight)
        
Max = GoodGuys('Max')

print( Max.sayHello() )
15/84:
# good guys / bad guys
class Characters():
    def __init__(self, name, team, height, weight):
        self.name, self.team, self.height, self.weight = name, team, height, weight
        
    def sayHello(self):
        print( 'My name is {} and I\'m the on the {} guys'.format(self.name, self.team) )

class GoodGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'good', height, weight)

class BadGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'bad', height, weight)
        
Max = GoodGuys('Max', '', '', '')

print( Max.sayHello() )
15/85:
# good guys / bad guys
class Characters():
    def __init__(self, name, team, height, weight):
        self.name, self.team, self.height, self.weight = name, team, height, weight
        
    def sayHello(self):
        print( 'Hello, my name is {} and I\'m the on the {} guys'.format(self.name, self.team) )

class GoodGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'good', height, weight)

class BadGuys(Characters):
    def __init__(self, name, team, height, weight):
        super().__init__(name, 'bad', height, weight)
        
Max = GoodGuys('Max', '', '', '')

print( Max.sayHello() )

Tony = BadGuys('Tony', '', '', '')

print( Tony.sayHello() )
15/86:
# import necessary functions
from random import ranint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
game = Blackjack()
game.makeDeck()
print(game.deck)
15/87:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )

game = Blackjack()
game.makeDeck()
print(game.deck)
15/88:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck - )) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/89:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/90:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/91:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/92:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/93:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/94:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/95:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/96:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/97:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
    
        
game = Blackjack()
game.makeDeck()
print( game.pullCard(), len(game.deck) )
15/98:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
game = Blackjack()
game.makeDeck()
name = input('What is your name?')
player = Player(name)
dealer = Player('Dealer')

print( player.name, dealer.name )
15/99:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
game = Blackjack()
game.makeDeck()

name = input('What is your name?')
player = Player(name)
dealer = Player('Dealer')

# add two cards to the dealer and player hand
for i in range(2):
    player.addCard( game.pullCard() )
    dealer.addCard( game.pullCard() )

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

print( player.name, dealer.name )
15/100:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
game = Blackjack()
game.makeDeck()

#name = input('What is your name?')
player = Player(name)
dealer = Player('Dealer')

# add two cards to the dealer and player hand
for i in range(2):
    player.addCard( game.pullCard() )
    dealer.addCard( game.pullCard() )

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

#print( player.name, dealer.name )
15/101:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
game = Blackjack()
game.makeDeck()

#name = input('What is your name?')
#player = Player(name)
#dealer = Player('Dealer')

# add two cards to the dealer and player hand
for i in range(2):
    player.addCard( game.pullCard() )
    dealer.addCard( game.pullCard() )

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

#print( player.name, dealer.name )
15/102:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
# if not dealer's turn then only show one of his cards, otherwise show all
def showHand(self, dealer_start = True):
    print( '\n{}'.format(self.name) )
    print('========================')
    for i in range( len(self.hand) ):
        if self.name == 'Dealer' and i == 0 and dealer_start:
            print('- of -')
        else:
            card = self.hand[i]
            print( '{} of {}'.format( card[0], card[1] ) )
    
game = Blackjack()
game.makeDeck()

name = input('What is your name?')
player = Player(name)
dealer = Player('Dealer')

# add two cards to the dealer and player hand
for i in range(2):
    player.addCard( game.pullCard() )
    dealer.addCard( game.pullCard() )
    
#show both hands using method
player.showHand()
dealer.showHand()

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

print( player.name, dealer.name )
15/103:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
# if not dealer's turn then only show one of his cards, otherwise show all
def showHand(self, dealer_start = True):
    print( '\n{}'.format(self.name) )
    print('========================')
    for i in range( len(self.hand) ):
        if self.name == 'Dealer' and i == 0 and dealer_start:
            print('- of -')
        else:
            card = self.hand[i]
            print( '{} of {}'.format( card[0], card[1] ) )
        print( 'Total = {}'.format( slef.calcHand(dealer_start) ) )
        
#if not dealer's turn then only give back total of second card
def calcHand(self, dealer_start = True):
    total = 0
    aces = 0
    card_values = { 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10, 'J':10, 'Q': 10, 'K':10, 'A':11 }
    if self.name == 'Dealer' and dealer_start:
        card = self.hand[1]
        return card_values[ card[0] ]
    for card in self.hand:
        if card[0] == 'A':
            aces += 1
        else:
            total += card_values[ card[0] ]
    for i in range(aces):
        if total + 11 > 21:
            total += 1
        else:
            total += 11
    return total

game = Blackjack()
game.makeDeck()

name = input('What is your name?')
player = Player(name)
dealer = Player('Dealer')

# add two cards to the dealer and player hand
for i in range(2):
    player.addCard( game.pullCard() )
    dealer.addCard( game.pullCard() )

player_bust = False
while input('Would you like to stay or hit?').lower() != 'stay':
    clear_output()
    player.addCard( game.pullCard() )
    #show both hands using method
    player.showHand()
    dealer.showHand()
    # check if over 21
    if player.calcHand > 21:
        player_bust = True
        print( 'You lose!' )
        break
        

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

print( player.name, dealer.name )
15/104:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
# if not dealer's turn then only show one of his cards, otherwise show all
def showHand(self, dealer_start = True):
    print( '\n{}'.format(self.name) )
    print('========================')
    for i in range( len(self.hand) ):
        if self.name == 'Dealer' and i == 0 and dealer_start:
            print('- of -')
        else:
            card = self.hand[i]
            print( '{} of {}'.format( card[0], card[1] ) )
        print( 'Total = {}'.format( slef.calcHand(dealer_start) ) )
        
#if not dealer's turn then only give back total of second card
def calcHand(self, dealer_start = True):
    total = 0
    aces = 0
    card_values = { 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10, 'J':10, 'Q': 10, 'K':10, 'A':11 }
    if self.name == 'Dealer' and dealer_start:
        card = self.hand[1]
        return card_values[ card[0] ]
    for card in self.hand:
        if card[0] == 'A':
            aces += 1
        else:
            total += card_values[ card[0] ]
    for i in range(aces):
        if total + 11 > 21:
            total += 1
        else:
            total += 11
    return total

game = Blackjack()
game.makeDeck()

#

player_bust = False
while input('Would you like to stay or hit?').lower() != 'stay':
    clear_output()
    player.addCard( game.pullCard() )
    #show both hands using method
    player.showHand()
    dealer.showHand()
    # check if over 21
    if player.calcHand > 21:
        player_bust = True
        print( 'You lose!' )
        break
        

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

print( player.name, dealer.name )
15/105:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
# if not dealer's turn then only show one of his cards, otherwise show all
def showHand(self, dealer_start = True):
    print( '\n{}'.format(self.name) )
    print('========================')
    for i in range( len(self.hand) ):
        if self.name == 'Dealer' and i == 0 and dealer_start:
            print('- of -')
        else:
            card = self.hand[i]
            print( '{} of {}'.format( card[0], card[1] ) )
        print( 'Total = {}'.format( slef.calcHand(dealer_start) ) )
        
#if not dealer's turn then only give back total of second card
def calcHand(self, dealer_start = True):
    total = 0
    aces = 0
    card_values = { 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10, 'J':10, 'Q': 10, 'K':10, 'A':11 }
    if self.name == 'Dealer' and dealer_start:
        card = self.hand[1]
        return card_values[ card[0] ]
    for card in self.hand:
        if card[0] == 'A':
            aces += 1
        else:
            total += card_values[ card[0] ]
    for i in range(aces):
        if total + 11 > 21:
            total += 1
        else:
            total += 11
    return total

game = Blackjack()
game.makeDeck()

name = input('What is your name?')
player = Player(name)
dealer = Player('Dealer')

# add two cards to the dealer and player hand
for i in range(2):
    player.addCard( game.pullCard() )
    dealer.addCard( game.pullCard() )

player_bust = False
while input('Would you like to stay or hit?').lower() != 'stay':
    clear_output()
    player.addCard( game.pullCard() )
    #show both hands using method
    player.showHand()
    dealer.showHand()
    # check if over 21
    if player.calcHand > 21:
        player_bust = True
        print( 'You lose!' )
        break
# handling the dealers turn only if the player didn't bust
dealer_bust = False
if not player_bust:
    while dealer.calcHand(False) < 17:
        # pull card and put in player's hand
        dealer.addCard( game.pullCard() )
        # check if over 21
        if dealer.calcHand(False) > 21:
            dealer_bust = True
            print( 'You win!' )
            break;

clear_output()
player.showHand()
dealer.showHand(False)

# calculate a winner
if player_bust:
    print( 'You busted, better luck next time!' )
elif dealer_bust:
    print( 'The dealer busted, you win!' )
elif dealer.calcHand(False) > player.calcHand():
    print( 'Dealer has higher cards, you lose!' )
elif dealer.calcHand(False) < player.calcHand():
    print( 'You beat the dealer, congrats' )
else:
    print( 'You pushed, no one wins!' )
        

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

print( player.name, dealer.name )
15/106:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
# if not dealer's turn then only show one of his cards, otherwise show all
def showHand(self, dealer_start = True):
    print( '\n{}'.format(self.name) )
    print('========================')
    for i in range( len(self.hand) ):
        if self.name == 'Dealer' and i == 0 and dealer_start:
            print('- of -')
        else:
            card = self.hand[i]
            print( '{} of {}'.format( card[0], card[1] ) )
        print( 'Total = {}'.format( slef.calcHand(dealer_start) ) )
        
#if not dealer's turn then only give back total of second card
def calcHand(self, dealer_start = True):
    total = 0
    aces = 0
    card_values = { 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10, 'J':10, 'Q': 10, 'K':10, 'A':11 }
    if self.name == 'Dealer' and dealer_start:
        card = self.hand[1]
        return card_values[ card[0] ]
    for card in self.hand:
        if card[0] == 'A':
            aces += 1
        else:
            total += card_values[ card[0] ]
    for i in range(aces):
        if total + 11 > 21:
            total += 1
        else:
            total += 11
    return total

game = Blackjack()
game.makeDeck()
#

# add two cards to the dealer and player hand
for i in range(2):
    player.addCard( game.pullCard() )
    dealer.addCard( game.pullCard() )

player_bust = False
while input('Would you like to stay or hit?').lower() != 'stay':
    clear_output()
    player.addCard( game.pullCard() )
    #show both hands using method
    player.showHand()
    dealer.showHand()
    # check if over 21
    if player.calcHand > 21:
        player_bust = True
        print( 'You lose!' )
        break
# handling the dealers turn only if the player didn't bust
dealer_bust = False
if not player_bust:
    while dealer.calcHand(False) < 17:
        # pull card and put in player's hand
        dealer.addCard( game.pullCard() )
        # check if over 21
        if dealer.calcHand(False) > 21:
            dealer_bust = True
            print( 'You win!' )
            break;

clear_output()
player.showHand()
dealer.showHand(False)

# calculate a winner
if player_bust:
    print( 'You busted, better luck next time!' )
elif dealer_bust:
    print( 'The dealer busted, you win!' )
elif dealer.calcHand(False) > player.calcHand():
    print( 'Dealer has higher cards, you lose!' )
elif dealer.calcHand(False) < player.calcHand():
    print( 'You beat the dealer, congrats' )
else:
    print( 'You pushed, no one wins!' )
        

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

print( player.name, dealer.name )
15/107:
# import necessary functions
from random import randint
from IPython.display import clear_output

# create the blackjack class, which will hold all game methods and attributes
class Blackjack():
    def __init__(self):
        self.deck = []
        self.suits = ('Spades', 'Hearts', 'Diamonds', 'Clubs')
        self.values = ( 2, 3, 4, 5, 6, 7, 8, 9, 10, 'J', 'Q', 'K', 'A' )
        
    # create a method that creates a deck of 52 cards, each card should be a tuple with a value and suit
    def makeDeck(self):
        for suit in self.suits:
            for value in self.values:
                self.deck.append( (value, suit) )
                
    # method to pop a card from deck using a random index value
    def pullCard(self):
        return self.deck.pop( randint(0, len(self.deck) - 1) )
    
# create the class for the dealer and the player objects
class Player():
    def __init__(self, name):
        self.name = name
        self.hand = []
        
# take a tuple and append it to the hand
def addCard(self, card):
    self.hand.append(card)
    
# if not dealer's turn then only show one of his cards, otherwise show all
def showHand(self, dealer_start = True):
    print( '\n{}'.format(self.name) )
    print('========================')
    for i in range( len(self.hand) ):
        if self.name == 'Dealer' and i == 0 and dealer_start:
            print('- of -')
        else:
            card = self.hand[i]
            print( '{} of {}'.format( card[0], card[1] ) )
        print( 'Total = {}'.format( slef.calcHand(dealer_start) ) )
        
#if not dealer's turn then only give back total of second card
def calcHand(self, dealer_start = True):
    total = 0
    aces = 0
    card_values = { 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10, 'J':10, 'Q': 10, 'K':10, 'A':11 }
    if self.name == 'Dealer' and dealer_start:
        card = self.hand[1]
        return card_values[ card[0] ]
    for card in self.hand:
        if card[0] == 'A':
            aces += 1
        else:
            total += card_values[ card[0] ]
    for i in range(aces):
        if total + 11 > 21:
            total += 1
        else:
            total += 11
    return total

game = Blackjack()
game.makeDeck()

player_bust = False
while input('Would you like to stay or hit?').lower() != 'stay':
    clear_output()
    player.addCard( game.pullCard() )
    #show both hands using method
    player.showHand()
    dealer.showHand()
    # check if over 21
    if player.calcHand > 21:
        player_bust = True
        print( 'You lose!' )
        break
# handling the dealers turn only if the player didn't bust
dealer_bust = False
if not player_bust:
    while dealer.calcHand(False) < 17:
        # pull card and put in player's hand
        dealer.addCard( game.pullCard() )
        # check if over 21
        if dealer.calcHand(False) > 21:
            dealer_bust = True
            print( 'You win!' )
            break;

clear_output()
player.showHand()
dealer.showHand(False)

# calculate a winner
if player_bust:
    print( 'You busted, better luck next time!' )
elif dealer_bust:
    print( 'The dealer busted, you win!' )
elif dealer.calcHand(False) > player.calcHand():
    print( 'Dealer has higher cards, you lose!' )
elif dealer.calcHand(False) < player.calcHand():
    print( 'You beat the dealer, congrats' )
else:
    print( 'You pushed, no one wins!' )
        

print( 'Player Hand: {} \nDealer Hand: {}'.format(player.hand, dealer.hand) )

print( player.name, dealer.name )
16/1:
# create a list of numbers using list comprehension
nums = [ x for x in range(100) ]
print(nums)
16/2:
# using if statements within list comprehension
nums = [ x for x in range(10) if x % 2 == 0 ]
16/3:
# using if statements within list comprehension
nums = [ x for x in range(10) if x % 2 == 0 ]
print( nums )
16/4:
# using if/else statements within list comprehension
nums = [ 'Even' if x % 2 == 0 else 'Odd' for x in range(10) ]
print( nums )
16/5:
# creating a list of squared numbers from another list of numbers using list comprehension
nums = [ 2, 4, 6, 8 ]
squared_nums = [ num ** num for num in nums ]
print( squared_nums )
16/6:
# creating a list of squared numbers from another list of numbers using list comprehension
nums = [ 2, 4, 6, 8 ]
squared_nums = [ num ** 2 for num in nums ]
print( squared_nums )
16/7:
# creating a list of squared numbers from another list of numbers using list comprehension
nums = [ 2, 4, 6, 8 ]
squared_nums = [ num * num for num in nums ]
print( squared_nums )
16/8:
# creating a list of squared numbers from another list of numbers using list comprehension
nums = [ 2, 4, 6, 8 ]
squared_nums = [ num ** 2 for num in nums ]
print( squared_nums )
16/9:
# creating a dictionary of even numbers and square values using comprehension
numbers = [ x for x in range(10) ]
squares = { num: num ** 2 for num in numbers if num % 2 == 0 }
print( squares )
16/10:
degrees = [ 12, 21, 15, 32 ]
fahrenheit = [ (9 / 5) * degree + 32 for degree in degrees ]
print( fahrenheit )
16/11:
degrees = [ 12, 21, 15, 32 ]
fahrenheit = [ float((9 / 5) * degree + 32) for degree in degrees ]
print( fahrenheit )
16/12:
degrees = [ 12, 21, 15, 32 ]
fahrenheit = [ (9 / 5) * degree + 32 for degree in degrees ]
print( fahrenheit )
16/13:
# Degree Conversion
degrees = [ 12, 21, 15, 32 ]
fahrenheit = [ (9 / 5) * degree + 32 for degree in degrees ]
print( fahrenheit )
16/14:
# user input
user_input = int(input('Enter a number'))
nums = [ user_input for user_input in range(100) ]
print( nums )
16/15:
# user input
user_input = int(input('Enter a number'))
nums = [ user_input for x in range(100) if x % user_input == 0 ]
print( nums )
16/16:
# user input
user_input = int(input('Enter a number'))
nums = [ for x in range(100) if x % user_input == 0 ]
print( nums )
16/17:
# user input
user_input = int(input('Enter a number'))
nums = [ x for x in range(100) if x % user_input == 0 ]
print( nums )
16/18:
# user input
user_input = int(input('Enter a number'))
nums = [ x for x in range(100) if x % user_input == 0 ]
print( nums )
16/19:
# user input
user_input = int(input('Enter a number'))
nums = [ x for x in range(101) if x % user_input == 0 ]
print( nums )
16/20:
# user input
user_input = int(input('Enter a number'))
nums = [ x for x in range(101) if x % user_input == 0 ]
print( nums )
16/21:
# user input
user_input = int(input('Enter a number'))
nums = [ x for x in range(101) if x % user_input == 0 ]
print( nums )
16/22:
# user input
user_input = int(input('Enter a number'))
nums = [ x for x in range(101) if x % user_input == 0 ]
print( nums )
16/23:
# user input
user_input = int(input('Enter a number'))
nums = [ x for x in range(user_input, 101) if x % user_input == 0 ]
print( nums )
16/24:
# using a lambda to square a number
( lambda x: x ** 2 )( 4 )
16/25:
# passing multiple arguments into a lambda
( lambda x, y : x * y )( 10, 5 )
16/26:
# saving a lamda function in a variable
square = lambda x, y : x * y
print( square )
result = square(10, 5)
print( result )
16/27:
# using if/else statement within a lambda to return the greater number
greater = lambda x, y : x if x > y else y
result = greater(5, 10)
print( result )
16/28:
# returning a lambda function from another function
def my_func(n):
    return lambda x: x * n

doubler = my_func(2)
print( doubler(5) )
tripler = my_func(3)
print( tripler(5) )
16/29:
# Fill in the blanks
lambda x : True if x > 50 else False
16/30:
# degree conversion
def degreeConversion(celcius):
    return lambda celcius: (9 / 5) * celcius + 32

print( degreeConversion(12) )
16/31:
# degree conversion
def degreeConversion(celcius):
    return lambda celcius: (9 / 5) * celcius + 32

result = degreeConversion(12)

print( result )
16/32:
# returning a lambda function from another function
def my_func(x):
    return lambda x: x * x

doubler = my_func(2)
print( doubler(5) )
tripler = my_func(3)
print( tripler(5) )
16/33:
# degree conversion
def degreeConversion(celcius):
    return lambda celcius: (9 / 5) * celcius + 32

result = degreeConversion(12)

print( result )
16/34:
# degree conversion
def degreeConversion(celcius):
    return lambda celcius: (9 / 5) * celcius + 32

result = degreeConversion(12)

print( lambda celcius: (9 / 5) * 12 + 32 )
16/35:
# degree conversion
def degreeConversion(celcius):
    lambda celcius: (9 / 5) * celcius + 32

result = degreeConversion(12)

print( result )
16/36:
# degree conversion
# def degreeConversion(celcius):
result lambda celcius: (9 / 5) * celcius + 32

# result = degreeConversion(12)

print( result(12) )
16/37:
# degree conversion
# def degreeConversion(celcius):
result = lambda celcius: (9 / 5) * celcius + 32

# result = degreeConversion(12)

print( result(12) )
16/38:
# degree conversion
def degreeConversion(celcius):
    result = lambda celcius: (9 / 5) * celcius + 32
    return result

result = degreeConversion(12)

print( result(12) )
16/39:
# degree conversion
def degreeConversion(celcius):
    result = lambda celcius: (9 / 5) * celcius + 32
    return result

print( degreeConversion(12) )
16/40:
# degree conversion
def degreeConversion(celcius):
    result = lambda celcius: (9 / 5) * celcius + 32
    return result

x = degreeConversion(12)

print( x )
16/41:
# degree conversion
def degreeConversion(celcius):
    result = lambda celcius: (9 / 5) * celcius + 32
    return result

x = degreeConversion(12)

print( x(12) )
16/42:
# degree conversion
def degreeConversion(celcius):
     return lambda celcius: (9 / 5) * celcius + 32

x = degreeConversion(12)

print( x(12) )
16/43:
# degree conversion
def degreeConversion(celcius):
     return lambda celcius: (9 / 5) * celcius + 32

result = degreeConversion(12)

print( result(12) )
16/44:
# using the map function without lambdas
def convertDeg(C):
    return (9 / 5) * C + 32

temps = [ 12.5, 13.6, 15, 9.2 ]
converted_temps = map(convertDegs, temps)
print( converted_temps )
16/45:
# using the map function without lambdas
def convertDeg(C):
    return (9 / 5) * C + 32

temps = [ 12.5, 13.6, 15, 9.2 ]
converted_temps = map(convertDeg, temps)
print( converted_temps )
16/46:
# using the map function without lambdas
def convertDeg(C):
    return (9 / 5) * C + 32

temps = [ 12.5, 13.6, 15, 9.2 ]
converted_temps = map(convertDeg, temps)
print( converted_temps )
converted_temps = list(converted_temps)
16/47:
# using the map function without lambdas
def convertDeg(C):
    return (9 / 5) * C + 32

temps = [ 12.5, 13.6, 15, 9.2 ]
converted_temps = map(convertDeg, temps)
print( converted_temps )
converted_temps = list(converted_temps)
print( converted_temps )
16/48:
#using a map function with lambdas
temps = [ 12.5, 13.6, 15, 9.2 ]
converted_temps = list(map( lambda C: (9 / 5) * C + 32, temps ))
print( converted_temps )
16/49:
# using the filter functions without lamda functions, filter out temps below 55F
def filterTemps(C):
    converted = (9 / 5) * C + 32
    return True if converted > 55 else False

temps = [ 12.5, 13.6, 25, 9.2 ]
filtered_temps = filter(filterTemps(), temps)
print( filtered_temps )
filtered_temps = list(filtered_temps)
print( filtered_temps )
16/50:
# using the filter functions without lamda functions, filter out temps below 55F
def filterTemps(C):
    converted = (9 / 5) * C + 32
    return True if converted > 55 else False

temps = [ 12.5, 13.6, 25, 9.2 ]
filtered_temps = filter(filterTemps(temps), temps)
print( filtered_temps )
filtered_temps = list(filtered_temps)
print( filtered_temps )
16/51:
# using the filter functions without lamda functions, filter out temps below 55F
def filterTemps(C):
    converted = (9 / 5) * C + 32
    return True if converted > 55 else False

temps = [ 12.5, 13.6, 25, 9.2 ]
filtered_temps = filter(filterTemps, temps)
print( filtered_temps )
filtered_temps = list(filtered_temps)
print( filtered_temps )
16/52:
# using the filter functions without lamda functions, filter out temps below 55F
def filterTemps(C):
    converted = (9 / 5) * C + 32
    return True if converted > 55 else False

temps = [ 12.5, 13.6, 25, 9.2 ]
filtered_temps = filter(filterTemps, temps)
print( filtered_temps )
filtered_temps = list(filtered_temps)
print( filtered_temps )
16/53:
# using filter functions with lamda functions to filter out temps below 55F
temps = [ 12.5, 13.6, 25, 9.2 ]
filtered_temps = list( filter( lambda C: True if (9 / 5) * C + 32 > 55 else False, temps ) )
print( filtered_temps )
16/54:
temps = [ 12.5, 13.6, 15, 9.2 ]
filtered_temps = list( filter( lambda C : True if (9/5) * C + 32 > 55 else False, temps) ) # type convert the filter
print(filtered_temps)
16/55:
# using filter functions with lamda functions to filter out temps below 55F
temps = [ 12.5, 13.6, 25, 9.2 ]
filtered_temps = list( filter( lambda C: True if (9 / 5) * C + 32 > 55 else False, temps ) )
print( filtered_temps )
16/56:
# for information purposes this is how you use the reduce function
from functools import reduce
nums = [ 1, 2, 3, 4 ]
result = reduce( lambda a, b : a * b, nums )
print( result )
16/57:
names = [ "   ryan", "PAUL", "kevin connors     " ]
mapped_names = list( map( lambda name: name.title() ) )
print( mapped_names )
16/58:
names = [ "   ryan", "PAUL", "kevin connors     " ]
mapped_names = list( map( lambda name: name.title(), names ) )
print( mapped_names )
16/59:
names = [ "   ryan", "PAUL", "kevin connors     " ]
mapped_names = list( map( lambda name: name.title().strip(), names ) )
print( mapped_names )
16/60:
names = [ "   ryan", "PAUL", "kevin connors     " ]
converted_names = list( map( lambda name: name.title().strip(), names ) )
print( converted_names )
16/61:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( map( lambda name: name.lower() if name.lower()[0] == 'A', names ) )
print( filtered_names )
16/62:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( map( lambda name: True if name.lower()[0] == 'A' else False, names ) )
print( filtered_names )
16/63:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( map( lambda name: name.lower() if name.lower()[0] == 'A' else name.lower(), names ) )
print( filtered_names )
16/64:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( map( lambda name: name.lower() if name.lower()[0] == 'A' else False, names ) )
print( filtered_names )
16/65:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( map( lambda name: name.lower() if name.lower()[0] == 'A' else '', names ) )
print( filtered_names )
16/66:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( map( lambda name: name.lower() if name.[0] == 'A', names ) )
print( filtered_names )
16/67:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( map( lambda name: name.lower() if name[0] == 'A', names ) )
print( filtered_names )
16/68:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( map( lambda name: name.lower() if name.lower()[0] == 'A' else name.lower(), names ) )
print( filtered_names )
16/69:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name.lower() if name.lower()[0] == 'A' else name.lower(), names ) )
print( filtered_names )
16/70:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name.lower() if (name.lower())[0] == 'A', names ) )
print( filtered_names )
16/71:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name.lower() if (name.lower())[0] == 'A' else False, names ) )
print( filtered_names )
16/72:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name.lower() if name.lower()[0] == 'A' else False, names ) )
print( filtered_names )
16/73:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name if name[0] == 'A' else False, names ) )
print( filtered_names )
16/74:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name if name[0] != 'A' else False, names ) )
print( filtered_names )
16/75:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name.title() if name[0] != 'A' else False, names ) )
print( filtered_names )
16/76:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name.title() if name.lower()[0] != 'A' else False, names ) )
print( filtered_names )
16/77:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name.title() if name[0] != 'A' or name[0] != 'a' else False, names ) )
print( filtered_names )
16/78:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name.title() if name[0] != 'A' and name[0] != 'a' else False, names ) )
print( filtered_names )
16/79:
names = [ "Amanda", "Frank", "abby", "Ripal", "Adam" ]
filtered_names = list( filter( lambda name: name if name[0] != 'A' and name[0] != 'a' else False, names ) )
print( filtered_names )
16/80:
# writing a factorial using recursive functions
def factorial(n):
    if n <= 1:
        return 1
    else:
        return factorial(n - 1) * n

print( factorial(5) )
16/81:
# writing recursive fibonacci sequence
def fib(n):
    if n <= 1:
        return n
    else:
        return fib(n - 1) + fib(n - 2)
    
print( fib(5) )
16/82:
# using memoization with the fibonacci sequence
cache = {}
def fib(n):
    if n in cache:
        return cache[ n ]
    result = 0
    if n <= 1:
        result = n
    else:
        result = fib( n - 1 ) + fib( n - 2 )
    cache[ n ] = result
    
    return result

print( fib(50) )
16/83:
# writing recursive fibonacci sequence
def fib(n):
    if n <= 1:
        return n
    else:
        return fib(n - 1) + fib(n - 2)
    
print( fib(50) )
16/84:
# writing recursive fibonacci sequence
def fib(n):
    if n <= 1:
        return n
    else:
        return fib(n - 1) + fib(n - 2)
    
print( fib(5) )
16/85:
# using @lru_cache, Python's default moization/caching technique
from functools import lru_cache
@lru_cache
def fib(n):
    if n <= 1:
        return n
    else:
        return fib(n - 1) + fib(n - 2)
    
fib(50)
16/86:
# using @lru_cache, Python's default moization/caching technique
from functools import lru_cache
@lru_cache()
def fib(n):
    if n <= 1:
        return n
    else:
        return fib(n - 1) + fib(n - 2)
    
fib(50)
16/87:
# factorial caching
from functools import lru_cache
@lru_cache()
def factorial(n):
    if n <= 1:
        return n
    else:
        return factorial(n - 1) * n

factorial(5)
16/88:
# factorial caching
from functools import lru_cache
@lru_cache()
def factorial(n):
    if n <= 1:
        return n
    else:
        return factorial(n - 1) * n

factorial(5000)
16/89:
# factorial caching
from functools import lru_cache
@lru_cache()
def factorial(n):
    if n <= 1:
        return n
    else:
        return factorial(n - 1) * n

factorial(500)
16/90:
# factorial caching
from functools import lru_cache
@lru_cache()
def factorial(n):
    if n <= 1:
        return n
    else:
        return factorial(n - 1) * n

factorial(50)
16/91:
# factorial caching
from functools import lru_cache
@lru_cache()
def factorial(n):
    if n <= 1:
        return n
    else:
        return factorial(n - 1) * n

factorial(5)
17/1:
# searching data
def searchData(mList, item):
    return list( filter( lambda mList, item: True if item in mList else False ) )

searchData([ 2, 3, [ 18, 22 ], 6 ], 22)
17/2:
# searching data
def searchData(mList, item):
    return list( filter( lambda mList, item: True if item in mList else False ) )

searchData([ 2, 3, 18, 22, 6 ], 22)
17/3:
# searching data
def searchList(mList, item):
    return item
    # return list( filter( lambda mList, item: True if item in mList else False ) )

searchList([ 2, 3, 18, 22, 6 ], 22)
17/4:
# searching data
def searchList(mList, item):
    return if item in mList True else False
    # return list( filter( lambda mList, item: True if item in mList else False ) )

searchList([ 2, 3, 18, 22, 6 ], 22)
17/5:
# searching data
def searchList(mList, item):
    return True if item in mList else False
    # return list( filter( lambda mList, item: True if item in mList else False ) )

searchList([ 2, 3, 18, 22, 6 ], 22)
17/6:
# searching data
def searchList(mList, item):
    return True if item in mList else False
    # return list( filter( lambda mList, item: True if item in mList else False ) )

searchList([ 2, 3, [18, 22], 6 ], 22)
17/7:
# searching data
def searchList(mList, item):
    return True if item in mList else searchList(mList, item)
    # return list( filter( lambda mList, item: True if item in mList else False ) )

searchList([ 2, 3, [18, 22], 6 ], 22)
17/8:
# searching data
def searchList(mList, item):
    return True if item in mList else False
    # return list( filter( lambda mList, item: True if item in mList else False ) )

searchList([ 2, 3, [18, 22], 6 ], 22)
17/9:
# searching data
def searchList(mList, item):
    return True if item in mList else False
    # return list( filter( lambda mList, item: True if item in mList else False ) )

searchList([ 2, 3, [18, 22], 6 ], 22)
17/10:
# searching data
def searchList(mList, item):
    for num in mList:
        if item != num:
            return False
        else:
            searchList(mList, item)
    return True

searchList([ 2, 3, [18, 22], 6 ], 22)
17/11:
# searching data
def searchList(mList, item):
    for num in mList:
        if item != num:
            return False
        else:
            searchList(mList, item)
    return True

searchList([ 2, 3, [18, 22], 6 ], 22)
17/12:
# searching data
def searchList(mList, item):
    for num in mList:
        if item == num:
            return True
        else:
            searchList(mList, item)
    return False

searchList([ 2, 3, [18, 22], 6 ], 22)
17/13:
# searching data
def searchList(mList, item):
    for num in mList:
        if item == num:
            return True
        else:
            searchList(mList, item)
    return False

searchList([ 2, 3, [18, 22], 6 ], 22)
17/14:
# searching data
from functools import lru_cache
@lru_cache()
def searchList(mList, item):
    for num in mList:
        if item == num:
            return True
        else:
            searchList(mList, item)
    return False

searchList([ 2, 3, [18, 22], 6 ], 22)
17/15:
# searching data
from functools import lru_cache
@lru_cache()
def searchList(mList, item):
    for num in mList:
        if item == num:
            return True
        else:
            searchList(mList, item)
    return False

searchList([ 2, 3, [18, 22], 6 ], 22)
17/16:
# searching data
from functools import lru_cache
@lru_cache()
def searchList(mList, item):
    for num in mList:
        if item == num:
            return True
        else:
            searchList(mList, item)
    return False

searchList([ 2, 3, [18, 22], 6 ], 22)
17/17:
# searching data
from functools import lru_cache
@lru_cache()
def searchList(mList, item):
    for num in mList:
        if item == num:
            return True
        else:
            searchList(mList, item)
        return False

searchList([ 2, 3, [18, 22], 6 ], 22)
17/18:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

print( nums )
17/19:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

print( sorted(nums) )
17/20:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()

print( sorted(nums) )
print( binarySearch( nums, 3 ) )
17/21:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    mid = len(aList)
    print(mid)
print( sorted(nums) )
print( binarySearch( nums, 3 ) )
17/22:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    mid = len(aList)
    return True
    if aList[mid] == num:
        return True
    elif aList[mid] > num:
        aList = aList[ : mid ]
        print( aList )
print( sorted(nums) )
print( binarySearch( nums, 3 ) )
17/23:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    mid = len(aList)
    return True
    if aList[mid] == num:
        return True
    elif aList[mid] > num:
        aList = aList[ : mid ]
    elif aList[mid] < num:
        aList = aList[ mid + 1 : ]
    print( aList )
print( sorted(nums) )
print( binarySearch( nums, 3 ) )
17/24:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    mid = len(aList)
    return True
    if aList[mid] == num:
        return True
    elif aList[mid] > num:
        aList = aList[ : mid ]
    elif aList[mid] < num:
        aList = aList[ mid + 1 : ]
        print( aList )
print( sorted(nums) )
print( binarySearch( nums, 3 ) )
17/25:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    mid = len(aList)
    return True
    if aList[mid] == num:
        return True
    elif aList[mid] > num:
        aList = aList[ : mid ]
    elif aList[mid] < num:
        aList = aList[ mid + 1 : ]
    print( aList )
17/26:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    mid = len(aList)
    return True
    if aList[mid] == num:
        return True
    elif aList[mid] > num:
        aList = aList[ : mid ]
    elif aList[mid] < num:
        aList = aList[ mid + 1 : ]
        print( aList )
17/27:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    mid = len(aList)
    return True
    if aList[mid] == num:
        return True
    elif aList[mid] > num:
        aList = aList[ : mid ]
    elif aList[mid] < num:
        aList = aList[ mid + 1 : ]
        print( aList )
17/28:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    while aList:
        mid = len(aList)
        return True
        if aList[mid] == num:
            return True
        elif aList[mid] > num:
            aList = aList[ : mid ]
        elif aList[mid] < num:
            aList = aList[ mid + 1 : ]
        print( aList )
17/29:
# setting up imports and generating a list of random numbers to work with
import random
nums = [ random.randint(0, 20) for i in range(10) ]

def binarySearch(aList, num):
    aList.sort()
    while aList:
        mid = len(aList)
        return True
        if aList[mid] == num:
            return True
        elif aList[mid] > num:
            aList = aList[ : mid ]
        elif aList[mid] < num:
            aList = aList[ mid + 1 : ]
    return False

print( sorted(nums) )
print( binarySearch(nums, 3) )
18/1:
# creating a basic iterator from iterable
sports = [ 'baseball', 'soccer', 'football', 'hockey', 'basketball' ]
my_iter = iter(sports)
print( next(my_iter) )
18/2:
# creating a basic iterator from iterable
sports = [ 'baseball', 'soccer', 'football', 'hockey', 'basketball' ]
my_iter = iter(sports)
print( next(my_iter) )
18/3:
# creating a basic iterator from iterable
sports = [ 'baseball', 'soccer', 'football', 'hockey', 'basketball' ]
my_iter = iter(sports)
print( next(my_iter) )
18/4:
# creating a basic iterator from iterable
sports = [ 'baseball', 'soccer', 'football', 'hockey', 'basketball' ]
my_iter = iter(sports)
print( next(my_iter) )
print( next(my_iter) )
18/5:
# creating a basic iterator from iterable
sports = [ 'baseball', 'soccer', 'football', 'hockey', 'basketball' ]
my_iter = iter(sports)
print( next(my_iter) )
print( next(my_iter) )
for item in next(my_iter):
    print( item )
18/6:
# creating a basic iterator from iterable
sports = [ 'baseball', 'soccer', 'football', 'hockey', 'basketball' ]
my_iter = iter(sports)
print( next(my_iter) )
print( next(my_iter) )
for item in my_iter:
    print( item )
18/7:
# creating a basic iterator from iterable
sports = [ 'baseball', 'soccer', 'football', 'hockey', 'basketball' ]
my_iter = iter(sports)
print( next(my_iter) )
print( next(my_iter) )
for item in my_iter:
    print( item )
print( next(my_iter) )
18/8:
# creating our own iterator
class Alphabet():
    def __iter__(self):
        self.letters = 'abcdefghijklmnopqrstuvwxyz'
        self.index = 0
        return self
    def __next__(self):
        if self.index <= 25:
            char = self.letters[ self.index ]
            self.index += 1
            return char
        else:
            raise StopIteration

for char in Alphabet():
    print( char )
18/9:
# creating our own generator with start, stop and step parameters
def myRange(stop, start = 0, step = 1):
    print( 'Generator Start Value: {}'.format(start) )
    yield start
    start += step
for x in range(5):
    print( 'For Loop x Value: {}'.format(x) )
19/1:
# creating and applying our own decorator using the @ symbol
def decorator(func):
    def wrap():
        print('=======')
        func()
        print('=======')
    return wrap()
@decorator
def printName():
    print('John!')
printName()
19/2:
# creating and applying our own decorator using the @ symbol
def decorator(func):
    def wrap():
        print('=======')
        func()
        print('=======')
    return wrap
@decorator
def printName():
    print('John!')
printName()
19/3:
# creating a decorator that takes in parameters
def run_times(num):
    def wrap(func):
        for i in range(num):
            func()
    return wrap
@run_times(4)
def sayHello():
    print('Hello!')
19/4:
# creating a decorator that takes in parameters
def run_times(num):
    def wrapl(func):
        for i in range(num):
            func()
    return wrapl
@run_times(4)
def sayHello():
    print('Hello!')
19/5:
# creating a decorator that takes in parameters
def run_times(num):
    def wrap(func):
        for i in range(num):
            func()
    return wrap
@run_times(4)
def sayHello():
    print('Hello!')
19/6:
# creating a decorator for a function that accepts parameters
def birthday(func):
    def wrap(name, age):
        func(name, age + 1)
    return wrap
@birthday
def celebrate(name, age):
    print( 'Happy birthday {}, you are now {}.'.format(name, age) )
celebrate( 'Paul', 43 )
19/7:
# real world sim, restricting function access
def login_required(func):
    def wrap(user):
        password = input('What is the password?')
        if password == user['password']:
            func(user)
        else:
            print( 'Access Denied' )
    return wrap
@login_required
def restrictedFunc(user):
    print( 'Access granted, welcome {}'.format(user['name']) )
user = { 'name': 'Jess', 'password': 'ilywpf' }
restrictedFunc(user)
19/8:
# user input
def decorator(func):
    def wrap():
        user_input = input('Enter a number: ')
        if user_input < 100:
            func()
    return wrap
@decorator
def numbers():
    print("Less than 100")
19/9:
# user input
def decorator(func):
    def wrap():
        user_input = input('Enter a number: ')
        if user_input < 100:
            func()
    return wrap
@decorator
def numbers():
    print("Less than 100")
numbers()
19/10:
# user input
def decorator(func):
    def wrap():
        user_input = int(input('Enter a number: '))
        if user_input < 100:
            func()
    return wrap
@decorator
def numbers():
    print("Less than 100")
numbers()
19/11:
# creating a route
def route(x):
    def wrap(func):
        print('{}'.format(x))

@route("/index")
def index():
    print("This is how web pages are made in Flask")
19/12:
# creating a route
def route(x):
    def wrap(func):
        print('{}'.format(x))
    return wrap

@route("/index")
def index():
    print("This is how web pages are made in Flask")
19/13:
# import the entire math module
import math
print( math.floor(2.5) )
print( math.ceil(2.5) )
print( math.pi )
19/14:
# importing only variables a functions rather than an entire module, better efficency
from math import floor, pi
print( floor(2.5) )
19/15:
# importing only variables a functions rather than an entire module, better efficency
from math import floor, pi
print( floor(2.5) )
print( pi )
19/16:
# using the 'as' keyword to create an alias for imports
from math import floor as f
print( f(2.5) )
19/17:
# using the run command with Jupiter Notebook to access our own modules
%run test.py
print(length, width)
printInfo('John Smith', 37)
19/18:
# Time Module
from time import sleep
sleep(5000)
print( 'Time Module Imported' )
21/1:
# Time Module
from time import sleep
sleep(5)
print( 'Time Module Imported' )
21/2:
%run calculation.py
calcArea(15, 30)
21/3:
a, c = 'bo', 'bob'
b = a
print( hash(a), hash(b), hash(c) )
21/4:
# creating data collection to test for time complexity
import time
d = {}
for i in range(1000000):
    d[i] = 'value'
big_list = [ x for x in range(1000000) ]
21/5:
# creating data collection to test for time complexity
import time
d = {}
for i in range(1000000):
    d[i] = 'value'
big_list = [ x for x in range(1000000) ]
# retrieving information and tracking time to see which is faster
start_time = time.time()
if 9999999 in d:
    print( 'Found in dictionary' )
end_time = time.time() - start_time
print( 'Elapsed time for list: {}'.format(end_time) )
21/6:
# creating data collection to test for time complexity
import time
d = {}
for i in range(1000000):
    d[i] = 'value'
big_list = [ x for x in range(1000000) ]
# retrieving information and tracking time to see which is faster
start_time = time.time()
if 9999999 in d:
    print( 'Found in dictionary' )
end_time = time.time() - start_time
print( 'Elapsed time for list: {}'.format(end_time) )
21/7:
# creating data collection to test for time complexity
import time
d = {}
for i in range(1000000):
    d[i] = 'value'
big_list = [ x for x in range(1000000) ]
# retrieving information and tracking time to see which is faster
start_time = time.time()
if 9999999 in d:
    print( 'Found in dictionary' )
end_time = time.time() - start_time
print( 'Elapsed time for list: {}'.format(end_time) )
21/8:
# creating data collection to test for time complexity
import time
d = {}
for i in range(1000000):
    d[i] = 'value'
big_list = [ x for x in range(1000000) ]
# retrieving information and tracking time to see which is faster
start_time = time.time()
if 9999999 in d:
    print( 'Found in dictionary' )
end_time = time.time() - start_time
print( 'Elapsed time for list: {}'.format(end_time) )
21/9:
# creating data collection to test for time complexity
import time
d = {}
for i in range(1000000):
    d[i] = 'value'
big_list = [ x for x in range(1000000) ]
# retrieving information and tracking time to see which is faster
start_time = time.time()
if 9999999 in d:
    print( 'Found in dictionary' )
end_time = time.time() - start_time
print( 'Elapsed time for list: {}'.format(end_time) )
21/10:
# testing burble sort vs. insertion sort
def bubbleSort(aList):
    for i in range( len(aList) ):
        switched = False
        for j in range( len(aList) - 1 ):
            if aList[ j ] > aList[ j + 1 ]:
                aList[ j ], aList[ j + 1 ] = aList[ j + 1 ], aList[ j ]
                switched = True
        if switched == False:
            break
    return aList
def insertionSort(aList):
    for i in range( 1, len(aList) ):
        if aList[ i ] < aList[ i - 1 ]:
            for j in range( i, 0, -1):
                if aList[ j ] < aList[ j - 1 ]:
                    aList[ j ], aList[ j + 1 ] = aList[ j + 1 ], aList[ j ]
                else:
                    break
    return aList
21/11:
# testing burble sort vs. insertion sort
def bubbleSort(aList):
    for i in range( len(aList) ):
        switched = False
        for j in range( len(aList) - 1 ):
            if aList[ j ] > aList[ j + 1 ]:
                aList[ j ], aList[ j + 1 ] = aList[ j + 1 ], aList[ j ]
                switched = True
        if switched == False:
            break
    return aList
def insertionSort(aList):
    for i in range( 1, len(aList) ):
        if aList[ i ] < aList[ i - 1 ]:
            for j in range( i, 0, -1):
                if aList[ j ] < aList[ j - 1 ]:
                    aList[ j ], aList[ j + 1 ] = aList[ j + 1 ], aList[ j ]
                else:
                    break
    return aList
21/12:
# calling bubble sort and insertion sort to test time complexity
from random import randint
nums = [ randint(0, 100) for x in range(5000) ]
start_time = time.time()
bubbleSort(nums)
end_time = time.time() - start_time
print( 'Elapsed time for Bubble Sort: {}'.format(end_time) )
start_time = time.time()
insertionSort(nums)
end_time = time.time() - start_time
print( 'Elapsed time for Insertion Sort: {}'.format(end_time) )
21/13:
# calling bubble sort and insertion sort to test time complexity
from random import randint
nums = [ randint(0, 100) for x in range(5000) ]
start_time = time.time()
bubbleSort(nums)
end_time = time.time() - start_time
print( 'Elapsed time for Bubble Sort: {}'.format(end_time) )
start_time = time.time()
insertionSort(nums)
end_time = time.time() - start_time
print( 'Elapsed time for Insertion Sort: {}'.format(end_time) )
22/1:
# sending a request and logging the response code
import requests
r = requests.get('https://api.github.com/users/Connor-SM')
print( r )
print( type( r ) )
22/2:
# sending a request and logging the response code
import requests
r = requests.get('https://api.github.com/users/Connor-SM')
print( r )
print( type( r ) )

# accessing the content that we requested from the URL
data = r.content
print( data )
22/3:
# sending a request and logging the response code
import requests
r = requests.get('https://api.github.com/users/simononen')
print( r )
print( type( r ) )

# accessing the content that we requested from the URL
data = r.content
print( data )
22/4:
# sending a request and logging the response code
import requests
r = requests.get('https://api.github.com/users/Connor-SM')
print( r )
print( type( r ) )

# accessing the content that we requested from the URL
data = r.content
print( data )
22/5:
# sending a request and logging the response code
import requests
r = requests.get('https://api.github.com/users/Connor-SM')
print( r )
print( type( r ) )

# accessing the content that we requested from the URL
data = r.content
print( data )

# converting data from JSON into a Python dictionary and outputting all key-value pairs
data = r.json()
for k, v in data.items():
    print( 'Key: {} \t Value: {}'.format(k, v) )
print( data['name'] )
22/6:
# sending a request and logging the response code
import requests
r = requests.get('https://api.github.com/users/simononen')
print( r )
print( type( r ) )

# accessing the content that we requested from the URL
data = r.content
print( data )

# converting data from JSON into a Python dictionary and outputting all key-value pairs
data = r.json()
for k, v in data.items():
    print( 'Key: {} \t Value: {}'.format(k, v) )
print( data['name'] )
22/7:
# sending a request and logging the response code
import requests
r = requests.get('https://api.github.com/users/Connor-SM')
print( r )
print( type( r ) )

# accessing the content that we requested from the URL
data = r.content
print( data )

# converting data from JSON into a Python dictionary and outputting all key-value pairs
data = r.json()
for k, v in data.items():
    print( 'Key: {} \t Value: {}'.format(k, v) )
print( data['name'] )
22/8:
# outputting specific key-value pairs from data
r = requests.get('https://api.github.com/search/repositories?q=language:python')
data = r.json()
print( data['total_count'] )
22/9:
# importing the pandas library
import pandas as pd
22/10:
# importing the pandas library
import pandas as pd
# using the from_dict method to convert a dictionary into a pandas DataFrame
import random
random.seed(3)
names = [ 'Jess', 'Jordan', 'Sandy', 'Ted', 'Barney', 'Tyler', 'Rebecca' ]
ages = [ random.randint(18, 35) for x in range( len( names ) ) ]
people = { 'names': names, 'ages': ages }
df = pd.DataFrame.from_dict(people)
print( df )
23/1:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

x = np.random.randn(100)
plt.plot(x)
plt.show()
24/1:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
24/2:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617['School_type']
24/3:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617['School_Type']
24/4:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
# sy1617['School_Type']
24/5:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617['School_Type']['Short_Name']
24/6:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617['School_Type'] sy1617['Short_Name']
24/7:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617['School_Type']
24/8:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617['School_Type']
24/9:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617['School_Type', 'Primary_Category']
24/10:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617[['School_Type', 'Primary_Category']]
24/11:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617[['School_Type', 'Primary_Category']]
sy1617.loc[400062]
24/12:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617[['School_Type', 'Primary_Category']]
sy1617.iloc[0]
24/13:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617[['School_Type', 'Primary_Category']]
sy1617.iloc[0]
24/14:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617[['School_Type', 'Primary_Category']]
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
24/15:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617[['School_Type', 'Primary_Category']].descibe()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
24/16:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].descibe()
24/17:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].descibe
24/18:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617[['School_Type', 'Primary_Category']].descibe()
24/19:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
24/20:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
24/21:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
24/22:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
#sy1617.iloc[0]
#sy1617.loc[400062]['School_Type']
#sy1617[['School_Type', 'Primary_Category']].describe()
#sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
24/23:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
24/24:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
is_charter = sy1617['School_Type'] == 'Charter'
sy1617[is_charter]
24/25:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
is_charter = sy1617['School_Type'] == 'Charter'
sy1617[is_charter].describe()
24/26:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
is_charter = sy1617['School_Type'] == 'Charter'
sy1617[is_charter]
24/27:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
is_charter = sy1617['School_Type'] == 'Charter'
sy1617[is_charter]
charter_magnet = sy1617['School_Type'].isin(['charter', 'magnet'])
sy1617[charter_magnet]
24/28:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
is_charter = sy1617['School_Type'] == 'Charter'
sy1617[is_charter]
charter_magnet = sy1617['School_Type'].isin(['Charter', 'Magnet'])
sy1617[charter_magnet]
24/29:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
is_charter = sy1617['School_Type'] == 'Charter'
sy1617[is_charter]
charter_magnet = sy1617['School_Type'].isin(['Charter', 'Magnet'])
sy1617[charter_magnet].describe()
24/30:
import pandas as pd
sy1617 = pd.read_csv('Chicago_Public_Schools_-_School_Progress_Reports_SY1617.csv', index_col='School_ID')
sy1617.head()
sy1617.iloc[0]
sy1617.loc[400062]['School_Type']
sy1617[['School_Type', 'Primary_Category']].describe()
sy1617[['School_Survey_Student_Response_Rate_Pct', 'Suspensions_Per_100_Students_Year_1_Pct']].describe()
is_charter = sy1617['School_Type'] == 'Charter'
sy1617[is_charter]
charter_magnet = sy1617['School_Type'].isin(['Charter', 'Magnet'])
sy1617[charter_magnet]
24/31:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80]
24/32:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80]
24/33:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80].describe()
24/34:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80].describe(['School_Survey_Student_Response_Rate_Pct'])
24/35:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80].describe('School_Survey_Student_Response_Rate_Pct')
24/36:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80['School_Survey_Student_Response_Rate_Pct']].describe()
24/37:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80].describe()
24/38:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80].describe()
24/39: sy1617[is_charter & gt80]
24/40:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80]
24/41:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80].describe()
24/42:
gt80 = sy1617['School_Survey_Student_Response_Rate_Pct'] >= 80
sy1617[gt80]
24/43: sy1617.sort_values(by=['Primary_Category', 'Short_Name'])
24/44: sy1617.sort_values(by=['Primary_Category', 'Short_Name']).describe()
24/45: sy1617.sort_values(by=['Primary_Category', 'Short_Name'])
24/46: pd.pivot_table(sy1617, values = 'School_Survey_Student_Response_Rate_Pct', index = 'School_Type', columns = 'Primary_Category', aggfunc = np.mean)
24/47: pd.pivot_table(sy1617, values = 'School_Survey_Student_Response_Rate_Pct', index = 'School_Type', columns =['Primary_Category'], aggfunc = np.mean)
24/48:
pd.pivot_table(sy1617, 
               values = 'School_Survey_Student_Response_Rate_Pct',
               index = 'School_Type',
               columns=['Primary_Category'],
               aggfunc = np.mean)
24/49:
pd.pivot_table(sy1617, 
               values = 'School_Survey_Student_Response_Rate_Pct',
               index = 'School_Type',
               columns=['Primary_Category'],
               aggfunc=np.mean)
24/50:
pd.pivot_table(sy1617, 
               values = 'School_Survey_Student_Response_Rate_Pct',
               index = 'School_Type',
               columns=['Primary_Category'],
               aggfunc=np.mean)
24/51:
pd.pivot_table(sy1617, 
               values = 'School_Survey_Student_Response_Rate_Pct',
               index = 'School_Type',
               columns=['Primary_Category'],
               aggfunc=np.mean)
24/52:
import numpy as np
pd.pivot_table(sy1617, 
               values = 'School_Survey_Student_Response_Rate_Pct',
               index = 'School_Type',
               columns=['Primary_Category'],
               aggfunc=np.mean)
24/53:
import numpy as np
data_output = pd.pivot_table(sy1617, 
               values = 'School_Survey_Student_Response_Rate_Pct',
               index = 'School_Type',
               columns=['Primary_Category'],
               aggfunc=np.mean)

data_output.to_excel("data_file.xlsx")
24/54:
import numpy as np
data_output = pd.pivot_table(sy1617, 
               values = 'School_Survey_Student_Response_Rate_Pct',
               index = 'School_Type',
               columns=['Primary_Category'],
               aggfunc=np.mean)

data_output.to_csv('data_file.csv', index=False)
data_output.to_excel("data_file.xlsx")
24/55:
import numpy as np
data_output = pd.pivot_table(sy1617, 
               values = 'School_Survey_Student_Response_Rate_Pct',
               index = 'School_Type',
               columns=['Primary_Category'],
               aggfunc=np.mean)

data_output.to_csv('data_file.csv')
data_output.to_excel("data_file.xlsx")
24/56:
r1=['My nickname is ft.jgt','Someone is going to my place']

df=pd.DataFrame(r1,columns=['text'])

df.text.apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0)
24/57:
r1=sy1617['Student_Attainment_Description']

df=pd.DataFrame(r1,columns=['text'])

df.text.apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0)
24/58:
r1=sy1617['Student_Attainment_Description']

df=pd.DataFrame(r1,columns=['Student_Attainment_Description'])

df.text.apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0)
24/59:
r1=sy1617['Student_Attainment_Description']

df=pd.DataFrame(r1,columns=['text'])

df.text.apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0)
24/60:
r1=sy1617['Student_Attainment_Description']

rl
24/61: sy1617['Student_Attainment_Description']
24/62: sy1617['Student_Attainment_Description'].value_count()
24/63: sy1617['Student_Attainment_Description'].value_counts()
24/64: sy1617['CPS_School_Profile'].value_counts()
24/65: sy1617['CPS_School_Profile'].value_counts()
24/66: sy1617['CPS_School_Profile'].value_counts()
24/67: sy1617['CPS_School_Profile'].value_counts()
24/68: sy1617['CPS_School_Profile'].value_counts()
25/1:
import pandas as pd
ppe1920 = readCsv('procurement_plan_entries.csv')
25/2:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
25/3:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
25/4:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.describe()
25/5:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.describe()
25/6:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920[['subject_of_procurement', 'estimated_amount']].describe()
#ppe1920.describe()
25/7:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/8:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/9:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount');
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/10:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
print(ppe1920.sort_values(by='estimated_amount'))
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/11:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
pd.options.display.max_rows
pd.get_option("display.max_rows")
print(ppe1920.sort_values(by='estimated_amount')
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/12:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
print(ppe1920.sort_values(by='estimated_amount')
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/13:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/14:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/15:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/16:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.groupBy(by='procurement_method')
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/17:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/18:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/19:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/20:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/21:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/22:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920[procurement_method].isin(['Direct procurement'])
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/23:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920[procurement_method].isin(['Direct procurement'])
ppe1920[direct]
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/24:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct procurement'])
ppe1920[direct]
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/25:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct procurement'])
# ppe1920[direct]
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/26:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct procurement'])
# ppe1920[direct]
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/27:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct procurement'])
ppe1920[direct]
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/28:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct procurement'])
#ppe1920[direct]
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/29:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
ppe1920[direct]
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/30:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct Procurement']).describe()
ppe1920[direct]
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/31:
import pandas as pd
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/32:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
25/33: ppe1920.groupBy('procurement_method')
25/34:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
ppe1920.sort_values(by='estimated_amount')
direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920.groupBy('procurement_method')
25/35:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920.groupBy('procurement_method')
25/36:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920.group('procurement_method')
25/37:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920.group(by = 'procurement_method')
25/38:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920.groupBy('procurement_method')
25/39:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920['subject_of_procurement'].groupBy('procurement_method')
25/40:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
df = ppe1920['subject_of_procurement']
df.groupBy('procurement_method')
25/41:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920.groupBy('procurement_method')
25/42:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
#ppe1920.groupBy('procurement_method')
25/43:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
25/44:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
ppe1920[['procurement_method', 'estimated_amount']].groupby('procurement_method').sum()
25/45:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
ppe1920[['procurement_method', 'estimated_amount']].groupby('procurement_method')
25/46:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
ppe1920[['procurement_method', 'estimated_amount']].groupby('procurement_method').sum()
25/47:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
ppeByMethod = ppe1920[['procurement_method', 'estimated_amount']].groupby('procurement_method').sum()
25/48:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
ppeByMethod = ppe1920[['procurement_method', 'estimated_amount']].groupby('procurement_method').sum()
print(ppeByMethod)
25/49:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
ppeByMethod = ppe1920[['procurement_method', 'estimated_amount']].groupby('procurement_method').sum()
ppeByPde = ppeByMethod[['pde_title']].groupby('pde_title').sum()
print(ppeByPde)
25/50:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
25/51:
# Group By Method
ppe1920[['procurement_method', 'estimated_amount']].groupby('procurement_method').sum()
25/52:
# Group By Type
ppe1920[['procurement_type', 'estimated_amount']].groupby('procurement_type').sum()
25/53:
# Group By Pde
ppe1920[['pde_title', 'id']].groupby('pde_title').count()
25/54:
# Group By Pde
ppe1920[['pde_title', 'id' as 'Number Of Entries']].groupby('pde_title').count()
25/55:
# Group By Pde
ppe1920[['pde_title', 'id' => 'Number Of Entries']].groupby('pde_title').count()
25/56:
# Group By Pde
ppe1920[['pde_title', 'id']].groupby('pde_title').count()
25/57:
# Group By Pde
ppe1920[['pde_title', 'procurement_method', 'id']].groupby('pde_title').count()
25/58:
# Group By Pde
ppe1920[['pde_title', 'id']].groupby('pde_title').count()
25/59:
# Group By Pde
byPde = ppe1920[['pde_title', 'id']].groupby('pde_title').count()
byPde.sort_values(id)
25/60:
# Group By Pde
byPde = ppe1920[['pde_title', 'id']].groupby('pde_title').count()
byPde.sort_values(byPde[id])
25/61:
# Group By Pde
byPde = ppe1920[['pde_title', 'id']].groupby('pde_title').count()
byPde.sort_values(byPde['id'])
25/62:
# Group By Pde
byPde = ppe1920[['pde_title', 'id']].groupby('pde_title').count()
25/63:
# Group By Pde
ppe1920[['pde_title', 'id']].groupby('pde_title').count()
25/64:
# Group By Pde
ppe1920[['pde_title', 'id']].groupby('pde_title').count()
25/65:
# Group By Pde
ppe1920[['pde_title', 'id']].groupby('pde_title').count()
25/66:
# Common items in the plan
ppe1920.describe()
25/67:
# Common items in the plan
ppe1920[['id', 'subject_of_procurement']].describe()
25/68:
# Common items in the plan
ppe1920[['id', 'subject_of_procurement']]
25/69:
# Common items in the plan
ppe1920[['id', 'subject_of_procurement', 'estimated_amount']]
25/70:
# Common items in the plan
ppe1920[['id', 'subject_of_procurement', 'estimated_amount']].sort_values('estimated_amount')
25/71:
# Common items in the plan
ppe1920[['id', 'subject_of_procurement', 'estimated_amount']].sort_values('estimated_amount', 'desc')
25/72:
# Common items in the plan
ppe1920[['id', 'subject_of_procurement', 'estimated_amount']].sort_values('estimated_amount', ascending=True)
25/73:
# Common items in the plan
ppe1920[['id', 'subject_of_procurement', 'estimated_amount']].sort_values('estimated_amount', ascending=False)
25/74:
# Common items in the plan
ppe1920[['id', 'subject_of_procurement', 'estimated_amount', 'pde_title']].sort_values('estimated_amount', ascending=False)
25/75:
# Common methods used at plan
ppe1920[['procurement_method', 'estimated_amount']].groupby('procurement_method').count()
25/76:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement']].groupby('procurement_method').count()
25/77:
# Common methods used at plan
ppe1920[['procurement_method', 'procurements']].groupby('procurement_method').count()
25/78:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement']].groupby('procurement_method').count()
25/79:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement']].groupby('procurement_method').sum()
25/80:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement']].groupby('procurement_method').count()
25/81:
# Common types used at plan
ppe1920[['procurement_type', 'subject_of_procurement']].groupby('procurement_type').count()
25/82:
# Common types used at plan
ppe1920[['procurement_type', 'subject_of_procurement']].groupby('procurement_type').count()
25/83:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement']].groupby('procurement_method').count()
25/84:
# Most budgeted for items in the plan
ppe1920[['id', 'subject_of_procurement', 'estimated_amount', 'pde_title']].sort_values('estimated_amount', ascending=False)
25/85:
# Group By Type
ppe1920[['procurement_type', 'estimated_amount']].groupby('procurement_type').sum()
25/86:
# Common types used at plan
ppe1920[['procurement_type', 'subject_of_procurement']].groupby('procurement_type').count()
ppe1920[['procurement_type', 'estimated_amount']].groupby('procurement_type').sum()
25/87:
# Common types used at plan
ppe1920[['procurement_type', 'subject_of_procurement']].groupby('procurement_type').count()
25/88:
# Group By Pde
ppe1920[['pde_title', 'id']].groupby('pde_title').count()
25/89:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']]
.groupby('procurement_method')
.agg(
    max_amount('estimated_amount', 'max')
)
25/90:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(
    max_amount('estimated_amount', 'max')
)
25/91:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount('estimated_amount', 'max'))
25/92:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/93:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/94:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max')).sort_values(max_amount, ascending=False)
25/95:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/96:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max').sort_values(max_amount, ascending=False))
25/97:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/98:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/99:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/100:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('subject_of_procurement').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/101:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/102:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('subject_of_procurement').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/103:
# Most budgeted procurement
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/104:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/105:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].isin('Non-Consultancy Services')
25/106:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920[['procurement_method']].isin('Non-Consultancy Services')
25/107:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920[['subject_of_procurement', 'procurement_type', 'estimated_amount']].isin('Non-Consultancy Services')
25/108:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920[['procurement_type']].isin('Non-Consultancy Services')
25/109:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920['procurement_type'].isin('Non-Consultancy Services')
25/110:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
25/111:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
print(nonConsultancy)
25/112:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
25/113:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
nonConsultancy[['subject_of_procurement']]
25/114:
# Procurements Under non-consultancy

# ppe1920[['subjec_of_procurement', 'procurement_method', 'estimated_amount']]

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy]
25/115:
# Procurements Under non-consultancy

supplies = ppe1920['procurement_type'].isin(['Supplies'])
ppe1920[supplies]
25/116:
# Procurements Under Works

works = ppe1920['procurement_type'].isin(['Works'])
ppe1920[works]
25/117:
# Procurements Under Works

consultancyServices = ppe1920['procurement_type'].isin(['Consultancy Services   '])
ppe1920[consultancyServices]
25/118:
# Procurements Under Works

consultancyServices = ppe1920['procurement_type'].isin(['Consultancy Services'])
ppe1920[consultancyServices]
25/119:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
filterAmount = ppe1920['estimated_amount'].agg(max_amount = pd.NamedAgg(columns='estimated_amount', aggfunc='max'))
ppe1920[nonConsultancy]
25/120:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
filterAmount = ppe1920[['estimated_amount']].agg(max_amount = pd.NamedAgg(columns='estimated_amount', aggfunc='max'))
ppe1920[nonConsultancy]
25/121:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy]
25/122:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy.sort_values['estimated_amount']]
25/123:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy.sort_values('estimated_amount')]
25/124:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy.sort_values('estimated_amount', ascending=True)]
25/125:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy.sort_values(by=['estimated_amount'], ascending=True)]
25/126:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy.sort_values(by=['estimated_amount'])]
25/127:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy.sort_values(by='estimated_amount')]
25/128:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy.sort_values(['estimated_amount'])]
25/129:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy.sort_values('estimated_amount')]
25/130:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy]
25/131:
# Procurements Under non-consultancy

nonConsultancy = ppe1920['procurement_type'].isin(['Non-Consultancy Services'])
ppe1920[nonConsultancy]
25/132:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('pde_title').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/133:
# Most budgeted procurement by pde
ppe1920[['subject_of_procurement', 'pde_title', 'estimated_amount']].groupby('pde_title').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/134:
# Most budgeted procurement by pde
ppe1920[['pde_title', 'subject_of_procurement', 'estimated_amount']].groupby('pde_title').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/135:
# Most budgeted procurement by pde
ppe1920[['pde_title', 'subject_of_procurement', 'estimated_amount']].groupby('pde_title').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/136:
# Most budgeted procurement by pde
ppe1920[['pde_title', 'subject_of_procurement', 'estimated_amount']].groupby('subject_of_procurement').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/137:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/138:
# Search for the existence of a string
searchString = input('Enter search string')
ppe1920[['subject_of_procurement']].str.contains(searchString)
25/139:
# Common types used at plan
ppe1920[['procurement_type', 'subject_of_procurement']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum')
25/140:
# Common types used at plan
ppe1920[['procurement_type', 'subject_of_procurement']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max')
25/141:
# Common types used at plan
ppe1920[['procurement_type', 'subject_of_procurement']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/142:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/143:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/144:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
25/145:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type')
.agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/146:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/147:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'
25/148:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/149:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min')
25/150:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
25/151:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
25/152:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entirs = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
25/153:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
25/154:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
25/155:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
25/156:
# Group By Pde
ppe1920[['pde_title', 'id', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
25/157:
# Group By Pde
ppe1920[['pde_title', 'id', 'subject_of_procurement', 'estimated_amount']].groupby('id').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
25/158:
# Group By Pde
ppe1920[['pde_title', 'id', 'subject_of_procurement', 'estimated_amount']].groupby('pde_title').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/1:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/2:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
ppe1920[['subject_of_procurement', 'procurement_method']].describe()
26/3:
# Group By Type
ppe1920[['procurement_type', 'estimated_amount']].groupby('procurement_type').sum()
26/4:
# Pdes with their plans and value
ppe1920[['pde_title', 'id', 'subject_of_procurement', 'estimated_amount']].groupby('pde_title').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/5:
# Most budgeted for items in the plan
ppe1920[['id', 'subject_of_procurement', 'estimated_amount', 'pde_title']].sort_values('estimated_amount', ascending=False)
26/6:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
26/7:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
26/8:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NameAgg(column='estimated_amount', aggfunc='sum'))
26/9:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/10:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/11:
# Common methods used at plan
ppe1920[['procurement_method', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/12:
# Most budgeted for items in the plan
ppe1920[['pde_title', 'subject_of_procurement', 'procurement_method', 'estimated_amount']].sort_values('estimated_amount', ascending=False)
26/13:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum')).sort_values('estimated_amount', ascending=False)
26/14:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/15:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum').sort_values('estimated_amount', ascending=False))
26/16:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/17:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
26/18:
# Most budgeted procurement by method


mostBudgeted = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
mostBudgeted['sum_amount'].sort_values('estimated_amount', ascending=False)
26/19:
# Most budgeted procurement by method


mostBudgeted = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
mostBudgeted['sum_amount'].sort_values('sum_amount', ascending=False)
26/20:
# Most budgeted procurement by method


mostBudgeted = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
mostBudgeted[sum_amount].sort_values(sum_amount, ascending=False)
26/21:
# Most budgeted procurement by method


mostBudgeted = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
mostBudgeted.sort_values(sum_amount, ascending=False)
26/22:
# Most budgeted procurement by method


mostBudgeted = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
sort_values(mostBudgeted, ascending=False)
26/23:
# Most budgeted procurement by method


mostBudgeted = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
sort_values(mostBudgeted['sum_amount'], ascending=False)
26/24:
# Most budgeted procurement by method


ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/25:
# Most budgeted procurement by method


ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/26:
# Most budgeted procurement by method


ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/27:
# Most budgeted procurement by method


ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/28:
# Most budgeted procurement by method


ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/29:
# Most budgeted for items in the plan
ppe1920[['pde_title', 'subject_of_procurement', 'procurement_method', 'estimated_amount']].sort_values('estimated_amount', ascending=False)
26/30:
# Common types used at plan

commonTypes = ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
commonTypes.apply(commonTypes['sum_amount']);
26/31:
# Common types used at plan

commonTypes = ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
print(commonTypes)
26/32:
# Common types used at plan

ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
26/33:
# Common types used at plan

commonTypes = ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
commonTypes.sortValues(('sum_amount', ascending=False))
26/34:
# Common types used at plan

commonTypes = ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
commonTypes.sortValues(('sum_amount'))
26/35:
# Common types used at plan

commonTypes = ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
commonTypes.sort_values(('sum_amount'))
26/36:
# Common types used at plan

commonTypes = ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
commonTypes.sort_values('sum_amount')
26/37:
# Common types used at plan

commonTypes = ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
commonTypes.sort_values('sum_amount', ascending=True)
26/38:
# Common types used at plan

commonTypes = ppe1920[['procurement_type', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_type').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
commonTypes.sort_values('sum_amount', ascending=False)
26/39:
# Most budgeted procurement by method


mostBudgetedProcurements = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
mostBudgetedProcurements.sort_values('sum_amount', ascending=False)
26/40:
# Pdes with their plans and value
highSpent = ppe1920[['pde_title', 'id', 'subject_of_procurement', 'estimated_amount']].groupby('pde_title').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))

highSpent.sort_values('sum_amount', ascending=False)
26/41:
# Search for the existence of a string
searchString = input('Enter search string')
ppe1920['subject_of_procurement'].str.contains(searchString)
26/42:
# Search for the existence of a string
searchString = input('Enter search string').toLowerCase()
ppe1920['subject_of_procurement'].str.contains(searchString)
26/43:
# Search for the existence of a string
searchString = input('Enter search string').lower()
ppe1920['subject_of_procurement'].str.contains(searchString)
26/44:
# Search for the existence of a string
searchString = input('Enter search string').lower()
print(searchString)
ppe1920['subject_of_procurement'].str.contains(searchString)
26/45:
# Search for the existence of a string
searchString = input('Enter search string').lower()
res = ppe1920['subject_of_procurement'].str.contains(searchString)
res['subject_of_procurement']
26/46:
# Search for the existence of a string
searchString = input('Enter search string').lower()
res = ppe1920['subject_of_procurement'].str.contains(searchString)
ppe1920[res['subject_of_procurement']]
26/47:
# Search for the existence of a string
searchString = input('Enter search string').lower()
res = ppe1920['subject_of_procurement'].str.contains(searchString)
ppe1920[['subject_of_procurement']]
26/48:
# Search for the existence of a string
searchString = input('Enter search string').lower()
res = ppe1920['subject_of_procurement'].str.contains(searchString)
res
26/49:
# Search for the existence of a string
searchString = input('Enter search string').lower()
res = ppe1920['subject_of_procurement'].str.contains(searchString)
res
26/50:
# Search for the existence of a string
searchString = input('Enter search string ').lower()
res = ppe1920['subject_of_procurement'].str.contains(searchString)
res
26/51:
# Most budgeted procurement by method
ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'))
26/52:
# Most budgeted procurement by method


mostBudgetedProcurements = ppe1920[['subject_of_procurement', 'procurement_method', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
mostBudgetedProcurements.sort_values('sum_amount', ascending=False)
26/53:
# Search for the existence of a string
searchString = input('Enter search string ').lower()
#res = ppe1920['subject_of_procurement'].str.contains(searchString)
ppe1920[['subject_of_procurement']].values_count()
26/54:
# Search for the existence of a string
searchString = input('Enter search string ').lower()
#res = ppe1920['subject_of_procurement'].str.contains(searchString)
ppe1920[['subject_of_procurement']].values_count()
26/55:
# Search for the existence of a string
# searchString = input('Enter search string ').lower()
#res = ppe1920['subject_of_procurement'].str.contains(searchString)
ppe1920[['subject_of_procurement']].values_counts()
26/56:
# Search for the existence of a string
# searchString = input('Enter search string ').lower()
#res = ppe1920['subject_of_procurement'].str.contains(searchString)
ppe1920[['subject_of_procurement']].value_counts()
26/57:
# Search for the existence of a string
# searchString = input('Enter search string ').lower()
#res = ppe1920['subject_of_procurement'].str.contains(searchString)
ppe1920['subject_of_procurement'].value_counts()
26/58:
# Search for the existence of a string
# searchString = input('Enter search string ').lower()
#res = ppe1920['subject_of_procurement'].str.contains(searchString)
valueCounts = ppe1920['subject_of_procurement'].value_counts()
valueCounts.to_csv('value counts')
26/59:
# Search for the existence of a string
# searchString = input('Enter search string ').lower()
#res = ppe1920['subject_of_procurement'].str.contains(searchString)
valueCounts = ppe1920['subject_of_procurement'].value_counts()
valueCounts.to_csv('value_counts.csv')
26/60:
# Search for the existence of a string
valueCounts = ppe1920['subject_of_procurement', 'estimated_amount'].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/61:
# Search for the existence of a string
valueCounts = ppe1920['subject_of_procurement'].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/62:
# Search for the existence of a string
ppe1920['subject_of_procurement'].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/63:
# Search for the existence of a string
ppe1920[['subject_of_procurement']].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/64:
# Search for the existence of a string
ppe1920['subject_of_procurement'].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/65:
# Search for the existence of a string
valueCounts = ppe1920['subject_of_procurement'].value_counts()
valueCOunts = ppe1920['estimated_amount']
# valueCounts.to_csv('value_counts.csv')
26/66:
# Search for the existence of a string
valueCounts = ppe1920['subject_of_procurement'].value_counts()
ppe1920['estimated_amount']
# valueCounts.to_csv('value_counts.csv')
26/67:
# Search for the existence of a string
valueCounts = ppe1920['subject_of_procurement'].value_counts()
ppe1920['estimated_amount'].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/68:
# Search for the existence of a string
ppe1920['subject_of_procurement'].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/69:
# Search for the existence of a string
valueCounts = ppe1920['subject_of_procurement'].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/70:
# Search for the existence of a string
valueCounts = ppe1920['subject_of_procurement'].value_counts()
valueCounts
# valueCounts.to_csv('value_counts.csv')
26/71:
# Search for the existence of a string
valueCounts = ppe1920['subject_of_procurement'].value_counts()
print(valueCounts)
# valueCounts.to_csv('value_counts.csv')
26/72:
# Search for the existence of a string
ppe1920['subject_of_procurement'].value_counts()
# valueCounts.to_csv('value_counts.csv')
26/73:
# Common methods used at plan
commonMethods = ppe1920[['procurement_method', 'subject_of_procurement', 'estimated_amount']].groupby('procurement_method').agg(number_of_entries = pd.NamedAgg(column='subject_of_procurement', aggfunc='count'), min_amount = pd.NamedAgg(column='estimated_amount', aggfunc='min'), max_amount = pd.NamedAgg(column='estimated_amount', aggfunc='max'), sum_amount = pd.NamedAgg(column='estimated_amount', aggfunc='sum'))
commonMethods.sort_values('number_of_entries', ascending=False)
29/1:
import pandas as pd
import numpy as np

awardedContracts1920 = pd.read_csv('awarded_contracts.csv')
awardedContracts1920.head()
29/2:
# commonest provider per entity

awardedContracts1920['provider'].value_counts()
29/3:
# commonest provider per entity

awardedContracts1920['Provider'].value_counts()
29/4:
# commonest provider per entity

commonestProvider = awardedContracts1920['Provider'].value_counts()
commonestProvider.to_csv('commonest_provider.csv')
29/5:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
29/6:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
pd.set_option('display.max_rows', '10')
29/7:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
pd.set_option('display.max_rows', 10)
29/8:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
pd.set_option('display.max_rows', 10)
29/9:
# Huge amounts of contracts per entity
pd.set_option('display.max_rows', 10)
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
29/10:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
pd.set_option('display.max_rows', 10)
29/11:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
pd.set_option('display.max_rows', 10)
29/12:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
29/13:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
29/14:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
29/15:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
29/16:
# Huge amounts of contracts per entity
awardedContracts1920[['title', 'subject_of_procurement', 'Type', 'Provider', 'amount']].sort_values('amount', ascending=False)
29/17:
# Sum duplicated
awardedContracts1920.groupby(['subject_of_procurement', 'amount']).sum()
29/18:
# Sum duplicated
awardedContracts1920.groupby(['provider', 'amount']).sum()
29/19:
# Sum duplicated
awardedContracts1920.groupby(['Provider', 'amount']).sum()
29/20:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].isin['japan']
29/21:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].isin(['japan'])
29/22:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].isin(['japan'])
awardedContracts(japanAsProvider)
29/23:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('japan')
awardedContracts(japanAsProvider)
29/24:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('japan')
awardedContracts1920(japanAsProvider)
29/25:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('japan')
awardedContracts1920[japanAsProvider]
29/26:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('Japan')
awardedContracts1920[japanAsProvider]
29/27:
# Filter out Simba as provider
simbaAsProvider = awardedContracts1920['Provider'].str.contains('Simba')
awardedContracts1920[simbaAsProvider]
29/28:
# Filter out Katwaalo as provider
katwaloAsProvider = awardedContracts1920['Provider'].str.contains('Katwaalo')
awardedContracts1920[katwaloAsProvider]
29/29:
# Filter out mac east africa ltd as provider
macEastAfricaAsProvider = awardedContracts1920['Provider'].str.contains('mac east africa ltd')
awardedContracts1920[macEastAfricaAsProvider]
29/30:
# Filter out mac east africa ltd as provider
macEastAfricaAsProvider = awardedContracts1920['Provider'].str.lower().contains('mac east africa ltd')
awardedContracts1920[macEastAfricaAsProvider]
29/31:
# Filter out mac east africa ltd as provider
macEastAfricaAsProvider = awardedContracts1920['Provider'].lower().str.contains('mac east africa ltd')
awardedContracts1920[macEastAfricaAsProvider]
29/32:
# Filter out mac east africa ltd as provider
macEastAfricaAsProvider = awardedContracts1920['Provider'].str.contains('mac east africa ltd')
awardedContracts1920[macEastAfricaAsProvider]
29/33:
# Filter out mac east africa ltd as provider
macEastAfricaAsProvider = awardedContracts1920['Provider'].str.contains('east africa ltd')
awardedContracts1920[macEastAfricaAsProvider]
29/34:
# Filter out pearl entertainment as provider
pearlEntertainmentAsProvider = awardedContracts1920['Provider'].str.contains('Pearl entertainment limited')
awardedContracts1920[pearlEntertainmentAsProvider]
29/35:
# Filter out pearl entertainment as provider
pearlEntertainmentAsProvider = awardedContracts1920['Provider'].str.contains('Pearl entertainment limited')
awardedContracts1920[pearlEntertainmentAsProvider]
29/36:
# Filter out city tyres as Provider
cityTyresAsProvider = awardedContracts1920['Provider'].str.contains('city tyres')
awardedContracts1920[cityTresAsProvider]
29/37:
# Filter out city tyres as Provider
cityTyresAsProvider = awardedContracts1920['Provider'].str.contains('city tyres')
awardedContracts1920[cityTyresAsProvider]
29/38:
# Filter out summer auto services as provider
summerAutoServicesAsProvider = awardedContracts1920['Provider'].str.contains('summer auto services')
awardedContracts1929[summerAutoServiceAsProvider]
29/39:
# Filter out summer auto services as provider
summerAutoServicesAsProvider = awardedContracts1920['Provider'].str.contains('summer auto services')
awardedContracts1929[summerAutoServicesAsProvider]
29/40:
# Filter out summer auto services as provider
summerAutoServicesAsProvider = awardedContracts1920['Provider'].str.contains('summer auto services')
awardedContracts1929[summerAutoServicesAsProvider]
29/41:
# Filter out summer auto services as provider
summerAutoServicesAsProvider = awardedContracts1920['Provider'].str.contains('summer auto services')
awardedContracts1920[summerAutoServicesAsProvider]
29/42:
# Filter out carolrene services as provider
carolreneServicesAsProvider = awardedContracts1920['Provider'].str.contains('carolrene services')
awardedContracts1920[carolreneServicesAsProvider]
29/43:
# Filter out Daks Toyota
daksToyotaAsProvider = awardedContracts1920['Provider'].str.contains('Daks Toyota')
awardedContracts1920[daksToyotaAsProvider]
29/44:
# Filter out Daks Toyota
daksToyotaAsProvider = awardedContracts1920['Provider'].str.contains('Daks toyota')
awardedContracts1920[daksToyotaAsProvider]
29/45:
# Filter out Lina construction as provider
linaAsProvider = awardedContracts1920['Provider'].str.contains('Lina construction')
awardedContracts1920[linaAsProvider]
29/46:
# Filter out Daks Toyota
daksToyotaAsProvider = awardedContracts1920['Provider'].str.contains('Daks toyota')
awardedContracts1920[daksToyotaAsProvider]
# Filter out Lina construction as provider
linaAsProvider = awardedContracts1920['Provider'].str.contains('Lina construction')
awardedContracts1920[linaAsProvider]
29/47:
# Filter out Lina construction as provider
linaAsProvider = awardedContracts1920['Provider'].str.contains('Lina construction')
awardedContracts1920[linaAsProvider]
29/48:
# Filter out new vision as Provider
newVisionAsProvider = awardedContracts1920['Provider'].str.contains('new vision')
awardedContracts1920[newVisionAsProvider]
29/49:
# Filter ridar hotel as provider
ridarHotelAsProvider = awardedContracts1920['Provider'].str.contains('ridar hotel')
awardedContracts1920[ridarHotelAsProvider]
29/50:
# Filter Tendo catering as Provider
tendoCateringAsProvider = awardedContracts1920['Provider'].str.contains('Tendo catering')
awardedContracts1920[tendoCateringProvider]
29/51:
# Filter Tendo catering as Provider
tendoCateringAsProvider = awardedContracts1920['Provider'].str.contains('Tendo catering')
awardedContracts1920[tendoCateringAsProvider]
29/52:
# Filter Gittoes Pharmaceuticals as Provider
gittoesAsProvider = awardedContracts1920['Provider'].str.contains('Gittoes Pharmaceuticals')
awardedContracts1920[gittoesAsProvider]
29/53:
# Filter KANGAROO (U) Ltd as Provider
kangarooAsProvider = awardedContracts1920['Provider'].str.contains('KANGAROO')
awardedContracts1920[kangarooAsProvider]
29/54:
# Filter Grace lubega motors 2000 ltd as Provider
graceLubegaAsProvider = awardedContracts1920['Provider'].str.contains('Grace Lubega')
awardedContracts1920[graceLubegaAsProvider]
29/55:
# Filter Grace lubega motors 2000 ltd as Provider
graceLubegaAsProvider = awardedContracts1920['Provider'].str.contains('Grace lubega')
awardedContracts1920[graceLubegaAsProvider]
29/56:
# Filter M/S ARROW CENTRE(U)LTD as Provider
arrowCentreAsProvider = awardedContracts1920['Provider'].str.contains('arrow')
awardedContracts1920[arrowCentreAsProvider]
29/57:
# Filter M/S ARROW CENTRE(U)LTD as Provider
arrowCentreAsProvider = awardedContracts1920['Provider'].str.contains('ARROW')
awardedContracts1920[arrowCentreAsProvider]
29/58:
# Filter Veve and Sons Ltd as Provider
veveAsProvider = awardedContracts1920['Provider'].str.contains('Veve and Sons')
awardedContracts1920[veveAsProvider]
29/59:
# Filter Mans Plastics as Provider
mansPlasticAsProvider = awardedContracts1920['Provider'].str.contains('Mans Plastics')
awardedContracts1920[mansPlasticAsProvider]
29/60:
# Filter Mans Plastics as Provider
mansPlasticAsProvider = awardedContracts1920['Provider'].str.contains('Mans plastics')
awardedContracts1920[mansPlasticAsProvider]
29/61:
# Filter Mans Plastics as Provider
mansPlasticAsProvider = awardedContracts1920['Provider'].str.contains('Mans')
awardedContracts1920[mansPlasticAsProvider]
29/62:
# Filter Mans Plastics as Provider
mansPlasticAsProvider = awardedContracts1920['Provider'].str.contains('Mans Plastics')
awardedContracts1920[mansPlasticAsProvider]
29/63:
# Filter chemix & tech limited as Provider
chemixAsProvider = awardedContracts1920['Provider'].str.contains('chemix & tech')
awardedContracts1920[chemixAsProvider]
29/64:
# Filter Toyota uganda limited as Provider
toyotaUgandaAsProvider = awardedContracts1920['Provider'].str.contains('Toyota Uganda')
awardedContracts1920[toyotaUgandaAsProvider]
29/65:
# Filter Toyota uganda limited as Provider
toyotaUgandaAsProvider = awardedContracts1920['Provider'].str.contains('Toyota uganda')
awardedContracts1920[toyotaUgandaAsProvider]
29/66:
# Filter Toyota uganda limited as Provider
toyotaUgandaAsProvider = awardedContracts1920['Provider'].str.contains('Toyota')
awardedContracts1920[toyotaUgandaAsProvider]
29/67:
# Filter Monitor publications ltd as Provider
monitorAsProvider = awardedContracts1920['Provider'].str.contains('monitor')
awardedContracts1920[monitorAsProvider]
29/68:
# Filter Monitor publications ltd as Provider
monitorAsProvider = awardedContracts1920['Provider'].str.contains('Monitor')
awardedContracts1920[monitorAsProvider]
29/69:
# Filter Monitor publications ltd as Provider
monitorAsProvider = awardedContracts1920['Provider'].str.contains('monitor', regex='G[a-b].*')
awardedContracts1920[monitorAsProvider]
29/70:
# Filter Monitor publications ltd as Provider
monitorAsProvider = awardedContracts1920['Provider'].str.contains('monitor', case=False)
awardedContracts1920[monitorAsProvider]
29/71:
# Filter The cooper motor corporation (u) ltd as Provider
cooperAsProvider = awardedContracts1920['Provider'].str.contains('cooper motor', case=False)
awardedContracts1920[cooperAsProvider]
29/72:
# Filter Toyota uganda limited as Provider
toyotaUgandaAsProvider = awardedContracts1920['Provider'].str.contains('Toyota', case=False)
awardedContracts1920[toyotaUgandaAsProvider]
29/73:
# Filter Toyota uganda limited as Provider
toyotaUgandaAsProvider = awardedContracts1920['Provider'].str.contains('Toyota', case=True)
awardedContracts1920[toyotaUgandaAsProvider]
29/74:
# Filter Toyota uganda limited as Provider
toyotaUgandaAsProvider = awardedContracts1920['Provider'].str.contains('Toyota')
awardedContracts1920[toyotaUgandaAsProvider]
29/75:
# Filter Monitor publications ltd as Provider
monitorAsProvider = awardedContracts1920['Provider'].str.contains('monitor', case=False)
awardedContracts1920[monitorAsProvider]
29/76:
# Filter The cooper motor corporation (u) ltd as Provider
cooperAsProvider = awardedContracts1920['Provider'].str.contains('cooper motor', case=False)
awardedContracts1920[cooperAsProvider]
29/77:
# Filter Singh motor garage ltd as Provider
singhAsProvider = awardedContracts1920['Provider'].str.contains('Singh motor', case=False)
awardedContracts1920[singhAsProvider]
29/78:
# Filter M/S ARROW CENTRE(U)LTD as Provider
arrowCentreAsProvider = awardedContracts1920['Provider'].str.contains('ARROW', case=False)
awardedContracts1920[arrowCentreAsProvider]
29/79:
# Filter Astra pharma (u) ltd
astraAsProvider = awardedContracts1920['Provider'].str.contains('Astra', case=False)
awardedContracts1920[astraAsProvider]
29/80:
# Filter T&B General Enterprises LTD as Provider
TBGeneralAsProvider = awardedContracts1920['Provider'].str.contains('T&B General', case=False)
awardedContracts1920[TBGeneralAsProvider]
29/81:
# Filter Silverbacks Pharmacy Ltd as Provider
SilverbacksAsProvider = awardedContracts1920['Provider'].str.contains('Silverbacks', case=False)
awardedContracts1920[SilverbacksAsProvider]
29/82:
# Filter Silverbacks Pharmacy Ltd as Provider
SilverbacksAsProvider = awardedContracts1920['Provider'].str.contains('Silverbacks', case=False)
awardedContracts1920[SilverbacksAsProvider]
29/83:
# Filter M/S SKANZ ENGINEERING SERVICES LTD as Provider
skanzbacksAsProvider = awardedContracts1920['Provider'].str.contains('SKANZ', case=False)
awardedContracts1920[skanzbacksAsProvider]
29/84:
# Filter M/S SKANZ ENGINEERING SERVICES LTD as Provider
skanzbacksAsProvider = awardedContracts1920['Provider'].str.contains('SKANZ', case=False)
awardedContracts1920[skanzbacksAsProvider]
29/85:
# Filter M/S SKANZ ENGINEERING SERVICES LTD as Provider
skanzAsProvider = awardedContracts1920['Provider'].str.contains('SKANZ', case=False)
awardedContracts1920[skanzAsProvider]
29/86:
# Filter Sheraton kampala hotel as provider
sheratonAsProvider = awardedContracts1920['Provider'].str.contains('Sheraton', case=False)
awardedContracts1920[sheratonAsProvider]
29/87:
# Filter Sheraton kampala hotel as provider
sheratonAsProvider = awardedContracts1920['Provider'].str.contains('Sheraton', case=False)
awardedContracts1920[sheratonAsProvider]
29/88:
# Filter Luna group of companies as provider
lunaAsProvider = awardedContracts1920['Provider'].str.contains('Luna group', case=False)
awardedContracts1920[lunaAsProvider]
29/89:
# Filter Luna group of companies as provider
lunaAsProvider = awardedContracts1920['Provider'].str.contains('Luna group', case=False)
awardedContracts1920[lunaAsProvider]
29/90:
# Filter M/s Nunu Ventures Ltd as provider
nunuAsProvider = awardedContracts1920['Provider'].str.contains('Nunu', case=False)
awardedContracts1920[nunuAsProvider]
29/91:
# Filter M/s Nunu Ventures Ltd as provider
nunuAsProvider = awardedContracts1920['Provider'].str.contains('Nunu', case=False)
awardedContracts1920[nunuAsProvider]
29/92:
# Filter Rodena Printers & Stationers as provider
rodenaPrintersAsProvider = awardedContracts1920['Provider'].str.contains('Rodena', case=False)
awardedContracts1920[rodenaPrintersAsProvider]
29/93:
# Filter Rodena Printers & Stationers as provider
rodenaPrintersAsProvider = awardedContracts1920['Provider'].str.contains('Rodena', case=False)
awardedContracts1920[rodenaPrintersAsProvider]
29/94:
# Filter Balton (u) ltd as Provider
baltonAsProvider = awardedContracts1920['Provider'].str.contains('Balton', case=False)
awardedContracts1920[baltonAsProvider]
29/95:
# Filter Balton (u) ltd as Provider
baltonAsProvider = awardedContracts1920['Provider'].str.contains('Balton', case=False)
awardedContracts1920[baltonAsProvider]
29/96:
# Filter Maka motor works ltd as provider
makaMotorAsProvider = awardedContracts1920['Provider'].str.contains('Maka motor', case=False)
awardedContracts1920[makaMotorAsProvider]
29/97:
# Filter Lucaci Supplies Limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('lucaci', case=False)
awardedContracts1920[lucaciAsProvider]
29/98:
# Filter M/s imperial golf view hotel as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('imperial', case=False)
awardedContracts1920[lucaciAsProvider]
29/99:
# Filter M/s imperial golf view hotel as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('imperial golf', case=False)
awardedContracts1920[lucaciAsProvider]
29/100:
# Filter M/s imperial golf view hotel as provider
imperialAsProvider = awardedContracts1920['Provider'].str.contains('imperial golf', case=False)
awardedContracts1920[imperialAsProvider]
29/101:
# Filter Abacus pharma (a) ltd as provider
abacusAsProvider = awardedContracts1920['Provider'].str.contains('abacus', case=False)
awardedContracts1920[abacusAsProvider]
29/102:
# Filter Janine services limited as provider
janineAsProvider = awardedContracts1920['Provider'].str.contains('janine', case=False)
awardedContracts1920[janineAsProvider]
29/103:
# Filter M/S ROYAL AUTOMOBILES LTD as provider
royalAutomobilesAsProvider = awardedContracts1920['Provider'].str.contains('ROYAL AUTOMOBILES', case=False)
awardedContracts1920[royalAutomobilesAsProvider]
29/104:
# Filter M/S ROYAL AUTOMOBILES LTD as provider
royalAutomobilesAsProvider = awardedContracts1920['Provider'].str.contains('ROYAL', case=False)
awardedContracts1920[royalAutomobilesAsProvider]
29/105:
# Filter M/S ROYAL AUTOMOBILES LTD as provider
royalAutomobilesAsProvider = awardedContracts1920['Provider'].str.contains('ROYAL AUTO', case=False)
awardedContracts1920[royalAutomobilesAsProvider]
29/106:
# Filter Resteve international limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('lucaci', case=False)
awardedContracts1920[lucaciAsProvider]
29/107:
# Filter Resteve international limited as provider
resteveInternationalAsProvider = awardedContracts1920['Provider'].str.contains('Resteve international', case=False)
awardedContracts1920[resteveInternationalAsProvider]
29/108:
# Filter Skyline Holdings Ltd as provider
skylineAsProvider = awardedContracts1920['Provider'].str.contains('Skyline', case=False)
awardedContracts1920[skylineAsProvider]
29/109:
# Filter actros east africa limited as provider
actrosAsProvider = awardedContracts1920['Provider'].str.contains('actros', case=False)
awardedContracts1920[actrosAsProvider]
29/110:
# Filter Castilleo traders limited  as provider
castilleoAsProvider = awardedContracts1920['Provider'].str.contains('Castilleo', case=False)
awardedContracts1920[castilleoAsProvider]
29/111:
# Filter Kijoh Consulting and Marketing Limited as provider
KijohAsProvider = awardedContracts1920['Provider'].str.contains('Kijoh', case=False)
awardedContracts1920[KijohAsProvider]
29/112:
# Filter Grace lubega motors 2000 ltd as Provider
graceLubegaAsProvider = awardedContracts1920['Provider'].str.contains('Grace lubega', case=False)
awardedContracts1920[graceLubegaAsProvider]
29/113:
# Filter penina restaurant as provider
peninaAsProvider = awardedContracts1920['Provider'].str.contains('penina', case=False)
awardedContracts1920[peninaAsProvider]
29/114:
# Filter Joint medical stores as provider
jointAsProvider = awardedContracts1920['Provider'].str.contains('Joint medical', case=False)
awardedContracts1920[jointAsProvider]
29/115:
# Filter Baseline Suppliers ltd as provider
baselineAsProvider = awardedContracts1920['Provider'].str.contains('Baseline', case=False)
awardedContracts1920[baselineAsProvider]
29/116:
# Filter M/s almid clean services ltd as provider
almidAsProvider = awardedContracts1920['Provider'].str.contains('almid clean', case=False)
awardedContracts1920[almidAsProvider]
29/117:
# Filter Phillips Pharmaceuticals (U) Ltd as provider
PhillipsPharmaceuticalsAsProvider = awardedContracts1920['Provider'].str.contains('Phillips Pharmaceuticals', case=False)
awardedContracts1920[PhillipsPharmaceuticalsAsProvider]
29/118:
# Filter Viga pharmaceuticals ltd as provider
vigaAsProvider = awardedContracts1920['Provider'].str.contains('viga', case=False)
awardedContracts1920[vigaAsProvider]
29/119:
# Filter lunko enterprises limited as provider
lunkoAsProvider = awardedContracts1920['Provider'].str.contains('lunko', case=False)
awardedContracts1920[lunkoAsProvider]
29/120:
# Filter M/s prime impex 2001 ltd as provider
primeImpexAsProvider = awardedContracts1920['Provider'].str.contains('prime impex', case=False)
awardedContracts1920[primeImpexAsProvider]
29/121:
# Filter Lithocraft investments ltd as provider
LithocraftAsProvider = awardedContracts1920['Provider'].str.contains('Lithocraft', case=False)
awardedContracts1920[LithocraftAsProvider]
29/122:
# Filter Prime general supply ltd as provider
primeGeneralAsProvider = awardedContracts1920['Provider'].str.contains('prime general', case=False)
awardedContracts1920[primeGeneralAsProvider]
29/123:
# Filter DHL as provider
dhlAsProvider = awardedContracts1920['Provider'].str.contains('DHL', case=False)
awardedContracts1920[dhlAsProvider]
29/124:
# Filter Mid world enterprises ltd as provider
midAsProvider = awardedContracts1920['Provider'].str.contains('Mid world', case=False)
awardedContracts1920[midAsProvider]
29/125:
# Filter Reev consult international as provider
reevAsProvider = awardedContracts1920['Provider'].str.contains('Reev', case=False)
awardedContracts1920[reevAsProvider]
29/126:
# Filter Global scientific supplies ltd as provider
globalAsProvider = awardedContracts1920['Provider'].str.contains('Global', case=False)
awardedContracts1920[globalAsProvider]
29/127:
# Filter Global scientific supplies ltd as provider
globalAsProvider = awardedContracts1920['Provider'].str.contains('Global scientific', case=False)
awardedContracts1920[globalAsProvider]
29/128:
# Filter Critical care solutions as provider
criticalCareAsProvider = awardedContracts1920['Provider'].str.contains('Critical care', case=False)
awardedContracts1920[criticalCareAsProvider]
29/129:
# Filter Colas East Africa Limited as provider
colasAsProvider = awardedContracts1920['Provider'].str.contains('colas', case=False)
awardedContracts1920[colasAsProvider]
29/130:
# Filter Kiy auto repairs as provider
kiyAsProvider = awardedContracts1920['Provider'].str.contains('kiy', case=False)
awardedContracts1920[kiyAsProvider]
29/131:
# Filter KIBAO INVESTMENTS CO. LTD as provider
kibaoAsProvider = awardedContracts1920['Provider'].str.contains('kibao', case=False)
awardedContracts1920[kibaoAsProvider]
29/132:
# Filter JBN Consults & Planners Ltd as provider
jbnAsProvider = awardedContracts1920['Provider'].str.contains('JBN Consults', case=False)
awardedContracts1920[jbnAsProvider]
29/133:
# Filter Sobetra Uganda Limited as provider
SobetraAsProvider = awardedContracts1920['Provider'].str.contains('Sobetra', case=False)
awardedContracts1920[SobetraAsProvider]
29/134:
# Filter Ksb office and general supplies limited as provider
ksbAsProvider = awardedContracts1920['Provider'].str.contains('Ksb', case=False)
awardedContracts1920[ksbAsProvider]
29/135:
# Filter M/s Lea International Ltd as provider
LeaAsProvider = awardedContracts1920['Provider'].str.contains('Lea', case=False)
awardedContracts1920[LeaAsProvider]
29/136:
# Filter M/s Lea International Ltd as provider
LeaAsProvider = awardedContracts1920['Provider'].str.contains('Lea international', case=False)
awardedContracts1920[LeaAsProvider]
29/137:
# Filter M/S BURNS ENGINEERING&GENERAL SUPPLY as provider
burnsAsProvider = awardedContracts1920['Provider'].str.contains('burns', case=False)
awardedContracts1920[burnsAsProvider]
29/138:
# Filter M/S JAMUSHINE INTERNATIONALas provider
jamushineAsProvider = awardedContracts1920['Provider'].str.contains('JAMUSHINE', case=False)
awardedContracts1920[jamushineAsProvider]
29/139:
# Filter Riley media as provider
RileyAsProvider = awardedContracts1920['Provider'].str.contains('Riley', case=False)
awardedContracts1920[RileyAsProvider]
29/140:
# Filter Makdeus General Enterprises as provider
MakdeusAsProvider = awardedContracts1920['Provider'].str.contains('Makdeus', case=False)
awardedContracts1920[MakdeusAsProvider]
29/141:
# Filter Centric solutions (u) ltd as provider
centricAsProvider = awardedContracts1920['Provider'].str.contains('Centric', case=False)
awardedContracts1920[centricAsProvider]
29/142:
# Filter Abubaker technical services ltd as provider
AbubakerAsProvider = awardedContracts1920['Provider'].str.contains('Abubaker', case=False)
awardedContracts1920[AbubakerAsProvider]
29/143:
# Filter City Aluminium & Glass services ltd as provider
CityAluminiumAsProvider = awardedContracts1920['Provider'].str.contains('City Aluminium', case=False)
awardedContracts1920[CityAluminiumAsProvider]
29/144:
# Filter Rafah Communications (U) Limited as provider
RafahAsProvider = awardedContracts1920['Provider'].str.contains('Rafah', case=False)
awardedContracts1920[RafahAsProvider]
29/146:
# Filter M.M INTEGRATED STEEL MILLS (U) LTD as provider
mmIntegratedAsProvider = awardedContracts1920['Provider'].str.contains('M.M INTEGRATED', case=False)
awardedContracts1920[mmIntegratedAsProvider]
29/147:
# Filter M.M INTEGRATED STEEL MILLS (U) LTD as provider
mmIntegratedAsProvider = awardedContracts1920['Provider'].str.contains('M.M INTEGRATED', case=False)
awardedContracts1920[mmIntegratedAsProvider]
29/148:
# Filter M.M INTEGRATED STEEL MILLS (U) LTD as provider
mmIntegratedAsProvider = awardedContracts1920['Provider'].str.contains('M M INTEGRATED', case=False)
awardedContracts1920[mmIntegratedAsProvider]
29/149:
# Filter M.M INTEGRATED STEEL MILLS (U) LTD as provider
mmIntegratedAsProvider = awardedContracts1920['Provider'].str.contains('M INTEGRATED', case=False)
awardedContracts1920[mmIntegratedAsProvider]
29/150:
# Filter M.M INTEGRATED STEEL MILLS (U) LTD as provider
mmIntegratedAsProvider = awardedContracts1920['Provider'].str.contains('M.M INTEGRATED', case=False)
awardedContracts1920[mmIntegratedAsProvider]
29/151:
# Filter M/S JUCO COMPANY LOGISTICS as provider
jucoAsProvider = awardedContracts1920['Provider'].str.contains('juco', case=False)
awardedContracts1920[jucoAsProvider]
29/152:
# Filter M/S DR FORTUNATE MUYAMBI as provider
drAsProvider = awardedContracts1920['Provider'].str.contains('dr fortunate', case=False)
awardedContracts1920[drAsProvider]
29/153:
# Filter INSPIRE KIDZ EVENTS HEALTH CLUB LTD as provider
inspireAsProvider = awardedContracts1920['Provider'].str.contains('inspire', case=False)
awardedContracts1920[inspireAsProvider]
29/154:
# Filter Ttb investments ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('b investments', case=False)
awardedContracts1920[lucaciAsProvider]
29/155:
# Filter taste and tasty restaurant as provider
tasteAsProvider = awardedContracts1920['Provider'].str.contains('taste', case=False)
awardedContracts1920[tasteAsProvider]
29/156:
# Filter Stirling civil engineering ltd as provider
StirlingAsProvider = awardedContracts1920['Provider'].str.contains('Stirling', case=False)
awardedContracts1920[StirlingAsProvider]
29/157:
# Filter Stirling civil engineering ltd as provider
StirlingAsProvider = awardedContracts1920['Provider'].str.contains('Stirling', case=False)
awardedContracts1920[StirlingAsProvider]
29/158:
# Filter Jogoo 93.8 fm ltd as provider
jogooAsProvider = awardedContracts1920['Provider'].str.contains('jogoo', case=False)
awardedContracts1920[jogooAsProvider]
29/159:
# Filter A Plus Funeral Management Ltd as provider
aplusAsProvider = awardedContracts1920['Provider'].str.contains('A plus', case=False)
awardedContracts1920[aplusAsProvider]
29/160:
# Filter M/s Frema Trading Ltd as provider
fremaAsProvider = awardedContracts1920['Provider'].str.contains('Frema', case=False)
awardedContracts1920[fremaAsProvider]
29/161:
# Filter M/S BUKEDDE TV as provider
BUKEDDEAsProvider = awardedContracts1920['Provider'].str.contains('BUKEDDE', case=False)
awardedContracts1920[BUKEDDEAsProvider]
29/162:
# Filter Sybyl limited as provider
SybylAsProvider = awardedContracts1920['Provider'].str.contains('Sybyl', case=False)
awardedContracts1920[SybylAsProvider]
29/163:
# Filter Joy for Children Uganda as provider
joyAsProvider = awardedContracts1920['Provider'].str.contains('joy', case=False)
awardedContracts1920[joyAsProvider]
29/164:
# Filter M/S RICH-DAD AGRO CONSULTS&BUYERS LTD as provider
richAsProvider = awardedContracts1920['Provider'].str.contains('Rich', case=False)
awardedContracts1920[richAsProvider]
29/165:
# Filter M/S GRAPHIC SYSTEMS (U)LTD as provider
graphicAsProvider = awardedContracts1920['Provider'].str.contains('graphic', case=False)
awardedContracts1920[graphicAsProvider]
29/166:
# Filter M/S GRAPHIC SYSTEMS (U)LTD as provider
graphicAsProvider = awardedContracts1920['Provider'].str.contains('graphic systems', case=False)
awardedContracts1920[graphicAsProvider]
29/167:
# Filter WEBNET TECHNOLOGIES as provider
webNetAsProvider = awardedContracts1920['Provider'].str.contains('webnet', case=False)
awardedContracts1920[webNetAsProvider]
29/168:
# Filter Buscom international ltd as provider
BuscomAsProvider = awardedContracts1920['Provider'].str.contains('Buscom', case=False)
awardedContracts1920[BuscomAsProvider]
29/169:
# Filter M/S FOYER FIVE LTD as provider
foyerAsProvider = awardedContracts1920['Provider'].str.contains('FOYER', case=False)
awardedContracts1920[foyerAsProvider]
29/170:
# Filter Oscar general hardware ltd as provider
OscarAsProvider = awardedContracts1920['Provider'].str.contains('Oscar', case=False)
awardedContracts1920[OscarAsProvider]
29/171:
# Filter M2SYS INC. as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('lucaci', case=False)
awardedContracts1920[lucaciAsProvider]
29/172:
# Filter M2SYS INC. as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('M2SYS', case=False)
awardedContracts1920[lucaciAsProvider]
29/173:
# Filter M2SYS INC. as provider
M2SYSAsProvider = awardedContracts1920['Provider'].str.contains('M2SYS', case=False)
awardedContracts1920[M2SYSAsProvider]
29/174:
# Filter M/S KEMEN UGANDA LTD as provider
kemenAsProvider = awardedContracts1920['Provider'].str.contains('kemen', case=False)
awardedContracts1920[kemenAsProvider]
29/175:
# Filter China civil engineering construction corporation as provider
chinaCivilAsProvider = awardedContracts1920['Provider'].str.contains('China Civil', case=False)
awardedContracts1920[chinaCivilAsProvider]
29/176:
# Filter Nashidu traders as provider
NashiduAsProvider = awardedContracts1920['Provider'].str.contains('Nashidu', case=False)
awardedContracts1920[NashiduAsProvider]
29/177:
# Filter Blu Flammimgo Limited as provider
BluAsProvider = awardedContracts1920['Provider'].str.contains('Blu', case=False)
awardedContracts1920[BluAsProvider]
29/178:
# Filter Crane Crafts & Engravers Ltd as provider
craneCraftAsProvider = awardedContracts1920['Provider'].str.contains('Crane craft', case=False)
awardedContracts1920[craneCraftAsProvider]
29/179:
# Filter Zescom technologies ltd as provider
ZescomAsProvider = awardedContracts1920['Provider'].str.contains('Zescom', case=False)
awardedContracts1920[ZescomAsProvider]
29/180:
# Filter M/S KENYA VETERINARY VACCINES PRODUCTION INSTITUTEas provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('KENYA VETERINARY', case=False)
awardedContracts1920[lucaciAsProvider]
29/181:
# Filter Junaco U Limited as provider
JunacoAsProvider = awardedContracts1920['Provider'].str.contains('Junaco', case=False)
awardedContracts1920[JunacoAsProvider]
29/182:
# Filter Associated business machinery as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Associated', case=False)
awardedContracts1920[lucaciAsProvider]
29/183:
# Filter Lloyds Pharmacy Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Lloyds', case=False)
awardedContracts1920[lucaciAsProvider]
29/184:
# Filter M/S ESELLA COUNTRY HOTEL LTD
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('ESELLA', case=False)
awardedContracts1920[lucaciAsProvider]
29/185:
# Filter Kiru general services limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kiru', case=False)
awardedContracts1920[lucaciAsProvider]
29/186:
# Filter M/s Pembe General Supplies Ltd PO Box 37847, Kampala as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Pembe', case=False)
awardedContracts1920[lucaciAsProvider]
29/187:
# Filter M/S GLORY GUEST HOUSE
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('GLORY', case=False)
awardedContracts1920[lucaciAsProvider]
29/188:
# Filter M/s speke resort munyonyo
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('speke', case=False)
awardedContracts1920[lucaciAsProvider]
29/189:
# Filter M/S DYNAMIC TECHNICAL SERVICES as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('DYNAMIC', case=False)
awardedContracts1920[lucaciAsProvider]
29/190:
# Filter Kampala serena hotel as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('serena', case=False)
awardedContracts1920[lucaciAsProvider]
29/191:
# Filter Nila multi concepts limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Nila', case=False)
awardedContracts1920[lucaciAsProvider]
29/192:
# Filter level 5 associates as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('lucaci', case=False)
awardedContracts1920[lucaciAsProvider]
29/193:
# Filter Fabrication systems (u) ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Fabrication', case=False)
awardedContracts1920[lucaciAsProvider]
29/194:
# Filter Palcom Construction Company Ltdas provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Palcom', case=False)
awardedContracts1920[lucaciAsProvider]
29/195:
# Filter Hak computer and electronics uganda ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Hak', case=False)
awardedContracts1920[lucaciAsProvider]
29/196:
# Filter M/s Colline Hotel Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Colline Hotel Ltd', case=False)
awardedContracts1920[lucaciAsProvider]
29/197:
# Filter It office uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('It office', case=False)
awardedContracts1920[lucaciAsProvider]
29/198:
# Filter Prime General Supplies as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Prime General', case=False)
awardedContracts1920[lucaciAsProvider]
29/199:
# Filter EMPIRE PUBLISHING COMPANY(U)LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('EMPIRE', case=False)
awardedContracts1920[lucaciAsProvider]
29/200:
# Filter Henan Guoji Industry Group Co., Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Henan', case=False)
awardedContracts1920[lucaciAsProvider]
29/201:
# Filter M/S BOTSWANA VACCINE INSTITUTE LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('BOTSWANA', case=False)
awardedContracts1920[lucaciAsProvider]
29/202:
# Filter M/S TWECA UGANDA LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('TWECA', case=False)
awardedContracts1920[lucaciAsProvider]
29/203:
# Filter Muga services limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Muga', case=False)
awardedContracts1920[lucaciAsProvider]
29/204:
# Filter Africa look Travel as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Africa look', case=False)
awardedContracts1920[lucaciAsProvider]
29/205:
# Filter Sering Ingegneria as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Sering', case=False)
awardedContracts1920[lucaciAsProvider]
29/206:
# Filter SSESSE CIVIL WORKS & CONTRACTOR LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('SSESSE CIVIL', case=False)
awardedContracts1920[lucaciAsProvider]
29/207:
# Filter Bojoma Investments Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Bojoma Investments Ltd', case=False)
awardedContracts1920[lucaciAsProvider]
29/208:
# Filter Bojoma Investments Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Bojoma', case=False)
awardedContracts1920[lucaciAsProvider]
29/209:
# Filter Absolute global investment as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Absolute', case=False)
awardedContracts1920[lucaciAsProvider]
29/210:
# Filter CRANE COMPUTERS LIMITED as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('CRANE COMPUTERS', case=False)
awardedContracts1920[lucaciAsProvider]
29/211:
# Filter Nippon parts uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Nippon', case=False)
awardedContracts1920[lucaciAsProvider]
29/212:
# Filter Tyre express limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Tyre express', case=False)
awardedContracts1920[lucaciAsProvider]
29/213:
# Filter Innovative furniture limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Innovative', case=False)
awardedContracts1920[lucaciAsProvider]
29/214:
# Filter M/S CAPITAL RADIO as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('CAPITAL', case=False)
awardedContracts1920[lucaciAsProvider]
29/215:
# Filter M/S CAPITAL RADIO as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('CAPITAL RADIO', case=False)
awardedContracts1920[lucaciAsProvider]
29/216:
# Filter M/S E&S ENTERPRISES LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('E&S ENTERPRISES', case=False)
awardedContracts1920[lucaciAsProvider]
29/217:
# Filter Rene industries limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Rene industries limited', case=False)
awardedContracts1920[lucaciAsProvider]
29/218:
# Filter Rene industries limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Rene industrie', case=False)
awardedContracts1920[lucaciAsProvider]
29/219:
# Filter Rene industries limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Rene industries', case=False)
awardedContracts1920[lucaciAsProvider]
29/220:
# Filter M/s techman computer solutions ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('techman', case=False)
awardedContracts1920[lucaciAsProvider]
29/221:
# Filter Norvik enterprises ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Norvik', case=False)
awardedContracts1920[lucaciAsProvider]
29/222:
# Filter M/s blessed printers and stationers ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('blessed printers and stationers', case=False)
awardedContracts1920[lucaciAsProvider]
29/223:
# Filter LHIAMU & LITSA ENTERPRISES LTDas provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('LHIAMU', case=False)
awardedContracts1920[lucaciAsProvider]
29/224:
# Filter Technology associates limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Technology associates', case=False)
awardedContracts1920[lucaciAsProvider]
29/225:
# Filter M/S CONTINUUM ENGINEERING LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('CONTINUUM', case=False)
awardedContracts1920[lucaciAsProvider]
29/226:
# Filter M/S MANDELA NATIONAL STADIUM NAMBOOLE as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('CRANE COMPUTERS', case=False)
awardedContracts1920[lucaciAsProvider]
29/227:
# Filter M/S MANDELA NATIONAL STADIUM NAMBOOLE as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('MANDELA', case=False)
awardedContracts1920[lucaciAsProvider]
29/228:
# Filter Capital autotune limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Capital autotune', case=False)
awardedContracts1920[lucaciAsProvider]
29/229:
# Filter Capital autotune limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Capital autotune', case=False)
awardedContracts1920[lucaciAsProvider]
29/230:
# Filter M/s imperial royale hotel as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('imperial royale', case=False)
awardedContracts1920[lucaciAsProvider]
29/231:
# Filter Azu Propoeties Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Azu', case=False)
awardedContracts1920[lucaciAsProvider]
29/232:
# Filter Azu Propoeties Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Azu', case=False)
awardedContracts1920[lucaciAsProvider]
29/233:
# Filter Azu Propoeties Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Azu', case=False)
awardedContracts1920[lucaciAsProvider]
29/234:
# Filter energo projekt niskogradnja as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('energo', case=False)
awardedContracts1920[lucaciAsProvider]
29/235:
# Filter M/S DIGIPRINT SYSTEMS(U)LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('CRANE COMPUTERS', case=False)
awardedContracts1920[lucaciAsProvider]
29/236:
# Filter M/S DIGIPRINT SYSTEMS(U)LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('DIGIPRINT', case=False)
awardedContracts1920[lucaciAsProvider]
29/237:
# Filter UAP OLD Mutual Properties Uganda Limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('UAP OLD Mutual Properties', case=False)
awardedContracts1920[lucaciAsProvider]
29/238:
# Filter M/S JOHNICK SKY SOLUTIONS COMPANY LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('SKY SOLUTIONS', case=False)
awardedContracts1920[lucaciAsProvider]
29/239:
# Filter CRANE COMPUTERS LIMITED as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Palin corporation ltd', case=False)
awardedContracts1920[lucaciAsProvider]
29/240:
# Filter Palin corporation ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Palin corporation ltd', case=False)
awardedContracts1920[lucaciAsProvider]
29/241:
# Filter Mantra Technologies Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Mantra', case=False)
awardedContracts1920[lucaciAsProvider]
29/242:
# Filter Kontiki (u) ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kontiki', case=False)
awardedContracts1920[lucaciAsProvider]
29/243:
# Filter Aristoc as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Aristoc', case=False)
awardedContracts1920[lucaciAsProvider]
29/244:
# Filter Mulstar technical services as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Mulstar', case=False)
awardedContracts1920[lucaciAsProvider]
29/245:
# Filter M/S ROYAL AUTO MOBILES LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('ROYAL', case=False)
awardedContracts1920[lucaciAsProvider]
29/246:
# Filter M/S ROYAL AUTO MOBILES LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('ROYAL AUTO', case=False)
awardedContracts1920[lucaciAsProvider]
29/247:
# Filter Africalook & Tours as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Africalook', case=False)
awardedContracts1920[lucaciAsProvider]
29/248:
# Filter lunko enterprises as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('lunko', case=False)
awardedContracts1920[lucaciAsProvider]
29/249:
# Filter M/s ali sadik as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('ali sadik', case=False)
awardedContracts1920[lucaciAsProvider]
29/250:
# Filter DENIQUE SUPPLIES as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('DENIQUE', case=False)
awardedContracts1920[lucaciAsProvider]
29/251:
# Filter JOHN OTIM as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('JOHN OTIM', case=False)
awardedContracts1920[lucaciAsProvider]
29/252:
# Filter Melville engineering services limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Melville', case=False)
awardedContracts1920[lucaciAsProvider]
29/253:
# Filter Uganda printing and publishing corporation as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Uganda printing and publishing', case=False)
awardedContracts1920[lucaciAsProvider]
29/254:
# Filter CJoh Achelis & Gmbh as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('CRANE COMPUTERS', case=False)
awardedContracts1920[lucaciAsProvider]
29/255:
# Filter Joh Achelis & Gmbh as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Joh Achelis', case=False)
awardedContracts1920[lucaciAsProvider]
29/256:
# Filter Goodman International Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Goodman', case=False)
awardedContracts1920[lucaciAsProvider]
29/257:
# Filter Goodman International Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Good man', case=False)
awardedContracts1920[lucaciAsProvider]
29/258:
# Filter Goodman International Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Goodman', case=False)
awardedContracts1920[lucaciAsProvider]
29/259:
# Filter Nation media group ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Nation media', case=False)
awardedContracts1920[lucaciAsProvider]
29/260:
# Filter Karuri pharmaceuticals ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Karuri', case=False)
awardedContracts1920[lucaciAsProvider]
29/261:
# Filter M/s kazinga channel office world limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('kazinga channel', case=False)
awardedContracts1920[lucaciAsProvider]
29/262:
# Filter Kyopa general merchandise ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kyopa', case=False)
awardedContracts1920[lucaciAsProvider]
29/263:
# Filter Kain group limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kain', case=False)
awardedContracts1920[lucaciAsProvider]
29/264:
# Filter BELLEVUE LOGISTICS LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('BELLEVUE', case=False)
awardedContracts1920[lucaciAsProvider]
29/265:
# Filter M/s edge technologies ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('edge technologies', case=False)
awardedContracts1920[lucaciAsProvider]
29/266:
# Filter Victoria motors as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Victoria motor', case=False)
awardedContracts1920[lucaciAsProvider]
29/267:
# Filter Victoria motors as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Victoria motors', case=False)
awardedContracts1920[lucaciAsProvider]
29/268:
# Filter Mfi document solutions limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Mfi document solutions limited', case=False)
awardedContracts1920[lucaciAsProvider]
29/269:
# Filter M/s n2m company ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('n2m', case=False)
awardedContracts1920[lucaciAsProvider]
29/270:
# Filter M/s extraz industrial graphics co ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('extraz industrial', case=False)
awardedContracts1920[lucaciAsProvider]
29/271:
# Filter Kasese Nail and Wood Industry Limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kasese Nail and Wood', case=False)
awardedContracts1920[lucaciAsProvider]
29/272:
# Filter Josam technologies ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Josam', case=False)
awardedContracts1920[lucaciAsProvider]
29/273:
# Filter Eliveda general merchandise as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Eliveda general', case=False)
awardedContracts1920[lucaciAsProvider]
29/274:
# Filter Kayooza Office Solutions Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kayooza', case=False)
awardedContracts1920[lucaciAsProvider]
29/275:
# Filter GAGO U LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('GAGO', case=False)
awardedContracts1920[lucaciAsProvider]
29/276:
# Filter PUBLIC SPEAKING & LEADERSHIP SOLUTIONS as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('PUBLIC SPEAKING', case=False)
awardedContracts1920[lucaciAsProvider]
29/277:
# Filter Precise diagnostics and medical supplies ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Precise diagnostics', case=False)
awardedContracts1920[lucaciAsProvider]
29/278:
# Filter Cipla Quality Chemicals Industries Limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Cipla', case=False)
awardedContracts1920[lucaciAsProvider]
29/279:
# Filter Magic colours ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Magic colour', case=False)
awardedContracts1920[lucaciAsProvider]
29/280:
# Filter Magic colours ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Magic colours', case=False)
awardedContracts1920[lucaciAsProvider]
29/281:
# Filter Andy Lubega General Traders as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Andy Lubega', case=False)
awardedContracts1920[lucaciAsProvider]
29/282:
# Filter Luminous (U) Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Luminous', case=False)
awardedContracts1920[lucaciAsProvider]
29/283:
# Filter M/s radio one as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('radio one', case=False)
awardedContracts1920[lucaciAsProvider]
29/284:
# Filter M/S CBS RADIO STATION as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('CBS RADIO', case=False)
awardedContracts1920[lucaciAsProvider]
29/285:
# Filter Kimsy meds ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kimsy', case=False)
awardedContracts1920[lucaciAsProvider]
29/286:
# Filter Ecogas (U) Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Ecogas', case=False)
awardedContracts1920[lucaciAsProvider]
29/287:
# Filter de lamba co limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('lamba', case=False)
awardedContracts1920[lucaciAsProvider]
29/288:
# Filter M/S EUREKA PLACE HOTEL&SUITS as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('EUREKA PLACE', case=False)
awardedContracts1920[lucaciAsProvider]
29/289:
# Filter Corporate gifts ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Corporate gifts', case=False)
awardedContracts1920[lucaciAsProvider]
29/290:
# Filter Africa broadcasting (u) limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Africa broadcasting', case=False)
awardedContracts1920[lucaciAsProvider]
29/291:
# Filter Empower consult limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Empower', case=False)
awardedContracts1920[lucaciAsProvider]
29/292:
# Filter M/s nutu investments ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('nutu', case=False)
awardedContracts1920[lucaciAsProvider]
29/293:
# Filter Muddu Agonda Investments as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Muddu', case=False)
awardedContracts1920[lucaciAsProvider]
29/294:
# Filter Beyond Shley & Shanelle Co. Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Beyond Shley', case=False)
awardedContracts1920[lucaciAsProvider]
29/295:
# Filter M/s green and white enterprises as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('green and white enterprises', case=False)
awardedContracts1920[lucaciAsProvider]
29/296:
# Filter Super medic ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Super medic', case=False)
awardedContracts1920[lucaciAsProvider]
29/297:
# Filter Riley media uganda ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Riley media', case=False)
awardedContracts1920[lucaciAsProvider]
29/298:
# Filter E35 ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('E35', case=False)
awardedContracts1920[lucaciAsProvider]
29/299:
# Filter M/S ODOKASH GENERAL ENTERPRISES LIMITED as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('ODOKASH GENERAL', case=False)
awardedContracts1920[lucaciAsProvider]
29/300:
# Filter Delmaw Enterprises Limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Delmaw', case=False)
awardedContracts1920[lucaciAsProvider]
29/301:
# Filter M/s manifesto publications ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('manifesto', case=False)
awardedContracts1920[lucaciAsProvider]
29/302:
# Filter In - line print services ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('in-line', case=False)
awardedContracts1920[lucaciAsProvider]
29/303:
# Filter In - line print services ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('in - line', case=False)
awardedContracts1920[lucaciAsProvider]
29/304:
# Filter In - line print services ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('line print', case=False)
awardedContracts1920[lucaciAsProvider]
29/305:
# Filter Excel construction ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Excel construction', case=False)
awardedContracts1920[lucaciAsProvider]
29/306:
# Filter Measurement Solutions Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Measurement Solution', case=False)
awardedContracts1920[lucaciAsProvider]
29/307:
# Filter Ayden pharmaceuticals ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Ayden', case=False)
awardedContracts1920[lucaciAsProvider]
29/308:
# Filter Niletrac (u) ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Niletrac', case=False)
awardedContracts1920[lucaciAsProvider]
29/309:
# Filter Shreeji Pharmaceuticals Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Shreeji', case=False)
awardedContracts1920[lucaciAsProvider]
29/310:
# Filter M/s labx scientific ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('labx', case=False)
awardedContracts1920[lucaciAsProvider]
29/311:
# Filter Restaurant africa as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Restaurant', case=False)
awardedContracts1920[lucaciAsProvider]
29/312:
# Filter Restaurant africa as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Restaurant africa', case=False)
awardedContracts1920[lucaciAsProvider]
29/313:
# Filter Onward computer solutions as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Onward', case=False)
awardedContracts1920[lucaciAsProvider]
29/314:
# Filter CRANE COMPUTERS LIMITED as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kark technical services ltd', case=False)
awardedContracts1920[lucaciAsProvider]
29/315:
# Filter Kark technical services ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kark technical services ltd', case=False)
awardedContracts1920[lucaciAsProvider]
29/316:
# Filter Code Access Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Code Access', case=False)
awardedContracts1920[lucaciAsProvider]
29/317:
# Filter January Bamanzi as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('January Bamanzi', case=False)
awardedContracts1920[lucaciAsProvider]
29/318:
# Filter M/s top best cleaning services ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('top best cleaning services', case=False)
awardedContracts1920[lucaciAsProvider]
29/319:
# Filter M/S PRIZED HOLDINGS LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('PRIZED HOLDINGS', case=False)
awardedContracts1920[lucaciAsProvider]
29/320:
# Filter Fire and safety appliances limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Fire and safety', case=False)
awardedContracts1920[lucaciAsProvider]
29/321:
# Filter Fire and safety appliances limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Fire and safety', case=False)
awardedContracts1920[lucaciAsProvider]
29/322:
# Filter Fire and safety appliances limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Fire and safety', case=False)
awardedContracts1920[lucaciAsProvider]
29/323:
# Filter M/s kkn enterprises ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('kkn enterprises', case=False)
awardedContracts1920[lucaciAsProvider]
29/324:
# Filter Shajapa technical works ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Shajapa technical', case=False)
awardedContracts1920[lucaciAsProvider]
29/325:
# Filter M/S AFRICAN BOMA LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('AFRICAN BOMA', case=False)
awardedContracts1920[lucaciAsProvider]
29/326:
# Filter Eurasia Business Systems U Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Eurasia', case=False)
awardedContracts1920[lucaciAsProvider]
29/327:
# Filter M/S KEMTEC LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('KEMTEC', case=False)
awardedContracts1920[lucaciAsProvider]
29/328:
# Filter Reed fields catering services ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Reed field', case=False)
awardedContracts1920[lucaciAsProvider]
29/329:
# Filter ALPHA OFFICE SOLUTIONS ENTERPRISES LTD as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('ALPHA OFFICE', case=False)
awardedContracts1920[lucaciAsProvider]
29/330:
# Filter Africell uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Africell', case=False)
awardedContracts1920[lucaciAsProvider]
29/331:
# Filter Articulate services ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Articulate services ltd', case=False)
awardedContracts1920[lucaciAsProvider]
29/332:
# Filter Velvet pharma ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Velvet', case=False)
awardedContracts1920[lucaciAsProvider]
29/333:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('Japan', case=False)
awardedContracts1920[japanAsProvider]
29/334:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('Japan', case=False)
japan = awardedContracts1920[japanAsProvider]
japan.to_excel('providers/japan.xlsx')
29/335:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('Japan', case=False)
japan = awardedContracts1920[japanAsProvider]
japan.to_excel('providers/japan.xlsx')
29/336:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('Japan', case=False)
japan = awardedContracts1920[japanAsProvider]
japan.to_excel('japan.xlsx')
29/337:
# Filter out Japan as provider
japanAsProvider = awardedContracts1920['Provider'].str.contains('Japan', case=False)
japan = awardedContracts1920[japanAsProvider]
japan.to_excel('providers/japan.xlsx')
29/338:
# Filter out Simba as provider
simbaAsProvider = awardedContracts1920['Provider'].str.contains('Simba')
simba = awardedContracts1920[simbaAsProvider]
japan.to_excel('providers/simba.xlsx')
29/339:
# Filter out Simba as provider
simbaAsProvider = awardedContracts1920['Provider'].str.contains('Simba')
simba = awardedContracts1920[simbaAsProvider]
simba.to_excel('providers/simba.xlsx')
29/340:
# Filter out Katwaalo as provider
katwaloAsProvider = awardedContracts1920['Provider'].str.contains('Katwaalo')
katwaalo = awardedContracts1920[katwaloAsProvider]
katwaalo.to_excel('providers/katwaalo.xlsx')
29/341:
# Filter out mac east africa ltd as provider
macEastAfricaAsProvider = awardedContracts1920['Provider'].str.contains('east africa ltd')
mac = awardedContracts1920[macEastAfricaAsProvider]
mac.to_excel('providers/mac.xlsx')
29/342:
# Filter out pearl entertainment as provider
pearlEntertainmentAsProvider = awardedContracts1920['Provider'].str.contains('Pearl entertainment limited')
peal = awardedContracts1920[pearlEntertainmentAsProvider]
pearl.to_excel('providers/pearl.xlsx')
29/343:
# Filter out pearl entertainment as provider
pearlEntertainmentAsProvider = awardedContracts1920['Provider'].str.contains('Pearl entertainment limited')
pearl = awardedContracts1920[pearlEntertainmentAsProvider]
pearl.to_excel('providers/pearl.xlsx')
29/344:
# Filter out city tyres as Provider
cityTyresAsProvider = awardedContracts1920['Provider'].str.contains('city tyres')
city_tyres = awardedContracts1920[cityTyresAsProvider]
city_tyres.to_excel('providers/city_tyres.xlsx')
29/345:
# Filter out summer auto services as provider
summerAutoServicesAsProvider = awardedContracts1920['Provider'].str.contains('summer auto services')
summer_auto = awardedContracts1920[summerAutoServicesAsProvider]
summer_auto.to_excel('providers/summer_auto.xlsx')
29/346:
# Filter out carolrene services as provider
carolreneServicesAsProvider = awardedContracts1920['Provider'].str.contains('carolrene services')
carolrene = awardedContracts1920[carolreneServicesAsProvider]
carolrene.to_excel('providers/carolrene.xlsx')
29/347:
# Filter out Daks Toyota
daksToyotaAsProvider = awardedContracts1920['Provider'].str.contains('Daks toyota')
dak_toyota = awardedContracts1920[daksToyotaAsProvider]
dak_toyota.to_excel('providers/dak_toyota.xlsx')
29/348:
# Filter out Lina construction as provider
linaAsProvider = awardedContracts1920['Provider'].str.contains('Lina construction')
lina_construction = awardedContracts1920[linaAsProvider]
lina_construction.to_excel('providers/lina_construction.xlsx')
29/349:
# Filter out new vision as Provider
newVisionAsProvider = awardedContracts1920['Provider'].str.contains('new vision')
new_vision = awardedContracts1920[newVisionAsProvider]
new_vision.to_excel('providers/new_vision.xlsx')
29/350:
# Filter ridar hotel as provider
ridarHotelAsProvider = awardedContracts1920['Provider'].str.contains('ridar hotel')
ridah = awardedContracts1920[ridarHotelAsProvider]
ridah.to_excel('providers/ridah.xlsx')
29/351:
# Filter Tendo catering as Provider
tendoCateringAsProvider = awardedContracts1920['Provider'].str.contains('Tendo catering')
tendo = awardedContracts1920[tendoCateringAsProvider]
tendo.to_excel('providers/tendo.xlsx')
29/352:
# Filter Gittoes Pharmaceuticals as Provider
gittoesAsProvider = awardedContracts1920['Provider'].str.contains('Gittoes Pharmaceuticals')
gittoes = awardedContracts1920[gittoesAsProvider]
gittoes.to_excel('providers/gittoes.xlsx')
29/353:
# Filter KANGAROO (U) Ltd as Provider
kangarooAsProvider = awardedContracts1920['Provider'].str.contains('KANGAROO')
kangaroo = awardedContracts1920[kangarooAsProvider]
kangaroo.to_excel('providers/kangaroo.xlsx')
29/354:
# Filter Grace lubega motors 2000 ltd as Provider
graceLubegaAsProvider = awardedContracts1920['Provider'].str.contains('Grace lubega', case=False)
graceL = awardedContracts1920[graceLubegaAsProvider]
graceL.to_excel('providers/grace.xlsx')
29/355:
# Filter M/S ARROW CENTRE(U)LTD as Provider
arrowCentreAsProvider = awardedContracts1920['Provider'].str.contains('ARROW', case=False)
arrow = awardedContracts1920[arrowCentreAsProvider]
arrow.to_excel('providers/arrow.xlsx')
29/356:
# Filter Veve and Sons Ltd as Provider
veveAsProvider = awardedContracts1920['Provider'].str.contains('Veve and Sons')
veve = awardedContracts1920[veveAsProvider]
veve.to_excel('providers/veve.xlsx')
29/357:
# Filter Mans Plastics as Provider
mansPlasticAsProvider = awardedContracts1920['Provider'].str.contains('Mans Plastics')
mans_plastic = awardedContracts1920[mansPlasticAsProvider]
mans_plastic.to_excel('providers/mans_plastic.xlsx')
29/358:
# Filter chemix & tech limited as Provider
chemixAsProvider = awardedContracts1920['Provider'].str.contains('chemix & tech')
chemix = awardedContracts1920[chemixAsProvider]
chemix.to_excel('providers/chemix.xlsx')
29/359:
# Filter Toyota uganda limited as Provider
toyotaUgandaAsProvider = awardedContracts1920['Provider'].str.contains('Toyota')
toyota = awardedContracts1920[toyotaUgandaAsProvider]
toyota.to_excel('providers/toyota.xlsx')
29/360:
# Filter Monitor publications ltd as Provider
monitorAsProvider = awardedContracts1920['Provider'].str.contains('monitor', case=False)
monitor = awardedContracts1920[monitorAsProvider]
monitor.to_excel('providers/monitor.xlsx')
29/361:
# Filter The cooper motor corporation (u) ltd as Provider
cooperAsProvider = awardedContracts1920['Provider'].str.contains('cooper motor', case=False)
awardedContracts1920[cooperAsProvider]
29/362:
# Filter The cooper motor corporation (u) ltd as Provider
cooperAsProvider = awardedContracts1920['Provider'].str.contains('cooper motor', case=False)
cooper_moto = awardedContracts1920[cooperAsProvider]
cooper_moto.to_excel('providers/cooper_moto.xlsx')
29/363:
# Filter Singh motor garage ltd as Provider
singhAsProvider = awardedContracts1920['Provider'].str.contains('Singh motor', case=False)
singh_moto = awardedContracts1920[singhAsProvider]
singh_moto.to_excel('providers/singh_moto.xlsx')
29/364:
# Filter Astra pharma (u) ltd
astraAsProvider = awardedContracts1920['Provider'].str.contains('Astra', case=False)
astra = awardedContracts1920[astraAsProvider]
astra.to_excel('providers/astra.xlsx')
29/365:
# Filter T&B General Enterprises LTD as Provider
TBGeneralAsProvider = awardedContracts1920['Provider'].str.contains('T&B General', case=False)
tandB = awardedContracts1920[TBGeneralAsProvider]
tandB.to_excel('providers/tandB.xlsx')
29/366:
# Filter Silverbacks Pharmacy Ltd as Provider
SilverbacksAsProvider = awardedContracts1920['Provider'].str.contains('Silverbacks', case=False)
silverbacks = awardedContracts1920[SilverbacksAsProvider]
silverbacks.to_excel('providers/silverbacks.xlsx')
29/367:
# Filter M/S SKANZ ENGINEERING SERVICES LTD as Provider
skanzAsProvider = awardedContracts1920['Provider'].str.contains('SKANZ', case=False)
skanz = awardedContracts1920[skanzAsProvider]
skanz.to_excel('providers/skanz.xlsx')
29/368:
# Filter Sheraton kampala hotel as provider
sheratonAsProvider = awardedContracts1920['Provider'].str.contains('Sheraton', case=False)
sheraton = awardedContracts1920[sheratonAsProvider]
sheraton.to_excel('providers/sheraton.xlsx')
29/369:
# Filter Luna group of companies as provider
lunaAsProvider = awardedContracts1920['Provider'].str.contains('Luna group', case=False)
luna = awardedContracts1920[lunaAsProvider]
luna.to_excel('providers/luna.xlsx')
29/370:
# Filter M/s Nunu Ventures Ltd as provider
nunuAsProvider = awardedContracts1920['Provider'].str.contains('Nunu', case=False)
nunu = awardedContracts1920[nunuAsProvider]
nunu.to_excel('providers/nunu.xlsx')
29/371:
# Filter Rodena Printers & Stationers as provider
rodenaPrintersAsProvider = awardedContracts1920['Provider'].str.contains('Rodena', case=False)
rodena = awardedContracts1920[rodenaPrintersAsProvider]
rodena.to_excel('providers/rodena.xlsx')
29/372:
# Filter Balton (u) ltd as Provider
baltonAsProvider = awardedContracts1920['Provider'].str.contains('Balton', case=False)
balton = awardedContracts1920[baltonAsProvider]
balton.to_excel('providers/balton.xlsx')
29/373:
# Filter Maka motor works ltd as provider
makaMotorAsProvider = awardedContracts1920['Provider'].str.contains('Maka motor', case=False)
maka_moto = awardedContracts1920[makaMotorAsProvider]
maka_moto.to_excel('providers/maka_moto.xlsx')
29/374:
# Filter Lucaci Supplies Limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('lucaci', case=False)
licaci = awardedContracts1920[lucaciAsProvider]
licaci.to_excel('providers/licaci.xlsx')
29/375:
# Filter M/s imperial golf view hotel as provider
imperialAsProvider = awardedContracts1920['Provider'].str.contains('imperial golf', case=False)
imperial_golf = awardedContracts1920[imperialAsProvider]
imperial_golf.to_excel('providers/imperial_golf.xlsx')
29/376:
# Filter Abacus pharma (a) ltd as provider
abacusAsProvider = awardedContracts1920['Provider'].str.contains('abacus', case=False)
abacus = awardedContracts1920[abacusAsProvider]
abacus.to_excel('providers/abacus.xlsx')
29/377:
# Filter Janine services limited as provider
janineAsProvider = awardedContracts1920['Provider'].str.contains('janine', case=False)
janine = awardedContracts1920[janineAsProvider]
janine.to_excel('providers/janine.xlsx')
29/378:
# Filter M/S ROYAL AUTOMOBILES LTD as provider
royalAutomobilesAsProvider = awardedContracts1920['Provider'].str.contains('ROYAL AUTO', case=False)
royal_auto = awardedContracts1920[royalAutomobilesAsProvider]
royal_auto.to_excel('providers/royal_auto.xlsx')
29/379:
# Filter Resteve international limited as provider
resteveInternationalAsProvider = awardedContracts1920['Provider'].str.contains('Resteve international', case=False)
resteve = awardedContracts1920[resteveInternationalAsProvider]
resteve.to_excel('providers/resteve.xlsx')
29/380:
# Filter Skyline Holdings Ltd as provider
skylineAsProvider = awardedContracts1920['Provider'].str.contains('Skyline', case=False)
skyline = awardedContracts1920[skylineAsProvider]
skyline.to_excel('providers/resteve.xlsx')
29/381:
# Filter Resteve international limited as provider
resteveInternationalAsProvider = awardedContracts1920['Provider'].str.contains('Resteve international', case=False)
resteve = awardedContracts1920[resteveInternationalAsProvider]
resteve.to_excel('providers/resteve.xlsx')
29/382:
# Filter Skyline Holdings Ltd as provider
skylineAsProvider = awardedContracts1920['Provider'].str.contains('Skyline', case=False)
skyline = awardedContracts1920[skylineAsProvider]
skyline.to_excel('providers/skyline.xlsx')
29/383:
# Filter actros east africa limited as provider
actrosAsProvider = awardedContracts1920['Provider'].str.contains('actros', case=False)
actros = awardedContracts1920[actrosAsProvider]
actros.to_excel('providers/actros.xlsx')
29/384:
# Filter Castilleo traders limited  as provider
castilleoAsProvider = awardedContracts1920['Provider'].str.contains('Castilleo', case=False)
castilleo = awardedContracts1920[castilleoAsProvider]
castilleo.to_excel('providers/castilleo.xlsx')
29/385:
# Filter Castilleo traders limited  as provider
castilleoAsProvider = awardedContracts1920['Provider'].str.contains('Castilleo', case=False)
castilleo = awardedContracts1920[castilleoAsProvider]
castilleo.to_excel('providers/castilleo.xlsx')
29/386:
# Filter Kijoh Consulting and Marketing Limited as provider
KijohAsProvider = awardedContracts1920['Provider'].str.contains('Kijoh', case=False)
kijoh = awardedContracts1920[KijohAsProvider]
kijoh.to_excel('providers/kijoh.xlsx')
29/387:
# Filter penina restaurant as provider
peninaAsProvider = awardedContracts1920['Provider'].str.contains('penina', case=False)
penina = awardedContracts1920[peninaAsProvider]
penina.to_excel('providers/penina.xlsx')
29/388:
# Filter Joint medical stores as provider
jointAsProvider = awardedContracts1920['Provider'].str.contains('Joint medical', case=False)
joint_medical = awardedContracts1920[jointAsProvider]
joint_medical.to_excel('providers/joint_medical.xlsx')
29/389:
# Filter Baseline Suppliers ltd as provider
baselineAsProvider = awardedContracts1920['Provider'].str.contains('Baseline', case=False)
baseline = awardedContracts1920[baselineAsProvider]
baseline.to_excel('providers/baseline.xlsx')
29/390:
# Filter M/s almid clean services ltd as provider
almidAsProvider = awardedContracts1920['Provider'].str.contains('almid clean', case=False)
almid = awardedContracts1920[almidAsProvider]
almid.to_excel('providers/almid.xlsx')
29/391:
# Filter Phillips Pharmaceuticals (U) Ltd as provider
PhillipsPharmaceuticalsAsProvider = awardedContracts1920['Provider'].str.contains('Phillips Pharmaceuticals', case=False)
phillips = awardedContracts1920[PhillipsPharmaceuticalsAsProvider]
phillips.to_excel('providers/phillips.xlsx')
29/392:
# Filter Viga pharmaceuticals ltd as provider
vigaAsProvider = awardedContracts1920['Provider'].str.contains('viga', case=False)
viga = awardedContracts1920[vigaAsProvider]
viga.to_excel('providers/viga.xlsx')
29/393:
# Filter lunko enterprises limited as provider
lunkoAsProvider = awardedContracts1920['Provider'].str.contains('lunko', case=False)
lunko = awardedContracts1920[lunkoAsProvider]
lunko.to_excel('providers/lunko.xlsx')
29/394:
# Filter M/s prime impex 2001 ltd as provider
primeImpexAsProvider = awardedContracts1920['Provider'].str.contains('prime impex', case=False)
prime_impex = awardedContracts1920[primeImpexAsProvider]
prime_impex.to_excel('providers/prime_impex.xlsx')
29/395:
# Filter Lithocraft investments ltd as provider
LithocraftAsProvider = awardedContracts1920['Provider'].str.contains('Lithocraft', case=False)
litho_craft = awardedContracts1920[LithocraftAsProvider]
litho_craft.to_excel('providers/litho_craft.xlsx')
29/396:
# Filter Prime general supply ltd as provider
primeGeneralAsProvider = awardedContracts1920['Provider'].str.contains('prime general', case=False)
prime_general = awardedContracts1920[primeGeneralAsProvider]
prime_general.to_excel('providers/prime_general.xlsx')
29/397:
# Filter DHL as provider
dhlAsProvider = awardedContracts1920['Provider'].str.contains('DHL', case=False)
dhl = awardedContracts1920[dhlAsProvider]
dhl.to_excel('providers/dhl.xlsx')
29/398:
# Filter Mid world enterprises ltd as provider
midAsProvider = awardedContracts1920['Provider'].str.contains('Mid world', case=False)
mid_world = awardedContracts1920[midAsProvider]
mid_world.to_excel('providers/mid_world.xlsx')
29/399:
# Filter Reev consult international as provider
reevAsProvider = awardedContracts1920['Provider'].str.contains('Reev', case=False)
reev = awardedContracts1920[reevAsProvider]
reev.to_excel('providers/reev.xlsx')
29/400:
# Filter Global scientific supplies ltd as provider
globalAsProvider = awardedContracts1920['Provider'].str.contains('Global scientific', case=False)
global_scientific = awardedContracts1920[globalAsProvider]
global_scientific.to_excel('providers/global_scientific.xlsx')
29/401:
# Filter Critical care solutions as provider
criticalCareAsProvider = awardedContracts1920['Provider'].str.contains('Critical care', case=False)
critical_care = awardedContracts1920[criticalCareAsProvider]
critical_care.to_excel('providers/critical_care.xlsx')
29/402:
# Filter Global scientific supplies ltd as provider
globalAsProvider = awardedContracts1920['Provider'].str.contains('Global scientific', case=False)
global_scientific = awardedContracts1920[globalAsProvider]
global_scientific.to_excel('providers/global_scientific.xlsx')
29/403:
# Filter Colas East Africa Limited as provider
colasAsProvider = awardedContracts1920['Provider'].str.contains('colas', case=False)
colas = awardedContracts1920[colasAsProvider]
colas.to_excel('providers/colas.xlsx')
29/404:
# Filter Kiy auto repairs as provider
kiyAsProvider = awardedContracts1920['Provider'].str.contains('kiy', case=False)
kiy = awardedContracts1920[kiyAsProvider]
kiy.to_excel('providers/kiy.xlsx')
29/405:
# Filter KIBAO INVESTMENTS CO. LTD as provider
kibaoAsProvider = awardedContracts1920['Provider'].str.contains('kibao', case=False)
kibao = awardedContracts1920[kibaoAsProvider]
kibao.to_excel('providers/kibao.xlsx')
29/406:
# Filter JBN Consults & Planners Ltd as provider
jbnAsProvider = awardedContracts1920['Provider'].str.contains('JBN Consults', case=False)
jbn = awardedContracts1920[jbnAsProvider]
jbn.to_excel('providers/jbn.xlsx')
29/407:
# Filter Sobetra Uganda Limited as provider
SobetraAsProvider = awardedContracts1920['Provider'].str.contains('Sobetra', case=False)
sobetra = awardedContracts1920[SobetraAsProvider]
sobetra.to_excel('providers/sobetra.xlsx')
29/408:
# Filter Ksb office and general supplies limited as provider
ksbAsProvider = awardedContracts1920['Provider'].str.contains('Ksb', case=False)
ksb = awardedContracts1920[ksbAsProvider]
ksb.to_excel('providers/ksb.xlsx')
29/409:
# Filter M/s Lea International Ltd as provider
LeaAsProvider = awardedContracts1920['Provider'].str.contains('Lea international', case=False)
lea = awardedContracts1920[LeaAsProvider]
lea.to_excel('providers/lea.xlsx')
29/410:
# Filter M/S BURNS ENGINEERING&GENERAL SUPPLY as provider
burnsAsProvider = awardedContracts1920['Provider'].str.contains('burns', case=False)
burns = awardedContracts1920[burnsAsProvider]
burns.to_excel('providers/burns.xlsx')
29/411:
# Filter M/S JAMUSHINE INTERNATIONALas provider
jamushineAsProvider = awardedContracts1920['Provider'].str.contains('JAMUSHINE', case=False)
jamushine = awardedContracts1920[jamushineAsProvider]
jamushine.to_excel('providers/jamushine.xlsx')
29/412:
# Filter Riley media as provider
RileyAsProvider = awardedContracts1920['Provider'].str.contains('Riley', case=False)
riley = awardedContracts1920[RileyAsProvider]
riley.to_excel('providers/riley.xlsx')
29/413:
# Filter Makdeus General Enterprises as provider
MakdeusAsProvider = awardedContracts1920['Provider'].str.contains('Makdeus', case=False)
markdeus = awardedContracts1920[MakdeusAsProvider]
markdeus.to_excel('providers/mark_deus.xlsx')
29/414:
# Filter Riley media as provider
RileyAsProvider = awardedContracts1920['Provider'].str.contains('Riley', case=False)
riley = awardedContracts1920[RileyAsProvider]
riley.to_excel('providers/riley.xlsx')
29/415:
# Filter M/S JAMUSHINE INTERNATIONALas provider
jamushineAsProvider = awardedContracts1920['Provider'].str.contains('JAMUSHINE', case=False)
jamushine = awardedContracts1920[jamushineAsProvider]
jamushine.to_excel('providers/jamushine.xlsx')
29/416:
# Filter Centric solutions (u) ltd as provider
centricAsProvider = awardedContracts1920['Provider'].str.contains('Centric', case=False)
centric = awardedContracts1920[centricAsProvider]
centric.to_excel('providers/centric.xlsx')
29/417:
# Filter Abubaker technical services ltd as provider
AbubakerAsProvider = awardedContracts1920['Provider'].str.contains('Abubaker', case=False)
abubaker = awardedContracts1920[AbubakerAsProvider]
abubaker.to_excel('providers/abubaker.xlsx')
29/418:
# Filter Centric solutions (u) ltd as provider
centricAsProvider = awardedContracts1920['Provider'].str.contains('Centric', case=False)
centric = awardedContracts1920[centricAsProvider]
centric.to_excel('providers/centric.xlsx')
29/419:
# Filter City Aluminium & Glass services ltd as provider
CityAluminiumAsProvider = awardedContracts1920['Provider'].str.contains('City Aluminium', case=False)
city_alluminium = awardedContracts1920[CityAluminiumAsProvider]
city_alluminium.to_excel('providers/city_alluminium.xlsx')
29/420:
# Filter Rafah Communications (U) Limited as provider
RafahAsProvider = awardedContracts1920['Provider'].str.contains('Rafah', case=False)
rafah = awardedContracts1920[RafahAsProvider]
rafah.to_excel('providers/rafah.xlsx')
29/421:
# Filter Rafah Communications (U) Limited as provider
RafahAsProvider = awardedContracts1920['Provider'].str.contains('Rafah', case=False)
rafah = awardedContracts1920[RafahAsProvider]
rafah.to_excel('providers/rafah.xlsx')
29/422:
# Filter M.M INTEGRATED STEEL MILLS (U) LTD as provider
mmIntegratedAsProvider = awardedContracts1920['Provider'].str.contains('M.M INTEGRATED', case=False)
mm = awardedContracts1920[mmIntegratedAsProvider]
mm.to_excel('providers/mm.xlsx')
29/423:
# Filter M/S JUCO COMPANY LOGISTICS as provider
jucoAsProvider = awardedContracts1920['Provider'].str.contains('juco', case=False)
juco = awardedContracts1920[jucoAsProvider]
juco.to_excel('providers/juco.xlsx')
29/424:
# Filter M/S DR FORTUNATE MUYAMBI as provider
drAsProvider = awardedContracts1920['Provider'].str.contains('dr fortunate', case=False)
dr_fortunate = awardedContracts1920[drAsProvider]
dr_fortunate.to_excel('providers/dr_fortunate.xlsx')
29/425:
# Filter INSPIRE KIDZ EVENTS HEALTH CLUB LTD as provider
inspireAsProvider = awardedContracts1920['Provider'].str.contains('inspire', case=False)
inspire = awardedContracts1920[inspireAsProvider]
inspire.to_excel('providers/inspire.xlsx')
29/426:
# Filter Ttb investments ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('b investments', case=False)
tb_investment = awardedContracts1920[lucaciAsProvider]
tb_investment.to_excel('providers/tb_investment.xlsx')
29/427:
# Filter INSPIRE KIDZ EVENTS HEALTH CLUB LTD as provider
inspireAsProvider = awardedContracts1920['Provider'].str.contains('inspire', case=False)
inspire = awardedContracts1920[inspireAsProvider]
inspire.to_excel('providers/inspire.xlsx')
29/428:
# Filter taste and tasty restaurant as provider
tasteAsProvider = awardedContracts1920['Provider'].str.contains('taste', case=False)
taste = awardedContracts1920[tasteAsProvider]
taste.to_excel('providers/taste.xlsx')
29/429:
# Filter Stirling civil engineering ltd as provider
StirlingAsProvider = awardedContracts1920['Provider'].str.contains('Stirling', case=False)
stirling = awardedContracts1920[StirlingAsProvider]
stirling.to_excel('providers/stirling.xlsx')
29/430:
# Filter Jogoo 93.8 fm ltd as provider
jogooAsProvider = awardedContracts1920['Provider'].str.contains('jogoo', case=False)
jogoo = awardedContracts1920[jogooAsProvider]
jogoo.to_excel('providers/jogoo.xlsx')
29/431:
# Filter A Plus Funeral Management Ltd as provider
aplusAsProvider = awardedContracts1920['Provider'].str.contains('A plus', case=False)
a_plus = awardedContracts1920[aplusAsProvider]
a_plus.to_excel('providers/a_plus.xlsx')
29/432:
# Filter M/s Frema Trading Ltd as provider
fremaAsProvider = awardedContracts1920['Provider'].str.contains('Frema', case=False)
frema = awardedContracts1920[fremaAsProvider]
frema.to_excel('providers/frema.xlsx')
29/433:
# Filter M/s eurasia business systems uganda ltd as provider
eurasiaAsProvider = awardedContracts1920['Provider'].str.contains('eurasia', case=False)
eurasia = awardedContracts1920[eurasiaAsProvider]
eurasia.to_excel('providers/eurasia.xlsx')
29/434:
# Filter M/S BUKEDDE TV as provider
BUKEDDEAsProvider = awardedContracts1920['Provider'].str.contains('BUKEDDE', case=False)
bukedde = awardedContracts1920[BUKEDDEAsProvider]
bukedde.to_excel('providers/bukedde.xlsx')
29/435:
# Filter Sybyl limited as provider
SybylAsProvider = awardedContracts1920['Provider'].str.contains('Sybyl', case=False)
sybl = awardedContracts1920[SybylAsProvider]
sybl.to_excel('providers/sybl.xlsx')
29/436:
# Filter Joy for Children Uganda as provider
joyAsProvider = awardedContracts1920['Provider'].str.contains('joy', case=False)
joy = awardedContracts1920[joyAsProvider]
joy.to_excel('providers/joy.xlsx')
29/437:
# Filter M/S RICH-DAD AGRO CONSULTS&BUYERS LTD as provider
richAsProvider = awardedContracts1920['Provider'].str.contains('Rich', case=False)
rich = awardedContracts1920[richAsProvider]
rich.to_excel('providers/rich.xlsx')
29/438:
# Filter M/S GRAPHIC SYSTEMS (U)LTD as provider
graphicAsProvider = awardedContracts1920['Provider'].str.contains('graphic systems', case=False)
graphic_systems = awardedContracts1920[graphicAsProvider]
graphic_systems.to_excel('providers/graphic_systems.xlsx')
29/439:
# Filter WEBNET TECHNOLOGIES as provider
webNetAsProvider = awardedContracts1920['Provider'].str.contains('webnet', case=False)
webnet = awardedContracts1920[webNetAsProvider]
webnet.to_excel('providers/webnet.xlsx')
29/440:
# Filter Buscom international ltd as provider
BuscomAsProvider = awardedContracts1920['Provider'].str.contains('Buscom', case=False)
buscom = awardedContracts1920[BuscomAsProvider]
buscom.to_excel('providers/buscom.xlsx')
29/441:
# Filter WEBNET TECHNOLOGIES as provider
webNetAsProvider = awardedContracts1920['Provider'].str.contains('webnet', case=False)
webnet = awardedContracts1920[webNetAsProvider]
webnet.to_excel('providers/webnet.xlsx')
29/442:
# Filter M/S GRAPHIC SYSTEMS (U)LTD as provider
graphicAsProvider = awardedContracts1920['Provider'].str.contains('graphic systems', case=False)
graphic_systems = awardedContracts1920[graphicAsProvider]
graphic_systems.to_excel('providers/graphic_systems.xlsx')
29/443:
# Filter M/S RICH-DAD AGRO CONSULTS&BUYERS LTD as provider
richAsProvider = awardedContracts1920['Provider'].str.contains('Rich', case=False)
rich = awardedContracts1920[richAsProvider]
rich.to_excel('providers/rich.xlsx')
29/444:
# Filter Joy for Children Uganda as provider
joyAsProvider = awardedContracts1920['Provider'].str.contains('joy', case=False)
joy = awardedContracts1920[joyAsProvider]
joy.to_excel('providers/joy.xlsx')
29/445:
# Filter Sybyl limited as provider
SybylAsProvider = awardedContracts1920['Provider'].str.contains('Sybyl', case=False)
sybl = awardedContracts1920[SybylAsProvider]
sybl.to_excel('providers/sybl.xlsx')
29/446:
# Filter M/S BUKEDDE TV as provider
BUKEDDEAsProvider = awardedContracts1920['Provider'].str.contains('BUKEDDE', case=False)
bukedde = awardedContracts1920[BUKEDDEAsProvider]
bukedde.to_excel('providers/bukedde.xlsx')
29/447:
# Filter M/S FOYER FIVE LTD as provider
foyerAsProvider = awardedContracts1920['Provider'].str.contains('FOYER', case=False)
foyer = awardedContracts1920[foyerAsProvider]
foyer.to_excel('providers/foyer.xlsx')
29/448:
# Filter Oscar general hardware ltd as provider
OscarAsProvider = awardedContracts1920['Provider'].str.contains('Oscar', case=False)
oscar = awardedContracts1920[OscarAsProvider]
oscar.to_excel('providers/oscar.xlsx')
29/449:
# Filter M2SYS INC. as provider
M2SYSAsProvider = awardedContracts1920['Provider'].str.contains('M2SYS', case=False)
m2sys = awardedContracts1920[M2SYSAsProvider]
m2sys.to_excel('providers/m2sys.xlsx')
29/450:
# Filter M/S KEMEN UGANDA LTD as provider
kemenAsProvider = awardedContracts1920['Provider'].str.contains('kemen', case=False)
kemen = awardedContracts1920[kemenAsProvider]
kemen.to_excel('providers/kemen.xlsx')
29/451:
# Filter China civil engineering construction corporation as provider
chinaCivilAsProvider = awardedContracts1920['Provider'].str.contains('China Civil', case=False)
china_civil = awardedContracts1920[chinaCivilAsProvider]
china_civil.to_excel('providers/china_civil.xlsx')
29/452:
# Filter Nashidu traders as provider
NashiduAsProvider = awardedContracts1920['Provider'].str.contains('Nashidu', case=False)
nashidu = awardedContracts1920[NashiduAsProvider]
nashidu.to_excel('providers/nashidu.xlsx')
29/453:
# Filter Blu Flammimgo Limited as provider
BluAsProvider = awardedContracts1920['Provider'].str.contains('Blu', case=False)
blu = awardedContracts1920[BluAsProvider]
blu.to_excel('providers/blu.xlsx')
29/454:
# Filter Crane Crafts & Engravers Ltd as provider
craneCraftAsProvider = awardedContracts1920['Provider'].str.contains('Crane craft', case=False)
crane_craft = awardedContracts1920[craneCraftAsProvider]
crane_craft.to_excel('providers/crane_craft.xlsx')
29/455:
# Filter Zescom technologies ltd as provider
ZescomAsProvider = awardedContracts1920['Provider'].str.contains('Zescom', case=False)
zescom = awardedContracts1920[ZescomAsProvider]
zescom.to_excel('providers/zescom.xlsx')
29/456:
# Filter Crane Crafts & Engravers Ltd as provider
craneCraftAsProvider = awardedContracts1920['Provider'].str.contains('Crane craft', case=False)
crane_craft = awardedContracts1920[craneCraftAsProvider]
crane_craft.to_excel('providers/crane_craft.xlsx')
29/457:
# Filter M/S KENYA VETERINARY VACCINES PRODUCTION INSTITUTEas provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('KENYA VETERINARY', case=False)
kenya = awardedContracts1920[lucaciAsProvider]
kenya.to_excel('providers/kenya.xlsx')
29/458:
# Filter Junaco U Limited as provider
JunacoAsProvider = awardedContracts1920['Provider'].str.contains('Junaco', case=False)
junaco = awardedContracts1920[JunacoAsProvider]
junaco.to_excel('providers/junaco.xlsx')
29/459:
# Filter Associated business machinery as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Associated', case=False)
associated = awardedContracts1920[lucaciAsProvider]
associated.to_excel('providers/associated.xlsx')
29/460:
# Filter Lloyds Pharmacy Ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Lloyds', case=False)
lloyds = awardedContracts1920[lucaciAsProvider]
lloyds.to_excel('providers/lloyds.xlsx')
29/461: %history -g
32/1:
import pandas as pd
awardedContracts = pd.read('awarded-contracts-2019-2020-groupBy-type.xlsx')
awardedContracts.head()
32/2:
import pandas as pd
awardedContracts = pd.read_excel('awarded-contracts-2019-2020-groupBy-type.xlsx')
awardedContracts.head()
32/3:
import pandas as pd
awardedContracts = pd.read_excel('Contracts-2019-2020-groupBy-type.xlsx')
awardedContracts.head()
32/4:
import pandas as pd
awardedContracts = pd.read_excel('Contracts-2019-2020-groupBy-type.xlsx')
awardedContracts.head()
awardedContract['subject of procurement']
32/5:
import pandas as pd
awardedContracts = pd.read_excel('Contracts-2019-2020-groupBy-type.xlsx')
awardedContracts.head()
awardedContracts['subject of procurement']
32/6:
import pandas as pd
awardedContracts = pd.read_excel('Contracts-2019-2020-groupBy-type.xlsx')
awardedContracts.head()
awardedContracts['Subject of procurement']
32/7:
import pandas as pd
awardedContracts = pd.read_excel('Contracts-2019-2020-groupBy-type.xlsx')
awardedContracts.head()
32/8:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    contract_amount = pd.namedAgg(column='amount', aggfunc="sum")
)

awardedContractsBytype
32/9:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    contract_amount = pd.NamedAgg(column='amount', aggfunc="sum")
)

awardedContractsBytype
32/10:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    'contract amount' = pd.NamedAgg(column='amount', aggfunc="sum")
)

awardedContractsBytype
32/11:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    contract amount = pd.NamedAgg(column='amount', aggfunc="sum")
)

awardedContractsBytype
32/12:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    contract_amount = pd.NamedAgg(column='amount', aggfunc="sum")
)

awardedContractsBytype
32/13:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type')

awardedContractsBytype
32/14:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    
    contract_count = pd.NamedAgg(column='subject_of_procurement', aggfunc="count")
)

awardedContractsBytype
32/15:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    
    contract_count = pd.NamedAgg(column='Subject of procurement', aggfunc="count")
)

awardedContractsBytype
32/16:
# Group By type

awardedContractsBytype = awardedContracts['Providers'].groupby('Procurement Type').agg(
    
    contract_count = pd.NamedAgg(column='Subject of procurement', aggfunc="count")
)

awardedContractsBytype
32/17:
# Group By type

awardedContractsBytype = awardedContracts['Provider'].groupby('Procurement Type').agg(
    
    contract_count = pd.NamedAgg(column='Subject of procurement', aggfunc="count")
)

awardedContractsBytype
32/18:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    
    contract_count = pd.NamedAgg(column='Subject of procurement', aggfunc="count")
)

awardedContractsBytype
32/19:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type')

awardedContractsBytype
32/20:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type')

awardedContractsBytype
32/21:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Procurement Type').agg(
    
    contract_count = pd.NamedAgg(column='Procurement Type', aggfunc="count")
)

awardedContractsBytype
32/22:
# Group By type

awardedContractsBytype = awardedContracts.groupby('Provider').agg(
    
    contract_count = pd.NamedAgg(column='Procurement Type', aggfunc="count")
)

awardedContractsBytype
32/23:
# Filter out UNRA procurements
UNRAContracts = awardedContracts['Uganda National Roads Authority'].isin['Entity']
UNRAContracts
32/24:
# Filter out UNRA procurements
UNRAContracts = awardedContracts['Uganda National Roads Authority'].isin(['Entity'])
UNRAContracts
32/25:
# Filter out UNRA procurements
UNRAContracts = awardedContracts['Uganda National Roads Authority'].isin(['Entity'])
awardedContracts[UNRAContracts]
32/26:
# Filter out UNRA procurements
UNRAContracts = awardedContracts['Uganda National Roads Authority'].isin('Entity')
awardedContracts[UNRAContracts]
32/27:
# Filter out UNRA procurements
UNRAContracts = awardedContracts
awardedContracts
32/28:
# Filter out UNRA procurements
UNRAContracts = awardedContracts['Entity'].isin(['Uganda National Roads Authority'])
awardedContracts[UNRAContracts]
32/29:
# Filter out KCCA procurements
UNRAContracts = awardedContracts['Entity'].isin(['Kampala Capital City Authority'])
awardedContracts[UNRAContracts]
32/30:
# Filter out MAAIF procurements
MAAIFContracts = awardedContracts['Entity'].isin(['Ministry of Agriculture, Animal Industry and Fisheries'])
awardedContracts[MAAIFContracts]
# maaif_dataset.to_excel('maaif_dataset.xlsx')
32/31:
# Filter out OPM procurements
OPMContracts = awardedContracts['Entity'].isin(['Office of the Prime Minister'])
awardedContracts[MAAIFContracts]
# opm_dataset.to_excel('opm_dataset.xlsx')
32/32:
# Filter out National Social Security Fund procurements
NSSFContracts = awardedContracts['Entity'].isin(['National Social Security Fund'])
awardedContracts[NSSFContracts]
# nssf_dataset.to_excel('nssf_dataset.xlsx')
32/33:
# Filter out National Medical Stores procurements
NMSContracts = awardedContracts['Entity'].isin(['National Medical Stores'])
awardedContracts[NMSContracts]
# nms_dataset.to_excel('nms_dataset.xlsx')
32/34:
# Filter out Ministry of Education, Science, Technology and Sports
MESTSContracts = awardedContracts['Entity'].isin(['Ministry of Education, Science, Technology and Sports'])
awardedContracts[MESTSContracts]
# mests_dataset.to_excel('mests_dataset.xlsx')
32/35:
# Filter out Ministry of Defence
MODContracts = awardedContracts['Entity'].isin(['Ministry of Defence'])
awardedContracts[MODContracts]
# mod_dataset.to_excel('mod_dataset.xlsx')
32/36:
# Filter out Ministry of Local Government
MLGContracts = awardedContracts['Entity'].isin(['Ministry of Local Government'])
awardedContracts[MLGContracts]
# mlg_dataset.to_excel('mlg_dataset.xlsx')
32/37:
# Filter out Ministry of Local Government
MLGContracts = awardedContracts['Entity'].isin(['Ministry of Local Government'])
mlg_dataset = awardedContracts[MLGContracts]
mlg_dataset.to_excel('mlg_dataset.xlsx')
32/38:
# Filter out UNRA procurements
UNRAContracts = awardedContracts['Entity'].isin(['Uganda National Roads Authority'])
unra_dataset = awardedContracts[UNRAContracts]
unra_dataset.to_excel('unra_dataset.xlsx')
32/39:
# Filter out KCCA procurements
KCCAContracts = awardedContracts['Entity'].isin(['Kampala Capital City Authority'])
kcca_dataset = awardedContracts[KCCAContracts]
kcca_dataset.to_excel('kcca_dataset.xlsx')
32/40:
# Filter out MAAIF procurements
MAAIFContracts = awardedContracts['Entity'].isin(['Ministry of Agriculture, Animal Industry and Fisheries'])
maaif_dataset = awardedContracts[MAAIFContracts]
maaif_dataset.to_excel('maaif_dataset.xlsx')
32/41:
# Filter out OPM procurements
OPMContracts = awardedContracts['Entity'].isin(['Office of the Prime Minister'])
opm_dataset = awardedContracts[OPMContracts]
opm_dataset.to_excel('opm_dataset.xlsx')
32/42:
# Filter out National Social Security Fund procurements
NSSFContracts = awardedContracts['Entity'].isin(['National Social Security Fund'])
nssf_dataset = awardedContracts[NSSFContracts]
nssf_dataset.to_excel('nssf_dataset.xlsx')
32/43:
# Filter out National Medical Stores procurements
NMSContracts = awardedContracts['Entity'].isin(['National Medical Stores'])
nms_dataset = awardedContracts[NMSContracts]
nms_dataset.to_excel('nms_dataset.xlsx')
32/44:
# Filter out Ministry of Education, Science, Technology and Sports
MESTSContracts = awardedContracts['Entity'].isin(['Ministry of Education, Science, Technology and Sports'])
mests_dataset = awardedContracts[MESTSContracts]
mests_dataset.to_excel('mests_dataset.xlsx')
32/45:
# Filter out Ministry of Defence
MODContracts = awardedContracts['Entity'].isin(['Ministry of Defence'])
mod_dataset = awardedContracts[MODContracts]
mod_dataset.to_excel('mod_dataset.xlsx')
32/46:
# Filter out Ministry of Local Government
MLGContracts = awardedContracts['Entity'].isin(['Ministry of Local Government'])
mlg_dataset = awardedContracts[MLGContracts]
mlg_dataset.to_excel('mlg_dataset.xlsx')
32/47:
# Filter out Ministry of Health
MOHContracts = awardedContracts['Entity'].isin(['Ministry of Health'])
moh_dataset = awardedContracts[MOHContracts]
moh_dataset.to_excel('moh_dataset.xlsx')
32/48:
# Filter out UNRA procurements
UNRAContracts = awardedContracts['Entity'].isin(['Uganda National Roads Authority'])
unra_dataset = awardedContracts[UNRAContracts]
unra_dataset.to_excel('unra_dataset.xlsx')
32/49:
# Filter out KCCA procurements
KCCAContracts = awardedContracts['Entity'].isin(['Kampala Capital City Authority'])
kcca_dataset = awardedContracts[KCCAContracts]
kcca_dataset.to_excel('kcca_dataset.xlsx')
32/50:
# Filter out MAAIF procurements
MAAIFContracts = awardedContracts['Entity'].isin(['Ministry of Agriculture, Animal Industry and Fisheries'])
maaif_dataset = awardedContracts[MAAIFContracts]
maaif_dataset.to_excel('maaif_dataset.xlsx')
32/51:
# Filter out OPM procurements
OPMContracts = awardedContracts['Entity'].isin(['Office of the Prime Minister'])
opm_dataset = awardedContracts[OPMContracts]
opm_dataset.to_excel('opm_dataset.xlsx')
32/52:
# Filter out National Social Security Fund procurements
NSSFContracts = awardedContracts['Entity'].isin(['National Social Security Fund'])
nssf_dataset = awardedContracts[NSSFContracts]
nssf_dataset.to_excel('nssf_dataset.xlsx')
32/53:
# Filter out National Medical Stores procurements
NMSContracts = awardedContracts['Entity'].isin(['National Medical Stores'])
nms_dataset = awardedContracts[NMSContracts]
nms_dataset.to_excel('nms_dataset.xlsx')
32/54:
# Filter out Ministry of Education, Science, Technology and Sports
MESTSContracts = awardedContracts['Entity'].isin(['Ministry of Education, Science, Technology and Sports'])
mests_dataset = awardedContracts[MESTSContracts]
mests_dataset.to_excel('mests_dataset.xlsx')
32/55:
# Filter out Ministry of Defence
MODContracts = awardedContracts['Entity'].isin(['Ministry of Defence'])
mod_dataset = awardedContracts[MODContracts]
mod_dataset.to_excel('mod_dataset.xlsx')
32/56:
# Filter out Ministry of Local Government
MLGContracts = awardedContracts['Entity'].isin(['Ministry of Local Government'])
mlg_dataset = awardedContracts[MLGContracts]
mlg_dataset.to_excel('mlg_dataset.xlsx')
32/57:
# Filter out Ministry of Health
MOHContracts = awardedContracts['Entity'].isin(['Ministry of Health'])
moh_dataset = awardedContracts[MOHContracts]
moh_dataset.to_excel('moh_dataset.xlsx')
32/58:
# Filter out Ministry of Water and Environment
MOWEContracts = awardedContracts['Entity'].isin(['Ministry of Water and Environment'])
mowe_dataset = awardedContracts[MOWEContracts]
mowe_dataset.to_excel('mowe_dataset.xlsx')
32/59:
# Filter out Ministry of Works and Transport
MOWTContracts = awardedContracts['Entity'].isin(['Ministry of Works and Transport'])
mowt_dataset = awardedContracts[MOWTContracts]
mowt_dataset.to_excel('mowt_dataset.xlsx')
32/60:
# Filter out National Water and Sewerage Corporation
NWSCContracts = awardedContracts['Entity'].isin(['National Water and Sewerage Corporation'])
nwsc_dataset = awardedContracts[NWSCContracts]
nwsc_dataset.to_excel('nwsc_dataset.xlsx')
32/61:
# Filter out National Water and Sewerage Corporation
NWSCContracts = awardedContracts['Entity'].isin(['National Water and Sewerage Corporation'])
nwsc_dataset = awardedContracts[NWSCContracts]
nwsc_dataset.to_excel('nwsc_dataset.xlsx')
32/62:
# Filter out Rural Electrification Agency
REAContracts = awardedContracts['Entity'].isin(['Rural Electrification Agency'])
rea_dataset = awardedContracts[MOHContracts]
rea_dataset.to_excel('rea_dataset.xlsx')
32/63:
# Filter out Ministry of Energy and Mineral Development
MEMDContracts = awardedContracts['Entity'].isin(['Ministry of Energy and Mineral Development'])
memd_dataset = awardedContracts[MEMDContracts]
memd_dataset.to_excel('memd_dataset.xlsx')
33/1:
# Filter out the contracts for National Social Security Fund
NSSFContracts = awardedContracts1920['title'].isin['National Social Security Fund']
awardedContracts1920[NSSFContracts]
33/2:
# Filter out the contracts for National Social Security Fund
NSSFContracts = awardedContracts1920['title'].isin['National Social Security Fund']
NSSFContracts
33/3:
# Filter out the contracts for National Social Security Fund
NSSFContracts = awardedContracts1920['title'].isin['National Social Security Fund']
awardedContracts1920[NSSFContracts]
33/4:
# Filter out the contracts for National Social Security Fund
NSSFContracts = awardedContracts1920['title'].isin['National Social Security Fund']
awardedContracts1920[NSSFContracts]
33/5:
# Filter out the contracts for National Social Security Fund
NSSFContracts = awardedContracts1920['title'].isin['National Social Security Fund']
awardedContracts1920[NSSFContracts]
33/6:
import pandas as pd
import numpy as np

awardedContracts1920 = pd.read_csv('awarded_contracts.csv')
awardedContracts1920.head()
33/7:
# Filter out the contracts for National Social Security Fund
NSSFContracts = awardedContracts1920['title'].isin['National Social Security Fund']
awardedContracts1920[NSSFContracts]
34/1:
# Filter out Rural Electrification Agency
REAContracts = awardedContracts['Entity'].isin(['Rural Electrification Agency'])
rea_dataset = awardedContracts[REAContracts]
rea_dataset.to_excel('rea_dataset.xlsx')
34/2:
import pandas as pd
awardedContracts = pd.read_excel('Contracts-2019-2020-groupBy-type.xlsx')
awardedContracts.head()
34/3:
# Filter out Rural Electrification Agency
REAContracts = awardedContracts['Entity'].isin(['Rural Electrification Agency'])
rea_dataset = awardedContracts[REAContracts]
rea_dataset.to_excel('rea_dataset.xlsx')
33/8:
# Filter out the contracts for National Social Security Fund
NSSFContracts = awardedContracts1920['title'].isin['National Social Security Fund']
awardedContracts1920[NSSFContracts]
33/9:
import pandas as pd
import numpy as np

awardedContracts1920 = pd.read_csv('awarded_contracts.csv')
awardedContracts1920.head()
33/10:
# Filter out the contracts for National Social Security Fund
NSSFContracts = awardedContracts1920['title'].isin['National Social Security Fund']
awardedContracts1920[NSSFContracts]
34/4:
# Filter out National Water and Sewerage Corporation
NWSCContracts = awardedContracts['Entity'].isin(['National Water and Sewerage Corporation'])
nwsc_dataset = awardedContracts[NWSCContracts]
nwsc_dataset.to_excel('nwsc_dataset.xlsx')
34/5:
# Filter out National Social Security Fund procurements
NSSFContracts = awardedContracts['Entity'].isin(['National Social Security Fund'])
nssf_dataset = awardedContracts[NSSFContracts]
nssf_dataset.to_excel('nssf_dataset.xlsx')
33/11:
# Filter Velvet TMA Architects ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('TMA Architect', case=False)
awardedContracts1920[lucaciAsProvider]
#tma.to_excel('tma.xlsx')
33/12:
# Filter Velvet TMA Architects ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('TMA Architect', case=False)
awardedContracts1920[lucaciAsProvider]
tma.to_excel('tma.xlsx')
33/13:
# Filter Velvet TMA Architects ltd as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('TMA Architect', case=False)
tma = awardedContracts1920[lucaciAsProvider]
tma.to_excel('tma.xlsx')
33/14:
# Filter out Technology Associates as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('technology associates', case=False)
techAssociates = awardedContracts1920[lucaciAsProvider]
techAssociates.to_excel('techAssociates.xlsx')
33/15:
# Filter out Kain group limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('TMA Architect', case=False)
kain = awardedContracts1920[lucaciAsProvider]
kain.to_excel('kain.xlsx')
33/16:
# Filter out Balton Uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('TMA Architect', case=False)
baltonUganda = awardedContracts1920[lucaciAsProvider]
baltonUganda.to_excel('baltonUganda.xlsx')
33/17:
# Filter out Kain group limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('Kain', case=False)
kain = awardedContracts1920[lucaciAsProvider]
kain.to_excel('kain.xlsx')
33/18:
# Filter out Balton Uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('balton uganda', case=False)
baltonUganda = awardedContracts1920[lucaciAsProvider]
baltonUganda.to_excel('baltonUganda.xlsx')
33/19:
# Filter out Balton Uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('balton uganda', case=False)
awardedContracts1920[lucaciAsProvider]
# rafah.to_excel('rafah.xlsx')
33/20:
# Filter out Balton Uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('rafah', case=False)
awardedContracts1920[lucaciAsProvider]
# rafah.to_excel('rafah.xlsx')
33/21:
# Filter out Balton Uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('rafah', case=False)
rafah = awardedContracts1920[lucaciAsProvider]
rafah.to_excel('rafah.xlsx')
33/22:
# Filter out Balton Uganda limited as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('rafah', case=False)
rafah = awardedContracts1920[lucaciAsProvider]
rafah.to_excel('rafah1.xlsx')
33/23:
# Filter out vision group as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('vision group', case=False)
visionGroup = awardedContracts1920[lucaciAsProvider]
visionGroup.to_excel('visionGroup.xlsx')
33/24:
# Filter out level 5 associates as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('level 5', case=False)
awardedContracts1920[lucaciAsProvider]
# level5Associates.to_excel('level5Associates.xlsx')
33/25:
# Filter out level 5 associates as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('level 5', case=False)
level5Associates = awardedContracts1920[lucaciAsProvider]
level5Associates.to_excel('level5Associates.xlsx')
33/26:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('level 5', case=False)
awardedContracts1920[lucaciAsProvider]
# castelleo.to_excel('castelleo.xlsx')
33/27:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('level 5', case=False)
castelleo = awardedContracts1920[lucaciAsProvider]
castelleo.to_excel('castelleo.xlsx')
33/28:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castelleo', case=False)
castelleo = awardedContracts1920[lucaciAsProvider]
castelleo.to_excel('castelleo.xlsx')
33/29:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castelleo', case=False)
castelleo = awardedContracts1920[lucaciAsProvider]
castelleo.to_excel('castelleo.xlsx')
33/30:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castelleo', case=False)
castelleo = awardedContracts1920[lucaciAsProvider]
castelleo.to_excel('castelleo.xlsx')
33/31:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castelleo', case=False)
castelleo = awardedContracts1920[lucaciAsProvider]
castelleo.to_excel('castelleo.xlsx')
33/32:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castelleo', case=False)
castelleo = awardedContracts1920[lucaciAsProvider]
castelleo.to_excel('castelleo.xlsx')
33/33:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castelleo', case=False)
castelleo = awardedContracts1920[lucaciAsProvider]
castelleo.to_excel('castelleo.xlsx')
33/34:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castelleo', case=False)
castelleo = awardedContracts1920[lucaciAsProvider]
castelleo.to_excel('castelleo.xlsx')
33/35:
# Filter out level 5 associates as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('level 5', case=False)
level5Associates = awardedContracts1920[lucaciAsProvider]
level5Associates.to_excel('level5Associates.xlsx')
33/36:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castelleo', case=False)
awardedContracts1920[lucaciAsProvider]
#castelleo.to_excel('castelleo.xlsx')
33/37:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castilleo', case=False)
awardedContracts1920[lucaciAsProvider]
#castelleo.to_excel('castelleo.xlsx')
33/38:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castilleo', case=False)
castilleo = awardedContracts1920[lucaciAsProvider]
castilleo.to_excel('castilleo.xlsx')
34/6:
# Filter out Rural Electrification Agency
REAContracts = awardedContracts['Entity'].isin(['Rural Electrification Agency'])
rea_dataset = awardedContracts[REAContracts]
rea_dataset.to_excel('rea_dataset.xlsx')
33/39:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('castilleo', case=False)
awardedContracts1920[lucaciAsProvider]
# niletrac.to_excel('niletrac.xlsx')
33/40:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('niletrac', case=False)
awardedContracts1920[lucaciAsProvider]
# niletrac.to_excel('niletrac.xlsx')
33/41:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('niletrac', case=False)
niletrac = awardedContracts1920[lucaciAsProvider]
# niletrac.to_excel('niletrac.xlsx')
33/42:
# Filter out castelleo as provider
lucaciAsProvider = awardedContracts1920['Provider'].str.contains('niletrac', case=False)
niletrac = awardedContracts1920[lucaciAsProvider]
niletrac.to_excel('niletrac.xlsx')
35/1:
import pandas as pd
import numpy as np
ppe1920 = pd.read_csv('procurement_plan_entries.csv')
ppe1920.head()
#ppe1920.sort_values(by='estimated_amount')
#direct = ppe1920['procurement_method'].isin(['Direct Procurement'])
#ppe1920[direct].describe()
#ppe1920[['subject_of_procurement', 'estimated_amount']]
#ppe1920.describe()
#ppe1920[['subject_of_procurement', 'procurement_method']].describe()
35/2:
# Export data sheet to excel
procurementPlanEntries = ppe1920;
procurementPlanEntries.to_excel('procurement_plan_entries.xlsx')
36/1: import psycopg
36/2: import psycopg2
36/3: import psycopg2
36/4: import psycopg3
36/5: import psycopg2
36/6: sudo apt-get install libpq-dev
36/7: import psycopg2
37/1: 1 + 1
37/2: 1 + 1
37/3: 1 + 1
37/4: 1 + 1
37/5: 1 + 1
37/6: 1 + 1
37/7: ## code header
37/8: ## code headerm
37/9:
x = 5
x + 3
37/10: 3 + 3
37/11: 2 + 3
37/12: 2 + 3
37/13: 2 + 3
37/14:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
37/15: pd.__version__
37/16: type(-100.82)
37/17: type(1)
37/18: type(True)
37/19:
# Lists
[ 1, 5, 10, 20]
37/20:
# Lists
[ 1, 5, 10, 20, "Hello world"]
37/21:
# Lists
lucky_numbers = [ 1, 5, 10, 20, "Hello world"]
37/22: type(lucky_numbers)
37/23:
# Lists
lucky_numbers = [ 1, 5, 10, 20 ]
37/24:
# Lists
lucky_numbers = [ 1, 5, 10, 20 ]
names = [ "Micheal", "Freddy", "Jason" ]
activated = [ True, True, False, False, True ]
37/25:
# Lists
lucky_numbers = [ 1, 5, 10, 20 ]
names = [ "Micheal", "Freddy", "Jason" ]
activated = [ True, True, False, False, True ]
37/26: len(lucky_numbers)
37/27: lucky_numbers[4]
37/28: lucky_numbers[2]
37/29: names[2]
37/30: names[100]
37/31: try names[100]
37/32:
try(names[100])
except(e):
    print('Error')
37/33:
try:
    names[100]
except(e):
    print('Error')
37/34:
# Dictionaries
menu = {
    "Filet Mignon": 29.99,
    "Big Mac": 3.99
    "Pizza": 0.99,
    "Salmon": 29.99
}
37/35:
# Dictionaries
menu = {
    "Filet Mignon": 29.99,
    "Big Mac": 3.99,
    "Pizza": 0.99,
    "Salmon": 29.99
}
37/36:
# Dictionaries
menu = {
    "Filet Mignon": 29.99,
    "Big Mac": 3.99,
    "Pizza": 0.99,
    "Salmon": 29.99
}
37/37: menu['Big Mac']
37/38: menu['Big']
37/39: menu['Big Mac']
37/40: "John" == 'john'
37/41:
def convert_to_fahrenheit(celsiusTemp):
    product = celsiusTemp * 1.8
    final = product + 32
    return final
37/42: convert_to_fahrenheit(35)
37/43: convert_to_fahrenheit(35)
37/44: convert_to_fahrenheit(20)
37/45: import pandas as pd
37/46: import pandas as pd
37/47: import pandas as pd
37/48:
ice_cream = ['Chocolate', 'Vanilla', 'Strawberry', 'Rum Raisan']
pd.Series(ice_cream)
37/49:
ages = [21, 18, 4, 25, 35]
pd.Series(ages)
37/50:
lottery = [4, 8, 15, 16, 23, 42]
pd.Series(lottery)
37/51:
registrations = [True, False, False, False, True]
pd.Series(registrations)
37/52:
lottery = [4, 8, 15, 16, 23, 42]
pd.Series(lottery)
37/53:
webster = {
    "Aardvark": "An Aninal",
    "Banana": "A delicious fruit",
    "Cyan": "A color"
}
pd.Series(webster)
39/1: import pandas as pd
39/2: pd.read_csv('nba.csv')
39/3: nba = pd.read_csv('nba.csv')
39/4: nba.head()
39/5: nba.head(7)
39/6: nba.head(1)
39/7: nba.tail()
39/8: nba.index
39/9: nba.values
39/10: nba.shape
39/11: nba.dType
39/12: nba.dTypes
39/13: nba.dtypes
39/14: nba.head(1)
39/15: nba.columns
39/16: nba.rows
39/17: nba.axes
39/18: nba.info()
39/19: nba.get_dtype_counts()
39/20: nba.get_dtype_counts()
39/21: nba.get_dtypes_counts()
39/22: nba.dtypes_counts()
39/23: nba.dtypes()
39/24: nba.dtypes
39/25: nba.dtypes_count()
39/26: nba.dtypes_counts()
39/27: nba.get_dtypes_counts()
39/28: pd.read_csv('revenue.csv')
39/29: pd.read_csv('revenue.csv', index_col = "Date")
39/30:
rev = pd.read_csv('revenue.csv', index_col = "Date")
rev.head(3)
39/31:
s = pd.Series([1, 2, 3])
s
39/32:
s = pd.Series([1, 2, 3])
s.sum()
39/33: rev.sum()
39/34:
s = pd.Series([1, 2, 3])
s.sum(axis = "index")
39/35: rev.sum(axis = "index")
39/36: rev.sum(axis = 1)
39/37: rev.sum(axis = "columns")
39/38:
nba = pd.read_csv('nba.csv')
nba.head(3)
39/39: nba.Name
39/40:
nba.Name
nba.Number
39/41:
nba.Name
nba.Number
nba.Salary
39/42:
nba.Name
nba.Number
nba.Salary

Output = None
39/43: nba['Name']
39/44: nba['name']
39/45: nba['name', case=False]
39/46: nba['name']
39/47: nba['Name']
39/48:
nba['Name']
name["Salary"]
39/49:
nba['Name']
nba["Salary"]
39/50:
nba["Name"]
name["Salary"]
39/51:
nba["Name"]
nba["Salary"]
39/52: type(nba["Name"])
39/53: nba["Name"]
39/54: nba["Name"].head()
39/55:
nba = pd.read_csv("nba.csv")
nbs.head(3)
39/56:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/57: nba[["Name", "Team"]]
39/58: nba[["Team", "Name"]]
39/59: nba[["Team", "Name"]].head(3)
39/60:
nba[["Team", "Name"]].head(3)
nba[["Number", "College"]]
39/61:
nba[["Team", "Name"]].head(3)
nba[["Number", "College"]]
nba[["Salary", "Team", "Name"]].tail()
39/62:
select = ["Salary", "Team", "Name"]
nba[select]
39/63:
nba = pd.read_csv('nba.csv')
nba.head(3)
39/64: nba["Sport"] = "Basketball"
39/65: nba["Sport"] = "Basketball"
39/66: nba.head(3)
39/67:
nba.head(3)
nba.tail()
39/68: nba["League"] = "National Basketball Association"
39/69: nba.head(3)
39/70:
nba = pd.read_csv('nba.csv')
nba.head(3)
39/71: nba.insert(3, column = "Sport", value = "Basketball")
39/72: nba.head(3)
39/73: nba.insert(7, column = "League", value = "National Basketball Association")
39/74: nba.head(3)
39/75:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/76: nba["Age"].add(5)
39/77: nba["Age"]
39/78: nba["Age"].add(5)
39/79:
nba["Age"].add(5)
nba["Age"] + 5
39/80:
nba["Age"].add(5)
nba["Team"] + 5
39/81:
nba["Age"].add(5)
nba["Age"] + 5
39/82:
nba["Age"].add(5)
nba["Age"] + 5

nba["Salary"].sub(5000000)
39/83:
nba["Age"].add(5)
nba["Age"] + 5

nba["Salary"].sub(5000000)
nba["Salary"] - 5000000
39/84:
nba["Age"].add(5)
nba["Age"] + 5

nba["Salary"].sub(5000000)
nba["Salary"] - 5000000

nba["Weight"].mul(0.453492)
39/85:
nba["Age"].add(5)
nba["Age"] + 5

nba["Salary"].sub(5000000)
nba["Salary"] - 5000000

nba["Weight"].mul(0.453492)
nba["Weight"] * 0.453592
39/86:
nba["Age"].add(5)
nba["Age"] + 5

nba["Salary"].sub(5000000)
nba["Salary"] - 5000000

nba["Weight"].mul(0.453492)
nba["Weight in Kilograms"] = nba["Weight"] * 0.453592
39/87: nba.head(3)
39/88: nba["Salary"].div(1000000)
39/89:
nba["Salary"].div(1000000)
nba["Salary"] / 1000000
39/90:
nba["Salary"].div(1000000)
nba["Salary in Millions"] = nba["Salary"] / 1000000
39/91: nba.head(3)
39/92:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/93: nba["Team"].value_counts()
39/94:
nba["Team"].value_counts()
nba["Position"].value_counts()
39/95:
nba["Team"].value_counts()
nba["Position"].value_counts().head(1)
39/96:
nba["Team"].value_counts()
nba["Position"].value_counts().head(1)
nba["Weight"].value_counts()
39/97:
nba["Team"].value_counts()
nba["Position"].value_counts().head(1)
nba["Weight"].value_counts().tail()
39/98:
nba["Team"].value_counts()
nba["Position"].value_counts().head(1)
nba["Weight"].value_counts().tail()
nba["Salary"]
39/99:
nba["Team"].value_counts()
nba["Position"].value_counts().head(1)
nba["Weight"].value_counts().tail()
nba["Salary"].value_counts()
39/100:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/101: nba.tail()
39/102: nba.dropna()
39/103:
nba = pd.read_csv("nba.csv")
nba.head()
39/104:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/105: nba.head()
39/106: nba.dropna()
39/107: nba.head()
39/108: nba.dropna(how = "all")
39/109: nba.dropna(how = "all", inplace = True)
39/110: nba.tail(3)
39/111: nba.dropna(axis = 1)
39/112: nba.dropna(axis = "columns")
39/113: nba.head()
39/114: nba.dropna(subset = ["Salary"])
39/115: nba.dropna(subset = ["Salary", "College"])
39/116:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/117: nba.fillna(0)
39/118: nba["Salary"].fillna(0, inplace = True)
39/119: nba.head()
39/120: nba["College"].fillna("No College")
39/121: nba["College"].fillna("No College", inplace = True)
39/122: nba.head()
39/123:
nba = read_csv("nba.csv")
nba.head(3)
39/124:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/125:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/126:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba.head(3)
39/127:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba.tail(3)
39/128:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba.head(3)
39/129:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba["Salary"].fillna(0)
nab.head(3)
39/130:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba["Salary"].fillna(0)
nba.head(3)
39/131:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba["Salary"].fillna(0)
# nba.head(3)
39/132:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba["Salary"].fillna(0)
nba.head(3)
39/133:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba["Salary"].fillna(0, inplace = True)
nba.head(3)
39/134:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba["Salary"].fillna(0, inplace = True)
nba["College"].fillna("None", inplace = True)
nba.head(3)
39/135: nba.dtypes
39/136:
nba.dtypes
nba.info()
39/137: nba["Salary"]
39/138: nba["Salary"].astype("int")
39/139: nba["Salary"] = nba["Salary"].astype("int")
39/140: nba.head(3)
39/141:
nba.dtypes
nba.info()
39/142:
nba["Number"] = nba["Number"].astype("int")
nba["Age"] = nba["Age"].astype("int")
39/143:
nba["Number"] = nba["Number"].astype("int")
nba["Age"] = nba["Age"].astype("int")
nba.head(3)
39/144: nba["Age"].astype("float")
39/145: nba["Position"].unique()
39/146: nba["Position"].nunique()
39/147: nba["Postion"] = nba["Position"].astype("category")
39/148:
nba.dtypes
nba.info()
39/149: nba["Team"].unique()
39/150: nba["Team"].nunique()
39/151: nba["Team"] = nba["Team"].astype("category")
39/152: nba.head(3)
39/153:
nba.dtypes
nba.info()
39/154:
nba = pd.read_csv("nba.csv")
nba.head()
39/155: nba.sort_values("Name")
39/156: nba.sort_values("Name", ascending=False)
39/157:
nba.sort_values("Name", ascending=False)

nba.sort_values("Age")
39/158:
nba.sort_values("Name", ascending=False)

nba.sort_values("Age". ascending = False)
39/159:
nba.sort_values("Name", ascending=False)

nba.sort_values("Age", ascending = False)
39/160:
nba.sort_values("Name", ascending = False)

nba.sort_values("Age", ascending = False)
39/161:
nba.sort_values("Name", ascending = False)

nba.sort_values("Age", ascending = False)

nba.sort_values("Salary", ascending = False)
39/162:
nba.sort_values("Name", ascending = False)

nba.sort_values("Age", ascending = False)

nba.sort_values("Salary", ascending = False, inplace = True)
39/163:
nba.sort_values("Name", ascending = False)

nba.sort_values("Age", ascending = False)

nba.sort_values("Salary", ascending = False, inplace = True)
nba.head(3)
39/164: nba.sort_values("Salary").tail()
39/165: nba.sort_values("Salary", na_position = "last")
39/166: nba.sort_values("Salary", na_position = "first")
39/167: nba.sort_values("Salary", ascending = False, na_position = "first")
39/168: nba.sort_values("Salary", ascending = False, na_position = "first").tail()
39/169:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/170: nba.sort_values(["Team", "Name"])
39/171: nba.sort_values(["Team", "Name"], ascending = False)
39/172: nba.sort_values(["Team", "Name"], ascending = [True, False])
39/173: nba.sort_values(["Team", "Name"], ascending = [True, False])
39/174: nba.sort_values(["Team", "Name"], ascending = [True, False])
39/175: nba.sort_values(["Team", "Name"], ascending = [True, False])
39/176:
nba.sort_values(["Team", "Name"], ascending = [True, False], inplace = True)
nba.head(3)
39/177:
nba = pd.read_csv("nba.csv")
nba.head(3)
39/178:
sortOrder = ["Number", "Salary", "Name"]
nba.sort_values(sortOrder)
39/179:
sortOrder = ["Number", "Salary", "Name"]
nba.sort_values(sortOrder, inplace = True)
nba.tail(3)
39/180: nba.sort_index()
39/181: nba.sort_index(ascending = False, inplace = True)
39/182:
nba.sort_index(ascending = False, inplace = True)
nba.head(3)
39/183:
nba = pd.read_csv("nba.csv")
nba.head()
39/184:
nba = pd.read_csv("nba.csv").dropna(how = "all")
nba["Salary"] = nba["Salary"].fillna(0).astype("int")
nba.head(3)
39/185: nba["Salary"].rank()
39/186: nba["Salary"].rank(ascending = False)
39/187: nba["Salary"].rank(ascending = False).astype("int")
39/188:
nba["Salary Rank"] = nba["Salary"].rank(ascending = False).astype("int")
nba.head()
39/189: nba.sort_values(by = "Salary")
39/190: nba.sort_values(by = "Salary", ascending = False)
40/1: import pandas as pd
40/2: pd.read_csv('employees.csv')
40/3: df = pd.read_csv('employees.csv')
40/4:
df = pd.read_csv('employees.csv')
df.head(3)
40/5: df.info()
40/6: df["Start Date"].head(3)
40/7: pd.to_datetime(df["Start Date"])
40/8: df["Start Date"] = pd.to_datetime(df["Start Date"])
40/9: pd.to_datetime(df["Last Login Time"])
40/10: df["Last Login Time"] = pd.to_datetime(df["Last Login Time"])
40/11: df["Senior Management"].astype("bool")
40/12: df["Senior Management"] = df["Senior Management"].astype("bool")
40/13: df["Gender"] = df["Gender"].astype("category")
40/14: df.info()
40/15:
df = pd.read_csv('employees.csv')
df["Gender"] = df["Gender"].astype("category")
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Last Login Time"] = pd.to_datetime(df["Last Login Time"])
df["Start Date"] = pd.to_datetime(df["Start Date"])
df.head(3)
40/16:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Gender"] = df["Gender"].astype("category")
df["Senior Management"] = df["Senior Management"].astype("bool")
df.head(3)
40/17: df.info()
40/18:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.head(3)
40/19: df["Gender"]
40/20: df["Gender"] == "Male"
40/21:
male = df["Gender"] == "Male"
male
40/22:
male = df["Gender"] == "Male"
df[male]
40/23: df[df["Team"] == "Finance"]
40/24: df[df["Team"] == "Finance"]
40/25: df["Senior Management"]
40/26: df[df["Senior Management"]]
40/27: df["Team"] != "Marketing"
40/28: df[df["Team"] != "Marketing"]
40/29: df["Salary"] > 110000
40/30: df[df["Salary"] > 110000]
40/31:
df[df["Salary"] > 110000]

df["Bonus %"] < 1.5
40/32:
df[df["Salary"] > 110000]

df[df["Bonus %"] < 1.5]
40/33: df["Start Date"] <= "1985-01-01"
40/34: df[df["Start Date"] <= "1985-01-01"]
40/35:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.head(3)
40/36: df["Gender"] == "Male"
40/37:
mask1 = df["Gender"] == "Male"
mask2 = df["Team"] == "Marketing"

df[mask1, mask2]
40/38:
mask1 = df["Gender"] == "Male"
mask2 = df["Team"] == "Marketing"

df[mask1 & mask2]
40/39:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.head(3)
40/40:
mask1 = df["Senior Management"]
df["Start Date"] < "1990-01-02"
40/41:
mask1 = df["Senior Management"]
mask2 = df["Start Date"] < "1990-01-02"

df[mask1 | mask2]
40/42: df["First Name"]
40/43: df["First Name"] == "Robert"
40/44:
mask1 = df["First Name"] == "Robert"
df["Team"]
40/45:
mask1 = df["First Name"] == "Robert"
df["Team"] == "Client Services"
40/46:
mask1 = df["First Name"] == "Robert"
mask2 = df["Team"] == "Client Services"

df["Start Date"]
40/47:
mask1 = df["First Name"] == "Robert"
mask2 = df["Team"] == "Client Services"

df["Start Date"] > "2016-06-01"
40/48:
mask1 = df["First Name"] == "Robert"
mask2 = df["Team"] == "Client Services"
mask3 = df["Start Date"] > "2016-06-01"

df[(mask1 & mask2) | mask3]
40/49:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.head(3)
40/50:
mask1 = df["Team"] == "Legal"
mask2 = df["Team"] == "Sales"
maske3 = df["Team"] == "Product"

df[mask1 | mask2 | mask3]
40/51: df["Team"].isin(["Leagl", "Sales", "Product"])
40/52: df[df["Team"].isin(["Leagl", "Sales", "Product"])]
40/53: df[df["Team"].isin(["Legal", "Sales", "Product"])]
40/54:
mask = df["Team"].isin(["Legal", "Sales", "Product"])
df[mask]
40/55:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.head(3)
40/56: df["Team"]
40/57: df["Team"].isnull()
40/58:
mask = df["Team"].isnull()
df[mask]
40/59: df["Gender"]
40/60: df["Gender"].notnull()
40/61: condition = df["Gender"].notnull()
40/62:
condition = df["Gender"].notnull()

df[condition]
40/63:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.head(3)
40/64: df["Salary"]
40/65: df["Salary"].between(60000, 70000)
40/66:
mask = df["Salary"].between(60000, 70000)
df[mask]
40/67: df["Bonus %"]
40/68: df["Bonus %"].between(2.0, 5.0)
40/69:
mask = df["Bonus %"].between(2.0, 5.0)
df[mask]
40/70: df["start Date"].between("1991-01-01", "1992-01-01")
40/71: df["Start Date"].between("1991-01-01", "1992-01-01")
40/72:
mask = df["Start Date"].between("1991-01-01", "1992-01-01")
df[mask]
40/73: df["Last Login Time"]
40/74: df["Last Login Time"].between("08:30AM", "12:00PM")
40/75:
mask = df["Last Login Time"].between("08:30AM", "12:00PM")
df[mask]
40/76:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.head(3)
40/77:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df["First Name"].sort_values("First Name", inplace = True)
df.head(3)
40/78:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.sort_values("First Name", inplace = True)
df.head(3)
40/79: df["First Name"]
40/80: df["First Name"].duplicated()
40/81:
mask = df["First Name"].duplicated()
df[mask]
40/82: df["First Name"]
40/83: df["First Name"].duplicated(keep = "")
40/84: df["First Name"].duplicated(keep = "last")
40/85: df["First Name"].duplicated(keep = "Last")
40/86: df["First Name"].duplicated(keep = "last")
40/87:
mask = df["First Name"].duplicated(keep = "last")
df[mask]
40/88:
mask = df["First Name"].duplicated(keep = False)
df[mask]
40/89: df["First Name"]
40/90: df["First Name"].duplicated()
40/91: df["First Name"].duplicated(keep = False)
40/92: ~df["First Name"].duplicated(keep = False)
40/93:
mask = ~df["First Name"].duplicated(keep = False)
df[mask]
40/94:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.sort_values("First Name", inplace = True)
df.head(3)
40/95: len(df)
40/96: df.drop_duplicates()
40/97: len(df.drop_duplicates())
40/98: df.drop_duplicates(subset = ["First Name"], keep = "first")
40/99: df.drop_duplicates(subset = ["First Name"], keep = "last")
40/100: df.drop_duplicates(subset = ["First Name"], keep = False)
40/101: df.drop_duplicates(subset = ["First Name"], keep = False)
40/102: df.drop_duplicates(subset = ["Team"], keep = False)
40/103:
df = pd.read_csv('employees.csv', parse_dates = ["Start Date", "Last Login Time"])
df["Senior Management"] = df["Senior Management"].astype("bool")
df["Gender"] = df["Gender"].astype("category")
df.head(3)
40/104: df["Gender"]
40/105: df["Gender"].unique()
40/106:
df["Gender"].unique()

df["Team"].unique()
40/107: len(df["Team"].unique())
40/108: df["Team"].nunique()
40/109: df["Team"].nunique(dropna = False)
41/1: import pandas as pd
41/2: pd.read_csv("jamesbond.csv")
41/3:
bond = pd.read_csv("jamesbond.csv")
bond.head(3)
41/4:
bond = pd.read_csv("jamesbond.csv", index_col = "")
bond.head(3)
41/5:
bond = pd.read_csv("jamesbond.csv")
bond.head(3)
41/6:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.head(3)
41/7:
bond = pd.read_csv("jamesbond.csv")
bond.head(3)
41/8: bond.set_index("Film")
41/9: bond.set_index("Film", inplace = True)
41/10:
bond.set_index("Film", inplace = True)
bond.head(3)
41/11:
bond.set_index("Film", inplace = True)
bond.head(3)
41/12:
bond.set_index("Film", inplace = False)
bond.head(3)
41/13:
bond.set_index("Film")
bond.head(3)
41/14: bond.set_index("Film")
41/15: bond.head()
41/16: bond.reset_index()
41/17: bond.reset_index(drop = True)
41/18: bond.reset_index(drop = False, inplace = True)
41/19:
bond.reset_index(drop = False, inplace = True)
bond.head()
41/20:
bond = pd.read_csv("jamesbond.csv")
bond.head(3)
41/21:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.head(3)
41/22:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.sort_index(inplace = True)
bond.head(3)
41/23: bond.loc["Goldfinger"]
41/24:
bond.loc["Goldfinger"]
bond.loc["GoldenEye"]
41/25:
bond.loc["Goldfinger"]
bond.loc["GoldenEye"]
bond.loc["Sacred bond"]
41/26:
bond.loc["Goldfinger"]
bond.loc["GoldenEye"]
41/27:
bond.loc["Goldfinger"]
bond.loc["GoldenEye"]
bond.loc["Casino Royale"]
41/28: bond.loc["Diamonds Are Forever": "Moonraker"]
41/29: bond.loc["Diamonds Are Forever"]
41/30: bond.loc["Diamonds Are Forever": "Moonraker"]
41/31: bond.loc["Diamonds Are Forever" : "Moonraker"]
41/32:
bond.loc["Diamonds Are Forever" : "Moonraker"]
bond.loc["Golden Eye" :]
41/33:
bond.loc["Diamonds Are Forever" : "Moonraker"]
bond.loc["GoldenEye" :]
41/34:
bond.loc["Diamonds Are Forever" : "Moonraker"]
bond.loc["GoldenEye" :]
bond.loc[: "On Her Magesty's Secret Service"]
41/35: bond.loc[["Moonraker", "Octopussy"]]
41/36: bond.loc[["Octopussy", "Moonraker"]]
41/37: bond.loc[["For your Eyes Only", "Live and let Die", "Gold Bond"]]
41/38: bond.loc[["For your Eyes Only", "Live and let Die"]]
41/39: bond.loc[["For Your Eyes Only", "Live and Let Die", "Gold Bond"]]
41/40: bond.loc[["For Your Eyes Only", "Live and Let Die"]]
41/41: "Gold Bond" in bond.index
41/42:
bond = pd.read_csv("jamesbond.csv")
bond.head(3)
41/43: bond.loc[15]
41/44:
bond.loc[15]
bond.iloc[15]
41/45:
bond.loc[15]
bond.iloc[15]

bind.ilco[[15, 20]]
41/46:
bond.loc[15]
bond.iloc[15]

bond.ilco[[15, 20]]
41/47:
bond.loc[15]
bond.iloc[15]

bond.iloc[[15, 20]]
41/48:
bond.loc[15]
bond.iloc[15]

bond.iloc[[15, 20]]
bond.iloc[:4]
41/49:
bond.loc[15]
bond.iloc[15]

bond.iloc[[15, 20]]
bond.iloc[:4]
bond.iloc[4:]
41/50:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.sort_index(inplace = True)
bond.head(3)
41/51: bond.loc["GoldenEye"]
41/52: bond.iloc[0]
41/53:
bond = pd.read_csv("jamesbond.csv")
bond.head(3)
41/54:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.sort_index(inplace = True)
bond.head(3)
41/55:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.sort_index(inplace = True)
bond.head(3)
41/56: bond.ix["GOldEye"]
41/57: bond.ix["GoldenEye"]
41/58: bond.ix["Casino Royale"]
41/59: bond.loc["GoldenEyw"]
41/60: bond.loc["GoldenEye"]
41/61: bond.iloc["GoldenEye"]
41/62: bond.loc["GoldenEye"]
41/63:
bond.loc["GoldenEye"]
bond.ix[["Diamonds are Forever", "Moonraker", "Spectre"]]
41/64:
bond.loc["GoldenEye"]
bond.loc[["Diamonds are Forever", "Moonraker", "Spectre"]]
41/65:
bond.loc["GoldenEye"]
bond.loc[["Moonraker", "Spectre"]]
41/66:
bond.loc["GoldenEye"]
bond.ix[["Moonraker", "Spectre"]]
41/67:
bond.loc["GoldenEye"]
bond.loc[["Moonraker", "Spectre"]]
41/68: bond.ix[10]
41/69:
bond = pd.read_csv("jamesbond.csv")
bond.head(3)
41/70:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.head(3)
41/71:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.sort_index(inplace = True)
bond.head(3)
41/72: bond.loc["Moonraker"]
41/73: bond.loc["Moonraker", "Actor"]
41/74: bond.loc["Moonraker", "Director" : "Bidget"]
41/75: bond.loc["Moonraker", "Director" : "Budget"]
41/76: bond.loc["Moonraker", ["Actor", "Budget", "Year"]]
41/77:
bond.loc["Moonraker", ["Actor", "Budget", "Year"]]

bond.iloc[14]
41/78:
bond.loc["Moonraker", ["Actor", "Budget", "Year"]]

bond.iloc[14, 2]
41/79:
bond.loc["Moonraker", ["Actor", "Budget", "Year"]]

bond.iloc[14, 2:5]
41/80:
bond.loc["Moonraker", ["Actor", "Budget", "Year"]]

bond.iloc[14, 2 : 10]
41/81:
bond.loc["Moonraker", ["Actor", "Budget", "Year"]]

bond.iloc[14, [5, 3, 2]]
41/82:
bond.loc["Moonraker", ["Actor", "Budget", "Year"]]

bond.iloc[14, [5, 3, 2]]

bond.ix[20]
41/83:
bond.loc["Moonraker", ["Actor", "Budget", "Year"]]

bond.iloc[14, [5, 3, 2]]
41/84:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.sort_index(inplace = True)
bond.head(3)
41/85:
bond = pd.read_csv("jamesbond.csv", index_col = "Film")
bond.sort_index(inplace = True)
bond.head(3)
41/86: bond
41/87: bond["Actor"]
41/88: bond["Actor"] == "Sean Connery"
41/89: mask = bond["Actor"] == "Sean Connery"
41/90: bond[mask]
41/91: bond[mask]["Actor"]
41/92: bond[mask]["Actor"] = "Sir Sean Connery"
41/93: bond
41/94: bond[mask]
41/95:
df2 = bond[mask]
df2["Actor"] = "Sir Sean Connery"
df2
41/96: bond
43/1: import pandas as pd
43/2: df = pd.read_csv('./Sales_Data/Sales_April_2019.csv')
43/3: df = pd.read_csv('./Sales_Analysis/Sales_Data/Sales_April_2019.csv')
43/4: df = pd.read_csv('./Sales_Data/Sales_April_2019.csv')
43/5: df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')
43/6:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')
df.head();
43/7:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')
df.head();
43/8:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

df.head();
43/9:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

df.tail();
43/10:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

df.tail()
43/11:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

df.head()
43/12: df.describe()
43/13:
df.describe()
df.info()
43/14:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

for file in files:
    print(file)
43/15:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

for file in files:
    print(file)
43/16:
import pandas as pd
import os
43/17:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

for file in files:
    print(file)
43/18:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = df.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.head()
43/19:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = df.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.head()
43/20:
import pandas as pd
import os
43/21:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = df.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.head()
43/22:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.head()
43/23:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.head()
43/24:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

df.head()
43/25:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.head()
43/26:
df = pd.read_csv('./SalesAnalysis/Sales_Data/Sales_April_2019.csv')

files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.to_csv('all_months', index = False)
43/27:
files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.to_csv('all_months', index = False)
43/28:
all_data = pd.read_csv('all_months.csv')
all_data.head()
43/29:
all_data = pd.read_csv('./SalesAnalysis/Sales_Data/all_months.csv')
all_data.head()
43/30:
all_data = pd.read_csv('./Sales_Data/all_months.csv')
all_data.head()
43/31:
all_data = pd.read_csv('./all_months.csv')
all_data.head()
43/32:
all_data = pd.read_csv('../all_months.csv')
all_data.head()
43/33:
all_data = pd.read_csv('.../all_months.csv')
all_data.head()
43/34:
all_data = pd.read_csv('all_months.csv')
all_data.head()
43/35:
files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.to_csv('all_months.csv', index = False)
43/36:
all_data = pd.read_csv('all_months.csv')
all_data.head()
43/37: all_data.sort_values(ascending=False)
43/38: all_data.sort_values("Price Ezch", ascending=False)
43/39: all_data.sort_values("Price Each", ascending=False)
43/40:
all_data.sort_values("Price Each", ascending=False)
all_data.head(10)
43/41:
all_data.sort_values("Price Each", ascending=False, inplace=True)
all_data.head(10)
43/42:
all_data.sort_values("Price Each", ascending=False)
all_data.head(10)
43/43: all_data['Month'] = 3
43/44:
all_data['Month'] = 3
all_data.head()
43/45:
all_data['Month'] = all_data['Order Date'].str[0:2]
all_data.head()
43/46:
all_data['Month'] = all_data['Order Date'].str[0:2]
all_data['Month'] = all_data['Month'].astype('int')
all_data.head()
43/47:
files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.to_csv('all_months.csv', index = False)
43/48:
files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.to_csv('all_months.csv', index = False)
all_months.head()
43/49:
files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.to_csv('all_months.csv', index = False)
all_months.head()
43/50:
files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months = all_months_data.to_csv('all_months.csv', index = False)
all_months.head()
43/51:
files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months = all_months_data.to_csv('all_months.csv', index = False)
all_months.head()
43/52:
files = [file for file in os.listdir('./SalesAnalysis/Sales_Data') ]

all_months_data = pd.DataFrame()

for file in files:
    df = pd.read_csv('./SalesAnalysis/Sales_Data/' + file)
    all_months_data = pd.concat([all_months_data, df])

all_months_data.to_csv('all_months.csv', index = False)
43/53:
all_data = pd.read_csv('all_months.csv')
all_data.head()
43/54:
all_data = pd.read_csv('all_months.csv')
all_data.head(10)
43/55:
all_data['Month'] = all_data['Order Date'].str[0:2]
all_data['Month'] = all_data['Month'].astype('int')
all_data.head()
43/56:
all_data['Month'] = all_data['Order Date'].str[0:2]
# all_data['Month'] = all_data['Month'].astype('int')
all_data.head()
43/57:
all_data['Month'] = all_data['Order Date'].str[0:2]
# all_data['Month'] = all_data['Month'].astype('int')
all_data.head(50)
43/58:
all_data['Month'] = all_data['Order Date'].str[0:2]
# all_data['Month'] = all_data['Month'].astype('int')
all_data.head(500)
43/59:
all_data['Month'] = all_data['Order Date'].str[0:2]
# all_data['Month'] = all_data['Month'].astype('int')
all_data.head(5000)
43/60:
all_data['Month'] = all_data['Order Date'].str[0:2]
# all_data['Month'] = all_data['Month'].astype('int')
all_data.tail(5000)
43/61:
all_data['Month'] = all_data['Order Date'].str[0:2]
all_data['Month'] = all_data['Month'].astype('int')
all_data.head()
43/62:
nan_df = all_data[all_data.isna().any(axis=1)]
nan_df.head()
43/63:
nan_df = all_data[all_data.isna().any(axis=1)]
nan_df.head()

all_data = all_data.dropna(how="all")
all_data.head()
43/64:
all_data['Month'] = all_data['Order Date'].str[0:2]
all_data['Month'] = all_data['Month'].astype('int32')
all_data.head()
43/65:
temp_df = all_data[all_data['Order Date'].str[0:2] == 'Or']
temp_df.head()
43/66:
temp_df = all_data[all_data['Order Date'] == 'Or']
temp_df.head()
43/67:
temp_df = all_data[all_data['Order Date'] == 'Or']
temp_df.head()
43/68:
temp_df = all_data[all_data['Order Date'].str[0:2] == 'Or']
temp_df.head()
43/69:
temp_df = all_data[all_data['Order Date'].str[0:2] == 'Or']
temp_df.head()
43/70: all_data = all_data[all_data['Order Date'].str[0:2] != 'Or']
43/71:
all_data['Month'] = all_data['Order Date'].str[0:2]
all_data['Month'] = all_data['Month'].astype('int32')
all_data.head()
43/72:
all_data.sort_values("Price Each", ascending=False)
all_data.head(10)
43/73: all_data['Sales'] = all_data['Quantity Ordered'] * all_data['Price Each']
43/74: all_data['Sales'] = all_data['Quantity Ordered'].astype('int') * all_data['Price Each']
43/75: all_data.head()
43/76: all_data.head(10)
43/77: all_data.head(15)
43/78:
quantity = all_data['Quantity Ordered'].astype('int32')
price = all_data['Price Each']
all_data['Sales'] = quantity * price
all_data.head(15)
43/79:
quantity = all_data['Quantity Ordered'].astype('int32')
price = all_data['Price Each'].astype('int32')
all_data['Sales'] = quantity * price
all_data.head(15)
43/80:
quantity = all_data['Quantity Ordered'].astype('int32')
price = all_data['Price Each'].astype('float')
all_data['Sales'] = quantity * price
all_data.head(15)
43/81:
quantity = all_data['Quantity Ordered'].astype('int32')
price = all_data['Price Each'].astype('float')
all_data['Sales'] = quantity * price
all_data.head(15)
43/82:
all_data.sort_values("Price Each", ascending=False)
all_data.head(10)
43/83:
all_data.sort_values("Sales", ascending=False)
all_data.head(10)
43/84:
all_data.sort_values("Sales", ascending=True)
all_data.head(10)
43/85: all_data.groupby('Month').sum()
43/86: all_data.groupby('Month').sum()['Sales']
43/87: all_data.groupby('Month').sum()
43/88: import matplotlib.pyplot as plt
43/89: results = all_data.groupby('Month').sum()
43/90:
import matplotlib.pyplot as plt

months = range(1, 13)

plt.bar(months, results['Sales'])
43/91:
import matplotlib.pyplot as plt

months = range(1, 13)

plt.bar(months, results['Sales'])
plt.show()
43/92:
import matplotlib.pyplot as plt

months = range(1, 13)

plt.bar(months, results['Sales'])
plt.xticks(months)
plt.show()
43/93:
import matplotlib.pyplot as plt

months = range(1, 13)

plt.bar(months, results['Sales'])
plt.xticks(months)
plt.ylabel('Sales in USD ($)')
plt.show()
43/94:
import matplotlib.pyplot as plt

months = range(1, 13)

plt.bar(months, results['Sales'])
plt.xticks(months)
plt.ylabel('Sales in USD ($)')
plt.xlabel('Month number')
plt.show()
43/95:
def get_city(address):
    return address.split(',')[1]

all_data['City'] = all_data['Purchase Address'].apply(lamda x: get_city(x))

all_data.head()
43/96:
def get_city(address):
    return address.split(',')[1]

all_data['City'] = all_data['Purchase Address'].apply(lambda x: get_city(x))

all_data.head()
43/97:
def get_city(address):
    return address.split(',')[1]

def get_state(address):
    return address.split(',')[2]

all_data['City'] = all_data['Purchase Address'].apply(lambda x: get_city(x) + ' ' + get_state(x))

all_data.head()
43/98:
def get_city(address):
    return address.split(',')[1]

def get_state(address):
    return address.split(',')[2].split(' ')[1]

all_data['City'] = all_data['Purchase Address'].apply(lambda x: get_city(x) + ' ' + get_state(x))

all_data.head()
43/99:
def get_city(address):
    return address.split(',')[1]

def get_state(address):
    return address.split(',')[2].split(' ')[1]

all_data['City'] = all_data['Purchase Address'].apply(lambda x: get_city(x) + ' (' + get_state(x) + ')')

all_data.head()
47/1: ### Sample note heading
47/2: ### Sample note heading
47/3: ### Sample note heading
47/4: ### Sample note heading
47/5: ### Sample note heading
47/6: ### Sample note heading
47/7: ### Sample note heading
47/8: ### Sample note heading
47/9: ### Sample note heading
50/1: print("Hello, World!"
50/2: print("Hello, World!")
50/3:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
50/4:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
50/5:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
50/6:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
50/7:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
50/8:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
50/9:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
50/10:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
50/11:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
50/12:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
50/13:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
50/14:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
50/15:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
50/16:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/1:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/2:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
54/3:
path_to_zip_dataset = "dataset.zip"
directory_to_extract_to = ""

import zipfile
zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
zip_ref.extractall(directory_to_extract_to)
zip_ref.close()
54/4:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/5:
#path_to_zip_dataset = "dataset.zip"
#directory_to_extract_to = ""

#import zipfile
#zip_ref = zipfile.ZipFile(path_to_zip_dataset, 'r')
#zip_ref.extractall(directory_to_extract_to)
#zip_ref.close()
54/6:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/7:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/8:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/9:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/10:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/11:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head()
54/12:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location, header=None)
df.head(5)
54/13:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
df = pd.read_csv(location)
df.head(5)
54/14:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
# display the first five rows with the headers
# df = pd.read_csv(location)
# displays the rows without headers (suitable for files without headers)
# df = pd.read_csv(location, header=None)

# Loading data with headers specified
df = pd.read_csv(location, names=['Region', 'Country', 'Item Type'])
df.head(5)
54/15:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
# display the first five rows with the headers
# df = pd.read_csv(location)
# displays the rows without headers (suitable for files without headers)
# df = pd.read_csv(location, header=None)

# Loading data with headers specified
df = pd.read_csv(location, names=['Region 1', 'Country 1', 'Item Type 1'])
df.columns = ['Region 1', 'Country 1', 'Item Type 1'])
df.head(5)
54/16:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
# display the first five rows with the headers
# df = pd.read_csv(location)
# displays the rows without headers (suitable for files without headers)
# df = pd.read_csv(location, header=None)

# Loading data with headers specified
df = pd.read_csv(location, names=['Region 1', 'Country 1', 'Item Type 1'])
df.columns = ['Region 1', 'Country 1', 'Item Type 1'])
df.head(5)
54/17:
import pandas as pd
location = "datasets/csv/SalesRecords.csv"
# display the first five rows with the headers
# df = pd.read_csv(location)
# displays the rows without headers (suitable for files without headers)
# df = pd.read_csv(location, header=None)

# Loading data with headers specified
df = pd.read_csv(location, names=['Region 1', 'Country 1', 'Item Type 1'])
df.columns = ['Region 1', 'Country 1', 'Item Type 1']
df.head(5)
54/18:
import pandas as pd
alabama_dataset = "datasets/csv/AlabamaState.csv"
df = pd.read_csv(alabama_dataset)
df.head()
54/19:
import pandas as pd
alabama_dataset = "datasets/csv/AlabamaState.csv"
df = pd.read_csv(alabama_dataset)
df.head()
54/20:
import pandas as pd
alabamadataset = "datasets/csv/AlabamaState.csv"
df = pd.read_csv(alabamadataset)
df.head()
55/1:
import pandas as pd
alabamadataset = "datasets/csv/AlabamaState.csv"
df = pd.read_csv(alabamadataset)
df.head()
55/2:
import pandas as pd
alabamadataset = "datasets/csv/AlabamaState.csv"
df = pd.read_csv(alabamadataset)
df.head(5)
55/3:
import pandas as pd
alabama_dataset = "datasets/csv/AlabamaState.csv"
df = pd.read_csv(alabama_dataset)
df.head(5)
55/4:
import pandas as pd
alabama_dataset = "datasets/csv/AlabamaSate.csv"
df = pd.read_csv(alabama_dataset)
df.head(5)
55/5:
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data=stateDetails, columns=[titles, values])
df.to_csv('datasets/csv/states_info.csv')
55/6:
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data=stateDetails, columns=[titles, values])
df.to_csv('datasets/csv/states_info.csv', index=False, header=False)
55/7:
import pandas as pd
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data = stateDetails, columns=[titles, values])
df.to_csv('datasets/csv/states_info.csv', index=False, header=False)
55/8:
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data = stateDetails, columns=[titles, values])
df.to_csv('datasets/csv/states_info.csv', index=False, header=False)
56/1:
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data = stateDetails, columns=['titles', 'values'])
df.to_csv('datasets/csv/states_info.csv', index=False, header=False)
56/2:
import pandas as pd
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data = stateDetails, columns=['titles', 'values'])
df.to_csv('datasets/csv/states_info.csv', index=False, header=False)
56/3:
import pandas as pd
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data = stateDetails, columns=['titles', 'values'])
df.to_csv('datasets/output/states_info.csv', index=False, header=False)
56/4:
import pandas as pd
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data = stateDetails, columns=['titles', 'values'])
df.to_csv('datasets/csv/states_info.csv', index=False, header=False)
56/5:
import pandas as pd
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data = stateDetails, columns=['titles', 'values'])
df.to_csv('datasets/csv/states_info.csv', index=False, header=False)
56/6:
import pandas as pd
titles = ['Name', 'POP100', 'HU100']
values = ['Alabama', '4779736', '2171853']
stateDetails = zip(titles, values)
df = pd.DataFrame(data = stateDetails, columns=['titles', 'values'])
df.to_csv('datasets/csv/states_info.csv', index=False, header=False)
56/7:
import pandas as pd
alabama_dataset = "datasets/csv/AlabamaSate.csv"
df = pd.read_csv(alabama_dataset)
df.head(5)
56/8:
import pandas as pd
df = pd.read_csv("datasets/csv/state_info.csv")
df.head()
56/9:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv")
df.head()
56/10:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index=False)
df.head()
56/11:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index_col=None)
df.head()
56/12:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index_col=None)
df.head()
56/13:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv")
df.head(index=None)
56/14:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv")
df.head()
56/15:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index_col=Name)
df.head()
56/16:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index_col='Name')
df.head()
56/17:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index_col='')
df.head()
56/18:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index_col='None')
df.head()
56/19:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index_col=None)
df.head()
56/20:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv", index_col=None)
df.head()
56/21:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv")
df.head()
56/22:
import pandas as pd
df = pd.read_csv("datasets/csv/states_info.csv")
df.head()
56/23:
import pandas as pd
location = "datasets/excel/fbiDataset.xls"
df = pd.read_excel(location)
df.head()
56/24:
import pandas as pd
location = "datasets/excel/fbiDataset.xls" 
df = pd.read_excel(location)
df.head()
56/25:
import pandas as pd
location = "datasets/excel/fbiDataset.xls" 
df = pd.read_excel(location)
df.head()
56/26:
import pandas as pd
location = "datasets/excel/fbiDataset.xlsx" 
df = pd.read_excel(location)
df.head()
56/27:
import pandas as pd
location = "datasets/excel/fbiDataset.xls" 
df = pd.read_excel(location)
df.head()
56/28:
import pandas as pd
location = "datasets/excel/fbiDataset.xls" 
df = pd.read_excel("datasets/excel/fbiDataset.xls")
df.head()
56/29:
import pandas as pd
location = "datasets/excel/fbiDataset.xls" 
df = pd.read_excel("datasets/excel/fbiDataset.xls")
df.head()
56/30:
import pandas as pd
location = "datasets/excel/fbiDataset.xls" 
df = pd.read_excel("datasets/excel/fbiDataset.xls")
df.head()
56/31:
import pandas as pd
location = "datasets/excel/fbiDataset.xls" 
df = pd.read_excel(location)
df.head()
56/32:
import zipfile
location_zip = "datasets/zip/Ref.zip"
zp = zipfile.ZipFile(location_zip, 'r')
zp.extractall("datasets/zip")
56/33:
import pandas as pd
location = "datasets/excel/Mastdata.xls"
df = pd.read_csv(location)
df.head()
56/34:
import pandas as pd
location = "datasets/zip/Mastdata.xls"
df = pd.read_csv(location)
df.head()
56/35:
import pandas as pd
location = "datasets/zip/Mastdata.xls"
df = pd.read_excel(location)
df.head()
56/36:
import pandas as pd
titles = [ 'Item ID', 'Item Description' ]
values = [ 'AFN110197D', 'Accommodation and Food Services' ]
dataList = zip(titles, values)
df = pd.DataFrame(data=dataList, columns=['titles', 'values'])
writer = pd.ExcelWriter('datasets/zip/dataframe.xlsx', engine='xlsxwriter')
df.to_excel(writer, sheet_name='Sheet1')
writer.save()
56/37:
import pandas as pd
import xlsxwriter as ExcelWriter
titles = [ 'Item ID', 'Item Description' ]
values = [ 'AFN110197D', 'Accommodation and Food Services' ]
dataList = zip(titles, values)
df = pd.DataFrame(data=dataList, columns=['titles', 'values'])
writer = pd.ExcelWriter('datasets/zip/dataframe.xlsx', engine='xlsxwriter')
df.to_excel(writer, sheet_name='Sheet1')
writer.save()
56/38:
import pandas as pd
import xlsxwriter as ExcelWriter
titles = [ 'Item ID', 'Item Description' ]
values = [ 'AFN110197D', 'Accommodation and Food Services' ]
dataList = zip(titles, values)
df = pd.DataFrame(data=dataList, columns=['titles', 'values'])
writer = pd.ExcelWriter('datasets/zip/dataframe.xlsx', engine='xlsxwriter')
df.to_excel('datasets/zip/dataframe.xls', sheet_name='Sheet1')
writer.save()
56/39:
import pandas as pd
titles = [ 'Item ID', 'Item Description' ]
values = [ 'AFN110197D', 'Accommodation and Food Services' ]
dataList = zip(titles, values)
df = pd.DataFrame(data=dataList, columns=['titles', 'values'])
writer = pd.ExcelWriter('datasets/zip/dataframe.xlsx', engine='xlsxwriter')
df.to_excel('datasets/zip/dataframe.xls', sheet_name='Sheet1')
writer.save()
56/40:
import pandas as pd
titles = [ 'Item ID', 'Item Description' ]
values = [ 'AFN110197D', 'Accommodation and Food Services' ]
dataList = zip(titles, values)
df = pd.DataFrame(data=dataList, columns=['titles', 'values'])
# writer = pd.ExcelWriter('datasets/zip/dataframe.xlsx', engine='xlsxwriter')
df.to_excel('datasets/zip/dataframe.xls', sheet_name='Sheet1')
writer.save()
56/41:
import pandas as pd
titles = [ 'Item ID', 'Item Description' ]
values = [ 'AFN110197D', 'Accommodation and Food Services' ]
dataList = zip(titles, values)
df = pd.DataFrame(data=dataList, columns=['titles', 'values'])
writer = pd.ExcelWriter('datasets/zip/dataframe.xlsx', engine='xlsxwriter')
df.to_excel('datasets/zip/dataframe.xls', sheet_name='Sheet1')
writer.save()
56/42:
import pandas as pd
titles = [ 'Item ID', 'Item Description' ]
values = [ 'AFN110197D', 'Accommodation and Food Services' ]
dataList = zip(titles, values)
df = pd.DataFrame(data=dataList, columns=['titles', 'values'])
writer = pd.ExcelWriter('datasets/zip/dataframe.xlsx', engine='xlsxwriter')
df.to_excel('datasets/zip/dataframe.xls', sheet_name='Sheet1')
writer.save()
56/43:
import pandas as pd
titles = [ 'Item ID', 'Item Description' ]
values = [ 'AFN110197D', 'Accommodation and Food Services' ]
dataList = zip(titles, values)
df = pd.DataFrame(data=dataList, columns=['titles', 'values'])
writer = pd.ExcelWriter('datasets/zip/dataframe.xlsx', engine='xlsxwriter')
df.to_excel('datasets/zip/dataframe.xls', sheet_name='Sheet1')
writer.save()
56/44:
import pandas as pd
import numpy as np
import glob

all_data = pd.DataFrame()
for f in glob.glob('datasets/Mast*.xls'):
    df = pd.read_excel(f)
    all_data = all_data.append(df, ignore_index=True)

all_data.describe()
56/45:
import pandas as pd
import numpy as np
import glob

all_data = pd.DataFrame()
for f in glob.glob('datasets/Mast*.xls'):
    df = pd.read_excel(f)
    all_data = all_data.append(df, ignore_index=True)

all_data.head()
56/46:
import pandas as pd
import numpy as np
import glob

all_data = pd.DataFrame()
for f in glob.glob('datasets/Mast*.xls'):
    df = pd.read_excel(f)
    all_data = all_data.append(df, ignore_index=True)

all_data.head()
56/47:
import pandas as pd
import numpy as np
import glob

all_data = pd.DataFrame()
for f in glob.glob('datasets/Mast*.xlsx'):
    df = pd.read_excel(f)
    all_data = all_data.append(df, ignore_index=True)

all_data.describe()
56/48:
import pandas as pd
import numpy as np
import glob

all_data = pd.DataFrame()
for f in glob.glob('datasets/Mast*.xls'):
    df = pd.read_excel(f)
    all_data = all_data.append(df, ignore_index=True)

all_data.head()
58/1: print(amazon_cart)
63/1: print(my_dict)
78/1:
import pandas as pd
pd.read_csv('data.csv')
78/2:
import pandas as pd
data_frame = pd.read_csv('data.csv')
data_frame.head()
78/3:
import pandas as pd
data_frame = pd.read_csv('data.csv')
data_frame.shape
78/4: data_frame.describe()
78/5: data_frame.values()
78/6: data_frame.values
78/7: data_frame[data_frame['Age'] > 40].head()
78/8: data_frame[[Age]]
78/9: data_frame[[Age, Club]]
78/10: data_frame.head()
78/11: data_frame[Age]
78/12: data_frame['Age']
78/13: data_frame[['Age', 'Wage']]
78/14: data_frame[['Club', 'Age', 'Value', 'Wage']]
78/15: data_frame[['Name', 'Club', 'Age', 'Value', 'Wage']]
78/16:
df1 = pd.DataFrame(data_frame, columns=['Name', 'Wage', 'Value'])
df1
78/17:
df1 = pd.DataFrame(data_frame, columns=['Name', 'Wage', 'Value'])
df1['difference'] = df1['Value'] = df1['Wage']
df1
78/18:
df1 = pd.DataFrame(data_frame, columns=['Name', 'Wage', 'Value'])
df1['difference'] = df1['Value'] - df1['Wage']
df1
78/19:
df1 = pd.DataFrame(data_frame, columns=['Name', 'Wage', 'Value'])

def value_to_float(x):
    if type(x) == float or type(x) == int:
        return x
    if 'K' in x:
        if len(x) > 1:
            return float(x.replace('K', '')) * 1000
        return 1000.0
    if 'M' in x:
        if len(x) > 1:
            return float(x.replace('M', '')) * 1000000
        return 1000000.0
    if 'B' in x:
        return float(x.replace('B', '')) * 1000000000
    return 0.0

wage = df1['Wage'].replace('[\,]', '', regex=True).apply(value_to_float)
value = df1['Value'].replace('[\,]', '', regex=True).apply(value_to_float)

df1['Wage'] = wage
df1['Value'] = value

df1['difference'] = df1['Value'] - df1['Wage']
df1
78/20:
df1 = pd.DataFrame(data_frame, columns=['Name', 'Wage', 'Value'])

def value_to_float(x):
    if type(x) == float or type(x) == int:
        return x
    if 'K' in x:
        if len(x) > 1:
            return float(x.replace('K', '')) * 1000
        return 1000.0
    if 'M' in x:
        if len(x) > 1:
            return float(x.replace('M', '')) * 1000000
        return 1000000.0
    if 'B' in x:
        return float(x.replace('B', '')) * 1000000000
    return 0.0

wage = df1['Wage'].replace('[\,]', '', regex=True).apply(value_to_float)
value = df1['Value'].replace('[\,]', '', regex=True).apply(value_to_float)

df1['Wage'] = wage
df1['Value'] = value

df1['difference'] = df1['Value'] - df1['Wage']
df1.sort_values('difference', ascending=False)
78/21:
import seaborn as sns
sns.set()

graph = sns.scatterplot(x='Wage', y='Value', data=df1)
78/22:
import seaborn as sns
sns.set()

graph = sns.scatterplot(x='Wage', y='Value', data=df1)
78/23:
from bokeh.plotting import figure, show
from bokeh.model import HoverTool

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[hover])
78/24:
from bokeh.plotting import figure, show
from bokeh.model import HoverTool

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[])
78/25:
from bokeh.plotting import figure, show
from bokeh.model import HoverTools

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[])
78/26:
from bokeh.plotting import figure, show
from bokeh.model import HoverTool

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[])
78/27:
from bokeh.plotting import figure, show
from bokeh.models import HoverTool

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[])
78/28:
from bokeh.plotting import figure, show
from bokeh.models import HoverTool

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[])
show(p)
78/29:
from bokeh.plotting import figure, show
from bokeh.models import HoverTool

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[])
p.circle('Wage', 'Values', size=10, source=df1)
show(p)
78/30:
from bokeh.plotting import figure, show
from bokeh.models import HoverTool

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[])
p.circle('Wage', 'Value', size=10, source=df1)
show(p)
78/31:
from bokeh.plotting import figure, show
from bokeh.models import HoverTool

TOOLTIPS= HoverTool(tooltips = [
    ("index", "$index"),
    ("(Wage, Value)", "(@Wage, @Value)")
    ("Name", "@Name")
])

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[TOOLTIPS])
p.circle('Wage', 'Value', size=10, source=df1)
show(p)
78/32:
from bokeh.plotting import figure, show
from bokeh.models import HoverTool

TOOLTIPS= HoverTool(tooltips=[
    ("index", "$index"),
    ("(Wage, Value)", "(@Wage, @Value)")
    ("Name", "@Name")]
)

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[TOOLTIPS])
p.circle('Wage', 'Value', size=10, source=df1)
show(p)
78/33:
from bokeh.plotting import figure, show
from bokeh.models import HoverTool

TOOLTIPS= HoverTool(tooltips=[
    ("index", "$index"),
    ("(Wage, Value)", "(@Wage, @Value)"),
    ("Name", "@Name")]
)

p = figure(title="Soccer 2019", x_axis_label='Wage', y_axis_label='Value', plot_width=700, plot_height=700, tools=[TOOLTIPS])
p.circle('Wage', 'Value', size=10, source=df1)
show(p)
79/1:
from sklearn.datasets import load_iris
iris = load_iris()
79/2: iris.data
79/3:
iris.data
iris.target
79/4:
X = iris.data
y = iris.target

feature_names = iris.feature_names
target_names = iris.target_names
79/5:
X = iris.data
y = iris.target

feature_names = iris.feature_names
target_names = iris.target_names

feature_names
79/6:
X = iris.data
y = iris.target

feature_names = iris.feature_names
target_names = iris.target_names

feature_names
target_names
79/7:
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.4)

print(X_train.shape)
79/8:
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.4)

print(X_train.shape)
print(X_test.shape)
79/9:
from sklearn.neighbors import kNeighborsClassifier

knn = kNeighborsClassifier(n_neighbors=3)
knn.fit(X_tain, y_train)
y_pred = knn.predict(X_test)
79/10:
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_tain, y_train)
y_pred = knn.predict(X_test)
79/11:
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_tain, y_train)
y_pred = knn.predict(X_test)
79/12:
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
79/13:
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)

print(X_train.shape)
print(X_test.shape)
79/14:
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
79/15:
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
79/16:
from sklearn import metrics
print(metrics.accuracy(y_test, y_pred))
79/17:
from sklearn import metrics
print(metrics.accuracy_score(y_test, y_pred))
79/18:
sample = [[3, 5, 4, 2], [2, 3, 5, 4]]
predictions = knn.predict(sample)
pred_species = [iris.target_names[p] for p in predictions]
print("predictions: ", pred_species)
79/19:
from sklearn.externals import joblib
 model = joblib.dump(knn, 'mlbrain.jonlib')
79/20:
from sklearn.externals import joblib
model = joblib.dump(knn, 'mlbrain.jonlib')
79/21:
from sklearn.externals import joblib
joblib.dump(knn, 'mlbrain.jonlib')
79/22:
sample = [[3, 5, 4, 2], [2, 3, 5, 4]]
predictions = knn.predict(sample)
pred_species = [iris.target_names[p] for p in predictions]
print("predictions: ", pred_species)
79/23:
model = joblib.load('mlbrain.joblib')
sample = [[3, 5, 4, 2], [2, 3, 5, 4]]
predictions = model.predict(sample)
pred_species = [iris.target_names[p] for p in predictions]
print("predictions: ", pred_species)
79/24:
model = joblib.load('mlbrain.joblib')
model.predict('X_test')
sample = [[3, 5, 4, 2], [2, 3, 5, 4]]
predictions = model.predict(sample)
pred_species = [iris.target_names[p] for p in predictions]
print("predictions: ", pred_species)
79/25:
from sklearn.externals import joblib
joblib.dump(knn, 'mlbrain.joblib')
79/26:
model = joblib.load('mlbrain.joblib')
sample = [[3, 5, 4, 2], [2, 3, 5, 4]]
predictions = model.predict(sample)
pred_species = [iris.target_names[p] for p in predictions]
print("predictions: ", pred_species)
80/1: import pandas as pd
80/2:
df = pd.read_csv('heart-disease.csv')
df.head()
80/3:
df = pd.read_csv('heart-disease.csv')
df.head(10)
80/4: df = pd.read_csv('heart-disease.csv')
80/5: df.head(10)
80/6: df.target.value_counts().plot(kind='bar')
80/7: df.target.value_counts().plot(kind='line')
80/8: df.target.value_counts().plot(kind='pie')
80/9: df.target.value_counts().plot(kind='bar')
80/10: !ls
80/11: ls
81/1: import pandas as pd
81/2:
# 2 main datatypes
series = pd.Series(['BMW', 'Toyota', 'Honda'])
81/3: series
81/4: # series = 1 dimensional
81/5: colours = pd.Series(['Red', 'Blue', 'White'])
81/6: coluors
81/7: colours
81/8: # DataFrame = 2-dimensional
81/9:
car_data = pd.DataFrame({'Car Make': series, 'Colours': colours})
car_data
81/10:
# import data
car_sales = pd.read_csv('car-sales.csv')
81/11: car_sales
81/12:
# Exporting a dataframe
car_sale.to_csv('exported-car-sales.csv')
81/13:
# Exporting a dataframe
car_sales.to_csv('exported-car-sales.csv')
81/14: !ls
81/15:
exported_car_sales = pd.read_csv('exported-car-sales.csv')
exported_car_sales
81/16:
# Exporting a dataframe
car_sales.to_csv('exported-car-sales.csv', index=False)
81/17: !ls
81/18:
exported_car_sales = pd.read_csv('exported-car-sales.csv')
exported_car_sales
81/19:
# Attribute
car_sales.dtypes
81/20:
# Attribute
car_sales.dtypes

# Function
# car_sales.to_csv
81/21: car_sales.columns
81/22: car_columns = car_sales.columns
81/23:
car_columns = car_sales.columns
car_columns
81/24: car_sales.index
81/25: car_sales.describe()
81/26: car_sales.info()
81/27: car_sales.mean()
81/28:
car_prices = pd.Series([3000, 1500, 111250])
car_prices.mean()
81/29: car_sales.sum()
81/30: car_sales['Doors'].sum()
81/31: len(car_sales)
81/32: car_sales
81/33: car_sales.head()
81/34: car_sales.head(10)
81/35: car_sales.head(11)
81/36: car_sales.head(100)
81/37: car_sales.head()
81/38: car_sales.tail()
81/39: car_sales.tail(2)
81/40: car_sales.tail(10)
81/41: car_sales.tail()
81/42:
# .loc & .iloc
animals = pd.Series(['cat', 'dog', 'bird', 'panda', 'Snake'])
81/43: animals
81/44:
# .loc & .iloc
animals = pd.Series(['cat', 'dog', 'bird', 'panda', 'Snake'], index=[0, 3, 9, 8, 3])
81/45: animals
81/46: animals.loc[3]
81/47: animals.loc[9]
81/48: car_sales
81/49: car_sales.loc[1]
81/50:
# .iloc
animals.iloc[3]
81/51: animals
81/52:
# .loc refers to position & .iloc refers to index
animals = pd.Series(['cat', 'dog', 'bird', 'panda', 'Snake'], index=[0, 3, 9, 8, 3])
81/53: animals.iloc[:3]
81/54: car_sales[:5]
81/55: car_sales.loc[:5]
81/56: car_sales.iloc[:5]
81/57: car_sales['Make']
81/58: car_sales[car_sales['Make'] == 'Toyota']
81/59: pd.crosstab(car_sales['Make'], car_sales['Doors'])
81/60:
# Groupby
car_sales.groupby(['Make']).mean()
81/61:
# Groupby
car_sales.groupby(['Make']).sum()
81/62:
# Groupby
car_sales.groupby(['Make']).sub()
81/63:
# Groupby
car_sales.groupby(['Make']).diff()
81/64:
# Groupby
car_sales.groupby(['Make']).sum()
81/65:
# Groupby
car_sales.groupby(['Make']).mean()
81/66: car_sales['Odometer (KM)'].mean()
81/67: car_sales['Odometer (KM)'].plot()
81/68: %matplotlib inline
81/69: car_sales['Odometer (KM)'].plot()
81/70: car_sales.groupBy(['Make']).mean().plot()
81/71: car_sales.groupBy(['Make']).plot()
81/72: car_sales.groupBy(['Make'].plot())
81/73: car_sales['Odometer (KM)'].hist()
81/74: car_sales['price'].plot()
81/75: car_sales['Price'].plot()
81/76: car_sales['Price'] = car_sales['Price'].str.replace('[\$\,\.]', '').astype(int)
81/77: car_sales['Price'] = car_sales['Price'].str.replace('[\$\,\.]', '').astype(int)
81/78: car_sales.dtypes
81/79: car_sales['Price'] = (str(car_sales['Price'])).str.replace('[\$\,\.]', '').astype(int)
81/80:
car_sales['Price'] = (str(car_sales['Price']))
car_sales.dtypes
81/81: car_sales['Price'] = (str(car_sales['Price'])).str.replace('[\$\,\.]', '').astype(int)
81/82: car_sales.head()
81/83: car_sales.
81/84: car_sales
81/85:
# import data
car_sales = pd.read_csv('car-sales.csv')
81/86: car_sales
81/87:
# Exporting a dataframe
car_sales.to_csv('exported-car-sales.csv', index=False)
81/88: !ls
81/89:
exported_car_sales = pd.read_csv('exported-car-sales.csv')
exported_car_sales
81/90:
# Attribute
car_sales.dtypes

# Function
# car_sales.to_csv
81/91: car_sales.columns
81/92:
car_columns = car_sales.columns
car_columns
81/93: car_sales.index
81/94: car_sales.describe()
81/95: car_sales.info()
81/96: car_sales.mean()
81/97:
car_prices = pd.Series([3000, 1500, 111250])
car_prices.mean()
81/98: car_sales['Doors'].sum()
81/99: len(car_sales)
81/100: car_sales
81/101: car_sales.dtypes
81/102: car_sales
81/103: car_sales['Price'] = (str(car_sales['Price'])).str.replace('[\$\,\.]', '').astype(int)
81/104: car_sales
81/105: car_sales['Price'] = (car_sales['Price']).str.replace('[\$\,\.]', '').astype(int)
81/106: car_sales['Price'].plot()
81/107: car_sales["Make"].str.lower()
81/108: car_sales["Make"].str.upper()
81/109: car_sales
81/110: car_sales["Make"] = car_sales["Make"].str.lower()
81/111: car_sales
81/112:
car_sales_missing = pd.read_csv('car-sales-missing-data.csv')
car_sales_missing
81/113: car_sales_missing["Odometer"].mean()
81/114: car_sales_missing["Odometer"].fillna(car_sales_missing["Odometer"].mean())
81/115: car_sales_missing
81/116: car_sales_missing["Odometer"] = car_sales_missing["Odometer"].fillna(car_sales_missing["Odometer"].mean())
81/117: car_sales_missing
81/118:
car_sales_missing = pd.read_csv('car-sales-missing-data.csv')
car_sales_missing
81/119: car_sales_missing["Odometer"] = car_sales_missing["Odometer"].fillna(car_sales_missing["Odometer"].mean(), inplace=True)
81/120: car_sales_missing
81/121:
car_sales_missing = pd.read_csv('car-sales-missing-data.csv')
car_sales_missing
81/122: car_sales_missing["Odometer"].mean()
81/123: car_sales_missing["Odometer"].fillna(car_sales_missing["Odometer"].mean(), inplace=True)
81/124: car_sales_missing
81/125:
car_sales_missing = pd.read_csv('car-sales-missing-data.csv')
car_sales_missing
81/126: car_sales_missing["Odometer"].mean()
81/127: car_sales_missing["Odometer"] = car_sales["Odometer"].fillna(car_sales_missing["Odometer"].mean())
81/128: car_sales_missing["Odometer"] = car_sales["Odometer"].fillna(car_sales_missing["Odometer"].mean())
81/129: car_sales_missing["Odometer"] = car_sales_missing["Odometer"].mean()
81/130: car_sales_missing
81/131:
car_sales_missing = pd.read_csv('car-sales-missing-data.csv')
car_sales_missing
81/132: car_sales_missing["Odometer"].fillna(car_sales_missing["Odometer"].mean(), inplace=True)
81/133: car_sales_missing_drop_na
81/134: car_sales_missing_drop_na()
81/135: car_sales_missing.drop_na()
81/136: car_sales_missing.dropna()
81/137: car_sales_missing
81/138: car_sales_missing.dropna(inplace=True)
81/139: car_sales_missing
81/140:
car_sales_missing = pd.read_csv('car-sales-missing-data.csv')
car_sales_missing
81/141:
car_sales_missing_dropped = car_sales_missing.dropna()
car_sales_missing_dropped
81/142: car_sales_missing_dropped.to_csv('car-sales-missing-dropped.csv')
81/143: ls
81/144:
# Column from series
seats_column = pd.Series([5, 5, 5, 5, 5])

# New column called sears
car_sales["Seats"] = seats_column
car_sales
81/145: car_sales["Seats"].fillna(5, inplace=True)
81/146: car_sales
81/147:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/148:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7, 4, 2, 3.6, 5.4]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/149:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7, 4, 2, 3.6, 5.4, 3.5, 2.4]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/150:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7, 4, 2, 3.6, 5.4, 3.5, 2.4, 10]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/151:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7, 4, 2, 3.6, 5.4, 3.5, 2.4, 10, 11, 12]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/152:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7, 4, 2, 3.6, 5.4, 3.5, 2.4, 10, 11, 12, 3.5, 12.2, 11.2]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/153:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7, 4, 2, 3.6, 5.4, 3.5, 2.4, 10.3, 11.2, 12.3, 3.5, 12.2, 11.2]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/154:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7, 4, 2, 3.6, 5.4, 3.5]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/155: car_sales["Total fuel used"] = car_sales["Odometer (KM)"] / 100 * car_sales['Fuel per 100KM']
81/156: car_sales
81/157: car_sales["Total fuel used (L)"] = car_sales["Odometer (KM)"] / 100 * car_sales['Fuel per 100KM']
81/158: car_sales
81/159: # Create a column from a single value
81/160:
# Create a column from a single value
car_sales["Number of wheels"] = 4
car_sales
81/161:
car_sales["Passed road safety"] = True
car_sales
81/162: car_sales.drop("Total fuel used(L)", axis=1)
81/163: car_sales.drop("Total fuel used (L)", axis=1)
81/164: car_sales
81/165: car_sales.drop("Total fuel used (L)", axis=1, inplace=True)
81/166: car_sales
81/167:
car_sales["Price X Seats"] = car_sales["Price"] * car_sales["Seats"]
car_sales
81/168: car_sales.drop("Price X Seats", inplace=True)
81/169:
car_sales.drop("Price X Seats", inplace=True)
car_sales
81/170:
car_sales.drop("Price X Seats", axis=1, inplace=True)
car_sales
81/171: car_sales.sample(frac=1)
81/172: car_sales = car_sales.sample(frac=1)
81/173: car_sales_shuffled = car_sales.sample(frac=1)
81/174:
car_sales_shuffled = car_sales.sample(frac=1)
car_sales_shuffled
81/175:
# import data
car_sales = pd.read_csv('car-sales.csv')
81/176:
# Only select 20% of the data
car_sales_shuffled.sample(frac=0.3)
81/177:
# Only select 20% of the data
car_sales_shuffled.sample(frac=0.2)
81/178: car_sales_shuffled
81/179: car_sales_shuffled.reset_index()
81/180: car_sales_shuffled.reset_index(index=False)
81/181: car_sales_shuffled.reset_index(drop=True, inplace=True)
81/182: car_sales_shuffled
81/183: car_sales
81/184:
car_sales["Passed road safety"] = True
car_sales
81/185:
# Create a column from a single value
car_sales["Number of wheels"] = 4
car_sales
81/186: car_sales["Total fuel used (L)"] = car_sales["Odometer (KM)"] / 100 * car_sales['Fuel per 100KM']
81/187:
# Column from Python list
fuel_economy = [7.5, 9.2, 5.0, 9.6, 8.7, 4, 2, 3.6, 5.4, 3.5]
car_sales["Fuel per 100KM"] = fuel_economy
car_sales
81/188: car_sales["Total fuel used (L)"] = car_sales["Odometer (KM)"] / 100 * car_sales['Fuel per 100KM']
81/189: car_sales
81/190:
car_sales["Odometer (KM)"] = car_sales["Odometer (KM)"].apply(lambda x: x / 1.6)
car_sales
81/191: car_sales['Price'] = (car_sales['Price']).str.replace('[\$\,\.]', '').astype(int)
81/192: car_sales['Price'].plot()
81/193:
car_sales["Odometer (KM)"] = car_sales["Odometer (KM)"].apply(lambda x: x / 1.6)
car_sales
82/1: import pandas as pd
82/2:
sample_series = pd.Series('Red', 'Orange', 'Green')
sample_series
82/3:
sample_series = pd.Series(['Red', 'Orange', 'Green'])
sample_series
82/4: sample_series = pd.Series(['Red', 'Orange', 'Green'])
82/5: sample_series = pd.Series(['Red', 'Orange', 'Green'])
82/6: sample_series
82/7: car_types = pd.Series(['Toyota', 'Honda', 'Mercedes', 'Nissan', 'Tesla'])
82/8: car_types
82/9: sample_colours = pd.Series(['Red', 'Orange', 'Green'])
82/10: sample_colours
82/11: car_data = pd.DataFrame(sample_colours, car_types)
82/12: car_data
82/13: car_data = pd.DataFrame({sample_colours, car_types})
82/14: car_data = pd.DataFrame({ 'colors': sample_colours, 'types': car_types})
82/15: car_data
82/16:
car_sales = pd.read_csv('car-sales.csv')
car_sales
82/17: car_sales.to_csv('export-car_sales.csv')
82/18: ls
82/19: car_sales.to_csv('export-car_sales')
82/20: ls
82/21: car_sales.to_csv('export-car_sales.csv')
82/22: ls
82/23: rm -r export-car_sales
82/24: ls
82/25: ls
82/26: car_sales.dtypes
82/27:
numbers = pd.Series(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
numbers.sum()
82/28:
numbers = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
numbers.sum()
82/29:
numbers = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
numbers.mean()
82/30: numbers.sum()
82/31: car_sales
82/32: len(car_sales)
82/33: car_sales
82/34: car_sales_columns = car_sales.columns
82/35:
car_sales_columns = car_sales.columns
car_sales_columns
82/36: car_sales.head()
82/37: car_sales.head(7)
82/38: car_sales.tail()
82/39: car_sales.loc(3)
82/40: car_sales.loc([:])
82/41: car_sales.loc([:3])
82/42: car_sales.loc([3])
82/43: car_sales.loc(car_sales[3])
82/44: car_sales.loc(3)
82/45: car_sales.loc([3])
82/46: car_sales.loc(['3'])
82/47: car_sales
82/48: car_sales.loc[3]
82/49: car_sales.iloc[3]
82/50: car_sales.iloc[:3]
82/51: car_sales.iloc[3]
82/52: car_sales.iloc[[3]]
82/53: car_sales.iloc[3]
82/54: car_sales
82/55: car_sales.loc[[3]]
82/56: car_sales.loc[3]
82/57: car_sales.iloc[3]
82/58: car_sales.iloc[:3]
82/59: car_sales.loc[:3]
82/60: car_sales.loc[3]
82/61: car_sales.iloc[3]
82/62: car_sales["Odometer (KM)"]
82/63: car_sales[["Odometer (KM)"]]
82/64: car_sales["Odometer (KM)"].mean()
82/65: car_sales[car_sales["Odometer (KM)"] > 100000]
82/66: car_sales["Make"].crosstab(car_sales["Doors"])
82/67: car_sales["Make"].crosstab([car_sales["Doors"]])
82/68: car_sales.crosstab([car_sales["Make"], car_sales["Doors"]])
82/69: make_and_doors = car_sales.crosstab([car_sales["Make"], car_sales["Doors"]])
82/70: make_and_doors = pd.crosstab([car_sales["Make"], car_sales["Doors"]])
82/71: make_and_doors = pd.crosstab(columns = [car_sales["Make"], car_sales["Doors"]])
82/72: make_and_doors = pd.crosstab(columns = [car_sales["Make"], car_sales["Doors"]], index=False)
82/73: make_and_doors
82/74: make_and_doors = pd.crosstab(columns = [car_sales["Make"], car_sales["Doors"]], index=True)
82/75: make_and_doors
82/76: make_and_doors = pd.crosstab( [car_sales["Make"], car_sales["Doors"]], index=True)
82/77: make_and_doors = pd.crosstab( [car_sales["Make"], car_sales["Doors"]], index=True, columns)
82/78: make_and_doors = pd.crosstab( [car_sales["Make"], car_sales["Doors"]], index=True, columns='')
82/79: make_and_doors = pd.crosstab( [car_sales["Make"], car_sales["Doors"]], columns='', index=True )
82/80: make_and_doors = pd.crosstab(columns=[car_sales["Make"], car_sales["Doors"]], index=True )
82/81: pd.crosstab(car_sales["Make"], car_sales["Doors"])
82/82: car_sales.groups()
82/83: car_sales.groups
82/84: car_sales.groups
82/85: car_sales.groups
82/86: car_sales.groupBy(["Make"])
82/87: car_sales.groupBy(car_sales["Make"])
82/88: car_sales.groupby(car_sales["Make"])
82/89: car_sales.groupby(["Make"])
82/90: car_groups = car_sales.groupby(car_sales["Make"])
82/91: car_groups
82/92: car_groups.mean()
82/93: car_groups
82/94: car_groups = car_sales.groupby(car_sales["Make"]).mean()
82/95: car_groups
82/96: car_sales["Odometer (KM)"].plot()
82/97: car_sales["Odometer (KM)"].bar()
82/98: car_sales["Odometer (KM)"].line()
82/99: car_sales["Odometer (KM)"].hist()
82/100: car_sales["Odometer (KM)"].pie()
82/101: car_sales["Odometer (KM)"].plot()
82/102: car_sales["Odometer (KM)"].hist()
82/103: car_sales
82/104: car_sales['Price'].plot()
82/105: car_sales["Price"].str.replace('(\D)', '')
82/106:
car_sales = car_sales["Price"].str.replace('(\D)', '')
car_sales
82/107:
car_sales = car_sales["Price"].str.replace('(\D)', '')
car_sales
82/108:
car_sales["Price"] = car_sales["Price"].str.replace('(\D)', '')
car_sales
82/109:
# car_sales["Price"] = car_sales["Price"].str.replace('(\D)', '')
car_sales
82/110: car_sales.to_csv('export-car_sales.csv')
82/111:
car_sales["Price"] = car_sales["Price"].str.replace('(\D)', '')
car_sales
82/112:
# car_sales["Price"] = car_sales["Price"].str.replace('(\D)', '')
car_sales
82/113:
car_sales = pd.read_csv('car-sales.csv')
car_sales
82/114:
# car_sales["Price"] = car_sales["Price"].str.replace('(\D)', '')
car_sales
82/115:
car_sales["Price"] = car_sales["Price"].str.replace('(\D)', '')
car_sales
82/116: car_sales['Price'].plot()
82/117:
car_sales["Price"] = car_sales["Price"].str.replace('(\D)', '').astype(int)
car_sales
82/118: car_sales['Price'].plot()
82/119: car_sales['Price'].hist()
82/120:
car_sales = pd.read_csv('car-sales.csv')
car_sales
82/121:
car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '').astype(int)
car_sales
82/122:
car_sales = pd.read_csv('car-sales.csv')
car_sales
82/123:
car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '').astype(int)
car_sales
82/124:
car_sales["Price"] = int(car_sales["Price"].str.replace('[\$,]', ''))
car_sales
82/125:
car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '').astype(int)
car_sales
82/126:
def remove_trailing_zeros(price):
    return int(price)

print(remove_trailing_zeros('400.00'))
82/127:
import math
def remove_trailing_zeros(price):
    return int(math.floor(price))

print(remove_trailing_zeros('400.00'))
82/128:
import math
def remove_trailing_zeros(price):
    price = int(price)
    return int(math.floor(price))

print(remove_trailing_zeros('400.00'))
82/129:
import re
tail_dot_regx = re.compile(r'(?:(\.)|(\.\d*?[1-9]\d*?))0+(?=\b|[^0-9])')
def remove_trailing_zeros(price):
    return tail_dot_regx.sub(r'\2', price)

print(remove_trailing_zeros('400.00'))
82/130:
car_sales["Price"] = remove_trailing_zeros(car_sales["Price"])
car_sales
82/131:
car_sales["Price"] = remove_trailing_zeros(car_sales["Price"])
car_sales
82/132:
import re
tail_dot_regx = re.compile(r'(?:(\.)|(\.\d*?[1-9]\d*?))0+(?=\b|[^0-9])')
def remove_trailing_zeros(price):
    return tail_dot_regx.sub(r'\2', price)
82/133:
import re
tail_dot_regx = re.compile(r'(?:(\.)|(\.\d*?[1-9]\d*?))0+(?=\b|[^0-9])')
def remove_trailing_zeros(price):
    return tail_dot_regx.sub(r'\2', price)
82/134:
car_sales["Price"] = remove_trailing_zeros(car_sales["Price"])
car_sales
82/135: car_sales
82/136: car_sales["Price"]
82/137:
car_sales["Price"]= remove_trailing_zeros(car_sales["Price"])
car_sales
82/138:
import re
tail_dot_regx = re.compile(r'(?:(\.)|(\.\d*?[1-9]\d*?))0+(?=\b|[^0-9])')
def remove_trailing_zeros(price):
    return tail_dot_regx.sub(r'\2', price)


car_sales["Price"]= remove_trailing_zeros(car_sales["Price"])
car_sales
82/139:
import re
tail_dot_regx = re.compile(r'(?:(\.)|(\.\d*?[1-9]\d*?))0+(?=\b|[^0-9])')

def remove_trailing_zeros(price):
    return tail_dot_regx.sub(r'\2', price)


car_sales["Price"]= remove_trailing_zeros(str(car_sales["Price"]))
car_sales
82/140:
car_sales = pd.read_csv('car-sales.csv')
car_sales
82/141: car_sales["Price"].str.replace('[\$,]', '')
82/142:
def remove_trailing_zeros(price):
    return price.str.replace('^(\d+\.\d*?[1-9])0+$', '').astype(int)

print(remove_trailing_zeros('400.00'))
82/143:
def remove_trailing_zeros(price):
    return price.replace('^(\d+\.\d*?[1-9])0+$', '').astype(int)

print(remove_trailing_zeros('400.00'))
82/144:
def remove_trailing_zeros(price):
    return price.replace('^(\d+\.\d*?[1-9])0+$', '')

print(remove_trailing_zeros('400.00'))
82/145:
def remove_trailing_zeros(price):
    return type(price) # .replace('^(\d+\.\d*?[1-9])0+$', '')

print(remove_trailing_zeros('400.00'))
82/146:
def remove_trailing_zeros(price):
    return price.replace('^(\d+\.\d*?[1-9])0+$', '')

print(remove_trailing_zeros('400.00'))
82/147:
def remove_trailing_zeros(price):
    return price.replace('(\.\d*?[1-9])0+$', '')

print(remove_trailing_zeros('400.00'))
82/148:
import re

tail_dot_rgx = re.compile(r'(?:(\.)|(\.\d*?[1-9]\d*?))0+(?=\b|[^0-9])')

def remove_tail_dot_zeros(a):
    return tail_dot_rgx.sub(r'\2',a)
82/149:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
# car_sales
print(remove_tail_dot_zeros(car_sales["Price"]))
82/150:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
# car_sales
print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/151:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_sales["Price"].str.replace('[\$,]', '')
# car_sales
print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/152:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_sales["Price"].str.replace('[\$,]', '')
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/153:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
type(car_sales["Price"].str.replace('[\$,]', ''))
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/154:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
type(car_sales["Price"].str.replace('[\$,]', ''))
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/155:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
type(car_sales["Price"].str.replace('[\$,]', '')[0])
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/156:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_sales["Price"].str.replace('[\$,]', '')
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/157:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_sales["Price"].str.replace('[\$,]', '').astype(int)
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/158:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
int(car_sales["Price"].str.replace('[\$,]', ''))
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/159:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
int(car_sales["Price"].str.replace('[\$,]', '')[0])
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/160:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_sales["Price"].str.replace('[\$,]', '')
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/161:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
Decimal(car_sales["Price"].str.replace('[\$,]', ''))
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/162:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_sales["Price"].str.replace('[\$,]', '').rstrip('0')
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/163:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_sales["Price"].str.replace('[\$,]', '')
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/164:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_sales["Price"].str.replace('[\$,]', '')[0].rstrip('0')
# car_sales
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/165:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_prices = car_sales["Price"].str.replace('[\$,]', '')
car_prices = car_prices[0].rstrip('0').rstrip('.') if '.' in car_prices[0] else car_prices[0]
car_prices
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/166:
def remove_trailing_zeros(prices):
    for price in prices:
        car_price = price.str.replace('[\$,]', '')
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return int(car_price)

print(remove_trailing_zeros('400.00'))
82/167:
def remove_trailing_zeros(prices):
    for price in prices:
        car_price = price.str.replace('[\$,]', '')
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return int(car_price)

print(remove_trailing_zeros('$400.00'))
82/168:
def remove_trailing_zeros(prices):
    for price in prices:
        car_price = price.str.replace('[\$,]', '')
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return (car_price)

print(remove_trailing_zeros('$4,000.00'))
82/169:
def remove_trailing_zeros(prices):
    for price in prices:
        car_price = price.str.replace('[\$,]', '')
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros('$4,000.00'))
82/170:
def remove_trailing_zeros(prices):
    for price in prices:
        car_price = price.replace('[\$,]', '')
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros('$4,000.00'))
82/171:
def remove_trailing_zeros(prices):
    for price in prices:
        car_price = price.replace('[\$,]', '')
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros('$4,000.00'))
82/172:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
        car_prices.append(car_price)
    return car_prices

print(remove_trailing_zeros('$4,000.00'))
82/173:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros('$4,000.00'))
82/174:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros('$4,000.00'))
82/175:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros('$4,000.00'))
82/176:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros(['$4,000.00', '$5,000.00']))
82/177:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros(['$4,000.00', '$5,000.00']))
82/178:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/179:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/180:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.str.replace('[\$,]', '')
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/181:
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace('[\$,]', '')
        car_prices.append(car_price)
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_prices

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/182:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = price.replace(r'[\$,]', '')
        car_prices.append(car_price)
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_prices

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/183:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = re.sub(r"[\$,]", '', price)
        car_prices.append(car_price)
#         car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_prices

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/184:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = re.sub(r"[\$,]", '', price)  
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
        car_prices.append(car_price)
    return car_prices

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/185:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = re.sub(r"[\$,]", '', price)  
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
        car_prices.append(int(car_price))
    return car_prices

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/186:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = re.sub(r"[\$,]", '', price)  
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros('$2,300.00')
82/187:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = re.sub(r"[\$,]", '', price)  
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
    return car_price

print(remove_trailing_zeros('$2,300.00'))
82/188:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    for item in price:  
        car_price = item.rstrip('0').rstrip('.') if '.' in price else price
    return car_price

print(remove_trailing_zeros('$2,300.00'))
82/189:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    for item in price:  
        car_price = item.rstrip('0').rstrip('.') if '.' in price else price
    return car_price

print(remove_trailing_zeros('$2,300.00'))
82/190:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
#     for item in car_price:  
    car_price = car_price.rstrip('0').rstrip('.') if '.' in price else price
    return car_price

print(remove_trailing_zeros('$2,300.00'))
82/191:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    return car_price.rstrip('0').rstrip('.') if '.' in price else price

print(remove_trailing_zeros('$2,300.00'))
82/192:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    return car_price.rstrip('0').rstrip('.') if '.' in price else price

print(type(remove_trailing_zeros('$2,300.00'))
82/193:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    return car_price.rstrip('0').rstrip('.') if '.' in price else price

print(type(remove_trailing_zeros('$2,300.00')))
82/194:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    return int(car_price.rstrip('0').rstrip('.') if '.' in price else price)

print(type(remove_trailing_zeros('$2,300.00')))
82/195:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    return int(car_price.rstrip('0').rstrip('.') if '.' in price else price)

print(remove_trailing_zeros('$2,300.00'))
82/196:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    return int(car_price.rstrip('0').rstrip('.') if '.' in price else price)

print(remove_trailing_zeros('$2,300.00'))
print(remove_trailing_zeros('$5,300.00'))
print(remove_trailing_zeros('$4,300.00'))
print(remove_trailing_zeros('$7,300.00'))
print(remove_trailing_zeros('$8,300.00'))
print(remove_trailing_zeros('$20,300.50'))
82/197:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    return int(car_price.rstrip('0').rstrip('.') if '.' in price else price)

print(remove_trailing_zeros('$2,300.00'))
print(remove_trailing_zeros('$5,300.00'))
print(remove_trailing_zeros('$4,300.00'))
print(remove_trailing_zeros('$7,300.00'))
print(remove_trailing_zeros('$8,300.00'))
print(remove_trailing_zeros('$20,300.00'))
82/198:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = re.sub(r"[\$,]", '', price)  
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
        car_prices.append(int(car_price))
    return car_prices

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/199:
import re
def remove_trailing_zeros(price):
    car_price = re.sub(r"[\$,]", '', price)
    return int(car_price.rstrip('0').rstrip('.') if '.' in price else price)
82/200:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = re.sub(r"[\$,]", '', price)  
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
        car_prices.append(int(car_price))
    return car_prices

print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/201:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_prices = remove_trailing_zeros(car_sales["Price"])
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/202:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_prices = remove_trailing_zeros(car_sales["Price"])
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/203: car_sales
82/204:
# car_sales["Price"] = car_sales["Price"].str.replace('[\$,]', '')
car_prices = remove_trailing_zeros(car_sales["Price"])
car_prices
# print(remove_tail_dot_zeros(car_sales["Price"].str.replace('[\$,]', '')))
82/205:
car_sales['Prices'] = remove_trailing_zeros(car_sales["Price"])
car_sales
82/206:
car_sales['Price'] = remove_trailing_zeros(car_sales["Price"])
car_sales
82/207:
car_sales['Price'] = remove_trailing_zeros(car_sales["Price"])
car_sales.drop('Prices')
82/208:
car_sales['Price'] = remove_trailing_zeros(car_sales["Price"])
car_sales.drop('Prices', axis=1)
82/209: car_sales['Price'] = remove_trailing_zeros(car_sales["Price"])
82/210:
car_sales = pd.read_csv('car-sales.csv')
car_sales
82/211:
import re
def remove_trailing_zeros(prices):
    car_prices = []
    for price in prices:
        car_price = re.sub(r"[\$,]", '', price)  
        car_price = car_price.rstrip('0').rstrip('.') if '.' in car_price else car_price
        car_prices.append(int(car_price))
    return car_prices

# print(remove_trailing_zeros(['$2,300.00', '$4,000.00', '$5,000.00']))
82/212: car_sales['Price'] = remove_trailing_zeros(car_sales["Price"])
82/213: car_sales
82/214: car_sales['Prices'] = remove_trailing_zeros(car_sales["Price"])
82/215: car_sales
82/216: car_sales['Price'].plot()
82/217: car_sales['Price'].hist()
82/218: ## Tracing Lead Time Report
82/219:
data_set = pd.read_csv('Lead-time-tacker.csv')
data_set.head()
82/220:
data_set = pd.read_csv('Lead-time-tacker.csv')
data_set.head()
82/221:
data_set = pd.read_csv('Lead-time-tacker.csv')
data_set
82/222:
dataset = pd.read_csv('Lead-time-tacker.csv')
data_set
82/223:
dataset = pd.read_csv('Lead-time-tacker.csv')
data_set
82/224: dataset = pd.read_csv('Lead-time-tacker.csv')
82/225: ls
82/226: df = pd.read_csv('Lead-time-tacker.csv')
82/227: ls
82/228: leadTimeDataset = pd.read_csv('Lead-time-tacker.csv')
82/229: leadTimeDataset.head()
82/230: df = pd.read_csv('Lead-time-tacker.csv')
82/231: df.head()
82/232: df.tail()
82/233: df.tail(500)
82/234: df.head(500)
82/235: df.head()
82/236: df[df['method'] == 'Expression Of Interest']
82/237: df_expression_of_interest = df[df['method'] == 'Expression Of Interest']
82/238: df_expression_of_interest
82/239: df_expression_of_interest.dropna(index=False)
82/240: df_expression_of_interest.dropna()
82/241: df_expression_of_interest
82/242: df_expression_of_interest_without_null_responsive_bids = df_expression_of_interest.dropna()
82/243: df_expression_of_interest_without_null_responsive_bids
82/244: df = pd.read_csv('lead_time_analysis.csv')
82/245: df.head()
82/246: #### Filter out contracts with bids greater than 1
82/247: df[df['Number Of Responsive Bids'] > 1]
82/248: df_expression_of_interest = df[df['method'] == 'Expression Of Interest']
82/249: df_expression_of_interest
82/250: df_expression_of_interest_without_null_responsive_bids = df_expression_of_interest.dropna()
82/251: df_expression_of_interest_without_null_responsive_bids
82/252: df = pd.read_csv('total-bids.csv')
82/253: df.head()
82/254: df_expression_of_interest = df[df['method'] == 'Expression Of Interest']
82/255: df[df['Number Of Responsive Bids'] > 1]
82/256: df.head()
82/257: df_expression_of_interest = df[df['method'] == 'Expression Of Interest']
82/258: df.head()
82/259: df_exp = df[df['method'] == 'Expression Of Interest']
82/260: df_exp = df[df['method'] == 'Expression Of Interest']
82/261: df_exp = df[df['method'] == 'Expression Of Interest']
82/262:
df_exp = df[df['method'] == 'Expression Of Interest']
df_exp
82/263: df_expression_of_interest = df[df['method'] == 'Expression Of Interest']
82/264: df_exp_dropnull = df_exp.dropna()
82/265:
df_exp_dropnull = df_exp.dropna()
df_exp_dropnull
82/266: df = pd.read_csv('total-bids.csv')
82/267: df.head()
82/268:
df_exp = df[df['method'] == 'Expression Of Interest']
df_exp
82/269:
df_exp_dropnull = df_exp.dropna()
df_exp_dropnull
82/270: df = pd.read_csv('total_bids.csv')
82/271: df.head()
82/272:
df_exp_dropnull = df_exp.dropna()
df_exp_dropnull
82/273: ls
82/274: df.head()
82/275:
df_exp = df[df['method'] == 'Expression Of Interest']
df_exp
82/276:
df_exp_dropnull = df_exp.dropna()
df_exp_dropnull
82/277: df = pd.read_csv('Lead-time-tacker.csv')
82/278: df.head()
82/279:
df_exp = df[df['method'] == 'Expression Of Interest']
df_exp
82/280:
df_exp_dropnull = df_exp.dropna()
df_exp_dropnull
82/281:
df_exp_dropnull = df_exp.dropna()
df_exp_dropnull
df_exp_dropnull.to_excel('exp_of_interest')
82/282:
df_exp_dropnull = df_exp.dropna()
df_exp_dropnull
df_exp_dropnull.to_csv('exp_of_interest')
82/283:
df_exp_dropnull = df_exp.dropna()
df_exp_dropnull
df_exp_dropnull.to_excel('exp_of_interest.xls')
82/284:
df_lead_time_report = pd.read_csv('lead_time_report.csv')
df_lead_time_report.head()
82/285: #### Import the lead time report
82/286: df_RIB = df_lead_time_report[df_lead_time_report['method'] == 'Restricted International Bidding (RIB)']
82/287: df_RIB
82/288:
df_lead_time_report = pd.read_csv('lead_time_report2.csv')
df_lead_time_report.head()
82/289: df_RIB = df_lead_time_report[df_lead_time_report['method'] == 'Restricted International Bidding (RIB)']
82/290: df_RIB
82/291:
df_RIB['date_difference'] = df_RIB['signed_date'] - df_RIB['BI initiation date']
df_RIB['date_difference'] = df_RIB['date_differnce'] / np.timedelta64(1,'D')
df_RIB
82/292:
df_RIB['date_difference'] = df_RIB['signed_date'] - df_RIB['BI initiation date']
df_RIB['date_difference'] = df_RIB['date_differnce'] /np.timedelta64(1,'D')
df_RIB
82/293:
df_RIB['date_difference'] = df_RIB['signed_date'] - df_RIB['BI initiation date']
# df_RIB['date_difference'] = df_RIB['date_differnce'] /np.timedelta64(1,'D')
82/294:
df_OIB = df_lead_time_report[df_lead_time_report['method'] == 'Open International Bidding(OIB)']
df_OIB
82/295:
df_RIB['date_difference'] = df_RIB['signed_date'] - df_RIB['BI initiation date']
# df_RIB['date_difference'] = df_RIB['date_differnce'] /np.timedelta64(1,'D')
82/296:
df_RIB['date_difference'] = pd.to_datetime(df_RIB['signed_date']) - pd.to_date_time(df_RIB['BI initiation date'])
# df_RIB['date_difference'] = df_RIB['date_differnce'] /np.timedelta64(1,'D')
82/297:
df_RIB['date_difference'] = pd.to_datetime(df_RIB['signed_date']) - pd.to_datetime(df_RIB['BI initiation date'])
# df_RIB['date_difference'] = df_RIB['date_differnce'] /np.timedelta64(1,'D')
82/298:
df_RIB['date_difference'] = pd.to_datetime(df_RIB['signed_date']) - pd.to_datetime(df_RIB['BI initiation date'])
df_RIB['date_difference'] = df_RIB['date_differnce'] /np.timedelta64(1,'D')
82/299:
df_RIB['date_difference'] = pd.to_datetime(df_RIB['signed_date']) - pd.to_datetime(df_RIB['BI initiation date'])
# df_RIB['date_difference'] = df_RIB['date_differnce'] /np.timedelta64(1,'D')
df_RIB['date_difference']
82/300: car_sales['Make'].str.lower()
82/301: car_sales['Make'].lower()
82/302: car_sales['Make'].str.lower()
82/303: car_sales['Make'] = car_sales['Make'].str.lower()
82/304: car_sales
82/305: car_sales
82/306: car_sales_missing_data = pd.read_csv('car-sales-missing-data.csv')
82/307: car_sales_missing_data
82/308: car_sales_missing_data['Odometer'].mean()
82/309: describe(car_sales_missing_data)
82/310: car_sales_missing_data.describe()
82/311: car_sales_missing_data['Odometer'] = car_sales_missing_data['Odometer'].mean()
82/312: car_sales_missing_data
82/313: car_sales_missing_data
82/314: car_sales_missing_data.describe()
82/315: car_sales_no_missing_data = car_sales_missing_data.dropna()
82/316: car_sales_no_missing_data
82/317: car_sales_no_missing_data['Seats'] = pd.Series([5, 5, 5, 5, 5, 5])
82/318: car_sales_no_missing_data
82/319:
import math
# car_sales_no_missing_data['Engine Size'] = math.random()
math.random()
82/320:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.random()
82/321:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.random(1.3, 4.5)
82/322:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.random([1.3, 4.5])
82/323:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.randrange([1.3, 4.5])
82/324:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.randrange(1.3, 4.5)
82/325:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.randrange(1.3, 4.5, '')
82/326:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.randrange(1, 5)
82/327:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.randrange(1, 5)
82/328:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.randrange(1, 5)
82/329:
import random
# car_sales_no_missing_data['Engine Size'] = math.random()
random.randrange(1, 5)
82/330:
import random

for item in range(len(car_sales_no_missing_data)):
    car_sales_no_missing_data['Engine Size'] = random.randrange(1, 5)
    
car_sales_no_missing_data
82/331:
import random

for item in range(len(car_sales_no_missing_data)):
    car_sales_no_missing_data['Engine Size'] = random.randrange(1, 5)
    
car_sales_no_missing_data
82/332:
import random

for item in range(len(car_sales_no_missing_data)):
    car_sales_no_missing_data['Engine Size'] = random.randrange(1, 5)
    
car_sales_no_missing_data
82/333:
import random

len(car_sales_no_missing_data)

for item in range(len(car_sales_no_missing_data)):
    car_sales_no_missing_data['Engine Size'] = random.randrange(1, 5)
    
car_sales_no_missing_data
82/334:
import random

print(len(car_sales_no_missing_data))

for item in range(len(car_sales_no_missing_data)):
    car_sales_no_missing_data['Engine Size'] = random.randrange(1, 5)
    
car_sales_no_missing_data
82/335:
import random

random_list = []
for item in range(len(car_sales_no_missing_data)):
    random_list.append(random.randrange(1, 5))
    
car_sales_no_missing_data['Engine Size'] = radom_list
car_sales_no_missing_data
82/336:
import random

random_list = []
for item in range(len(car_sales_no_missing_data)):
    random_list.append(random.randrange(1, 5))
    
car_sales_no_missing_data['Engine Size'] = random_list
car_sales_no_missing_data
82/337:
import random

random_list = []
for item in range(len(car_sales_no_missing_data)):
    random_list.append(random.randrange(1, 5))
    
car_sales_no_missing_data['Engine Size'] = random_list
car_sales_no_missing_data
82/338: car_sales_no_missing_data['Price Per KM']
82/339: car_sales_no_missing_data['Price Per KM'] = ''
82/340: car_sales_no_missing_data
82/341: car_sales_no_missing_data['Price Per KM'].drop()
82/342: car_sales_no_missing_data['Price Per KM'].drop(inplace=True)
82/343: car_sales_no_missing_data.drop(inplace=True, columns='Price Per KM')
82/344: car_sales_no_missing_data
82/345: car_sales_no_missing_data.frac(0.1)
82/346: car_samples = car_sales_no_missing_data.sample(frac=0.1)
82/347: car_samples
82/348: car_samples = car_sales_no_missing_data.sample(frac=1)
82/349: car_samples
82/350: car_samples = car_sales_no_missing_data.sample(frac=1)
82/351: car_samples
82/352: car_samples.reset()
82/353: car_samples.reset
82/354: car_samples.reset_index()
82/355: car_samples.reset_index(drop=True, inplace=True)
82/356: car_samples
82/357:
car_sales_no_missing_data['Odometer'] = car_sales_no_missing_data['Odometer'] * 1.6
car_sales_no_missing_data
82/358:
car_sales_no_missing_data['Odometer'] = car_sales_no_missing_data['Odometer'].apply(lambda x: x / 16)
car_sales_no_missing_data
82/359: car_sales_no_missing_data.rename({'Odometer', 'Odometer (M)'})
82/360: car_sales_no_missing_data.rename(column = {'Odometer': 'Odometer (M)'})
82/361: car_sales_no_missing_data.rename(columns = {'Odometer': 'Odometer (M)'})
83/1: import numpy as np
83/2:
#NumPy's main datatype is ndarray
al = np.array([1, 2, 3])
al
83/3: type(a1)
83/4:
#NumPy's main datatype is ndarray
al = np.array([1, 2, 3])
al
83/5: type(a1)
83/6: type(a1)
83/7:
#NumPy's main datatype is ndarray
a1 = np.array([1, 2, 3])
al
83/8:
#NumPy's main datatype is ndarray
a1 = np.array([1, 2, 3])
a1
83/9:
#NumPy's main datatype is ndarray
a1 = np.array([1, 2, 3])
a1
83/10: type(a1)
83/11:
a2 = np.array([[1, 2.0, 3.3], 4, 5, 6.5])

a3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],[[10, 11, 12], [13, 14, 15], [16, 17, 18]]])
83/12: a2
83/13: a3
83/14: a1.shape
83/15: a2.shape
83/16: a3.shape
83/17: a1.ndim, a2.ndim, a3.ndim
83/18: a1
83/19: a2
83/20:
a2 = np.array([[1, 2.0, 3.3],[4, 5, 6.5]])

a3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],[[10, 11, 12], [13, 14, 15], [16, 17, 18]]])
83/21: a2
83/22: a3
83/23: a1.shape
83/24: a2.shape
83/25: a3.shape
83/26: a1
83/27: a2
83/28: a3
83/29: a1.ndim, a2.ndim, a3.ndim
83/30: a1.dtype, a2.dtype, a3.dtype
83/31: a1.size, a2.size, a3.size
83/32: type(a1), type(a2), type(a3)
83/33:
# Create a DataFrame from a Numpy array
import pandas as pd

df = pd.DataFrame(a2)
df
83/34: ## Creating NumPy Arrays
83/35:
sample_array = np.array([1, 2, 3])
sample_array
83/36: sample_array.dtype
83/37: ones = np.ones(2, 3)
83/38: ones = np.ones((2, 3)
83/39: ones = np.ones((2, 3))
83/40: ones
83/41: type(ones)
83/42: ones.dtype
83/43: zeros = np.zeros((2, 3))
83/44: zeros
83/45: zeros.dtype
83/46: type(zeros)
83/47:
range_array = np.arange(0, 10, 2)
range_array
83/48:
random_array = np.random.randint(0, 10, size=(3, 5))
random_array
83/49: random_array.size
83/50: random_array.shape
83/51: np.random.random((5, 3))
83/52:
random_array_2 = np.random.random((5, 3))
random_array_2
83/53: random_array_3 = np.random.rand(5, 3)
83/54:
random_array_3 = np.random.rand(5, 3)
random_array_3
83/55:
random_array_3 = np.random.rand(5, 3)
random_array_3
83/56:
# Pseudo-random numbers
np.random.seed()
83/57:
# Pseudo-random numbers
# np.random.seed()
random_array_4 = np.random.randint(10, size(5, 3))
random_array_4
83/58:
# Pseudo-random numbers
# np.random.seed()
random_array_4 = np.random.randint(10, size=(5, 3))
random_array_4
83/59:
# Pseudo-random numbers
# np.random.seed()
random_array_4 = np.random.randint(10, size=(5, 3))
random_array_4
83/60:
# Pseudo-random numbers
# np.random.seed()
random_array_4 = np.random.randint(10, size=(5, 3))
random_array_4
83/61:
# Pseudo-random numbers
np.random.seed(seed=0)
random_array_4 = np.random.randint(10, size=(5, 3))
random_array_4
83/62:
# Pseudo-random numbers
np.random.seed(seed=0)
random_array_4 = np.random.randint(10, size=(5, 3))
random_array_4
83/63:
random_array_5 = np.random.random(5, 3)
random_array_5
83/64:
random_array_5 = np.random.random((5, 3))
random_array_5
83/65:

random_array_5 = np.random.random((5, 3))
random_array_5
83/66:
np.random.seed(1)
random_array_5 = np.random.random((5, 3))
random_array_5
83/67:
np.random.seed(1)
random_array_5 = np.random.random((5, 3))
random_array_5
83/68:
np.random.seed(7)
random_array_5 = np.random.random((5, 3))
random_array_5
83/69: ## Viewing arrays and matrices
83/70: np.unique(random_array_4)
83/71: a1
83/72: a2
83/73: a3
83/74: a2.shape
83/75: a1[0]
83/76: a2[0]
83/77: a3[0]
83/78: a2[1]
83/79: a3
83/80: a3[:2, :2, :2]
83/81:
a4 = np.random.randint(10, size=(2, 3, 4, 5))
a4
83/82: a4.shape
83/83: a4.shape, a4.ndim
83/84: # Get the first 4 numbers of the inner most arrays
83/85:
# Get the first 4 numbers of the inner most arrays
a4[:, :, :, :4]
83/86:
# Get the first 4 numbers of the inner most arrays
a4[:, :, :, :3]
83/87:
# Get the first 4 numbers of the inner most arrays
a4[:, :, :, :3]
83/88:
# Get the first 4 numbers of the inner most arrays
a4[:, :, :, :2]
83/89:
# Get the first 4 numbers of the inner most arrays
a4[:, :, :, :1]
83/90: ## Manipulating & comparing arrays
83/91: a1
83/92: ones = np.ones(3)
83/93:
ones = np.ones(3)
ones
83/94: a1 + ones
83/95: a1 - ones
83/96: a1 * ones
83/97: a2
83/98: a1 * a2
83/99: a2 ** 2
83/100: np.square(a2)
83/101: np.add(a1, ones)
83/102: a1 % 2
83/103: a2 % 2
83/104: np.exp(a1)
83/105: np.log(a1)
83/106: ### Aggregation
83/107:
listy_list = [1, 2, 3]
type(listy_list)
83/108: sum(listy_list)
83/109: type(a1)
83/110: sum(a1)
83/111: np.sum(a1)
83/112:
# Create a massive NumPy array
massive_array = np.random.random(100000)
massive_array.size
83/113: massive_array[:10]
83/114:
%timeit sum(massive_array)  # Python's sum()
%timeit np.sum(massive_array) # Numpy's sum()
83/115: a2
83/116: np.mean(a2)
83/117: np.max(a2)
83/118: np.min(a2)
83/119: np.std(a2)
83/120: np.var(a2)
83/121:
# Standard deviation = squareroot of variance
np.sqrt(np.var(a2))
83/122:
#Demo of std and var
high_var_array = np.array([1, 100, 200, 300, 400, 5000])
low_var_array = np.array([2, 4, 6, 8, 10])
83/123: np.var(high_var_array), np.var(low_var_array)
83/124: np.std(high_var_array), np.std(low_var_array)
83/125: np.std(high_var_array), np.std(low_var_array)
83/126: np.mean(high_var_array), np.mean(low_var_array)
83/127:
#Demo of std and var
high_var_array = np.array([1, 100, 200, 300, 4000, 5000])
low_var_array = np.array([2, 4, 6, 8, 10])
83/128: np.var(high_var_array), np.var(low_var_array)
83/129: np.std(high_var_array), np.std(low_var_array)
83/130: np.mean(high_var_array), np.mean(low_var_array)
83/131:
import matplotlib.plot as plt
plt.hist(high_var_array)
plt.show()
83/132:
import matplotlib.pyplot as plt
plt.hist(high_var_array)
plt.show()
83/133:
plt.hist(low_var_array)
plt.show()
83/134:
plt.hist2D(low_var_array)
plt.show()
83/135:
plt.hist(low_var_array)
plt.show()
83/136: ### Reshaping & Transposing
83/137: a2
83/138: a2.shape
83/139: a3
83/140: a3.shape
83/141: a2.reshape(2, 3, 1)
83/142: a2.shapw
83/143: a2.shap2
83/144: a2.shape
83/145: a2.reshape(2, 3, 1).shape
83/146: a3.shape
83/147:
a2_reshape = a2.reshape(2, 3, 1)
a2_reshape
83/148: a2_reshape * a3
83/149:
#Transpose
a2.T
83/150: a2
83/151: a2.shape
83/152: a2.T.shape
83/153: array_1 = np.array([[1, 2, 3], [4, 5, 6]])
83/154: array_1
83/155: array_1.shape
83/156: array_1.reshape(3, 2, 1)
83/157: array_1
83/158:
array_1_reshape = array_1.reshape(3, 2, 1)
array_1_reshape
83/159: array_1_reshape.shape()
83/160: array_1_reshape.shape
83/161:
array_1_reshape = array_1.reshape(2, 3, 1)
array_1_reshape
83/162: array_1_reshape.shape
83/163: array_1_reshape.T
83/164: ### Dot Product
83/165:
np.random.seed(0)

mat1 = np.random.randint(10, size=(5, 3))
mat2 = np.random.randint(10, size=(5, 3))
83/166: mat1
83/167: mat2
83/168: mat1.shape, mat2.shape
83/169: mat1* mat2
83/170:
# Element-wise multiplication (Hadamard product)
mat1* mat2
83/171:
# Dot Product
np.dot(mat1, mat2)
83/172:
# Transpose mat1
mat1.T
83/173:
# Transpose mat1
mat1.T.shape
83/174: mat2.shape, mat1.T.shape
83/175:
# Transpose mat1
mat2.T.shape
83/176: mat1.shape, mat2.T.shape
83/177:
mat3 = np.dot(mat1, mat2.T.shape)
mat3
83/178:
mat3 = np.dot(mat1, mat2.T)
mat3
83/179: mat3.shape
83/180:
# Dot Product
np.dot(mat1, mat2.T)
83/181:
np.random.seed(0)
sales_amounts = np.random.randint(20, size=(5, 3))
sales_amounts
83/182:
# Create a weekly_sales DataFrame
weekly_sales = pd.DataFrame(sales_amounts, 
                            index=["Mon", "Tue", "Wed", "Thurs", "Fri"],
                            columns=["Almond butter", "Peanut butter", "Cashew butter"])
weekly_sales
83/183:
# Create prices array
prices = np.array([10, 8, 12])
prices
83/184:
# Create butter_prices DataFrame
butter_prices = pd.DataFrame(prices,
                             index=["Price"],
                             columns=["Almond butter", "Peanut butter", "Cashew butter"])
butter_prices
83/185:
# Create butter_prices DataFrame
butter_prices = pd.DataFrame(prices.reshape(1, 3),
                             index=["Price"],
                             columns=["Almond butter", "Peanut butter", "Cashew butter"])
butter_prices
83/186: total_sales = prices.dot(sales_amounts)
83/187: prices
83/188: sales_amounts
83/189: prices.shape
83/190: sales_amounts.shape
83/191: total_sales = prices.dot(sales_amounts.T)
83/192:
total_sales = prices.dot(sales_amounts.T)
total_sales
83/193:
# Create daily_sales
butter_prices
83/194: weekly_sales
83/195:
# Create daily_sales
butter_prices.shape, weekly_sales.shape
83/196: weekly_sales.T.shape
83/197:
daily_sales = butter_prices.dot(weekly_sales.T)
daily_sales
83/198: weekly_sales
83/199: weekly_sales['Total sales ($)'] = daily_sales
83/200: weekly_sales.shape
83/201:
# Doesn't work not the same shape

weekly_sales['Total sales ($)'] = daily_sales.T
83/202:
# Doesn't work not the same shape

weekly_sales['Total sales ($)'] = daily_sales.T
weekly_sales
83/203: array_2 = np.array([[1, 2], [3, 4]])
83/204: array_2
83/205: array_2.shape
83/206: array_2.T
83/207: array_2.T
83/208: array_3
83/209: array_3
83/210: array_three
84/1: array_three = np.array([[1, 2], [2, 3], [3, 4]])
84/2: array_3 = np.array([[1, 2], [2, 3], [4, 5]])
84/3: array_3 = np.array([[1, 2], [2, 3], [4, 5]])
84/4: import numpy as np
84/5: array_3 = np.array([[1, 2], [2, 3], [4, 5]])
84/6:
array_3 = np.array([[1, 2], [2, 3], [4, 5]])
array_3
84/7: array_3.T
84/8: array_3.reshape(3, 2)
84/9: array_3.reshape(3, 1)
84/10: array_3.reshape(3, 2)
84/11: a2
84/12:
#NumPy's main datatype is ndarray
a1 = np.array([1, 2, 3])
a1
85/1: import numpy as np
85/2:
#NumPy's main datatype is ndarray
a1 = np.array([1, 2, 3])
a1
85/3: type(a1)
85/4:
a2 = np.array([[1, 2.0, 3.3],[4, 5, 6.5]])

a3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],[[10, 11, 12], [13, 14, 15], [16, 17, 18]]])
85/5: a2
85/6: a3
85/7: a1.shape
85/8: a2.shape
85/9: a3.shape
85/10: a1
85/11: a2
85/12: a3
85/13: a1.ndim, a2.ndim, a3.ndim
85/14: a1.dtype, a2.dtype, a3.dtype
85/15: a1.size, a2.size, a3.size
85/16: type(a1), type(a2), type(a3)
85/17:
# Create a DataFrame from a Numpy array
import pandas as pd

df = pd.DataFrame(a2)
df
85/18:
sample_array = np.array([1, 2, 3])
sample_array
85/19: sample_array.dtype
85/20: ones = np.ones((2, 3))
85/21: ones
85/22: ones.dtype
85/23: type(ones)
85/24: zeros = np.zeros((2, 3))
85/25: zeros
85/26: zeros.dtype
85/27: type(zeros)
85/28:
range_array = np.arange(0, 10, 2)
range_array
85/29:
random_array = np.random.randint(0, 10, size=(3, 5))
random_array
85/30: random_array.size
85/31: random_array.shape
85/32:
random_array_2 = np.random.random((5, 3))
random_array_2
85/33:
random_array_3 = np.random.rand(5, 3)
random_array_3
85/34:
# Pseudo-random numbers
np.random.seed(seed=0)
random_array_4 = np.random.randint(10, size=(5, 3))
random_array_4
85/35:
np.random.seed(7)
random_array_5 = np.random.random((5, 3))
random_array_5
85/36: np.unique(random_array_4)
85/37: a1
85/38: a2.shape
85/39: a2
85/40: a3
85/41: a1[0]
85/42: a2[0]
85/43: a3[0]
85/44: a2[1]
85/45: a3
85/46: a3[:2, :2, :2]
85/47:
a4 = np.random.randint(10, size=(2, 3, 4, 5))
a4
85/48: a4.shape, a4.ndim
85/49:
# Get the first 4 numbers of the inner most arrays
a4[:, :, :, :1]
85/50: a1
85/51:
ones = np.ones(3)
ones
85/52: a1 + ones
85/53: a1 - ones
85/54: a1 * ones
85/55: a2
85/56: a1 * a2
85/57: a2 ** 2
85/58: np.square(a2)
85/59: np.add(a1, ones)
85/60: a1 % 2
85/61: a2 % 2
85/62: np.exp(a1)
85/63: np.log(a1)
85/64:
listy_list = [1, 2, 3]
type(listy_list)
85/65: sum(listy_list)
85/66: type(a1)
85/67: sum(a1)
85/68: np.sum(a1)
85/69:
# Create a massive NumPy array
massive_array = np.random.random(100000)
massive_array.size
85/70: massive_array[:10]
85/71:
%timeit sum(massive_array)  # Python's sum()
%timeit np.sum(massive_array) # Numpy's np.sum()
85/72: a2
85/73: np.mean(a2)
85/74: np.max(a2)
85/75: np.min(a2)
85/76:
# Standard deviation = a measure of how spread out a group of numbers is from the mean
np.std(a2)
85/77:
# Variance = measure of the average degree to which each number is different
# Higher variance = wider range of numbers
# Lower Variance = lower range of numbers
np.var(a2)
85/78:
# Standard deviation = squareroot of variance
np.sqrt(np.var(a2))
85/79:
#Demo of std and var
high_var_array = np.array([1, 100, 200, 300, 4000, 5000])
low_var_array = np.array([2, 4, 6, 8, 10])
85/80: np.var(high_var_array), np.var(low_var_array)
85/81: np.std(high_var_array), np.std(low_var_array)
85/82: np.mean(high_var_array), np.mean(low_var_array)
85/83:
import matplotlib.pyplot as plt
plt.hist(high_var_array)
plt.show()
85/84:
plt.hist(low_var_array)
plt.show()
85/85: a2
85/86: a2.shape
85/87: a3
85/88: a3.shape
85/89: a2.reshape(2, 3, 1).shape
85/90: a2.shape
85/91: a3.shape
85/92:
a2_reshape = a2.reshape(2, 3, 1)
a2_reshape
85/93: a2_reshape * a3
85/94: a2
85/95: a2.shape
85/96:
#Transpose
a2.T
85/97: a2.T.shape
85/98: array_1 = np.array([[1, 2, 3], [4, 5, 6]])
85/99: array_1
85/100: array_1.shape
85/101: array_1
85/102:
array_1_reshape = array_1.reshape(2, 3, 1)
array_1_reshape
85/103: array_1_reshape.shape
85/104: array_1_reshape.T
85/105:
np.random.seed(0)

mat1 = np.random.randint(10, size=(5, 3))
mat2 = np.random.randint(10, size=(5, 3))
85/106: mat1
85/107: mat2
85/108: mat1.shape, mat2.shape
85/109:
# Element-wise multiplication (Hadamard product)
mat1* mat2
85/110:
# Dot Product
np.dot(mat1, mat2.T)
85/111:
# Transpose mat1
mat2.T.shape
85/112: mat1.shape, mat2.T.shape
85/113:
mat3 = np.dot(mat1, mat2.T)
mat3
85/114: mat3.shape
85/115:
np.random.seed(0)
sales_amounts = np.random.randint(20, size=(5, 3))
sales_amounts
85/116:
# Create a weekly_sales DataFrame
weekly_sales = pd.DataFrame(sales_amounts, 
                            index=["Mon", "Tue", "Wed", "Thurs", "Fri"],
                            columns=["Almond butter", "Peanut butter", "Cashew butter"])
weekly_sales
85/117:
# Create prices array
prices = np.array([10, 8, 12])
prices
85/118:
# Create butter_prices DataFrame
butter_prices = pd.DataFrame(prices.reshape(1, 3),
                             index=["Price"],
                             columns=["Almond butter", "Peanut butter", "Cashew butter"])
butter_prices
85/119: prices
85/120: prices.shape
85/121: sales_amounts
85/122: sales_amounts.shape
85/123:
total_sales = prices.dot(sales_amounts.T)
total_sales
85/124:
# Create daily_sales
butter_prices.shape, weekly_sales.shape
85/125: weekly_sales.T.shape
85/126:
daily_sales = butter_prices.dot(weekly_sales.T)
daily_sales
85/127: weekly_sales.shape
85/128:
weekly_sales['Total sales ($)'] = daily_sales.T
weekly_sales
85/129: array_2 = np.array([[1, 2], [3, 4]])
85/130: array_2
85/131: array_2.shape
85/132: array_2.T
85/133:
array_3 = np.array([[1, 2], [2, 3], [4, 5]])
array_3
85/134: array_3.T
85/135: array_3.reshape(3, 2)
85/136: a2
85/137: a2
85/138: a2
85/139: a1
85/140: a2
85/141: a1 > a2
85/142: a1 >= a2
85/143:
bool_array = a1 >= a2
bool_array
85/144: type(bool_array)
85/145: a1 < 5
85/146: a1 == a2
85/147: a1 == a2
85/148: a1 == a1
85/149: array_4 = np.array([[-1, 2], [4, -5]])
85/150: array_4
85/151: array_4.shape
85/152: array_4.T
85/153: ## Sorting arrays
85/154: random_arrays
85/155:
random_array = np.random.randint(10, size=(3, 5))
random_array
85/156: random_array.shape
85/157: np.sort
85/158: np.sort(random_array)
85/159: np.argsort()
85/160: np.argsort(random_array)
85/161: a1
85/162: np.argsort(a1)
85/163: np.argmin(a1)
85/164: np.argmax(a2)
85/165: np.argmax(a1)
85/166: np.argmax(random_array)
85/167: np.argmax(random_array, axis=0)
85/168: np.argmax(random_array, axis=1)
85/169: ## Practical Example - NumPy in Action
85/170: ls
85/171: <img src="./numpy-images/pandas.png" />
85/172: <img src="./numpy-images/panda.png" />
85/173: <img src="./numpy-images/pandas.jpeg" />
85/174: <img src="./numpy-images/panda.png" />
85/175: ![Image]('./numpy-images/panda.png')
85/176: ![Image]('numpy-images/panda.png')
85/177: ![Image]('./images/panda.png')
85/178: <img src="images/panda.png" />
85/179: ![Image](images/panda.png)
85/180: # Turn an image into a Numpy array
85/181:
# Turn an image into a Numpy array
from matplotlib.image import imread

panda = imread()
85/182:
# Turn an image into a Numpy array
from matplotlib.image import imread

panda = imread('images/panda.png')
print(type(panda))
85/183: panda
85/184: panda.size, panda.shape, panda.ndim
85/185: panda[:5]
85/186: cd images
85/187: ls
85/188: car = imread('images/car-photop.png')
85/189: car = imread('images/car-photo.png')
85/190: ls
85/191: pwd
85/192: cd ..
85/193: car = imread('images/car-photo.png')
85/194: car
85/195: type(car)
85/196: car.size, car.shape, car.ndim
85/197: car[:1]
85/198: dog = imread('images/dog-photo.png')
85/199: type(dog)
85/200: dog[:1]
86/1: import numpy as np
86/2: a1 = np.array([1, 2, 3])
86/3: a2 = np.array([[1, 2, 3], [4, 5, 6]])
86/4: a3 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
86/5: a1.shape, a1.ndim, a1.dtype, a1.size, type(a1)
86/6: a2.shape, a2.ndim, a2.dtype, a2.size, type(a2)
86/7: a3.shape, a3.ndim, a3.dtype, a3.size, type(a3)
86/8: import pandas as pd
86/9:
df_a1 = pd.DataFrame(a1)
df_a1
86/10:
df_a2 = pd.DataFrame(a2)
df_a2
86/11:
df_a3 = pd.DataFrame(a3)
df_a3
86/12:
ones = np.ones((10, 2))
ones
86/13: zeros = np.zeros((7, 2, 3))
86/14:
zeros = np.zeros((7, 2, 3))
zeros
86/15: array_0_100 = np.randrange(1, 100)
86/16: array_0_100 = np.range(1, 100)
86/17: array_0_100 = np.randomrange(1, 100)
86/18: array_0_100 = np.randrange(1, 100)
86/19: array_0_100 = np.randrange([1, 100])
86/20: array_0_100 = np.arange(1, 100)
86/21:
array_0_100 = np.arange(1, 100)
array_0_100
86/22:
array_0_100 = np.arange(1, 100, 3)
array_0_100
86/23: array_0_10 = np.random.randint(0, 10, size=(7, 2))
86/24:
array_0_10 = np.random.randint(0, 10, size=(7, 2))
array_0_10
86/25: array_0_1 = np.random.random((3, 5))
86/26:
array_0_1 = np.random.random((3, 5))
array_0_1
86/27: np.random.seed(42)
86/28:
np.random.seed(42)

array_0_to_10 = np.random.randint(0, 10, size=(4, 6))

array_0_to_10
86/29:
np.random.seed(42)

array_0_to_10 = np.random.randint(0, 10, size=(4, 6))

array_0_to_10
86/30:
np.random.seed(0)

array_0_to_10 = np.random.randint(0, 10, size=(4, 6))

array_0_to_10
86/31:
np.random.seed(42)

array_0_to_10 = np.random.randint(0, 10, size=(4, 6))

array_0_to_10
86/32:
new_array = np.random.randint(1, 10, size=(3, 7))

np.unique(new_array)
86/33:
new_array = np.random.randint(1, 10, size=(3, 7))

unique_array = np.unique(new_array)
86/34:
new_array = np.random.randint(1, 10, size=(3, 7))

unique_array = np.unique(new_array)

unique_array
86/35: unique_array[0]
86/36:
new_array = np.random.randint(1, 10, size=(3, 7))

unique_array = np.unique(new_array)

new_array
unique_array
86/37:
new_array = np.random.randint(1, 10, size=(3, 7))

unique_array = np.unique(new_array)

new_array
86/38:
new_array = np.random.randint(1, 10, size=(3, 7))

unique_array = np.unique(new_array)

new_array
unique_array
86/39: unique_array[0]
86/40: new_array[0]
86/41: new_array
86/42: new_array[:,:]
86/43: new_array[:,:]
86/44: new_array[[:],[:]]
86/45: new_array[[:,:]]
86/46: new_array[[:, :]]
86/47: new_array[[:1, :1]]
86/48: new_array[:1, :1]
86/49: new_array[[:1]]
86/50: new_array[[1]]
86/51: new_array[[0][1]]
86/52: new_array[[0],[1]]
86/53: new_array[[0]]
86/54: new_array[[:]]
86/55: new_array[[0]]
86/56: new_array[[0], [1]]
86/57: new_array[[0]]
86/58: random_array
86/59: random_array = np.random.randint(0, 10)
86/60: random_arrau
86/61: random_array
86/62: random_array
86/63: random_array
86/64: random_array = np.random.randint(0, 10, size=(3, 5))
86/65: random_array
86/66:
random_array = np.random.randint(0, 10, size=(3, 5))
array_of_ones = np.ones
86/67: random_array, array_of_ones
86/68:
random_array
array_of_ones
86/69:
#random_array
array_of_ones
86/70:
#random_array
array_of_ones
86/71:
random_array = np.random.randint(0, 10, size=(3, 5))
array_of_ones = np.ones()
86/72:
random_array = np.random.randint(0, 10, size=(3, 5))
array_of_ones = np.ones(3, 5)
86/73:
random_array = np.random.randint(0, 10, size=(3, 5))
array_of_ones = np.ones((3, 5))
86/74:
random_array
array_of_ones
86/75: random_array
86/76: array_of_ones
86/77: np.sum(random_array, array_of_ones)
86/78: np.sum([random_array, array_of_ones])
86/79: np.sum(random_array)
86/80: np.sum(array_of_ones)
86/81: 63 + 15
86/82: np.sum([random_array, array_of_ones])
86/83: np.sum([random_array, array_of_ones])
86/84: array_of_ones = np.ones((5, 3))
86/85: array_of_ones2 = np.ones((5, 3))
86/86:
random_array = np.random.randint(0, 10, size=(3, 5))
array_of_ones = np.ones((3, 5))
86/87: np.sum([array_of_ones, array_of_ones2])
86/88: np.sum([array_of_ones, array_of_ones2.T])
86/89: array_of_ones3 = np.ones(3, 5)
86/90: array_of_ones3 = np.ones((3, 5))
86/91: array_of_ones3
86/92: np.subtract([array_of_ones2 - array_of_ones3])
86/93: np.subtract([array_of_ones2 - array_of_ones3.T])
86/94: np.subtract([array_of_ones2, array_of_ones3.T])
86/95: np.subtract(array_of_ones2, array_of_ones3.T)
86/96: np.subtract(array_of_ones2, array_of_ones3)
86/97: np.subtract(array_of_ones2, array_of_ones3.T)
86/98: np.multiply(array_of_ones, subtract_array)
86/99: subtract_array = np.subtract(array_of_ones2, array_of_ones3.T)
86/100: np.multiply(array_of_ones, subtract_array)
86/101: np.multiply(array_of_ones, subtract_array.T)
86/102: multPlication_array = np.multiply(array_of_ones, subtract_array.T)
86/103: multPlication_array ** 2
86/104: np.power(multPlication_array, 2)
86/105: np.square(multPlication_array)
86/106: np.mean(multPlication_array)
86/107: np.max(multPlication_array)
86/108: np.min(multPlication_array)
86/109: np.std(multPlication_array)
86/110: np.std(array_of_ones3)
86/111: np.std(multPlication_array)
86/112: np.var(multPlication_array)
86/113: multPlication_array.reshape(3, 5, 1)
86/114: multPlication_array.T
86/115:
random_array_1 = np.random.randint(0, 10, size=(3, 3))
random_array_2 = np.random.randint(0, 10, size=(3, 2))
86/116: np.dot(random_array_1, random_array_2)
86/117:
random_array_3 = np.random.randint(0, 10, size=(4, 3))
random_array_4 = np.random.randint(0, 10, size=(4, 3))
86/118: np.dot(random_array_3, random_array_4.T)
86/119:
new_random_array_1 = np.random.randint(0, 10)
new_random_array_2 = np.random.randint(0, 10)
86/120: new_random_array_1 > new_random_array_2
86/121: new_random_array_1, new_random_array_2
86/122: new_random_array_1, new_random_array_2
86/123: new_random_array_1 > new_random_array_2
86/124:
new_random_array_1 = np.random.randint(0, 10)
new_random_array_2 = np.random.randint(0, 10)
86/125: new_random_array_1, new_random_array_2
86/126: new_random_array_1 >= new_random_array_2
86/127: np.sort(new_random_array_1)
86/128: import numpy as np
86/129: a1 = np.array([1, 2, 3])
86/130: a2 = np.array([[1, 2, 3], [4, 5, 6]])
86/131: a3 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
86/132: a1.shape, a1.ndim, a1.dtype, a1.size, type(a1)
86/133: a2.shape, a2.ndim, a2.dtype, a2.size, type(a2)
86/134: a3.shape, a3.ndim, a3.dtype, a3.size, type(a3)
86/135: import pandas as pd
86/136:
df_a1 = pd.DataFrame(a1)
df_a1
86/137:
df_a2 = pd.DataFrame(a2)
df_a2
86/138:
df_a3 = pd.DataFrame(a3)
df_a3
86/139:
ones = np.ones((10, 2))
ones
86/140:
zeros = np.zeros((7, 2, 3))
zeros
86/141:
array_0_100 = np.arange(1, 100, 3)
array_0_100
86/142:
array_0_10 = np.random.randint(0, 10, size=(7, 2))
array_0_10
86/143:
array_0_1 = np.random.random((3, 5))
array_0_1
86/144:
np.random.seed(42)

array_0_to_10 = np.random.randint(0, 10, size=(4, 6))

array_0_to_10
86/145:
new_array = np.random.randint(1, 10, size=(3, 7))

unique_array = np.unique(new_array)

unique_array
86/146: new_array
86/147: new_array[0]
86/148: new_array[[0]]
86/149:
random_array = np.random.randint(0, 10, size=(3, 5))
array_of_ones = np.ones((3, 5))
86/150: random_array
86/151: array_of_ones
86/152: np.sum([random_array, array_of_ones])
86/153: array_of_ones2 = np.ones((5, 3))
86/154: np.sum([array_of_ones, array_of_ones2.T])
86/155: array_of_ones3 = np.ones((3, 5))
86/156: array_of_ones3
86/157: subtract_array = np.subtract(array_of_ones2, array_of_ones3.T)
86/158: multPlication_array = np.multiply(array_of_ones, subtract_array.T)
86/159: multPlication_array ** 2
86/160: np.power(multPlication_array, 2)
86/161: np.square(multPlication_array)
86/162: np.mean(multPlication_array)
86/163: np.max(multPlication_array)
86/164: np.min(multPlication_array)
86/165: np.std(multPlication_array)
86/166: np.var(multPlication_array)
86/167: multPlication_array.reshape(3, 5, 1)
86/168: multPlication_array.T
86/169:
random_array_1 = np.random.randint(0, 10, size=(3, 3))
random_array_2 = np.random.randint(0, 10, size=(3, 2))
86/170: dot_product = np.dot(random_array_1, random_array_2)
86/171:
random_array_3 = np.random.randint(0, 10, size=(4, 3))
random_array_4 = np.random.randint(0, 10, size=(4, 3))
86/172: np.dot(random_array_3, random_array_4.T)
86/173:
new_random_array_1 = np.random.randint(0, 10)
new_random_array_2 = np.random.randint(0, 10)
86/174: new_random_array_1, new_random_array_2
86/175: new_random_array_1 > new_random_array_2
86/176: new_random_array_1 >= new_random_array_2
86/177: np.sort(new_random_array_1)
86/178: new_random_array_1
86/179: random_array_1
86/180: np.sort(random_array_1)
86/181: np.argmin(random_array_1)
86/182: np.min(random_array_1)
86/183: np.argmax(random_array_1)
86/184: np.max(random_array_1)
86/185: np.argmax(random_array_1)
86/186: np.max(random_array_1)
86/187: np.argmax(random_array_1, axis=0)
86/188: np.argmax(random_array_1, axis=1)
86/189: np.argmin(random_array_1, axis=0)
86/190: np.random.randint(0, 10, size=(4,4))
86/191: np.random.randint(0, 100, size=(4,4))
87/1:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
87/2: plt.plot()
87/3: plt.plot();
87/4:
plt.plot()
plt.show()
87/5: plt.plot([1, 2, 3, 4])
87/6: plt.plot([1, 2, 3, 4]);
87/7:
x = [1, 2, 3, 4]
y = [11, 22, 33, 44]
plt.plot(x, y);
87/8:
x = [1, 2, 3, 4]
y = [11, 22, 33, 44]
plt.plot(y, x);
87/9:
x = [1, 2, 3, 4]
y = [11, 22, 33, 44]
plt.plot(x, y);
87/10:
# 1st Method
fig = plt.figure() # creates a figure
ax = fig.add_subplot() # adds some axes
plt.show()
87/11:
# 2nd Method
fig = plt.figure() # creates a figure
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plt.show()
87/12:
# 3rd Method (recommended)
fig, ax = plt.subplots()
ax.plot(x, y); # add some data
87/13:
# 3rd Method (recommended)
fig, ax = plt.subplots()
ax.plot(x, [50, 100, 200, 250]); # add some data
87/14:
# 3rd Method (recommended)
fig, ax = plt.subplots()
ax.plot(x, [50, 100, 200, 250]); # add some data
type(fig), type(ax)
87/15:
# 0. import matplotlib and get it ready for plotting in Jupiter
%matplotlib inline
import matplotlib.pyplot as plt

# 1. Prepare data
x = [1, 2, 3, 4]
y = [11, 22, 33, 44]

# 2. Setup plot
fig, ax = plt.subplots(figsize=(10, 10))

# 3. Plot data
ax.plot(x, y)

# 4. Customize plot
ax.set(title="Simple Plot", xlabel="x-axis", y-label="y-axis")

# 5. Save & show (you save the whole figure)
fig.savefig('images/sample-plot.png')
87/16:
# 0. import matplotlib and get it ready for plotting in Jupiter
%matplotlib inline
import matplotlib.pyplot as plt

# 1. Prepare data
x = [1, 2, 3, 4]
y = [11, 22, 33, 44]

# 2. Setup plot
fig, ax = plt.subplots(figsize=(10, 10))

# 3. Plot data
ax.plot(x, y)

# 4. Customize plot
ax.set(title="Simple Plot", xlabel="x-axis", ylabel="y-axis")

# 5. Save & show (you save the whole figure)
fig.savefig('images/sample-plot.png')
87/17:
# 0. import matplotlib and get it ready for plotting in Jupiter
%matplotlib inline
import matplotlib.pyplot as plt

# 1. Prepare data
x = [1, 2, 3, 4]
y = [11, 22, 33, 44]

# 2. Setup plot
fig, ax = plt.subplots(figsize=(10, 10)) # (width, height)

# 3. Plot data
ax.plot(x, y)

# 4. Customize plot
ax.set(title="Simple Plot", xlabel="x-axis", ylabel="y-axis")

# 5. Save & show (you save the whole figure)
fig.savefig('images/sample-plot.png')
87/18:
# create some data
x = np.linspace(0, 10, 100)
x
87/19:
# create some data
x = np.linspace(0, 10, 100)
x[:10]
87/20:
# create some data
x = np.linspace(0, 10, 100)
x
87/21:
# create some data
x = np.linspace(0, 10, 100)
x[:10]
87/22:
# plot the data
fig, ax = plt.subplot()
ax.plot(x, x**2)
87/23:
# plot the data
fig, ax = plt.subplot()
ax.plot(x, x**2);
87/24:
# plot the data and create a line plot
fig, ax = plt.subplots()
ax.plot(x, x**2);
87/25:
# plot the data and create a line plot
fig, ax = plt.subplots()
ax.plot(x, np.power(x, 2));
87/26:
# plot the data and create a line plot
fig, ax = plt.subplots()
ax.plot(x, x**2);
ax.plot(x, np.power(x, 2));
87/27:
# plot the data and create a line plot
fig, ax = plt.subplots()
ax.plot(x, x**2)
87/28:
# plot the data and create a line plot
fig, ax = plt.subplots()
ax.plot(x, x**2);
87/29: # Use the scatter plot
87/30:
# Use same data to make a scatter plot
fig, ax = plt.subplots()
ac.scatter(x, np.exp(x))
87/31:
# Use same data to make a scatter plot
fig, ax = plt.subplots()
ac.scatter(x, np.exp(x));
87/32:
# Use same data to make a scatter plot
fig, ax = plt.subplots()
ax.scatter(x, np.exp(x));
87/33:
# Another scatter plot
fig, ax = plt.subplots()
ax.scatter(x, np.sin(x));
87/34:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x)
87/35:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x, 10, 1, align=center)
87/36:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x, 10, 1, align=centre)
87/37:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x, 10, 1, align="centre")
87/38:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x, 10, 1, align="center")
87/39:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x, 10, 0.5, align="center")
87/40:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x, 10, 0.5, align="center");
87/41:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(plot, 10, 0.5, align="center");
87/42:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(nut_buttter_prices, 10, 0.5, align="center");
87/43:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(data=nut_buttter_prices, 10, 0.5, align="center");
87/44:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(10, 0.5, align="center", data=nut_buttter_prices);
87/45:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x=nut_buttter_prices.keys(), y=nut_buttter_prices.values() 10, 0.5, align="center");
87/46:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(x=nut_buttter_prices.keys(), y=nut_buttter_prices.values(), 10, 0.5, align="center");
87/47:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(nut_buttter_prices.keys(), nut_buttter_prices.values(), 10, 0.5, align="center");
87/48:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(nut_buttter_prices.keys(), nut_buttter_prices.values(), 10, 0.8, align="center");
87/49:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(nut_buttter_prices.keys(), nut_buttter_prices.values());
87/50:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(nut_buttter_prices.keys(), height=nut_buttter_prices.values());
87/51:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(nut_buttter_prices.keys(), height=nut_buttter_prices.values())
ax.set(title="Dan's Nut Butter Store",
       ylabel="Price ($)")
87/52:
# Make a plot from a dictionary
nut_buttter_prices =  {"Almond butter": 10, "Peanut butter": 8, "Cashew butter": 12}
fig, ax = plt.subplots()
ax.bar(nut_buttter_prices.keys(), height=nut_buttter_prices.values())
ax.set(title="Dan's Nut Butter Store",
       ylabel="Price ($)");
87/53:
# Make a plot from a dictionary
nut_buttter_prices =  {
                        "Almond butter": 10, 
                        "Peanut butter": 8, 
                        "Cashew butter": 12
                      }
fig, ax = plt.subplots()
ax.bar(nut_buttter_prices.keys(), height=nut_buttter_prices.values())
ax.set(title="Dan's Nut Butter Store",
       ylabel="Price ($)");
87/54:
fig, ax = plt.subplots()
ax.barh(list(nut_buttter_prices.keys()), list(nut_buttter_prices.values()))
87/55:
fig, ax = plt.subplots()
ax.barh(list(nut_buttter_prices.keys()), list(nut_buttter_prices.values()));
87/56:
# Make some data
x = np.random.rand(1000)
87/57: x
87/58:
# Make some data
x = np.random.rand(1000)
fig, ax = plt.subplots()
ax.hist()
87/59:
# Make some data
x = np.random.rand(1000)
fig, ax = plt.subplots()
ax.hist(x)
87/60:
# Make some data
x = np.random.rand(1000)
fig, ax = plt.subplots()
ax.hist(x);
87/61:
# Make some data
x = np.random.randn(1000)
fig, ax = plt.subplots()
ax.hist(x);
87/62:
# Subplot option 1
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))
87/63:
# Subplot option 1
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))

# plot to each different axis
ax1.plot(x, x/2)
87/64:
# Subplot option 1
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))

# plot to each different axis
ax1.plot(x, x/2);
ax2.scatter(np.random.random(10), np.random.random(10));
87/65:
# Subplot option 1
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))

# plot to each different axis
ax1.plot(x, x/2);
ax2.scatter(np.random.random(10), np.random.random(10));
ax3.bar(nut_buttter_prices.keys(), nut_buttter_prices.values());
87/66:
# Subplot option 1
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))

# plot to each different axis
ax1.plot(x, x/2);
ax2.scatter(np.random.random(10), np.random.random(10));
ax3.bar(nut_buttter_prices.keys(), nut_buttter_prices.values());
ax4.hist(np.random.randn(1000));
87/67:
# Subplots option 2
fig, ax = plt.subplots(nrows=2,
                       ncols=2,
                       figsize=(10, 5))
# Plot to each different index
ax[0, 0].plot(x, x/2)
87/68:
# Subplots option 2
fig, ax = plt.subplots(nrows=2,
                       ncols=2,
                       figsize=(10, 5))
# Plot to each different index
ax[0, 0].plot(x, x/2);
87/69:
# Subplots option 2
fig, ax = plt.subplots(nrows=2,
                       ncols=2,
                       figsize=(10, 5))
# Plot to each different index
ax[0, 0].plot(x, x/2);
ax[0, 1].scatter(np.random.random(10), np.random.random(10));
87/70:
# Subplots option 2
fig, ax = plt.subplots(nrows=2,
                       ncols=2,
                       figsize=(10, 5))
# Plot to each different index
ax[0, 0].plot(x, x/2);
ax[0, 1].scatter(np.random.random(10), np.random.random(10));
ax[1, 0].bar(nut_buttter_prices.keys(), nut_buttter_prices.values());
ax[1, 1].hst(np.randon.randn(1000));
87/71:
# Subplots option 2
fig, ax = plt.subplots(nrows=2,
                       ncols=2,
                       figsize=(10, 5))
# Plot to each different index
ax[0, 0].plot(x, x/2);
ax[0, 1].scatter(np.random.random(10), np.random.random(10));
ax[1, 0].bar(nut_buttter_prices.keys(), nut_buttter_prices.values());
ax[1, 1].hist(np.randon.randn(1000));
87/72:
# Subplots option 2
fig, ax = plt.subplots(nrows=2,
                       ncols=2,
                       figsize=(10, 5))
# Plot to each different index
ax[0, 0].plot(x, x/2);
ax[0, 1].scatter(np.random.random(10), np.random.random(10));
ax[1, 0].bar(nut_buttter_prices.keys(), nut_buttter_prices.values());
ax[1, 1].hist(np.random.randn(1000));
87/73:
# Make a dataframe
car_sales = pd.read_csv('car-sales.csv')
car_sales
87/74:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts
87/75:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts.plot()
87/76:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts.cumsum()
87/77:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts.cumsum()
87/78:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts.cumsum()
87/79:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts.cumsum()
87/80:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts.cumsum()
87/81:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts.cumsum()
87/82:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts.cumsum()
87/83:
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2020', periods=1000))
ts = ts.cumsum()
ts.plot()
87/84: car_sales
87/85: car_sales
87/86:
car_sales["Price"] = car_sales["Price"].str.replace('[\$\,\.]', '')
car_sales
87/87: type(car_sales["Price"][0])
87/88:
# Remove last two zeros
car_sales["Price"] = car_sales["Price"].str[:-2]
car_sales
87/89:
car_sales["Sale Date"] = pd.date_range("1/1/2020", periods=len(car_sales))
car_sales
87/90: type(car_sales["Price"][0])
87/91:
car_sales["Total Sales"] = car_sales["Price"].astype(int).cumsum()
car_sales
87/92:
# Lets plot the car sales
car_sales.plot(x="Sale Date", y="Total Sales");
87/93:
#Reassign price column to int
car_sales["Price"] = car_sales["Price"].astype(int)

# Plot scatter plot with price column as numeric
car_sales.plot(x="Odometer (KM)", y="Price", kind="Scatter")
87/94:
#Reassign price column to int
car_sales["Price"] = car_sales["Price"].astype(int)

# Plot scatter plot with price column as numeric
car_sales.plot(x="Odometer (KM)", y="Price", kind="Scatter")
87/95:
#Reassign price column to int
car_sales["Price"] = car_sales["Price"].astype(int)

# Plot scatter plot with price column as numeric
car_sales.plot(x="Odometer (KM)", y="Price", kind="scatter")
87/96:
#Reassign price column to int
car_sales["Price"] = car_sales["Price"].astype(int)

# Plot scatter plot with price column as numeric
car_sales.plot(x="Odometer (KM)", y="Price", kind="scatter");
87/97:
# A bar graph
x = np.random.rand(10, 4)
x

# Turn it into a dataframe
df = pd.DataFrame(x, columns=['a', 'b', 'c', 'd'])
df
87/98: df.plot.bar()
87/99: df.plot.bar();
87/100: df.plot.bar();
87/101: df.plot(kind="bar");
87/102:
# A bar graph
x = np.random.rand(10, 4)
x

# Turn it into a dataframe
df = pd.DataFrame(x, columns=['a', 'b', 'c', 'd'])
df
87/103: df.plot.bar();
87/104: df.plot(kind="bar");
87/105: car_sales
87/106: car_sales.plot(x="Make", y="Odometer (KM)", kind="bar")
87/107: car_sales.plot(x="Make", y="Odometer (KM)", kind="bar");
87/108:
# Histograms

car_sales["Odometer (KM)"].plot.hist();
87/109: car_sales["Odometer (KM)"].plot(kind="hist");
87/110: car_sales["Odometer (KM)"].plot.hist(bins=30)
87/111: car_sales["Odometer (KM)"].plot.hist(bins=10);
87/112: ls
87/113:
# Using another dataset
heart_disease = pd.read_csv('heart-disease.csv')
heart_disease
87/114:
# Using another dataset
heart_disease = pd.read_csv('heart-disease.csv')
heart_disease.head()
87/115:
# Create a histogram
heart_disease["age"].plot.hist()
87/116:
# Create a histogram
heart_disease["age"].plot.hist();
87/117:
# Create a histogram
heart_disease["age"].plot.hist(bins=20);
87/118:
# Create a histogram
heart_disease["age"].plot.hist(bins=30);
87/119:
# Create a histogram
heart_disease["age"].plot.hist(bins=40);
87/120:
# Create a histogram
heart_disease["age"].plot.hist(bins=50);
87/121:
# Create a histogram
heart_disease["age"].plot.hist(bins=100);
87/122:
# Create a histogram
heart_disease["age"].plot.hist(bins=80);
87/123:
# Create a histogram
heart_disease["age"].plot.hist(bins=50);
87/124:
# Create a histogram
heart_disease["age"].plot.hist(bins=10);
87/125: heart_disease.head()
87/126: heart_disease.plot.hist(subplots=True);
87/127: heart_disease.plot.hist(figsize=(20, 30), subplots=True);
87/128:
### Which one should you use? (pyplot vs matplotlib OO method?)

* When plotting something quickly, okay to use the pyplot method
* When plotting something more advanced use the OO method
87/129: heart_disease.head()
87/130: over_50 = heart_disease[heart_disease["age"] > 50]
87/131:
over_50 = heart_disease[heart_disease["age"] > 50]
len(over_50)
87/132:
over_50 = heart_disease[heart_disease["age"] > 50]
over_50.head()
87/133:
over_50.plot(kind="scatter",
             x="age",
             y="chol",
             c="target");
87/134:
#OO method
fig, ax = plt.subplots(figsize=(10, 6))
over_50.plot(kind='scatter',
             x='age',
             y='chol',
             c='target',
             ax=ax)
87/135:
# Pyplot Method
over_50.plot(kind='scatter',
             x='age',
             y='chol',
             c='target');
87/136:
#OO method
fig, ax = plt.subplots(figsize=(10, 6))
over_50.plot(kind='scatter',
             x='age',
             y='chol',
             c='target',
             ax=ax)
ax.set_xlim([45, 200])
87/137:
#OO method
fig, ax = plt.subplots(figsize=(10, 6))
over_50.plot(kind='scatter',
             x='age',
             y='chol',
             c='target',
             ax=ax)
ax.set_xlim([45, 200]);
87/138:
#OO method
fig, ax = plt.subplots(figsize=(10, 6))
over_50.plot(kind='scatter',
             x='age',
             y='chol',
             c='target',
             ax=ax);
87/139:
# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])
87/140:
# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol");
87/141:
# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target");
87/142:
#OO method with Pyplot method
fig, ax = plt.subplots(figsize=(10, 6))
over_50.plot(kind='scatter',
             x='age',
             y='chol',
             c='target',
             ax=ax);
87/143:
# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target");
87/144:
# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target")

# Add a horizontal line
ax.axhline(over_50["chol"].mean());
87/145:
# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target")

# Add a horizontal line
ax.axhline(over_50["chol"].mean(), linestyle="-");
87/146:
# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target")

# Add a horizontal line
ax.axhline(over_50["chol"].mean(), linestyle="--");
87/147:
# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target")

# Add a horizontal line
ax.axhline(over_50["chol"].mean(), linestyle="--");
87/148: over_50.head()
87/149:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10));
87/150:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over-50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol");
87/151:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol");
87/152:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--")
87/153:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");
87/154:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"]);
87/155:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Thalac");
87/156:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Thalac")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target");
87/157:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Thalac")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalac"].mean(), linestyle='**');
87/158:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Thalach")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='**');
87/159:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Thalach")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle=':');
87/160:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Thalach")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');
87/161:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10))

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');
87/162:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        xlabel="Age",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');
87/163:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');
87/164:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold")
87/165:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/166:
# Subplots of chol, age, thalach
fig, (ax0, ax1, ax2) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/167:
# Subplots of chol, age, thalach
fig, (ax0, ax1), (ax2) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/168:
# Subplots of chol, age, thalach
fig, (ax0, ax1), (ax2, ax3) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/169:
# Subplots of chol, age, thalach
fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/170:
# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/171:  plt.style.available
87/172: car_sales["Price"].plot()
87/173: car_sales.head()
87/174: car_sales["Price"].plot();
87/175: plt.style.use('seaborn-whitegrid')
87/176: car_sales["Price"].plot();
87/177: car_sales["Price"].plot();
87/178: plt.style.use('seaborn')
87/179: car_sales["Price"].plot();
87/180: car_sales.plot(x="Odometer (KM)", y="Price", kind="scatter")
87/181: car_sales.plot(x="Odometer (KM)", y="Price", kind="scatter");
87/182:
plt.style.use('ggplot')
car_sales["Price"].plot();
87/183:
# Create some data
x = np.random.randn(10, 4)
x
87/184:
df = pd.DataFrame(x, columns=['a', 'b', 'c', 'd'])
df
87/185:
ax = df.plot(kind='bar')
type(ax)
87/186:
# Customize the plot with the set() method
ax = df.plot(kind="bar")

# Add some labels and a title
ax.set(title="Random Number Bar Graph from DataFrame"
       xlabel="Row number",
       ylabel="Random number")

# make the legend visible
ax.legend.set_visible(True)
87/187:
# Customize the plot with the set() method
ax = df.plot(kind="bar")

# Add some labels and a title
ax.set(title="Random Number Bar Graph from DataFrame",
       xlabel="Row number",
       ylabel="Random number")

# make the legend visible
ax.legend.set_visible(True)
87/188:
# Customize the plot with the set() method
ax = df.plot(kind="bar")

# Add some labels and a title
ax.set(title="Random Number Bar Graph from DataFrame",
       xlabel="Row number",
       ylabel="Random number")

# make the legend visible
ax.legend().set_visible(True)
87/189:
# Set the style
plt.style.use('seaborn-whitegrid')

# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target")

# Add a horizontal line
ax.axhline(over_50["chol"].mean(), linestyle="--");
87/190:
# Set the style
plt.style.use('seaborn')

# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"])

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target")

# Add a horizontal line
ax.axhline(over_50["chol"].mean(), linestyle="--");
87/191:
# Set the style
plt.style.use('seaborn-whitegrid')

# OO method from scratch
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the data
scatter = ax.scatter(x=over_50["age"],
                     y=over_50["chol"],
                     c=over_50["target"],
                     cmap="winter")

# Customize the plot
ax.set(title="Heart Disease and Cholesterol Levels",
       xlabel="Age",
       ylabel="Cholesterol")

# Add a legend
ax.legend(*scatter.legend_elements(), title="Target")

# Add a horizontal line
ax.axhline(over_50["chol"].mean(), linestyle="--");
87/192:
# Customizing the y and x axis limitations

# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"])

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/193:
# Customizing the y and x axis limitations

# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"],
                      cmap="winter")

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"])

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/194:
# Customizing the y and x axis limitations

# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"],
                      cmap="winter")

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"],
                      cmap="winter")

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/195:
# Customizing the y and x axis limitations

# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"],
                      cmap="winter")

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Change the x axis limit
ax0.set_xlim([50, 80])

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"],
                      cmap="winter")

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/196:
# Customizing the y and x axis limitations

# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"],
                      cmap="winter")

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Change the x axis limit
# ax0.set_xlim([50, 80])

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"],
                      cmap="winter")

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/197:
# Customizing the y and x axis limitations

# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"],
                      cmap="winter")

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Change the x axis limit
ax0.set_xlim([50, 80])

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"],
                      cmap="winter")

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/198:
# Customizing the y and x axis limitations

# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"],
                      cmap="winter")

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Change the x axis limit
ax0.set_xlim([50, 80])

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"],
                      cmap="winter")

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Change ax1 x axis limit
ax1.set_xlimit([50, 80])
ax1.set_ylimit([60, 200])

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/199:
# Customizing the y and x axis limitations

# Subplots of chol, age, thalach
fig, (ax0, ax1) = plt.subplots(nrows=2,
                               ncols=1,
                               figsize=(10, 10),
                               sharex=True)

# Add data to ax0
scatter = ax0.scatter(x=over_50["age"],
                      y=over_50["chol"],
                      c=over_50["target"],
                      cmap="winter")

# Customize ax0
ax0.set(title="Heart Disease and Cholesterol Levels",
        ylabel="Cholesterol")

# Change the x axis limit
ax0.set_xlim([50, 80])

# Add a legend to ax0
ax0.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax0.axhline(y=over_50["chol"].mean(), linestyle="--");

# Add data to ax1
scatter = ax1.scatter(x=over_50["age"],
                      y=over_50["thalach"],
                      c=over_50["target"],
                      cmap="winter")

# customize ax1
ax1.set(title="Heart Disease and Thalac",
        xlabel="Age",
        ylabel="Max Heart Rate")

# Change ax1 x axis limit
ax1.set_xlim([50, 80])
ax1.set_ylim([60, 200])

# Add a legend to ax1
ax1.legend(*scatter.legend_elements(), title="Target")

# Add a mean line
ax1.axhline(y=over_50["thalach"].mean(), linestyle='--');

# Add a title to the figure
fig.suptitle("Heart Disease Analysis", fontweight="bold");
87/200: This plot shows some information about the heart disease dataset
87/201: fig
87/202: fig.savefig('heart-disease-analysis-plot-saved-with-code.png')
87/203:
# Reset figure
fig, ax = plt.subplots()
87/204:
x = np.random.randn(0, 10, size=(10, 5))

    df = pd.DataFrame(x, columns=['a', 'b', 'c', 'd', 'e'])
87/205:
x = np.random.randn(0, 10, size=(10, 5))

df = pd.DataFrame(x, columns=['a', 'b', 'c', 'd', 'e'])
87/206:
x = np.random.randn(0, 10, size=(10, 5))

df = pd.DataFrame(x, columns=['a', 'b', 'c', 'd', 'e'])

df
87/207:
x = np.random.randn(0, 10)

df = pd.DataFrame(x, columns=['a', 'b', 'c', 'd', 'e'])

df
87/208: x = np.random.randn(0, 10)
87/209:
x = np.random.randn(0, 10)
x
87/210:
x = np.random.randn((0, 10))
x
87/211:
x = np.random.randn()
x
87/212:
x = np.random.randn(shape=(10, 5))
x
87/213:
x = np.random.randn(size=(10, 5))
x
87/214:
x = np.random.randn(0, 10, size=(10, 5))
x
87/215:
x = np.random.random(0, 10, size=(10, 5))
x
87/216:
x = np.random.randn(2, 4)
x
87/217:
x = np.random.randn(0, 10)
x
87/218:
x = np.random.randn(2, 10)
x
87/219:
x = np.random.randn(2, 10, size=(10, 5))
x
87/220:
x = np.random.randn(2, 10, size=(2, 5))
x
87/221:
x = np.random.randn(2, 10, (10, 5))
x
87/222:
x = np.random.randn(2, 10, shape=(10, 5))
x
87/223:
x = np.random.randn(2, 10, size(10, 5))
x
87/224:
x = np.random.randn(2, 10, size=(10, 5))
x
87/225:
x = np.random.randn(2, 10)
x
87/226:
x = np.random.randn(2, 10)
y = np.random.randn(50, 1000)
y
87/227:
#Potential function

data = {
    'x_values': np.random.randn(2, 10),
    'y_values': np.random.randn(50, 1000)
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(nrows=1, ncols=1, figuresize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/228:
#Potential function

data = {
    'x_values': np.random.randn(2, 10),
    'y_values': np.random.randn(50, 1000)
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/229:
#Potential function

temp_x = np.random.randn(2, 10)
temp_y = np.random.randn(50, 1000)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/230: data['x_values']
87/231: data['y_values']
87/232:
#Potential function

temp_x = np.random.randn(2, 10)
temp_y = np.random.randn(50, 100)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/233:
x = np.random.randn(2, 10)
y = np.random.randn(50, 1000)
y
87/234:
x = np.random.randrange(2, 10)
y = np.random.randn(50, 1000)
x
87/235:
x = np.randrange(2, 10)
y = np.random.randn(50, 1000)
x
87/236:
x = np.randrange(2, 10)
y = np.random.randn(50, 1000)
x
87/237:
x = np.range(2, 10)
y = np.random.randn(50, 1000)
x
87/238:
x = range(2, 10)
y = np.random.randn(50, 1000)
x
87/239:
x = np.random(0, 10)
y = np.random.randn(50, 1000)
x
87/240:
x = np.random.random(0, 10)
y = np.random.randn(50, 1000)
x
87/241:
x = np.random.random(10)
y = np.random.randn(50, 1000)
x
87/242:
x = np.random.random(10) * 10
y = np.random.randn(50, 1000)
x
87/243:
x = np.random.random(10) * 10
y = np.random.random(50) * 100
x
87/244:
x = np.random.random(10) * 10
y = np.random.random(50) * 100
y
87/245:
#Potential function

temp_x = np.random.random(10) * 10
temp_y = np.random.random(50) * 100

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/246:
x = np.random.random(10) * 10
y = list(np.random.random(50) * 100)
y
87/247:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(50) * 100)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/248: data['y_values']
87/249:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(50) * 100)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/250:
x = list(np.random.random(10) * 10)
y = list(np.random.random(50) * 100)
y
87/251:
x = list(np.random.random(10) * 10)
y = list(np.random.random(10) * 200)
y
87/252:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(50) * 100)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/253:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.save('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/254:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plot

plotting_workflow(data)
87/255:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data)
87/256:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/257:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y, kind="bar")
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/258:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
#     ax.plot(x, y)
    ax.hist()
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/259:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
#     ax.plot(x, y)
    ax.hist(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/260:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
#     ax.plot(x, y)
    ax.hist([x, y])
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/261:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
#     ax.plot(x, y)
    ax.hist([x])
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/262:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
#     ax.plot(x, y)
    ax.hist(x)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/263:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
#     ax.plot(x, y)
    ax.hist([x])
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/264:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
#     ax.plot(x, y)
    ax.hist(y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/265:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/266:
#Potential function

temp_x = list(np.random.random(10) * 10)
temp_y = list(np.random.random(10) * 200)

data = {
    'x_values': temp_x,
    'y_values': temp_y
}

def plotting_workflow(data):
    # 1. Manipulate data
    x = data['x_values']
    y = data['y_values']
    
    # 2. Create plot
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # 3. Plot data
    ax.plot(x, y)
    
    # 4. Customize plot
    ax.set(title="Sample plot", xlabel="X axis", ylabel="Y axis")
    
    # 5. Save plot
    fig.savefig('sample-plot-1.png')
    
    # 6. Return plot
    
    return plt

plotting_workflow(data);
87/267:
for n in range(1, 100):
    if n % 2 != 0:
        print("Weird")
    elif n % 2 == 0 and 2 <= n and n <= 5:
        print("Not Weird")
    elif n % 2 == 0 and 6 <= n and n <= 10:
        print("Weird")
    elif n % 2 == 0 and n > 20:
        print("Not Weird")
87/268:
for n in range(1, 100):
    if n % 2 != 0:
        print("{n}Weird")
    elif n % 2 == 0 and 2 <= n and n <= 5:
        print("{n}Not Weird")
    elif n % 2 == 0 and 6 <= n and n <= 10:
        print("{n}Weird")
    elif n % 2 == 0 and n > 20:
        print("{n}Not Weird")
87/269:
for n in range(1, 100):
    if n % 2 != 0:
        print(f"{n}Weird")
    elif n % 2 == 0 and 2 <= n and n <= 5:
        print(f"{n}Not Weird")
    elif n % 2 == 0 and 6 <= n and n <= 10:
        print(f"{n}Weird")
    elif n % 2 == 0 and n > 20:
        print(f"{n}Not Weird")
87/270:
for n in range(1, 100):
    if n % 2 != 0:
        print(f"{n}Weird")
    elif n % 2 == 0 and 2 <= n <= 5:
        print(f"{n}Not Weird")
    elif n % 2 == 0 and 6 <= n <= 10:
        print(f"{n}Weird")
    elif n % 2 == 0 and n > 20:
        print(f"{n}Not Weird")
87/271:
for n in range(1, 100):
    if n % 2 != 0:
        print(f"{n}Weird")
    elif n % 2 == 0 and 2 <= n and n <= 5:
        print(f"{n}Not Weird")
    elif n % 2 == 0 and 6 <= n and n <= 10:
        print(f"{n}Weird")
    elif n % 2 == 0 and n > 20:
        print(f"{n}Not Weird")
88/1:
# 1.Get the data ready
import pandas as pd
heart_disease = pd.read_csv('data/heart-disease.csv')
heart_disease.head()
88/2:
# Create X (features matrix)
X = heart_disease.drop("target", axis=1)

# Create y (labels)
y = heart_disease["target"]
88/3:
# 2. Choose the right model and hyperparameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We'll keep the default hyperparameters
clf.get_params_
88/4:
# 2. Choose the right model and hyperparameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We'll keep the default hyperparameters
clf.get_params
88/5:
# 2. Choose the right model and hyperparameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We'll keep the default hyperparameters
clf.get_params()
88/6:
# 3. Fit the model to the data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
88/7: clf.fit(X_train, y_train)
88/8: clf.fit(X_train, y_train);
88/9: clf.fit(X_train, y_train);
88/10: X_train
88/11:
# make a prediction
y_label = clf.predict(np.array([0, 2, 3, 4]))
88/12:
y_preds = clf.predict(X_test)
y_preds
88/13: y_test
88/14:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
88/15: clf.score(X_test, y_test)
88/16:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
88/17: confusion_matrix(y_test, y_preds)
88/18: accuracy_score(y_test, y_preds)
88/19:
# 5. Improve a model
# Try different amount of n_estimators
np.random.seed(42)
for i in range(10, 100, 10):
    print(f"Trying model with {i} estimators...")
    clf = RandomForestClassifier(n_estimators=i)
    print(f"Model Accurscy on test set: {clf.score(X_test, y_test) * 100:.2f}%")
    print("")
88/20: import numpy as np
88/21:
# 5. Improve a model
# Try different amount of n_estimators
np.random.seed(42)
for i in range(10, 100, 10):
    print(f"Trying model with {i} estimators...")
    clf = RandomForestClassifier(n_estimators=i)
    print(f"Model Accurscy on test set: {clf.score(X_test, y_test) * 100:.2f}%")
    print("")
88/22:
# 5. Improve a model
# Try different amount of n_estimators
np.random.seed(42)
for i in range(10, 100, 10):
    print(f"Trying model with {i} estimators...")
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f"Model Accurscy on test set: {clf.score(X_test, y_test) * 100:.2f}%")
    print("")
88/23:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forst_model_1.pk1', 'wb'))
88/24:
loaded_model = picke.load(open('random_forst_model_1.pk1', 'rb'))
loaded_model.score(X_test, y_test)
88/25:
loaded_model = pickle.load(open('random_forst_model_1.pk1', 'rb'))
loaded_model.score(X_test, y_test)
88/26:
# 2. Choose the right model and hyperparameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We'll keep the default hyperparameters
# clf.get_params()
88/27:
# 3. Fit the model to the data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
88/28: clf.fit(X_train, y_train);
88/29:
y_preds = clf.predict(X_test)
y_preds
88/30: y_test
88/31:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
88/32: clf.score(X_test, y_test)
88/33:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
88/34: confusion_matrix(y_test, y_preds)
88/35: accuracy_score(y_test, y_preds)
88/36:
# Standard imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
88/37: heart_disease.head()
88/38:
X = heart_disease.drop("target", axis=1)
X.head()
88/39:
y = heart_disease["target"]
y.head()
88/40:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
88/41: X_train.shape, X_test.shape, y_train.shape, y_test.shape
88/42: X.shape
88/43: len(heart_disease)
88/44: X.shape[0] * 0.8
88/45:
# read the car sales dataset
car_sales = pd.read_csv('data/car-sales.csv')
car_sales.head()
88/46:
# read the car sales dataset
car_sales = pd.read_csv('data/car-sales.csv')
car_sales
88/47:
car_sales.drop('colour', axis=1)
car_sales
88/48:
X = car_sales.drop('colour', axis=1)
X
88/49:
X = car_sales.drop('Colour', axis=1)
X
88/50:
y = car_sales["Colour"]
y
88/51:
# split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, text_size=0.2)
88/52:
# split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, text_size=0.4)
88/53:
# split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)
88/54: X_train.shape
88/55: X_train.shape, X_test.shape, y_train.shape, y_test.shape
88/56: len(heart_disease)
88/57:
car_sales = pd.read_csv('data/car-sales-extended.csv')
car_sales.head()
88/58: len(car_sales)
88/59: car_sales.dtypes
88/60:
# Split into X/y
X = car_sales.drop('Price', axis=1)
y = car_sales['Price']

# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
88/61:
# Build machine learning model
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model,score(X_test, y_test)
88/62: car_sales['Doors']
88/63: car_sales['Doors'].value_counts()
88/64:
# Turn the categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_feature = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)]
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X
88/65:
# Turn the categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_feature = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)]
                                   remainders="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X
88/66:
# Turn the categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_feature = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainders="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X
88/67:
# Turn the categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_feature = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X
88/68:
# Turn the categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_feature = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_feature)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X
88/69:
# Turn the categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X
88/70: pd.DataFrame(transformed_X)
88/71: X
88/72: X.head()
88/73:
dummies = pd.get_dummies(car_sales[["Make", "Colour", "Doors"]])
dummies
88/74:
# Let's refit the model
np.ransom.seed(42)
X_train, X_test, y_train, x_test = train_test_split(transformed_X,
                                                    y, 
                                                    test_size=0.2)

model.fit(X_train, y_train)
88/75:
# Let's refit the model
np.random.seed(42)
X_train, X_test, y_train, x_test = train_test_split(transformed_X,
                                                    y, 
                                                    test_size=0.2)

model.fit(X_train, y_train)
88/76: model.score(X_test, y_test)
88/77: model.score(X_test, y_test)
88/78: model.score(X_test, y_test)
88/79:
# Let's refit the model
np.random.seed(42)
X_train, X_test, y_train, x_test = train_test_split(transformed_X,
                                                    y, 
                                                    test_size=0.2)

model.fit(X_train, y_train)
88/80: model.score(X_test, y_test)
88/81:
# Build machine learning model
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/82:
# Build machine learning model
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/83:
# Let's refit the model
np.random.seed(42)
X_train, X_test, y_train, x_test = train_test_split(transformed_X,
                                                    y, 
                                                    test_size=0.2)

model.fit(X_train, y_train)
88/84:
# import car sales missing data
car_sales_missing = pd.read_csv("data/car-sales-extended-missing-data.csv")
car_sales.head()
88/85: car_sales_missing.isna()
88/86: car_sales_missing.isna().sum()
88/87:
# Create X & y
X = car_sales_missing.drop("Price", axis=1)
y = car_sales_missing["Price"]
88/88:
# Lets try and convert our data to numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X
88/89: #### Option 1:Fill missing data with Pandas
88/90:
# Fill the "Make" column
car_sale_missing["Make"].fillna("missing", inplace=True)

# Fill the "Colour" column
car_sales_missing["colour"].fillna("missing", inplace=True)

# Fill the "Odometer (KM)" column
car_sales_missing["Odometer (KM)"].fillna(car_sales_missing["Odometer (KM)"].mean(), inplace=True)

# Fill the "Doors" column
car_sales_missing["Doors"].fillna(4, inplace=True)
88/91:
# Fill the "Make" column
car_sales_missing["Make"].fillna("missing", inplace=True)

# Fill the "Colour" column
car_sales_missing["colour"].fillna("missing", inplace=True)

# Fill the "Odometer (KM)" column
car_sales_missing["Odometer (KM)"].fillna(car_sales_missing["Odometer (KM)"].mean(), inplace=True)

# Fill the "Doors" column
car_sales_missing["Doors"].fillna(4, inplace=True)
88/92:
# Fill the "Make" column
car_sales_missing["Make"].fillna("missing", inplace=True)

# Fill the "Colour" column
car_sales_missing["Colour"].fillna("missing", inplace=True)

# Fill the "Odometer (KM)" column
car_sales_missing["Odometer (KM)"].fillna(car_sales_missing["Odometer (KM)"].mean(), inplace=True)

# Fill the "Doors" column
car_sales_missing["Doors"].fillna(4, inplace=True)
88/93:
# Check our dataframe again
car_sales_missing.isna().sum()
88/94:
# Remove rows with missing Price value
car_sales_missing.dropna(inplace=True)
88/95: car_sales_missing.isna().sum()
88/96: len(car_sales_missing)
88/97:
X = car_sales_missing.drop("Price", axis=1)
y = car_sales_missing["Price"]
88/98:
# Lets try and convert our data to numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X
88/99:
# Lets try and convert our data to numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(car_sales_missing)
transformed_X
88/100:
car_sales_missing = pd.read_csv("data/car-sales-extended-missing-data.csv")
car_sales_missing.head()
88/101: car_sales_missing.isna().sum()
88/102:
#Drop the rows with no labels
car_sales_missing.dropna(subset=["Price"], inplace=True)
car_sales_missing.isna().sum()
88/103:
# Split into X & y
X = car_sales_missing.drop("Price", axis=1)
y = car_sales_missing["Price"]
88/104: X.isna().sum()
88/105:
# Fill missing valus with Scikit-Learn
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

# Fill categorical values with 'missing' & numerical values with mean
cat_imputer = SimpleImputer(strategy="constant", fil_value="missing")
door_imputer = SimpleImputer(strategy="constant", fill_value=4)
num_imputer = SimpleImputer(strategy="mean")

# Define columns
cat_features = ["Make", "Colour"]
door_feature = ["Doors"]
num_features = ["Odometer (KM)"]

# Create an imputer (something thst fills missing data)
imputer = ColumnTransformer([
    ("cat_imputer", cat_imputer, cat_features),
    ("door_imputer", door_imputer, door_feature),
    ("num_imputer", num_imputer, num_feature)
])

# Transform the data
filled_X = imputer.fit_transform(X)
filled_X
88/106:
# Fill missing valus with Scikit-Learn
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

# Fill categorical values with 'missing' & numerical values with mean
cat_imputer = SimpleImputer(strategy="constant", fill_value="missing")
door_imputer = SimpleImputer(strategy="constant", fill_value=4)
num_imputer = SimpleImputer(strategy="mean")

# Define columns
cat_features = ["Make", "Colour"]
door_feature = ["Doors"]
num_features = ["Odometer (KM)"]

# Create an imputer (something thst fills missing data)
imputer = ColumnTransformer([
    ("cat_imputer", cat_imputer, cat_features),
    ("door_imputer", door_imputer, door_feature),
    ("num_imputer", num_imputer, num_feature)
])

# Transform the data
filled_X = imputer.fit_transform(X)
filled_X
88/107:
# Fill missing valus with Scikit-Learn
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

# Fill categorical values with 'missing' & numerical values with mean
cat_imputer = SimpleImputer(strategy="constant", fill_value="missing")
door_imputer = SimpleImputer(strategy="constant", fill_value=4)
num_imputer = SimpleImputer(strategy="mean")

# Define columns
cat_features = ["Make", "Colour"]
door_feature = ["Doors"]
num_feature = ["Odometer (KM)"]

# Create an imputer (something thst fills missing data)
imputer = ColumnTransformer([
    ("cat_imputer", cat_imputer, cat_features),
    ("door_imputer", door_imputer, door_feature),
    ("num_imputer", num_imputer, num_feature)
])

# Transform the data
filled_X = imputer.fit_transform(X)
filled_X
88/108:
car_sales_filled = pd.DataFrame(filled_X,
                                columns=["Make", "Colours", "Doors", "Odometer (KM)"])
88/109:
car_sales_filled = pd.DataFrame(filled_X,
                                columns=["Make", "Colours", "Doors", "Odometer (KM)"])
car_sales_filled
88/110:
car_sales_filled = pd.DataFrame(filled_X,
                                columns=["Make", "Colours", "Doors", "Odometer (KM)"])
car_sales_filled.head()
88/111: car_sales_filled.isna().sum()
88/112:
# Lets try and convert our data to numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
88/113:
# Lets try and convert our data to numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
88/114:
car_sales_filled = pd.DataFrame(filled_X,
                                columns=["Make", "Colours", "Doors", "Odometer (KM)"])
car_sales_filled.head()
88/115:
# Lets try and convert our data to numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors", "Odometer (KM)"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
88/116:
# Lets try and convert our data to numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
88/117:
car_sales_filled = pd.DataFrame(filled_X,
                                columns=["Make", "Colour", "Doors", "Odometer (KM)"])
car_sales_filled.head()
88/118: car_sales_filled.isna().sum()
88/119:
# Lets try and convert our data to numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", 
                                   one_hot, 
                                   categorical_features)],
                                   remainder="passthrough")

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
88/120:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.esemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X,
                                                    y,
                                                    test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/121:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X,
                                                    y,
                                                    test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/122: len(car_sales_filled), len(car_sales)
88/123:
what_we_are_covering = [

'0. An end-to-end Scikit-Learn workflow',
'1. Getting the data ready',
'2. Choose the right estimator/algorithm for our problems',
'3. Fit the model/algorithm for our problems',
'4. Evaluating a model',
'6. Save and load a trained model',
'7. Putting it all together!'
]
88/124: what_we_are_covering
88/125: ### 2.1 Picking a machine learning model for a regression problem
88/126:
# Import Boston housing dataset
from sklearn.datasets import load_boston
boston = load_boston()
boston
88/127:
# Import Boston housing dataset
from sklearn.datasets import load_boston
boston = load_boston()
boston;
88/128:
boston_df = pd.DataFrame(boston["data"], columns=boston["feature_names"])
boston_df["target"] = pd.Series(boston["target"])
boston_df.head()
88/129:
# How may samples?
len(boston_df)
88/130:
# Let's try the Ridge Regression model
from sklearn.linear_model import Ridge

# Setup random seed
np.random.seed(42)

# Create the data
X = boston_df.drop("target", axis=1)
y = boston_df["target"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Ridge model
model = Ridge()
model.fit(X_train, y_train)

# Check the score of the Ridge model on the test data
model.score(X_test, y_test)
88/131:
# Let's try the Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

# Setup random seed
np.random.seed(42)

# Create the data
X = boston_df.drop("target", axis=1)
y = boston_df["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Random Forest Regressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)

# Evaluate the Random Forest Regressor
rf.score(X_test, y_test)
88/132:
# Let's try the Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

# Setup random seed
np.random.seed(42)

# Create the data
X = boston_df.drop("target", axis=1)
y = boston_df["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Random Forest Regressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)

# Evaluate the Random Forest Regressor
rf.score(X_test, y_test)
88/133:
# Check the Ridge model again
model.score(X_test, y_test)
88/134:
heart_disease = pd.read_csv('data/heart-disease.csv')
heart_disease.head()
88/135: len(heart_disease)
88/136:
# Import the LinearSVC estimator class
import sklearn.svm import LinearSVC

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instatiate LinearSVC
clf = LinearSVC()
clf.fit(X_train, y_train)

# Evaluate the LinearSVC
clf.score(X_test, y_test)
88/137:
# Import the LinearSVC estimator class
from sklearn.svm import LinearSVC

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instatiate LinearSVC
clf = LinearSVC()
clf.fit(X_train, y_train)

# Evaluate the LinearSVC
clf.score(X_test, y_test)
88/138:
# Import the LinearSVC estimator class
from sklearn.svm import LinearSVC

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instatiate LinearSVC
clf = LinearSVC(max_iter=1000)
clf.fit(X_train, y_train)

# Evaluate the LinearSVC
clf.score(X_test, y_test)
88/139: heart_disease["target"].value_counts()
88/140:
# Import the RandomForestClassifier estimator class
from sklearn.ensemble import RandomForestClassifier

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instatiate Random Forest Classifier
clf = RandomForestClassifier
clf.fit(X_train, y_train)

# Evaluate the Random Forest Classifier
clf.score(X_test, y_test)
88/141:
# Import the RandomForestClassifier estimator class
from sklearn.ensemble import RandomForestClassifier

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instatiate Random Forest Classifier
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Evaluate the Random Forest Classifier
clf.score(X_test, y_test)
88/142:
Tidbit:
    1. If you have structured data, use ensemble methods
    2. If you have unstructured data, use deep learning or transfer learning
88/143:
# Import the RandomForestClassifier estimator class
from sklearn.ensemble import RandomForestClassifier

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instatiate Random Forest Classifier
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Evaluate the Random Forest Classifier
clf.score(X_test, y_test)
88/144: what_we_are_covering
88/145: ## 3. Fit the model/algorithm on our data and use it to make predictions
88/146: X.head()
88/147: y.head()
88/148: y.tail()
88/149:
# Import the RandomForestClassifier estimator class
from sklearn.ensemble import RandomForestClassifier

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instatiate Random Forest Classifier
clf = RandomForestClassifier()

# Fit the model to the data (training the machine learning model)
clf.fit(X_train, y_train)

# Evaluate the Random Forest Classifier (use the patterns the model has learned)
clf.score(X_test, y_test)
88/150:
# Import the RandomForestClassifier estimator class
from sklearn.ensemble import RandomForestClassifier

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instatiate Random Forest Classifier
clf = RandomForestClassifier()

# Fit the model to the data (training the machine learning model)
clf.fit(X_train, y_train)

# Evaluate the Random Forest Classifier (use the patterns the model has learned)
clf.score(X_test, y_test)
88/151:
# Use a trained model to make predictions
clf.predict(np.array([1, 7, 8, 3, 4]))
88/152: X_test.head()
88/153: clf.predict(X_test)
88/154: y_test
88/155: np.array(y_test)
88/156:
# Compare predictions to truth labels to evaluate the model
y_preds = clf.predict(X_test)
np.mean(y_preds == y_test)
88/157: clf.score(X_test, y_test)
88/158:
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_preds)
88/159:
# predict_proba() returns probabilities of a classification label
clf.predict_proba(X_test[:5])
88/160:
# Let's predict() on the same data...
clf.predict(X_test[:5])
88/161:
from sklearn.ensemble import RandomForestRegressor

np.random.seed(42)

# Create the data
X = boston_df.drop("target", axis=1)
y = boston_df["target"]

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate and fit model
model = RandomForestRegressor().fit(X_train, y_train)

# Make predictions
y_preds = model.predict(X_test)
88/162: y_preds[:10]
88/163: np.array(y_test[:10])
88/164:
# Compare the predictions to be true
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test, y_preds)
88/165: what_we_are_covering
88/166:
# Compare the predictions to be true
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test, y_preds)
88/167:
heart_disease = pd.read_csv('data/heart-disease.csv')
heart_disease.head()
88/168:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)

y = heart_disease["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, text_size=0.2)

clf = RandomForestClassifier()
clf.fit(X_train, y_train)
88/169:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)

y = heart_disease["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = RandomForestClassifier()

clf.fit(X_train, y_train)
88/170:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)

y = heart_disease["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = RandomForestClassifier()

clf.fit(X_train, y_train);
88/171: clf.score(X_test, y_test)
88/172:
from sklearn.ensemble import RandomForestRegressor

np.random.seed(42)

# Create the data
X = boston_df.drop("target", axis=1)
y = boston_df["target"]

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate and fit model
model = RandomForestRegressor().fit(X_train, y_train)
88/173: model.score(X_test, y_test)
88/174:
from sklearn.model_selection import cross_val_score

from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)

y = heart_disease["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = RandomForestClassifier()

clf.fit(X_train, y_train);
88/175: clf.score(X_test, y_test)
88/176: cross_val_score(clf, X, y)
88/177: cross_val_score(clf, X, y, cv=5)
88/178: cross_val_score(clf, X, y)
88/179: cross_val_score(clf, X, y, cv=10)
88/180:
np.random.seed(42)

# Single training and test split score
clf_single_score clf.score(X_test, y_test)

# Take the mean of 5-fold cross-validation score
clf_single_score = np.mean(cross_val_score(clf, X, y))

# Compare the two
clf_single_score, clf_cross_val_score
88/181:
np.random.seed(42)

# Single training and test split score
clf_single_score = clf.score(X_test, y_test)

# Take the mean of 5-fold cross-validation score
clf_single_score = np.mean(cross_val_score(clf, X, y))

# Compare the two
clf_single_score, clf_cross_val_score
88/182:
np.random.seed(42)

# Single training and test split score
clf_single_score = clf.score(X_test, y_test)

# Take the mean of 5-fold cross-validation score
clf_cross_val_score = np.mean(cross_val_score(clf, X, y))

# Compare the two
clf_single_score, clf_cross_val_score
88/183:
# Default scoring parameter of classifier = mean accuracy
clf.score()
88/184:
# Scoring parameter set to None by default
cross_val_score(clf, X, y, scoring=None)
88/185:
np.random.seed(42)

# Single training and test split score
clf_single_score = clf.score(X_test, y_test)

# Take the mean of 5-fold cross-validation score
clf_cross_val_score = np.mean(cross_val_score(clf, X, y))

# Compare the two
clf_single_score, clf_cross_val_score
88/186:
# Default scoring parameter of classifier = mean accuracy
clf.score(X, y)
88/187:
# Default scoring parameter of classifier = mean accuracy
clf.score(X, y)
88/188:
# Default scoring parameter of classifier = mean accuracy
clf.score(X_test, y_test)
88/189:
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

clf = RandomForestClassifier()
cross_val_score(clf, X, y)
88/190:
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

clf = RandomForestClassifier()
cross_val_score(clf, X, y)
88/191:
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

clf = RandomForestClassifier()
cross_val_score = cross_val_score(clf, X, y)
88/192: np.mean(cross_val_score)
88/193: print(f"Heart Disease Classifier Cross-Validated Acccuracy: { np.mean(cross_val_score) * 100:.2f }%")
88/194: print(f"Heart Disease Classifier Cross-Validated Acccuracy: {np.mean(cross_val_score) * 100:.2f}%")
88/195:
from sklearn.metrics import roc_curve

# Make predictions with probabilities
y_probs = clf.predict_proba(X_test)

y_probs
88/196:
from sklearn.metrics import roc_curve

# Fit the classifier
clf.fit(X_train, y_train)

# Make predictions with probabilities
y_probs = clf.predict_proba(X_test)

y_probs
88/197:
from sklearn.metrics import roc_curve

# Fit the classifier
clf.fit(X_train, y_train)

# Make predictions with probabilities
y_probs = clf.predict_proba(X_test)

y_probs[:10]
88/198:
from sklearn.metrics import roc_curve

# Fit the classifier
clf.fit(X_train, y_train)

# Make predictions with probabilities
y_probs = clf.predict_proba(X_test)

y_probs[:10], len(y_probs)
88/199:
y_probs_positive = y_probs[:, 1]
y_probs_positive
88/200:
y_probs_positive = y_probs[:, 1]
y_probs_positive[:10]
88/201:
# Calculate fpr, tpr, and thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)

# Check the false positive rates
fpr
88/202:
# Create a function for plotting ROC curves
import matplotlib.pyplot as plt

def plot_roc_curve(fpr, tpr):
    """
    Plots a ROC curve given the false positive rate (fpr) and the true positive rate (tpr) of a model
    """
    
    # Plot roc curve
    plt.plot(fpr, tpr, color="orange", label="ROC")
    # Plot line with no predictive power (baseline)
    plt.plot([0, 1], [0, 1], color="darkblue", linearstyle="--", label="Guessing")
    
    # Customize the plot
    plt.xlabel("False positive rate (fpr)")
    plt.ylabel("True positive rate (tpr)")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend()
    plt.show()
    
plot_roc_curve(fpr, tpr);
88/203:
# Create a function for plotting ROC curves
import matplotlib.pyplot as plt

def plot_roc_curve(fpr, tpr):
    """
    Plots a ROC curve given the false positive rate (fpr) and the true positive rate (tpr) of a model
    """
    
    # Plot roc curve
    plt.plot(fpr, tpr, color="orange", label="ROC")
    # Plot line with no predictive power (baseline)
    plt.plot([0, 1], [0, 1], color="darkblue", linestyle="--", label="Guessing")
    
    # Customize the plot
    plt.xlabel("False positive rate (fpr)")
    plt.ylabel("True positive rate (tpr)")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend()
    plt.show()
    
plot_roc_curve(fpr, tpr);
88/204:
# Create a function for plotting ROC curves
import matplotlib.pyplot as plt

def plot_roc_curve(fpr, tpr):
    """
    Plots a ROC curve given the false positive rate (fpr) and the true positive rate (tpr) of a model
    """
    
    # Plot roc curve
    plt.plot(fpr, tpr, color="orange", label="ROC")
    # Plot line with no predictive power (baseline)
    plt.plot([0, 1], [0, 1], color="darkblue", linestyle="--", label="Guessing")
    
    # Customize the plot
    plt.xlabel("False positive rate (fpr)")
    plt.ylabel("True positive rate (tpr)")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend()
    plt.show()
    
plot_roc_curve(fpr, tpr);
88/205:
from sklearn.metrics import roc_auc_score

roc_auc_score(y_test, y_probs_positive)
88/206:
from sklearn.metrics import roc_auc_score

roc_auc_score(y_test, y_probs_positive)
88/207:
# Plot perfect ROC curve and AUC score
fpr, tpr, threshold = roc_auc_score(y_test, y_test)
plot_roc_curve(fpr, tpr)
88/208:
# Plot perfect ROC curve and AUC score
fpr, tpr, thresholds = roc_auc_score(y_test, y_test)
plot_roc_curve(fpr, tpr)
88/209:
# Plot perfect ROC curve and AUC score
fpr, tpr, thresholds = roc_curve(y_test, y_test)
plot_roc_curve(fpr, tpr)
88/210:
# Perfect AUC score
roc_auc_score(y_test, y_test)
88/211:
# Perfect AUC score
roc_auc_score(X_test, y_test)
88/212:
# Perfect AUC score
roc_auc_score(y_test, y_test)
88/213:
# Create a function for plotting ROC curves
import matplotlib.pyplot as plt

def plot_roc_curve(fpr, tpr):
    """
    Plots a ROC curve given the false positive rate (fpr) and the true positive rate (tpr) of a model
    """
    
    # Plot roc curve
    plt.plot(fpr, tpr, color="orange", label="ROC")
    # Plot line with no predictive power (baseline)
#     plt.plot([0, 1], [0, 1], color="darkblue", linestyle="--", label="Guessing")
    
    # Customize the plot
    plt.xlabel("False positive rate (fpr)")
    plt.ylabel("True positive rate (tpr)")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend()
    plt.show()
    
plot_roc_curve(fpr, tpr);
88/214:
# Create a function for plotting ROC curves
import matplotlib.pyplot as plt

def plot_roc_curve(fpr, tpr):
    """
    Plots a ROC curve given the false positive rate (fpr) and the true positive rate (tpr) of a model
    """
    
    # Plot roc curve
    plt.plot(fpr, tpr, color="orange", label="ROC")
    # Plot line with no predictive power (baseline)
    plt.plot([0, 1], [0, 1], color="darkblue", linestyle="--", label="Guessing")
    
    # Customize the plot
    plt.xlabel("False positive rate (fpr)")
    plt.ylabel("True positive rate (tpr)")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend()
    plt.show()
    
plot_roc_curve(fpr, tpr);
88/215:
# Create X_test... etc
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
88/216:
from sklearn.metrics import roc_curve

# Fit the classifier
clf.fit(X_train, y_train)

# Make predictions with probabilities
y_probs = clf.predict_proba(X_test)

y_probs[:10], len(y_probs)
88/217:
y_probs_positive = y_probs[:, 1]
y_probs_positive[:10]
88/218:
# Calculate fpr, tpr, and thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)

# Check the false positive rates
fpr
88/219:
# Create a function for plotting ROC curves
import matplotlib.pyplot as plt

def plot_roc_curve(fpr, tpr):
    """
    Plots a ROC curve given the false positive rate (fpr) and the true positive rate (tpr) of a model
    """
    
    # Plot roc curve
    plt.plot(fpr, tpr, color="orange", label="ROC")
    # Plot line with no predictive power (baseline)
    plt.plot([0, 1], [0, 1], color="darkblue", linestyle="--", label="Guessing")
    
    # Customize the plot
    plt.xlabel("False positive rate (fpr)")
    plt.ylabel("True positive rate (tpr)")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend()
    plt.show()
    
plot_roc_curve(fpr, tpr);
88/220: **Confusion Matrix**
88/221: from sklearn.metrics import confusion_matrix
88/222:
from sklearn.metrics import confusion_matrix

y_preds = clf.predict(X_test)

confusion_matrix(y_test, y_preds)
88/223:
# Visualize confusion matrix with pd.crosstab()
pd.crosstab(y_test, y_preds, rownames=["Actual Label"], colnames=["Predicted Labels"])
88/224: len(y_preds)
88/225: len(X_test)
88/226:
# Make our confusion matrix more visual with Seaborn's heatmap()
import seaborn as sns

# Set the font scale
sns.set(font_sale=1.5)

# Create a confusion matrix
conf_mat = confusion_matrix(y_test, y_preds)

# Plot it using Seaborn
sns.heatmap(conf_mat);
88/227:
# Make our confusion matrix more visual with Seaborn's heatmap()
import seaborn as sns

# Set the font scale
sns.set(font_scale=1.5)

# Create a confusion matrix
conf_mat = confusion_matrix(y_test, y_preds)

# Plot it using Seaborn
sns.heatmap(conf_mat);
88/228:
def plot_conf_mat(conf_mat):
    """
    Plots a confusion matrix using Seaborn's heatmap().
    """
    fig, ax = plt.subplots(figsize(3,3))
    ax = sms.heatmap(conf_mat,
                     anoot=True # Annotate the boxes with conf_mat info
                     cbar=False)
    plt.xlabel("True label")
    plt.ylabel("Predicted label")
    
plot_conf_mat(conf_mat)
88/229:
def plot_conf_mat(conf_mat):
    """
    Plots a confusion matrix using Seaborn's heatmap().
    """
    fig, ax = plt.subplots(figsize(3,3))
    ax = sms.heatmap(conf_mat,
                     anoot=True, # Annotate the boxes with conf_mat info
                     cbar=False)
    plt.xlabel("True label")
    plt.ylabel("Predicted label")
    
plot_conf_mat(conf_mat)
88/230:
def plot_conf_mat(conf_mat):
    """
    Plots a confusion matrix using Seaborn's heatmap().
    """
    fig, ax = plt.subplots(figSize(3,3))
    ax = sms.heatmap(conf_mat,
                     anoot=True, # Annotate the boxes with conf_mat info
                     cbar=False)
    plt.xlabel("True label")
    plt.ylabel("Predicted label")
    
plot_conf_mat(conf_mat)
88/231:
def plot_conf_mat(conf_mat):
    """
    Plots a confusion matrix using Seaborn's heatmap().
    """
    fig, ax = plt.subplots(figsize=(3,3))
    ax = sms.heatmap(conf_mat,
                     anoot=True, # Annotate the boxes with conf_mat info
                     cbar=False)
    plt.xlabel("True label")
    plt.ylabel("Predicted label")
    
plot_conf_mat(conf_mat)
88/232:
def plot_conf_mat(conf_mat):
    """
    Plots a confusion matrix using Seaborn's heatmap().
    """
    fig, ax = plt.subplots(figsize=(3,3))
    ax = sns.heatmap(conf_mat,
                     anoot=True, # Annotate the boxes with conf_mat info
                     cbar=False)
    plt.xlabel("True label")
    plt.ylabel("Predicted label")
    
plot_conf_mat(conf_mat)
88/233:
def plot_conf_mat(conf_mat):
    """
    Plots a confusion matrix using Seaborn's heatmap().
    """
    fig, ax = plt.subplots(figsize=(3,3))
    ax = sns.heatmap(conf_mat,
                     annot=True, # Annotate the boxes with conf_mat info
                     cbar=False)
    plt.xlabel("True label")
    plt.ylabel("Predicted label")
    
plot_conf_mat(conf_mat)
88/234:
from sklearn.metrics import classification_report

print(classification_report(y_test, y_preds))
88/235:
# Where precision and recall become valuable
disease_true = np.zeros(10000)
disease_true[0] = 1 # only one positive case

disease_preds = np.zers(10000) # model predicts every case as 0

pd.DataFrame(classification_report(disease_true, disease_preds, output_dict=True))
88/236:
# Where precision and recall become valuable
disease_true = np.zeros(10000)
disease_true[0] = 1 # only one positive case

disease_preds = np.zeros(10000) # model predicts every case as 0

pd.DataFrame(classification_report(disease_true, disease_preds, output_dict=True))
88/237:
from sklearn.ensemble import RandomForestRegressor

np.random.seed(42)

X = boston_df.drop("target", axis=1)
y = boston_df["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train);
88/238: model.score(X_test, y_test)
88/239:
from sklearn.metrics import r2_score

# Fill an array with y_test_mean
y_test_mean = np.full(len(y_test), y_test.mean())
88/240: y_test
88/241: y_test.mean()
88/242: r2_score(y_test, y_test_mean)
88/243: y_test_mean
88/244: r2_score(y_test, y_test)
88/245: r2_score(y_test, y_test_mean)
88/246: **Mean absolute error (MAE)**
88/247:
# Mean absolute error
from sklearn.metrics import mean_absolute_error

y_preds = model.predict(X_test)
mae = mean_absolute_error(y_test, y_preds)
mae
88/248: df = pd.DataFrame(data={"actual values": y_test, "predicted values": y_preds})
88/249:
df = pd.DataFrame(data={"actual values": y_test, "predicted values": y_preds})
df
88/250:
df = pd.DataFrame(data={"actual values": y_test, "predicted values": y_preds})
df["difference"] = df["predicted values"] - df["actual values"]
88/251:
df = pd.DataFrame(data={"actual values": y_test, "predicted values": y_preds})
df["difference"] = df["predicted values"] - df["actual values"]
df
88/252: **Mean squared error (MSE)**
88/253:
from sklearn.metrics import mean_squared_error

y_preds = model.predict(X_test)
mse = mean_absolute_error(y_test, y_preds)
mse
88/254:
from sklearn.metrics import mean_squared_error

y_preds = model.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
mse
88/255:
# Calculate MSE by hand
np.square(df["differences"])
88/256:
# Calculate MSE by hand
np.square(df["difference"])
88/257:
# Calculate MSE by hand
np.square(df["difference"]).mean()
88/258:
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

clf = RandomForestClassifier()
88/259:
np.random.seed(42)
cv_acc = cross_val_score(clf, C, y, cv=5)
cv_acc
88/260:
np.random.seed(42)
cv_acc = cross_val_score(clf, X, y, cv=5)
cv_acc
88/261:
# Cross-validated accuracy
print(f'The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%')
88/262:
np.random.seed(42)
cv_acc = cross_val_score(clf, X, y, cv=5, scoring=None)
cv_acc
88/263:
# Cross-validated accuracy
print(f'The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%')
88/264:
cv_acc = cross_val_score(clf, X, y, cv=5, scoring="accuracy")
print(f'The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%')
88/265:
np.random.seed(42)
cv_acc = cross_val_score(clf, X, y, cv=5, scoring="accuracy")
print(f'The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%')
88/266:
cv_precision = cross_val_score(clf, X, y, cv=5, scoring="precision")
cv_precision
88/267:
cv_precision = cross_val_score(clf, X, y, cv=5, scoring="precision")
np.mean(cv_precision)
88/268:
# Recall
cv_recall = cross_val_score(clf, X, y, cv=5, scoring="recall")
np.mean(cv_recall)
88/269:
cv_f1 = cross_val_score(clf, X, y, cv=5, scoring="f1")
np.mean(cv_f1)
88/270:
np.random.seed(42)
cv_r2 = cross_val_score(model, X, y, cv=5, scoring=None)
cv_r2
88/271:
np.random.seed(42)
cv_r2 = cross_val_score(model, X, y, cv=5, scoring="r2")
cv_r2
88/272:
np.random.seed(42)
cv_r2 = cross_val_score(model, X, y, scoring=None)
cv_r2
88/273:
np.random.seed(42)
cv_r2 = cross_val_score(model, X, y, scoring="r2")
cv_r2
88/274:
# Mean absolute error
cv_mae = cross_val_score(model, X, y, cv=5, scoring="neg_mean_absolute_error")
cv_mae
88/275:
# Mean squared error
cv_mse = cross_val_score(model, X, y, cv=5, scoring="neg_mean_squared_error")
cv_mse
88/276:
# Mean squared error
cv_mse = cross_val_score(model, X, y, cv=5, scoring="neg_mean_squared_error")
cv_mse
np.mean(cv_mse)
88/277: ### 4.3 Using different evaluation metrics as Scikit-Learn functions
88/278:
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

np.random.seed(42)

X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Make some predictions
y_preds = clf.predict(X_test)

# Evaluate the classifier
print("Classifier metrics on the test set")
print(f"Accuracy: {accuracy_score(y_test, y_preds)*100:.2f}%")
print(f"Precision: {precision_score(y_test, y_preds)}")
print(f"Recall: {recall_score(y_test, y_preds)}")
print(f"F1: {f1_score(y_test, y_preds)}")
88/279: **Regression evaluation functions**
88/280:
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

np.random.split(42)

X = boston_df.drop("target", axis=1)
y = boston_df["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)

# Make predictions using our regression model
y_preds = model.predict(X_test)

# Evaluate the regression model
print("Regression model metrics on the test set")
print(f"R^2: {r2_score(y_test, y_preds)}")
print(f"MAE: {mean_absolute_error(y_test, preds)}")
print(f"MSE: {mean_sqaured_error(y_test, y_preds)}")
88/281:
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

np.random.seed(42)

X = boston_df.drop("target", axis=1)
y = boston_df["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)

# Make predictions using our regression model
y_preds = model.predict(X_test)

# Evaluate the regression model
print("Regression model metrics on the test set")
print(f"R^2: {r2_score(y_test, y_preds)}")
print(f"MAE: {mean_absolute_error(y_test, preds)}")
print(f"MSE: {mean_sqaured_error(y_test, y_preds)}")
88/282:
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

np.random.seed(42)

X = boston_df.drop("target", axis=1)
y = boston_df["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)

# Make predictions using our regression model
y_preds = model.predict(X_test)

# Evaluate the regression model
print("Regression model metrics on the test set")
print(f"R^2: {r2_score(y_test, y_preds)}")
print(f"MAE: {mean_absolute_error(y_test, y_preds)}")
print(f"MSE: {mean_sqaured_error(y_test, y_preds)}")
88/283:
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

np.random.seed(42)

X = boston_df.drop("target", axis=1)
y = boston_df["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)

# Make predictions using our regression model
y_preds = model.predict(X_test)

# Evaluate the regression model
print("Regression model metrics on the test set")
print(f"R^2: {r2_score(y_test, y_preds)}")
print(f"MAE: {mean_absolute_error(y_test, y_preds)}")
print(f"MSE: {mean_squared_error(y_test, y_preds)}")
88/284: what_we_are_covering
88/285:
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier()
clf.get_params()
88/286: clf.get_params()
88/287:
def evaluation_preds(y_true, y_preds):
    """
    Performs evaluation comparison on y_true labels vs. y_pred labels.
    """
    accuracy = accuracy(y_true, y_preds)
    precision = precision(y_true, y_preds)
    recall = recall(y_true, y_preds)
    f1 = f1_score(y_true, y_preds)
    metric_dict = {
        "accuracy": round(accuracy, 2),
        "precision": round(precision, 2),
        "recall": round(recall, 2),
        "f1": round(f1, 2)
    }
    
    print(f"Acc: {accuracy * 100:.2f}%")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 score: {f1:.2f}")
    
    
    return metric_dict
88/288:
from sklearn.ensemble import RandomForestClassifier

np,random.seed(42)

# Shuffle the data
heart_disease_shuffled = heart_disease.sample(frac=1)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease["target"]

# Split the data into train, validation & test sets
train_split = round(0.7 * len(heart_disease_shuffled)) #70% of data
valid_split = round(trained_split + 0.15 * len(heart_disease_shuffled)) # 15% of the data
X_train, y_train = X[:train_split], y[:train_split]
X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]
X_test, y_test = X[valid_split:], y[valid_split:]

len(X_train), len(X_valid), len(X_test)
88/289:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

# Shuffle the data
heart_disease_shuffled = heart_disease.sample(frac=1)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease["target"]

# Split the data into train, validation & test sets
train_split = round(0.7 * len(heart_disease_shuffled)) #70% of data
valid_split = round(trained_split + 0.15 * len(heart_disease_shuffled)) # 15% of the data
X_train, y_train = X[:train_split], y[:train_split]
X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]
X_test, y_test = X[valid_split:], y[valid_split:]

len(X_train), len(X_valid), len(X_test)
88/290:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

# Shuffle the data
heart_disease_shuffled = heart_disease.sample(frac=1)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease["target"]

# Split the data into train, validation & test sets
train_split = round(0.7 * len(heart_disease_shuffled)) #70% of data
valid_split = round(train_split + 0.15 * len(heart_disease_shuffled)) # 15% of the data
X_train, y_train = X[:train_split], y[:train_split]
X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]
X_test, y_test = X[valid_split:], y[valid_split:]

len(X_train), len(X_valid), len(X_test)
88/291: clf.get_params()
88/292:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

# Shuffle the data
heart_disease_shuffled = heart_disease.sample(frac=1)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split the data into train, validation & test sets
train_split = round(0.7 * len(heart_disease_shuffled)) #70% of data
valid_split = round(train_split + 0.15 * len(heart_disease_shuffled)) # 15% of the data
X_train, y_train = X[:train_split], y[:train_split]
X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]
X_test, y_test = X[valid_split:], y[valid_split:]

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Make baseline predictions
y_preds = clf.predict(X_valid)

# Evaluate the classifier on validation set
baseline_metrics = evaluation_preds(y_valid, y_preds)
88/293:
def evaluation_preds(y_true, y_preds):
    """
    Performs evaluation comparison on y_true labels vs. y_pred labels.
    """
    accuracy = accuracy_score(y_true, y_preds)
    precision = precision(y_true, y_preds)
    recall = recall(y_true, y_preds)
    f1 = f1_score(y_true, y_preds)
    metric_dict = {
        "accuracy": round(accuracy, 2),
        "precision": round(precision, 2),
        "recall": round(recall, 2),
        "f1": round(f1, 2)
    }
    
    print(f"Acc: {accuracy * 100:.2f}%")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 score: {f1:.2f}")
    
    
    return metric_dict
88/294:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

# Shuffle the data
heart_disease_shuffled = heart_disease.sample(frac=1)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split the data into train, validation & test sets
train_split = round(0.7 * len(heart_disease_shuffled)) #70% of data
valid_split = round(train_split + 0.15 * len(heart_disease_shuffled)) # 15% of the data
X_train, y_train = X[:train_split], y[:train_split]
X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]
X_test, y_test = X[valid_split:], y[valid_split:]

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Make baseline predictions
y_preds = clf.predict(X_valid)

# Evaluate the classifier on validation set
baseline_metrics = evaluation_preds(y_valid, y_preds)
baseline_metrics
88/295:
def evaluation_preds(y_true, y_preds):
    """
    Performs evaluation comparison on y_true labels vs. y_pred labels.
    """
    accuracy = accuracy_score(y_true, y_preds)
    precision = precision_score(y_true, y_preds)
    recall = recall(y_true, y_preds)
    f1 = f1_score(y_true, y_preds)
    metric_dict = {
        "accuracy": round(accuracy, 2),
        "precision": round(precision, 2),
        "recall": round(recall, 2),
        "f1": round(f1, 2)
    }
    
    print(f"Acc: {accuracy * 100:.2f}%")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 score: {f1:.2f}")
    
    
    return metric_dict
88/296:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

# Shuffle the data
heart_disease_shuffled = heart_disease.sample(frac=1)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split the data into train, validation & test sets
train_split = round(0.7 * len(heart_disease_shuffled)) #70% of data
valid_split = round(train_split + 0.15 * len(heart_disease_shuffled)) # 15% of the data
X_train, y_train = X[:train_split], y[:train_split]
X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]
X_test, y_test = X[valid_split:], y[valid_split:]

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Make baseline predictions
y_preds = clf.predict(X_valid)

# Evaluate the classifier on validation set
baseline_metrics = evaluation_preds(y_valid, y_preds)
baseline_metrics
88/297:
def evaluation_preds(y_true, y_preds):
    """
    Performs evaluation comparison on y_true labels vs. y_pred labels.
    """
    accuracy = accuracy_score(y_true, y_preds)
    precision = precision_score(y_true, y_preds)
    recall = recall_score(y_true, y_preds)
    f1 = f1_score(y_true, y_preds)
    metric_dict = {
        "accuracy": round(accuracy, 2),
        "precision": round(precision, 2),
        "recall": round(recall, 2),
        "f1": round(f1, 2)
    }
    
    print(f"Acc: {accuracy * 100:.2f}%")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 score: {f1:.2f}")
    
    
    return metric_dict
88/298:
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

# Shuffle the data
heart_disease_shuffled = heart_disease.sample(frac=1)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split the data into train, validation & test sets
train_split = round(0.7 * len(heart_disease_shuffled)) #70% of data
valid_split = round(train_split + 0.15 * len(heart_disease_shuffled)) # 15% of the data
X_train, y_train = X[:train_split], y[:train_split]
X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]
X_test, y_test = X[valid_split:], y[valid_split:]

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Make baseline predictions
y_preds = clf.predict(X_valid)

# Evaluate the classifier on validation set
baseline_metrics = evaluation_preds(y_valid, y_preds)
baseline_metrics
88/299:
np.random.seed(42)

# Create a second classifier with different hyperparameters
clf_2 = RandomForestClassifier(n_estimators=100)
clf_2.fit(X_train, y_train)

# Make predictions with different hyperparameters
y_preds_2 = clf_2.predict(X_valid)

# Evaluate the 2nd classifier
clf_2_metrics = evaluation_preds(y_valid, y_preds)
88/300:
np.random.seed(42)

# Create a second classifier with different hyperparameters
clf_2 = RandomForestClassifier(n_estimators=10)
clf_2.fit(X_train, y_train)

# Make predictions with different hyperparameters
y_preds_2 = clf_2.predict(X_valid)

# Evaluate the 2nd classifier
clf_2_metrics = evaluation_preds(y_valid, y_preds)
88/301:
np.random.seed(42)

# Create a second classifier with different hyperparameters
clf_2 = RandomForestClassifier(n_estimators=80)
clf_2.fit(X_train, y_train)

# Make predictions with different hyperparameters
y_preds_2 = clf_2.predict(X_valid)

# Evaluate the 2nd classifier
clf_2_metrics = evaluation_preds(y_valid, y_preds)
88/302:
np.random.seed(42)

# Create a second classifier with different hyperparameters
clf_2 = RandomForestClassifier(n_estimators=100)
clf_2.fit(X_train, y_train)

# Make predictions with different hyperparameters
y_preds_2 = clf_2.predict(X_valid)

# Evaluate the 2nd classifier
clf_2_metrics = evaluation_preds(y_valid, y_preds)
88/303:
np.random.seed(42)

clf_3 = RandomForestClassifier(max_depth=10)

clf_3.fit(X_train, y_train)

y_preds_3 = clf_3.predict(X_valid)

clf_3_metics = evaluation_preds(y_valid, y_preds)
88/304:
np.random.seed(41)

clf_3 = RandomForestClassifier(max_depth=10)

clf_3.fit(X_train, y_train)

y_preds_3 = clf_3.predict(X_valid)

clf_3_metics = evaluation_preds(y_valid, y_preds)
88/305:
np.random.seed(41)

clf_3 = RandomForestClassifier(max_depth=100)

clf_3.fit(X_train, y_train)

y_preds_3 = clf_3.predict(X_valid)

clf_3_metics = evaluation_preds(y_valid, y_preds)
88/306:
np.random.seed(41)

clf_3 = RandomForestClassifier(max_depth=10)

clf_3.fit(X_train, y_train)

y_preds_3 = clf_3.predict(X_valid)

clf_3_metics = evaluation_preds(y_valid, y_preds)
88/307:
np.random.seed(42)

clf_3 = RandomForestClassifier(max_depth=10)

clf_3.fit(X_train, y_train)

y_preds_3 = clf_3.predict(X_valid)

clf_3_metics = evaluation_preds(y_valid, y_preds)
88/308:
from sklearn.model_selection import RandomizedSearchCV

grid = {
    "n_estimator": [10, 100, 200, 500, 1000, 1200],
    "max_depth": [None, 5, 10, 20, 30],
    "max_features": ["auto", "sqrt"],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 4]
}

np.random.seed(42)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled("target")

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# Setup RandomizedSearchCV
rs_clf = RandomizedSearchCV(estimator=clf,
                            param_distributions=grid,
                            n_iter=10, #number of models to try
                            cv=5,
                            verbose=2)
88/309: heart_disease_shuffled
88/310:
from sklearn.model_selection import RandomizedSearchCV

grid = {
    "n_estimator": [10, 100, 200, 500, 1000, 1200],
    "max_depth": [None, 5, 10, 20, 30],
    "max_features": ["auto", "sqrt"],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 4]
}

np.random.seed(42)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# Setup RandomizedSearchCV
rs_clf = RandomizedSearchCV(estimator=clf,
                            param_distributions=grid,
                            n_iter=10, #number of models to try
                            cv=5,
                            verbose=2)
88/311:
from sklearn.model_selection import RandomizedSearchCV

grid = {
    "n_estimator": [10, 100, 200, 500, 1000, 1200],
    "max_depth": [None, 5, 10, 20, 30],
    "max_features": ["auto", "sqrt"],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 4]
}

np.random.seed(42)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# Setup RandomizedSearchCV
rs_clf = RandomizedSearchCV(estimator=clf,
                            param_distributions=grid,
                            n_iter=10, #number of models to try
                            cv=5,
                            verbose=2)

# Fit the RandomizedSearchCV version of clf
rs_clf.fit(X_train, y_train);
88/312:
from sklearn.model_selection import RandomizedSearchCV

grid = {
    "n_estimator": [10, 20, 50, 80, 100],
    "max_depth": [None, 5, 10, 20, 30],
    "max_features": ["auto", "sqrt"],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 4]
}

np.random.seed(42)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# Setup RandomizedSearchCV
rs_clf = RandomizedSearchCV(estimator=clf,
                            param_distributions=grid,
                            n_iter=10, #number of models to try
                            cv=5,
                            verbose=2)

# Fit the RandomizedSearchCV version of clf
rs_clf.fit(X_train, y_train);
88/313:
from sklearn.model_selection import RandomizedSearchCV

grid = {
    "n_estimator": [10, 20, 50, 80, 100],
    "max_depth": [None, 5, 10, 20, 30],
    "max_features": ["auto", "sqrt"],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 4]
}

np.random.seed(41)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# Setup RandomizedSearchCV
rs_clf = RandomizedSearchCV(estimator=clf,
                            param_distributions=grid,
                            n_iter=10, #number of models to try
                            cv=5,
                            verbose=2)

# Fit the RandomizedSearchCV version of clf
rs_clf.fit(X_train, y_train);
88/314:
from sklearn.model_selection import RandomizedSearchCV

grid = {
    "n_estimators": [10, 100, 200, 500, 1000, 1200],
    "max_depth": [None, 5, 10, 20, 30],
    "max_features": ["auto", "sqrt"],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 4]
}

np.random.seed(42)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# Setup RandomizedSearchCV
rs_clf = RandomizedSearchCV(estimator=clf,
                            param_distributions=grid,
                            n_iter=10, #number of models to try
                            cv=5,
                            verbose=2)

# Fit the RandomizedSearchCV version of clf
rs_clf.fit(X_train, y_train);
88/315: rs_clf.best_params_
88/316:
# Make predictions with the best hyperparameters
rs_y_preds = rs_clf.predict(X_test)

# Evaluate the predictions
rs_metrics = evaluation_preds(y_test, rs_y_preds)
88/317: dgrid
88/318: grid
88/319: grid
88/320: 6*5*2*3*3
88/321: 6*5*2*3*3*5
88/322:
grid_2 = {
        'n_estimators': [10, 100, 200, 500],
        'max_depth': [None],
        'max_features': ['auto', 'sqrt'],
        'min_samples_split': [6],
        'min_samples_leaf': [1, 2]
}
88/323:
from sklearn.model_selection import GridSearchCV, train_test_split

np.random.seed(42)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# Setup GridSearchCV
rs_clf = GridSearchCV(estimator=clf,
                      param_grid=grid_2,
                      cv=5,
                      verbose=2)

# Fit the RandomizedSearchCV version of clf
rs_clf.fit(X_train, y_train);
88/324:
from sklearn.model_selection import GridSearchCV, train_test_split

np.random.seed(42)

# Split into X & y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# Setup GridSearchCV
gs_clf = GridSearchCV(estimator=clf,
                      param_grid=grid_2,
                      cv=5,
                      verbose=2)

# Fit the RandomizedSearchCV version of clf
gs_clf.fit(X_train, y_train);
88/325: gs_clf.best_params_
88/326:
gs_y_preds = gs_clf.predict(X_test)

# evaluate the prediction
gs_metrics = evaluation_preds(y_test, gs_y_preds)
88/327:
compare_metrics = pd.DataFrame({"baseline": baseline_metrics},
                                "clf_2": clf_2_metrics,
                                "random search": rs_metrics,
                                "grid search": gs_metrics)

compare_metrics.plot.bar(figsize=(10, 8));
88/328:
compare_metrics = pd.DataFrame({"baseline": baseline_metrics,
                                "clf_2": clf_2_metrics,
                                "random search": rs_metrics,
                                "grid search": gs_metrics})

compare_metrics.plot.bar(figsize=(10, 8));
88/329: what_we_are_covering
88/330:
import pickle

# Save an existing model to file
pickle.dump(gs_clf, open("gs_random_random_forest_model_1.pk", "wb"))
88/331:
import pickle

# Save an existing model to file
pickle.dump(gs_clf, open("gs_random_forest_model_1.pk", "wb"))
88/332: !ls
88/333:
# Load a saved model
loaded_picke_model = pickle.load("gs_random_forest_model_1.pk", "rb")
88/334:
# Load a saved model
loaded_picke_model = pickle.load(open("gs_random_forest_model_1.pk", "rb"))
88/335:
# Make some predictions
pickle_y_preds = loaded_picke_model.predict(X_test)
evaluation_preds(y_test, pickle_y_preds)
88/336: **Joblib**
88/337:
from joblib import dump, load

# Save model to file
dump(gs_clf, filename="gs_random_forest_model_1.joblib")
88/338: !ls
88/339:
# Import a saved jonlib model
loaded_job_model = load(filename="gs_random_forest_model_1.joblib")
88/340:
# Make and evaluate joblib predictions
joblib_y_preds = loaded_joblib_model.predict(X_test)
evaluation_preds(y_test, joblib_y_preds)
88/341:
# Import a saved jonlib model
loaded_joblib_model = load(filename="gs_random_forest_model_1.joblib")
88/342:
# Make and evaluate joblib predictions
joblib_y_preds = loaded_joblib_model.predict(X_test)
evaluation_preds(y_test, joblib_y_preds)
88/343: what_we_are_covering
88/344:
data = pd.read_csv("data/car-sales-extended-missing-data.csv")
data
88/345: data.dtypes
88/346: data.isna().sum()
88/347:
# Getting data ready
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Modelling
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

# Setup random seed
import numpy as np
np.random.seed(42)

# Import data and drop rows with missing labels
data = pd.read_csv("data/car-sales-extended-missing-data.csv")
data.dropna(subset="Price", inplace=True)

# DEfine different features and transformer pipelines
categorical_features = ["Make", "Colour"]
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

door_feature = ["Doors"]
door_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value=4))
])

numeric_features = ["Odometer (KM)"]
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleInputer(strategy="mean"))
])

# Setup the preprocessing steps (fill missing values, then convert to numbers)
preprocessor = ColumnTransformer(
                    transformers=[
                        ("cat", categorical_transformer, categorical_features),
                        ("door", door_transformer, door_feature),
                        ("num", numeric_transformer, numeric_features)
                    ])
# Create a preprocessing and modelling pipeline
model = Pipeline(steps=[("preprocessor", preprocessor),
                        ("model", RandomForestRegressor)])

# Split data
X = data.drop("Price", axis=1)
y = data["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fit and score the model
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/348:
# Getting data ready
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Modelling
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

# Setup random seed
import numpy as np
np.random.seed(42)

# Import data and drop rows with missing labels
data = pd.read_csv("data/car-sales-extended-missing-data.csv")
data.dropna(subset=["Price"], inplace=True)

# DEfine different features and transformer pipelines
categorical_features = ["Make", "Colour"]
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

door_feature = ["Doors"]
door_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value=4))
])

numeric_features = ["Odometer (KM)"]
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleInputer(strategy="mean"))
])

# Setup the preprocessing steps (fill missing values, then convert to numbers)
preprocessor = ColumnTransformer(
                    transformers=[
                        ("cat", categorical_transformer, categorical_features),
                        ("door", door_transformer, door_feature),
                        ("num", numeric_transformer, numeric_features)
                    ])
# Create a preprocessing and modelling pipeline
model = Pipeline(steps=[("preprocessor", preprocessor),
                        ("model", RandomForestRegressor)])

# Split data
X = data.drop("Price", axis=1)
y = data["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fit and score the model
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/349:
# Getting data ready
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Modelling
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

# Setup random seed
import numpy as np
np.random.seed(42)

# Import data and drop rows with missing labels
data = pd.read_csv("data/car-sales-extended-missing-data.csv")
data.dropna(subset=["Price"], inplace=True)

# DEfine different features and transformer pipelines
categorical_features = ["Make", "Colour"]
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

door_feature = ["Doors"]
door_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value=4))
])

numeric_features = ["Odometer (KM)"]
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean"))
])

# Setup the preprocessing steps (fill missing values, then convert to numbers)
preprocessor = ColumnTransformer(
                    transformers=[
                        ("cat", categorical_transformer, categorical_features),
                        ("door", door_transformer, door_feature),
                        ("num", numeric_transformer, numeric_features)
                    ])
# Create a preprocessing and modelling pipeline
model = Pipeline(steps=[("preprocessor", preprocessor),
                        ("model", RandomForestRegressor)])

# Split data
X = data.drop("Price", axis=1)
y = data["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fit and score the model
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/350:
# Getting data ready
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Modelling
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

# Setup random seed
import numpy as np
np.random.seed(42)

# Import data and drop rows with missing labels
data = pd.read_csv("data/car-sales-extended-missing-data.csv")
data.dropna(subset=["Price"], inplace=True)

# DEfine different features and transformer pipelines
categorical_features = ["Make", "Colour"]
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

door_feature = ["Doors"]
door_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value=4))
])

numeric_features = ["Odometer (KM)"]
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean"))
])

# Setup the preprocessing steps (fill missing values, then convert to numbers)
preprocessor = ColumnTransformer(
                    transformers=[
                        ("cat", categorical_transformer, categorical_features),
                        ("door", door_transformer, door_feature),
                        ("num", numeric_transformer, numeric_features)
                    ])
# Create a preprocessing and modelling pipeline
model = Pipeline(steps=[("preprocessor", preprocessor),
                        ("model", RandomForestRegressor)])

# Split data
X = data.drop("Price", axis=1)
y = data["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fit and score the model
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/351:
# Getting data ready
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Modelling
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

# Setup random seed
import numpy as np
np.random.seed(42)

# Import data and drop rows with missing labels
data = pd.read_csv("data/car-sales-extended-missing-data.csv")
data.dropna(subset=["Price"], inplace=True)

# DEfine different features and transformer pipelines
categorical_features = ["Make", "Colour"]
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

door_feature = ["Doors"]
door_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value=4))
])

numeric_features = ["Odometer (KM)"]
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean"))
])

# Setup the preprocessing steps (fill missing values, then convert to numbers)
preprocessor = ColumnTransformer(
                    transformers=[
                        ("cat", categorical_transformer, categorical_features),
                        ("door", door_transformer, door_feature),
                        ("num", numeric_transformer, numeric_features)
                    ])
# Create a preprocessing and modelling pipeline
model = Pipeline(steps=[("preprocessor", preprocessor),
                        ("model", RandomForestRegressor())])

# Split data
X = data.drop("Price", axis=1)
y = data["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fit and score the model
model.fit(X_train, y_train)
model.score(X_test, y_test)
88/352:
# Use GridSearchCV with our regression Pipeline
Pipe_grid = {
    "preprocessor__num__imputer__strategy": ["mean", "median"],
    "model__n_estimators": [100, 1000],
    "model__max_depth": [None, 5],
    "model__max_features": ["auto"],
    "model__min_samples_split": [2, 4]
}
88/353:
# Use GridSearchCV with our regression Pipeline
from sklearn.model_selection import GridSearchCV
pipe_grid = {
    "preprocessor__num__imputer__strategy": ["mean", "median"],
    "model__n_estimators": [100, 1000],
    "model__max_depth": [None, 5],
    "model__max_features": ["auto"],
    "model__min_samples_split": [2, 4]
}

gs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2)
gs_model.fit(X_train, y_train)
88/354: gs_model.score(X_test, y_test)
88/355: what_we_are_covering
90/1:
## Preparing the tools

We're going to use pandas, Matplotlib and NumPy for data analysis and manipulation
90/2:
# Import all the tools we need

# Regular EDA (explotary data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline # we want our plots to appear inside the notebook

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifierre

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
90/3:
# Import all the tools we need

# Regular EDA (explotary data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
%matplotlib inline

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifierre

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
90/4:
# Import all the tools we need

# Regular EDA (explotary data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
%matplotlib inline

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
90/5:
df = pd.read-_csv('heart-disease.csv')
df.shape
90/6:
df = pd.read_csv('heart-disease.csv')
df.shape
90/7: df.head()
90/8: df.tail()
90/9:
# Lets find out how many of each class are there
df["target"].value_counts()
90/10: df["target"].value_counts().plot(kind="bar", color=["salmon", "lightblue"]);
90/11: df.info()
90/12:
# Are there any missing values?
df.isna().sum()
90/13: df.describe()
90/14: df.describe().plot(kind="bar")
90/15: df.describe().plot(kind="hist")
90/16: df.describe().plot(kind="pie")
90/17: df.describe().plot(kind="line")
90/18: df.describe()
90/19: df.describe().plot(kind="bar", colors=["blue"])
90/20: df.describe().plot(kind="bar", color=["blue"])
90/21: df.describe().plot(kind="line", color=["blue"])
90/22: df.describe().plot(kind="line", color=["blue", 'yellow'])
90/23: df.describe().plot(kind="line", color=["blue", 'yellow', 'grey'])
90/24: df.describe().plot(kind="line", color=["blue", 'yellow', 'grey', 'green'])
90/25: df.describe().plot()
90/26: df.describe().plot()
90/27: df.describe().plot();
90/28: df.describe()
90/29: ### Heart disease Frequency according to sex
90/30: df.sex.value_counts()
90/31:
# Compare target column with sex column
pd.crosstab(df.target, df.sex)
90/32:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot()
90/33:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar")
90/34:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(50, 50))
90/35:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 10))
90/36:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 10));
90/37:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="barh", figsize=(10, 10));
90/38:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 10));
90/39:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 10), legend: 'reverse');
90/40:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 10), legend: {'reverse'});
90/41:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 10), legend: false);
90/42:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 10), legend: False);
90/43:
# Compare target column with sex column
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 10));
90/44:
# Compare target column with sex column
pd.crosstab(df.target, df.sex)
90/45:
# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"])
90/46:
# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"]);
90/47:
# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"])

plt.title("Heart Disease Frequency for Sex")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"]);
90/48:
# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"])

plt.title("Heart Disease Frequency for Sex")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/49:
# Comparing the chol - Cholesteral column with the target column
pd.crosstab(df.chol, df.target)
90/50:
# Plotting the chol Vs. the target column
pd.crosstab(df.chol, df.target).plot(kind="bar")
90/51:
# Plotting the chol Vs. the target column
pd.crosstab(df.chol, df.target).plot(kind="bar", figsize=(10, 8))
90/52:
# Comparing the age column with the target column
pd.crosstab(df.age, df.target)
90/53:
# Plotting the age Vs. the target column
pd.crosstab(df.age, df.target).plot(kind="bar", figsize=(10, 8))
90/54:
# Plotting the age Vs. the target column
pd.crosstab(df.age, df.target).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for age")
90/55:
# Plotting the age Vs. the target column
pd.crosstab(df.age, df.target).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
90/56:
# Comparing the age column with the target column
pd.crosstab(df.target, df.age)
90/57:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("Age")
plt.ylabel("")
90/58:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Age")
90/59:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Age")
plt.legend(["Female", "Male"])
90/60:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Age")
plt.legend(["Female", "Male"])
plt.xtics(rotation=0)
90/61:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Age")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0)
90/62:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Age")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/63:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Age")
# plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/64:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("AMount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/65:
# Plotting the age Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/66:
# Comparing the chol column with the target column
pd.crosstab(df.target, df.age)
90/67:
# Plotting the chol Vs. the target column
pd.crosstab(df.target, df.age).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/68:
# Comparing the chol column with the target column
pd.crosstab(df.target, df.chol)
90/69:
# Plotting the chol Vs. the target column
pd.crosstab(df.target, df.chol).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/70:
# Plotting the chol Vs. the target column
pd.crosstab(df.target, df.chol).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
# plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/71:
# Comparing the chol column with the target column
pd.crosstab(df.target, df.chol)
90/72:
# Comparing the fbs column with the target column
pd.crosstab(df.target, df.fbs)
90/73:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
# plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
90/74:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Age")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/75:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/76:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Fbs, 1 = Fbs")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/77:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/78:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Food blood sugar", "Food blood sugar"])
plt.xticks(rotation=0);
90/79:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 10))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Food blood sugar", "Food blood sugar"])
plt.xticks(rotation=0);
90/80:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Food blood sugar", "Food blood sugar"])
plt.xticks(rotation=0);
90/81:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/82:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["lead", "purple"]
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/83:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["lead", "purple"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/84:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["sky_blue", "purple"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/85:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["blue", "purple"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/86:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["pink", "purple"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/87:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["pink", "maroon"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/88:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["salmon", "light_blue"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/89:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["salmon", "lightblue"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
90/90: df.head()
90/91: df.thalach.value_counts()
90/92: df.age[df.target == 1]
90/93:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1])
90/94:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color=["salmon"])
90/95:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color=["salmon"]);
90/96:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color=["salmon"])

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            color="lightblue");
90/97:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color="salmon")

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            color="lightblue");
90/98:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color="salmon")

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            color="lightblue")

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);
90/99:
# Check the distribution of the age column with a histogram
df.age.plot(kind=hist)
90/100:
# Check the distribution of the age column with a histogram
df.age.plot(kind="hist")
90/101:
# Check the distribution of the age column with a histogram
df.age.plot(kind="hist");
90/102: pd.crosstab(df.cp, df.target)
90/103:
# make the crosstab more visual
pd.crosstab(df.cp, df.target).plot(kind="bar",
                                   figsize=(10, 6),
                                   color=["salmon", "lightblue"])

# Add some communication
plt.title("Heart Disease Frequency Per Chest Pain Type")
plt.xlabel("Chest Pain Type")
plt.ylabel("Amount")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation=0);
90/104: df.head()
90/105:
# Make a correlation matrix
df.corr()
90/106:
# Lets make our correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidth=0.5,
                 fmt=".2f",
                 cmap="Y1GnBu");
90/107:
# Lets make our correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidth=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
90/108: df.head()
90/109:
# Split data into X and y
X = df.drop("target", axis=1)
y = df.target
90/110: X
90/111: y
90/112:
# Split data into train and test sets
np.random.seed(42)

# Split into train & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
90/113: X_train, len(X_train)
90/114: X_train
90/115: y_train, len(y_train)
90/116:
# Put models in a dictionary
models = {"Logistic Regresion": LogisticRegression(),
          "KNN": KNeighborsClassifier(),
          "Random Forest": RandomForestClassifier()
         }

# Create a function to fit and score models
def fit_and_score(models, X_train, X_test, y_train, y_test):
    """
    Fits and evaluates given machine learning models.
    models: a dict of differnt Scikit-Learn machine learning models
    X_train: training data (no labels)
    X_test: testing data (no labels)
    y_train: training labels
    y_test: test labels
    """
    
    # Set random seed
    np.random.seed(42)
    
    # Make a dictionary to keep model scores
    model_scores = {}
    
    # Loot through models
    for name, model in models.items():
        # Fit the model to the data
        model.fit(X_train, y_train)
        #Evaluate the model and append its score to model_scores
        model_scores[name] = model.score(X_test, y_test)
    return model_scores
90/117:
model_scores = fit_and_score(models=models,
                             X_train = X_train,
                             X_test = X_test,
                             y_train = y_train,
                             y_test = y_test)

model_scores
90/118:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar");
90/119:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="barh");
90/120:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar");
90/121:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0);
90/122:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar", color="sanlam")
plt.xticks(rotation=0);
90/123:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar", color="salmon")
plt.xticks(rotation=0);
90/124:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0);
90/125:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=2);
90/126:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=50);
90/127:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=10);
90/128:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=90);
90/129:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=180);
90/130:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=270);
90/131:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0);
90/132:
# Lets tune KNN

train_scores = []
test_scores = []

# Create a list of different values for n neighbors
neighbors = range(1, 21)

# Setup KNN instance
knn = KNeighborsClassifier()

# Loop through different n_neighbors
for i in neighbors:
    knn.set_params(n_neighbors=i)
    
    # Fit the algorithm
    knn.fit(X_train, y_train)
    
    # Update the training scores list
    train_scores.append(knn.score(X_train, y_train))
    
    # Update the test scores list
    test_scores.append(knn.score(X_test, y_test))
90/133: train_scores
90/134: test_scores
90/135:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%")
90/136:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xtics(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%")plt.
90/137:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xtics(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
90/138:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
91/1:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.plot(kind="bar")
plt.xticks(rotation=0)
plt.setxtics(labels)
91/2:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0)
plt.setxtics(labels)
91/3:
# Import all the tools we need

# Regular EDA (explotary data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
%matplotlib inline

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
91/4:
df = pd.read_csv('heart-disease.csv')
df.shape
91/5: df.head()
91/6: df.tail()
91/7:
# Lets find out how many of each class are there
df["target"].value_counts()
91/8: df["target"].value_counts().plot(kind="bar", color=["salmon", "lightblue"]);
91/9: df.info()
91/10:
# Are there any missing values?
df.isna().sum()
91/11: df.describe()
91/12: df.sex.value_counts()
91/13:
# Compare target column with sex column
pd.crosstab(df.target, df.sex)
91/14:
# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"])

plt.title("Heart Disease Frequency for Sex")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
91/15:
# Comparing the fbs column with the target column
pd.crosstab(df.target, df.fbs)
91/16:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["salmon", "lightblue"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
91/17: df.head()
91/18: df.thalach.value_counts()
91/19:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color="salmon")

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            color="lightblue")

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);
91/20:
# Check the distribution of the age column with a histogram
df.age.plot(kind="hist");
91/21: pd.crosstab(df.cp, df.target)
91/22:
# make the crosstab more visual
pd.crosstab(df.cp, df.target).plot(kind="bar",
                                   figsize=(10, 6),
                                   color=["salmon", "lightblue"])

# Add some communication
plt.title("Heart Disease Frequency Per Chest Pain Type")
plt.xlabel("Chest Pain Type")
plt.ylabel("Amount")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation=0);
91/23: df.head()
91/24:
# Make a correlation matrix
df.corr()
91/25:
# Lets make our correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidth=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
91/26: df.head()
91/27:
# Split data into X and y
X = df.drop("target", axis=1)
y = df.target
91/28: X
91/29: y
91/30:
# Split data into train and test sets
np.random.seed(42)

# Split into train & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
91/31: X_train
91/32: y_train, len(y_train)
91/33:
# Put models in a dictionary
models = {
    "Logistic Regresion": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Create a function to fit and score models
def fit_and_score(models, X_train, X_test, y_train, y_test):
    """
    Fits and evaluates given machine learning models.
    models: a dict of differnt Scikit-Learn machine learning models
    X_train: training data (no labels)
    X_test: testing data (no labels)
    y_train: training labels
    y_test: test labels
   """    
    # Set random seed
    np.random.seed(42)
    
    # Make a dictionary to keep model scores
    model_scores = {}
    
    # Loot through models
    for name, model in models.items():
        # Fit the model to the data
        model.fit(X_train, y_train)
        #Evaluate the model and append its score to model_scores
        model_scores[name] = model.score(X_test, y_test)
    return model_scores
91/34:
model_scores = fit_and_score(models=models,
                             X_train = X_train,
                             X_test = X_test,
                             y_train = y_train,
                             y_test = y_test)

model_scores
91/35:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0)
plt.setxtics(labels)
91/36:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0)
plt.setxtics(labels)
91/37:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0)
plt.setxticks(labels)
91/38:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0)
91/39:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0);
91/40:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.plot(kind="bar")
plt.xticks(rotation=0);
91/41:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0);
92/1:
# Lets tune KNN

train_scores = []
test_scores = []

# Create a list of different values for n neighbors
neighbors = range(1, 21)

# Setup KNN instance
knn = KNeighborsClassifier()

# Loop through different n_neighbors
for i in neighbors:
    knn.set_params(n_neighbors=i)
    
    # Fit the algorithm
    knn.fit(X_train, y_train)
    
    # Update the training scores list
    train_scores.append(knn.score(X_train, y_train))
    
    # Update the test scores list
    test_scores.append(knn.score(X_test, y_test))
92/2:
# Import all the tools we need

# Regular EDA (explotary data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
%matplotlib inline

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
92/3:
df = pd.read_csv('heart-disease.csv')
df.shape
92/4: df.head()
92/5: df.tail()
92/6:
# Lets find out how many of each class are there
df["target"].value_counts()
92/7: df["target"].value_counts().plot(kind="bar", color=["salmon", "lightblue"]);
92/8: df.info()
92/9:
# Are there any missing values?
df.isna().sum()
92/10: df.describe()
92/11: df.sex.value_counts()
92/12:
# Compare target column with sex column
pd.crosstab(df.target, df.sex)
92/13:
# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"])

plt.title("Heart Disease Frequency for Sex")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
92/14:
# Comparing the fbs column with the target column
pd.crosstab(df.target, df.fbs)
92/15:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["salmon", "lightblue"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
92/16: df.head()
92/17: df.thalach.value_counts()
92/18:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color="salmon")

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            color="lightblue")

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);
92/19:
# Check the distribution of the age column with a histogram
df.age.plot(kind="hist");
92/20: pd.crosstab(df.cp, df.target)
92/21:
# make the crosstab more visual
pd.crosstab(df.cp, df.target).plot(kind="bar",
                                   figsize=(10, 6),
                                   color=["salmon", "lightblue"])

# Add some communication
plt.title("Heart Disease Frequency Per Chest Pain Type")
plt.xlabel("Chest Pain Type")
plt.ylabel("Amount")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation=0);
92/22: df.head()
92/23:
# Make a correlation matrix
df.corr()
92/24:
# Lets make our correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidth=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
92/25: df.head()
92/26:
# Split data into X and y
X = df.drop("target", axis=1)
y = df.target
92/27: X
92/28: y
92/29:
# Split data into train and test sets
np.random.seed(42)

# Split into train & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
92/30: X_train
92/31: y_train, len(y_train)
92/32:
# Put models in a dictionary
models = {
    "Logistic Regresion": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Create a function to fit and score models
def fit_and_score(models, X_train, X_test, y_train, y_test):
    """
    Fits and evaluates given machine learning models.
    models: a dict of differnt Scikit-Learn machine learning models
    X_train: training data (no labels)
    X_test: testing data (no labels)
    y_train: training labels
    y_test: test labels
   """    
    # Set random seed
    np.random.seed(42)
    
    # Make a dictionary to keep model scores
    model_scores = {}
    
    # Loot through models
    for name, model in models.items():
        # Fit the model to the data
        model.fit(X_train, y_train)
        #Evaluate the model and append its score to model_scores
        model_scores[name] = model.score(X_test, y_test)
    return model_scores
92/33:
model_scores = fit_and_score(models=models,
                             X_train = X_train,
                             X_test = X_test,
                             y_train = y_train,
                             y_test = y_test)

model_scores
92/34:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0);
92/35:
# Lets tune KNN

train_scores = []
test_scores = []

# Create a list of different values for n neighbors
neighbors = range(1, 21)

# Setup KNN instance
knn = KNeighborsClassifier()

# Loop through different n_neighbors
for i in neighbors:
    knn.set_params(n_neighbors=i)
    
    # Fit the algorithm
    knn.fit(X_train, y_train)
    
    # Update the training scores list
    train_scores.append(knn.score(X_train, y_train))
    
    # Update the test scores list
    test_scores.append(knn.score(X_test, y_test))
92/36: train_scores
92/37: test_scores
92/38:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/39:
# Lets tune KNN

train_scores = []
test_scores = []

# Create a list of different values for n neighbors
neighbors = range(1, 51)

# Setup KNN instance
knn = KNeighborsClassifier()

# Loop through different n_neighbors
for i in neighbors:
    knn.set_params(n_neighbors=i)
    
    # Fit the algorithm
    knn.fit(X_train, y_train)
    
    # Update the training scores list
    train_scores.append(knn.score(X_train, y_train))
    
    # Update the test scores list
    test_scores.append(knn.score(X_test, y_test))
92/40: train_scores
92/41: test_scores
92/42:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 51, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/43:
plt.plot(neighbors, train_scores, label="Train score", figsize=(10, 10))
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 51, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/44:
plt.plot(neighbors, train_scores, label="Train score", figsize=(10, 10))
plt.plot(neighbors, test_scores, label="Test score", figsize=(10, 10))
plt.xticks(np.arange(1, 51, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/45:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 51, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()
plt.figure(figsize=(10,10))

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/46:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 51, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()
plt.figure(figsize=(2.))

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/47:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 51, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()
plt.figure(figsize=plt.figaspect(2.))

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/48:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 51, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/49:
# Lets tune KNN

train_scores = []
test_scores = []

# Create a list of different values for n neighbors
neighbors = range(1, 21)

# Setup KNN instance
knn = KNeighborsClassifier()

# Loop through different n_neighbors
for i in neighbors:
    knn.set_params(n_neighbors=i)
    
    # Fit the algorithm
    knn.fit(X_train, y_train)
    
    # Update the training scores list
    train_scores.append(knn.score(X_train, y_train))
    
    # Update the test scores list
    test_scores.append(knn.score(X_test, y_test))
92/50: train_scores
92/51: test_scores
92/52:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/53:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}
92/54: np.logspace(-4, 4, 20)
92/55:
# Tune Logistic Regression

np.random.seed(42)

# Setup random hyperparameter search for Logistic Regression
rs_log_reg = RandomizedSearchCV(LogisticRegression(),
                                param_distributions=log_reg_grid,
                                cv=5,
                                n_iter=20,
                                verbose=True)

# Fit random hyperparameter search model for Logistic Regression
rs_log_reg.fit(X_train, y_train)
92/56: rs_log_reg.best_params_
92/57: rs_log_reg.scorer_
92/58: rs_log_reg.scorer(X_test, y_test)
92/59: rs_log_reg.score(X_test, y_test)
92/60:
# Setup random seed
np.random.seed(42)

# Setup random hyperparameter search for RandomForestClassifier
rs_rf = RandomizedSearchCV(RandomForestClassifier(),
                           param_distributions=rf_grid,
                           cv=5,
                           n_iter=20,
                           verbose=True)

# Fit random hyperparameter search model for RandomForestClassifier()
rs_rf.fit(X_train, y_train)
92/61:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.range(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.range(2, 20, 2),
    "min_samples_lead": np.arange(1, 20, 2)
}
92/62:
# Import all the tools we need

# Regular EDA (explotary data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
%matplotlib inline

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
92/63:
df = pd.read_csv('heart-disease.csv')
df.shape
92/64: df.head()
92/65: df.tail()
92/66:
# Lets find out how many of each class are there
df["target"].value_counts()
92/67: df["target"].value_counts().plot(kind="bar", color=["salmon", "lightblue"]);
92/68: df.info()
92/69:
# Are there any missing values?
df.isna().sum()
92/70: df.describe()
92/71: df.sex.value_counts()
92/72:
# Compare target column with sex column
pd.crosstab(df.target, df.sex)
92/73:
# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"])

plt.title("Heart Disease Frequency for Sex")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
92/74:
# Comparing the fbs column with the target column
pd.crosstab(df.target, df.fbs)
92/75:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["salmon", "lightblue"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
92/76: df.head()
92/77: df.thalach.value_counts()
92/78:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color="salmon")

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            color="lightblue")

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);
92/79:
# Check the distribution of the age column with a histogram
df.age.plot(kind="hist");
92/80: pd.crosstab(df.cp, df.target)
92/81:
# make the crosstab more visual
pd.crosstab(df.cp, df.target).plot(kind="bar",
                                   figsize=(10, 6),
                                   color=["salmon", "lightblue"])

# Add some communication
plt.title("Heart Disease Frequency Per Chest Pain Type")
plt.xlabel("Chest Pain Type")
plt.ylabel("Amount")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation=0);
92/82: df.head()
92/83:
# Make a correlation matrix
df.corr()
92/84:
# Lets make our correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidth=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
92/85: df.head()
92/86:
# Split data into X and y
X = df.drop("target", axis=1)
y = df.target
92/87: X
92/88: y
92/89:
# Split data into train and test sets
np.random.seed(42)

# Split into train & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
92/90: X_train
92/91: y_train, len(y_train)
92/92:
# Put models in a dictionary
models = {
    "Logistic Regresion": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Create a function to fit and score models
def fit_and_score(models, X_train, X_test, y_train, y_test):
    """
    Fits and evaluates given machine learning models.
    models: a dict of differnt Scikit-Learn machine learning models
    X_train: training data (no labels)
    X_test: testing data (no labels)
    y_train: training labels
    y_test: test labels
   """    
    # Set random seed
    np.random.seed(42)
    
    # Make a dictionary to keep model scores
    model_scores = {}
    
    # Loot through models
    for name, model in models.items():
        # Fit the model to the data
        model.fit(X_train, y_train)
        #Evaluate the model and append its score to model_scores
        model_scores[name] = model.score(X_test, y_test)
    return model_scores
92/93:
model_scores = fit_and_score(models=models,
                             X_train = X_train,
                             X_test = X_test,
                             y_train = y_train,
                             y_test = y_test)

model_scores
92/94:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0);
92/95:
# Lets tune KNN

train_scores = []
test_scores = []

# Create a list of different values for n neighbors
neighbors = range(1, 21)

# Setup KNN instance
knn = KNeighborsClassifier()

# Loop through different n_neighbors
for i in neighbors:
    knn.set_params(n_neighbors=i)
    
    # Fit the algorithm
    knn.fit(X_train, y_train)
    
    # Update the training scores list
    train_scores.append(knn.score(X_train, y_train))
    
    # Update the test scores list
    test_scores.append(knn.score(X_test, y_test))
92/96: train_scores
92/97: test_scores
92/98:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/99:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.range(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.range(2, 20, 2),
    "min_samples_lead": np.arange(1, 20, 2)
}
92/100:
# Setup random seed
np.random.seed(42)

# Setup random hyperparameter search for RandomForestClassifier
rs_rf = RandomizedSearchCV(RandomForestClassifier(),
                           param_distributions=rf_grid,
                           cv=5,
                           n_iter=20,
                           verbose=True)

# Fit random hyperparameter search model for RandomForestClassifier()
rs_rf.fit(X_train, y_train)
92/101:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.arange(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.range(2, 20, 2),
    "min_samples_lead": np.arange(1, 20, 2)
}
92/102:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.range(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.range(2, 20, 2),
    "min_samples_lead": np.arange(1, 20, 2)
}
92/103: np.range(10, 1000, 50)
92/104: range(10, 1000, 50)
92/105:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.range(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.range(2, 20, 2),
    "min_samples_leaf": np.arange(1, 20, 2)
}
92/106:
# Import all the tools we need

# Regular EDA (explotary data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
%matplotlib inline

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
92/107:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.range(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.range(2, 20, 2),
    "min_samples_leaf": np.arange(1, 20, 2)
}
92/108:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.arange(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.range(2, 20, 2),
    "min_samples_leaf": np.arange(1, 20, 2)
}
92/109:
# Import all the tools we need

# Regular EDA (explotary data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
%matplotlib inline

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
92/110:
df = pd.read_csv('heart-disease.csv')
df.shape
92/111: df.head()
92/112: df.tail()
92/113:
# Lets find out how many of each class are there
df["target"].value_counts()
92/114: df["target"].value_counts().plot(kind="bar", color=["salmon", "lightblue"]);
92/115: df.info()
92/116:
# Are there any missing values?
df.isna().sum()
92/117: df.describe()
92/118: df.sex.value_counts()
92/119:
# Compare target column with sex column
pd.crosstab(df.target, df.sex)
92/120:
# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"])

plt.title("Heart Disease Frequency for Sex")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0);
92/121:
# Comparing the fbs column with the target column
pd.crosstab(df.target, df.fbs)
92/122:
# Plotting the fbs Vs. the target column
pd.crosstab(df.target, df.fbs).plot(kind="bar",
                                    color=["salmon", "lightblue"],
                                    figsize=(10, 8))

plt.title("Heart Disease Frequency for Fbs")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["No Fbs", "Fbs"])
plt.xticks(rotation=0);
92/123: df.head()
92/124: df.thalach.value_counts()
92/125:
# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            color="salmon")

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            color="lightblue")

# Add some helpful info
plt.title("Heart Disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);
92/126:
# Check the distribution of the age column with a histogram
df.age.plot(kind="hist");
92/127: pd.crosstab(df.cp, df.target)
92/128:
# make the crosstab more visual
pd.crosstab(df.cp, df.target).plot(kind="bar",
                                   figsize=(10, 6),
                                   color=["salmon", "lightblue"])

# Add some communication
plt.title("Heart Disease Frequency Per Chest Pain Type")
plt.xlabel("Chest Pain Type")
plt.ylabel("Amount")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation=0);
92/129: df.head()
92/130:
# Make a correlation matrix
df.corr()
92/131:
# Lets make our correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidth=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
92/132: df.head()
92/133:
# Split data into X and y
X = df.drop("target", axis=1)
y = df.target
92/134: X
92/135: y
92/136:
# Split data into train and test sets
np.random.seed(42)

# Split into train & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
92/137: X_train
92/138: y_train, len(y_train)
92/139:
# Put models in a dictionary
models = {
    "Logistic Regresion": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Create a function to fit and score models
def fit_and_score(models, X_train, X_test, y_train, y_test):
    """
    Fits and evaluates given machine learning models.
    models: a dict of differnt Scikit-Learn machine learning models
    X_train: training data (no labels)
    X_test: testing data (no labels)
    y_train: training labels
    y_test: test labels
   """    
    # Set random seed
    np.random.seed(42)
    
    # Make a dictionary to keep model scores
    model_scores = {}
    
    # Loot through models
    for name, model in models.items():
        # Fit the model to the data
        model.fit(X_train, y_train)
        #Evaluate the model and append its score to model_scores
        model_scores[name] = model.score(X_test, y_test)
    return model_scores
92/140:
model_scores = fit_and_score(models=models,
                             X_train = X_train,
                             X_test = X_test,
                             y_train = y_train,
                             y_test = y_test)

model_scores
92/141:
model_scores = pd.DataFrame(model_scores, index=["accuracy"])
model_scores.T.plot(kind="bar")
plt.xticks(rotation=0);
92/142:
# Lets tune KNN

train_scores = []
test_scores = []

# Create a list of different values for n neighbors
neighbors = range(1, 21)

# Setup KNN instance
knn = KNeighborsClassifier()

# Loop through different n_neighbors
for i in neighbors:
    knn.set_params(n_neighbors=i)
    
    # Fit the algorithm
    knn.fit(X_train, y_train)
    
    # Update the training scores list
    train_scores.append(knn.score(X_train, y_train))
    
    # Update the test scores list
    test_scores.append(knn.score(X_test, y_test))
92/143: train_scores
92/144: test_scores
92/145:
plt.plot(neighbors, train_scores, label="Train score")
plt.plot(neighbors, test_scores, label="Test score")
plt.xticks(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximun KNN score on the test data: {max(test_scores)*100:.2f}%");
92/146:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.arange(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.range(2, 20, 2),
    "min_samples_leaf": np.arange(1, 20, 2)
}
92/147:
# Create a hyperparameter grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {
    "n_estimators": np.arange(10, 1000, 50),
    "max_depth": [None, 3, 5, 10],
    "min_samples_split": np.arange(2, 20, 2),
    "min_samples_leaf": np.arange(1, 20, 2)
}
92/148:
# Setup random seed
np.random.seed(42)

# Setup random hyperparameter search for RandomForestClassifier
rs_rf = RandomizedSearchCV(RandomForestClassifier(),
                           param_distributions=rf_grid,
                           cv=5,
                           n_iter=20,
                           verbose=True)

# Fit random hyperparameter search model for RandomForestClassifier()
rs_rf.fit(X_train, y_train)
92/149:
# Find the best hyperparameters
rs_rf.best_params_
92/150:
# Evaluate the randomized search RandomForestClassifier model
rs_rf.score(X_test, y_test)
92/151: model_scores
92/152: rs_log_reg.best_params_
92/153: rs_log_reg.score(X_test, y_test)
92/154:
# Different hyperparameters for our LogisticRegression model
log_reg_grid = {"C": np.logspace(-4, 4, 30),
                "solver": ["liblinear"]}

# Setup grid hyperparameter search for LogisticRegression
gs_log_reg = GridSearchCV(LogisticRegression(),
                          param_grid=log_reg_grid,
                          cv=5,
                          verbose=True)

# Fit grid hyperparametr search model
gs_log_reg.fit(X_train, y_train)
92/155:
# Check the best hyperparameters
gs_log_reg.best_params_
92/156:
# Different hyperparameters for our LogisticRegression model
log_reg_grid = {"C": np.logspace(-4, 4, 30),
                "solver": ["liblinear"]}

# Setup grid hyperparameter search for LogisticRegression
gs_log_reg = GridSearchCV(LogisticRegression(),
                          param_grid=log_reg_grid,
                          cv=5,
                          verbose=True)

# Fit grid hyperparametr search model
gs_log_reg.fit(X_train, y_train);
92/157:
# Check the best hyperparameters
gs_log_reg.best_params_
92/158:
# Evaluate the grid search LogisticRegression model
gs_log_reg.score(X_test, y_test)
92/159: model_scores
92/160:
# Evaluate the grid search LogisticRegression model
gs_log_reg.score(X_test, y_test)
92/161:
# make predictions with trained model
y_preds = gs_log_reg.predict(X_test, y_test)
92/162:
# make predictions with trained model
y_preds = gs_log_reg.predict(X_test)
92/163: y_preds
92/164: y_test
92/165:
# Plot the ROC curve and calculate AUC curve metrics
# Always evaluate machine learning models on test dataset
plot_roc_curve(gs_log_reg, X_test, y_test)
92/166:
# Plot the ROC curve and calculate AUC curve metrics
# Always evaluate machine learning models on test dataset
plot_roc_curve(gs_log_reg, X_test, y_test);
92/167:
# Confusion matrix
print(confusion_matrix(y_test, y_preds))
92/168:
sns.set(font_scale=1.5)

def plot_conf_matrix(y_test, y_preds):
    """
    Plots a nice looking confusion matrix using seaborn's heatmap()
    """
    fig, ax = plt.subplots(figsize=(3,3))
    ax = sns.heatmap(confusion_matrix(y_test, y_preds),
                     annot=True)
    plt.xlabel("True label")
    plt.ylabel("Predicted label")
    
plot_conf_matrix(y_test, y_preds)
92/169: print(classification_report(y_test, y_preds))
92/170:
# Check best hyperparameters
gs_log_reg.best_params_
92/171:
# Create a new classifier with best parameters
clf = LogisticRegression(C=0.20433597178569418, solver='liblinear')
92/172:
# Cross-validated accuracy
cv_acc = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="accuracy")
cv_acc
92/173:
# Cross-validated accuracy
cv_acc = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="accuracy")
cv_acc
92/174:
# Cross-validated accuracy
cv_acc = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="accuracy")
cv_acc
92/175:
# Cross-validated accuracy
cv_acc = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="accuracy")
cv_acc
92/176:
# Cross-validated accuracy
cv_acc = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="accuracy")
cv_acc
92/177: np.mean(cv_acc)
92/178:
cv_acc = np.mean(cv_acc)
cv_acc
92/179:
# Cross-validated precision
cv_precision = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="precision")
cv_precision
92/180:
# Cross-validated precision
cv_precision = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="precision")
cv_precision = np.mean(cv_precision)
cv_precision
92/181:
# Cross-validated recall
cv_recall = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="recall")
cv_recall = np.mean(cv_recall)
cv_recall
92/182:
# Cross-validated f1-score
cv_f1 = cross_val_score(clf,
                         X,
                         y,
                         cv=5,
                         scoring="f1")
cv_f1 = np.mean(cv_f1)
cv_f1
92/183:
# Visualize cross-validated matrics
cv_metrics = pd.DataFrame({
    "Accuracy": cv_acc,
    "Precision": cv_precision,
    "recall": cv_recall
    "F1": cv_f1
}, index=[0])

cv_metrics.T.plot(kind="bar", title="Cross-validated classification metrics",
                  legend=False);
92/184:
# Visualize cross-validated matrics
cv_metrics = pd.DataFrame({
    "Accuracy": cv_acc,
    "Precision": cv_precision,
    "recall": cv_recall,
    "F1": cv_f1
}, index=[0])

cv_metrics.T.plot(kind="bar", title="Cross-validated classification metrics",
                  legend=False);
92/185:
# Visualize cross-validated matrics
cv_metrics = pd.DataFrame({
    "Accuracy": cv_acc,
    "Precision": cv_precision,
    "recall": cv_recall,
    "F1": cv_f1
}, index=[0])

cv_metrics.T.plot(kind="bar", title="Cross-validated classification metrics",
                  legend=False)
plt.xtricks(rotation=0)
92/186:
# Visualize cross-validated matrics
cv_metrics = pd.DataFrame({
    "Accuracy": cv_acc,
    "Precision": cv_precision,
    "recall": cv_recall,
    "F1": cv_f1
}, index=[0])

cv_metrics.T.plot(kind="bar", title="Cross-validated classification metrics",
                  legend=False)
plt.xticks(rotation=0)
95/1:
import requests
import json

res = requests.get('https://www.tenders.go.ke/website/contracts/advancedSearch?draw=1&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095')

contract_data = res.json()

print(contract_data)
95/2:
import requests
import json

res = requests.get('https://www.tenders.go.ke/website/contracts/advancedSearch?draw=1&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095')

contract_data = json.dumps(res.json())

print(contract_data)
95/3:
import requests
import json

res = requests.get('https://www.tenders.go.ke/website/contracts/advancedSearch?draw=1&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095')

contract_data = json.dumps(res.json(), indent=2)

print(contract_data)
95/4:
# imports
import requests
import json

import pandas as pd
95/5:
# imports
import requests
import json

import pandas as pd
95/6:


# pagination_max_original_value = 183
pagination_max_value = 5 

for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
    res = requests.get(url=end_point)
    print(f'Hit pagination page = {pagination_value}')

contract_data = json.dumps(res.json(), indent=2)

print(contract_data)
95/7:


# pagination_max_original_value = 183
pagination_max_value = 5 

for pagination_value in range(0, pagination_max_value):
    end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
    res = requests.get(url=end_point)
    print(f'Hit pagination page = {pagination_value}')

contract_data = json.dumps(res.json(), indent=2)

print(contract_data)
95/8:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value)
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(0, pagination_max_value):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data()

print(contract_data)
95/9:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value)
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(0, pagination_max_value):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(contract_data)
95/10:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(0, pagination_max_value):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(contract_data)
95/11:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(contract_data)
95/12:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]

print(contract_data.to_csv())
95/13:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

contract_data = json_normalize(contract_data)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]

print(contract_data.to_csv())
95/14:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

contract_data = pd.json_normalize(contract_data)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]

print(contract_data.to_csv())
95/15:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

contract_data = pd.read_json(contract_data, index=None, header=True)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]

print(contract_data.to_csv())
95/16:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

contract_data = pd.read_json(contract_data)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]

contract_data.to_csv('contract_data.csv', , index=None, header=True)
95/17:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

contract_data = pd.read_json(contract_data)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]

contract_data.to_csv('contract_data.csv', index=None, header=True)
95/18:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]

print(contract_data)
95/19:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]

for item in contract_data:
    print(item['data'])

# print(contract_data)
95/20:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]


print(type contract_data)
95/21:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]


print(type(contract_data))
95/22:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]


print(type(contract_data))

print(type(json.loads(contract_data)[0]))
95/23:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]


print(type(contract_data))

print(type(json.loads(contract_data)))
95/24:


# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

# Convert the data to csv
header_titles = [
    'org_name', 'type', 'created_at', 'tender_title', 'tender_ref_no', 'supplier_name', 'pin_number'
    'contract_amount', 'tender_award_date', 'year', 'contract_code', 'month', 'contract_status', 'updated_at'
    'expected_completion_date'
]


print(type(contract_data))

print(type(json.loads(contract_data)))
95/25:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

for i in contract_data_dict:
    print(i.data)
95/26:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

for i in contract_data_dict:
    print(i['data'])
95/27:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

for i in contract_data_dict:
    print(contract_data_dict['data'])
95/28:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

for i in contract_data_dict:
    print(contract_data_dict.data)
95/29:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

for i in contract_data_dict:
    print(contract_data_dict['data'])
95/30:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

for item in contract_data_dict:
    print(contract_data_dict['data'])
95/31:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
95/32: data
95/33:
# Convert the contract_data_dict to a pandas dataframe
df = pd.DataFrame(contract_data_dict)
df.head()
95/34:
# Convert the contract_data_dict to a pandas dataframe
df = pd.DataFrame(data)
df.head()
95/35:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df.head()
95/36:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, headers=True)
95/37:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
95/38:
# list the file
!ls
95/39:
# Read the created csv file using pandas
ppip_contracts = pd.read_csv('ppip_contracts.csv')
ppip_contracts.head()
95/40: ppip_contracts['tender_title']
95/41: ppip_contracts['tender_title'][0]
95/42: data['title']
95/43: data[0]['title']
95/44:
for item in data:
    print(data['title'])
95/45: type(data)
95/46:
titles = []
for item in data:
    print(title.append(data['title'])
95/47:
titles = []
for item in data:
    title.append(data['title']
print(title)
95/48:
titles = []
for item in data:
    titles.append(data['title']
print(titles)
95/49:
titles = []
for item in data:
    print(item)
95/50:
titles = []
for item in data:
    print(item.title)
95/51:
titles = []
for item in data:
    print(type(item))
95/52:
titles = {}
for item in data:
    titles = data['tender_title']
95/53:
titles = {}
for item in data:
    print(item)
95/54:
titles = {}
for item in data:
    print(item['tender_title'])
95/55:
titles = []
for item in data:
    titles.append(item['tender_title'])
print(titles)
95/56: from b4 import BeautifulSoup
95/57: from b4 import BeautifulSoup
96/1:
# imports
import requests
import json
import pandas as pd
96/2:
# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/3:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/4: data
96/5:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df.head()
96/6:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/7:
# list the file
!ls
96/8:
# Read the created csv file using pandas
ppip_contracts = pd.read_csv('ppip_contracts.csv')
ppip_contracts.head()
96/9: ppip_contracts['tender_title'][0]
96/10:
titles = []
for item in data:
    titles.append(item['tender_title'])
print(titles)
96/11: from b4 import BeautifulSoup
96/12: from b4 import beautifulsoup
96/13: from bs4 import beautifulsoup
96/14: from bs4 import BeautifulSoup
96/15:
titles = []
for item in data:
    titles.append(item['tender_title'])
print(titles)
print(type(titles))
96/16: contract_data
96/17:
# imports
import requests
import json
import re
import pandas as pd
96/18:
# pagination_max_original_value = 183
pagination_max_value = 3 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/19: contract_data
96/20:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/21: data
96/22:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df.head()
96/23:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/24:
# list the file
!ls
96/25:
# Read the created csv file using pandas
ppip_contracts = pd.read_csv('ppip_contracts.csv')
ppip_contracts.head()
96/26: ppip_contracts['tender_title'][0]
96/27:
titles = []
for item in data:
    titles.append(item['tender_title'])
print(titles)
print(type(titles))
96/28:
from bs4 import BeautifulSoup

soup =
96/29: re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/30:
# Remove html tags from the render title
link = re.search('<[^<]+?>', '', ppip_contracts['tender_title'][0])
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/31:
# Remove html tags from the render title
link = re.search('<[^<]+?>', ppip_contracts['tender_title'][0])
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/32: link
96/33: link.match
96/34: link.match()
96/35: link
96/36:
# Remove html tags from the render title
link = re.fullmatch('<[^<]+?>', ppip_contracts['tender_title'][0])
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/37: link
96/38: link
96/39:
# Remove html tags from the render title
link = re.fullmatch('<[^<]+?>', ppip_contracts['tender_title'][0])
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/40: link
96/41:
# Remove html tags from the render title
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0])
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/42: link
96/43: type(link)
96/44:
# Remove html tags from the render title
link = re.compile('<[^<]+?>', ppip_contracts['tender_title'][0])
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/45:
# Remove html tags from the render title
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/46: type(link), link
96/47:
# Remove html tags from the render title
link = `{re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)}{re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(1)}
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/48:
# Remove html tags from the render title
link = `{re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)}{re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(1)}`
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/49:
# Remove html tags from the render title
link = `{re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)}{re.match('<[^<]+?>', ppip_contracts['tender_title'][1]).group(1)}`
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/50:
# Remove html tags from the render title
link = `{re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)}`
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/51:
# Remove html tags from the render title
link = f'{re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)}''
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/52:
# Remove html tags from the render title
link = f'{re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/53:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(1)
link = link_group_1 + link_group_2
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/54:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(1)
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/55:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/56: type(link), link
96/57: link_group_1, type(link), link
96/58:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(-1)
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/59:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(2)
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/60:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).groups()
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).groups()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/61: link_group_1, type(link), link
96/62:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(1)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(2)
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/63:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/64: link_group_1, type(link), link
96/65:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(2)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/66:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(1)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/67:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(0)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/68:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group(2)
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/69:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/70: link_group_1, type(link), link
96/71:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/72:
# Remove html tags from the render title
link_group_1 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
link_group_2 = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group()
link = f'{link_group_1}{link_group_2}'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/73: link_group_1, type(link), link
96/74: type(link), link
96/75:
# Remove html tags from the render title
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/76: type(link), link
96/77: from bs4 import BeautifulSoup
96/78:
from bs4 import BeautifulSoup
import pprint
96/79:
# use regex to find all urls froma string
contract_detail_url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
96/80:
# use regex to find all urls froma string
contract_detail_url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
contract_detail_url
96/81:
# Get the response for contract detais
res_details = requests.get(contract_detail_url)
print(res_details)
96/82:
# Get the response for contract detais
res_details = requests.get(contract_detail_url)
print(res_details)
96/83:
# Get the response for contract detais
res_details = requests.get(contract_detail_url)
print(res_details)
96/84:
# Get the response for contract detais
res_details = requests.get(contract_detail_url)
print(res_details)
96/85:
# use regex to find all urls from a url link
contract_detail_url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
contract_detail_url
96/86:
# Get the response for contract detais
res_details = requests.get(contract_detail_url)
print(res_details)
96/87:
from bs4 import BeautifulSoup
import pprint
96/88:
# use regex to find all urls from a url link
contract_detail_url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
contract_detail_url
96/89:
# Get the response for contract detais
res_details = requests.get(contract_detail_url)
print(res_details)
96/90: type(res_details)
96/91: type(contract_detail_url)
96/92: type(contract_detail_url[0])
96/93:
# Get the response for contract detais
res_details = requests.get(contract_detail_url[0])
print(res_details)
96/94:
# Get the response for contract detais
res_details = requests.get(contract_detail_url[0])
print(res_details)
96/95:
# Get the response for contract detais
res_details = requests.get(contract_detail_url[0])
print(res_details)
96/96:
# Get the response for contract detais
res_details = requests.get(contract_detail_url[0])
print(res_details)
print(res_detsils.content)
96/97:
# Get the response for contract detais
res_details = requests.get(contract_detail_url[0])
print(res_details)
print(res_details.content)
96/98:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table-responsive')
print(table_data)
96/99:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table')
print(table_data)
96/100:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr')
print(table_data)
96/101:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr')
print(table_data.getText())
96/102:
# Get the response for contract detais
res_details = requests.get(contract_detail_url[0])
print(res_details)
print(res_details.content.pretty())
96/103:
# Get the response for contract detais
res_details = requests.get(contract_detail_url[0])
print(res_details)
print(res_details.pretty())
96/104:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr')
print(table_data)
96/105:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr td')
print(table_data)
96/106:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr td')
print(table_data.prettify())
96/107:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr td')
print(soup.prettify())
96/108:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr td')
# print(soup.prettify())
print(table_data)
96/109:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr td')
# print(soup.prettify())
# print(table_data)

content_details = []

for idx, item in enumerate(table_data):
    if id%2 is 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

content_details.append({'title': title, 'details': title_details})

print(content_details)
96/110:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr td')
# print(soup.prettify())
# print(table_data)

content_details = []

for idx, item in enumerate(table_data):
    if idx%2 is 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

content_details.append({'title': title, 'details': title_details})

print(content_details)
96/111:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr td')
# print(soup.prettify())
print(table_data)

content_details = []

for idx, item in enumerate(table_data):
    if idx%2 is 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

content_details.append({'title': title, 'details': title_details})

print(content_details)
96/112:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tr td')
# print(soup.prettify())
print(table_data)

content_details = []

for idx, item in enumerate(table_data):
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

content_details.append({'title': title, 'details': title_details})

print(content_details)
96/113:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tbody')
# print(soup.prettify())
print(table_data)

content_details = []

for idx, item in enumerate(table_data):
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

content_details.append({'title': title, 'details': title_details})

print(content_details)
96/114:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tbody')
# print(soup.prettify())
print(table_data[0])

# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/115:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tbody')
# print(soup.prettify())
print(table_data[0])
print(table_data[1])
print(table_data[2])
print(table_data[3])
print(table_data[4])
print(table_data[5])

# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/116:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tbody')
# print(soup.prettify())
print(table_data[0])
print(table_data[1])
print(table_data[2])
print(table_data[3])
print(table_data[4])

# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/117:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tbody')
# print(soup.prettify())
print(table_data[0])
print(table_data[1])
print(table_data[2])
print(table_data[3])
print(table_data[4])

# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/118:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tbody')
# print(soup.prettify())
print(table_data[0])
print(table_data[1])
print(table_data[2])
print(table_data[3])

# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/119:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tbody')
# print(soup.prettify())
print(table_data[0])
print(table_data[1])
print(table_data[2])
print(table_data[3].getText())

# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/120: content_details
96/121:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.select('.table tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
# print(table_data[0])
# print(table_data[1])
# print(table_data[2])
# print(table_data[3].getText())

print(contract_detail_data)

# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/122:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('.table tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data[0])
print(table_data[1])
print(table_data[2])
print(table_data[3])


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/123:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('.table tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data)
print(table_data[1])
print(table_data[2])
print(table_data[3])


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/124:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('.table tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/125:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/126:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('tbody')
contract_detail_data = soup.select('#home')

# print(soup.prettify())
print(table_data)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/127:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('table')
contract_detail_data = soup.select('#home')

# print(soup.prettify())
print(table_data)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/128:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('table')
contract_detail_data = soup.select('#home')

# print(soup.prettify())
print(table_data.getText())
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/129:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('table')
contract_detail_data = soup.select('#home')

# print(soup.prettify())
print(table_data[0].getText())
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/130:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('table')
contract_detail_data = soup.select('#home')

# print(soup.prettify())
print(type(table_data[0].getText())
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/131:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('table')
contract_detail_data = soup.select('#home')

# print(soup.prettify())
print(type(table_data[0].getText()))
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/132:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('table')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(type(table_data[0].getText()))
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/133:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('table')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data[0].getText())
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/134:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('table')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/135:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/136:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find_all('tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data.content)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/137:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data.content)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/138:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/139:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home tbody')

print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/140:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home tbody')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/141:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/142:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody tr')
contract_detail_data = soup.select('#home')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/143:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home tr')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


# content_details = []

# for idx, item in enumerate(table_data):
#     if idx%2 == 0:
#         title_details = table_data[idx].getText()
#     else:
#         title = table_data[idx].getText()

# content_details.append({'title': title, 'details': title_details})

# print(content_details)
96/144:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = []

for idx, item in enumerate(content_details_list):
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

content_details_list.append({'title': title, 'details': title_details})

print(content_details_list)
96/145:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = []

for idx, item in enumerate(content_details_list):
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

    content_details_list.append({'title': title, 'details': title_details})

print(content_details_list)
96/146:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = []

for idx, item in enumerate(content_details_list):
    print(idx)
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

    content_details_list.append({'title': title, 'details': title_details})

print(content_details_list)
96/147:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = []

for idx, item in enumerate(contract_detail_data):
    print(idx)
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

    content_details_list.append({'title': title, 'details': title_details})

print(content_details_list)
96/148:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
    print(idx)
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

    content_details_list = ({'title': title, 'details': title_details})

print(content_details_list)
96/149:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
    print(idx)
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

    content_details_list = {'title': title, 'details': title_details}

print(content_details_list)
96/150:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
    print(idx)
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/151:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
    print(idx)
    if idx%2 == 0:
        title_details = table_data[idx].getText()
    else:
        title = table_data[idx].getText()

#     content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/152:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
    print(idx)
    if idx%2 == 0:
        title_details = contract_detail_data[idx].getText()
    else:
        title = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/153:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()
    else:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/154:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
#     if idx%2 == 0:
#         title = contract_detail_data[idx].getText()
#         title_details = contract_detail_data[idx].getText()
#     else:
    title = contract_detail_data[idx].getText()
    title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/155:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

# print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
#     if idx%2 == 0:
#         title = contract_detail_data[idx].getText()
#         title_details = contract_detail_data[idx].getText()
#     else:
    title = contract_detail_data[idx].getText()
    title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/156:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()
    else:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/157:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data, start=1):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()
    else:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/158:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data)


content_details_list = {}

for idx, item in enumerate(contract_detail_data, 0):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()
    else:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/159:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(len(contract_detail_data))


content_details_list = {}

for idx, item in enumerate(contract_detail_data, 0):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()
    else:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/160:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data[0])
print(contract_detail_data[1])
print(contract_detail_data[2])
print(contract_detail_data[3])
print(contract_detail_data[4])
print(contract_detail_data[5])

content_details_list = {}

for idx, item in enumerate(contract_detail_data, 0):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()
    else:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/161: 0 / 2
96/162: 1 / 2
96/163:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data[0])
print(contract_detail_data[1])
print(contract_detail_data[2])
print(contract_detail_data[3])
print(contract_detail_data[4])
print(contract_detail_data[5])

content_details_list = {}

for idx, item in enumerate(contract_detail_data, 0):
    print(idx)
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()
    else:
        title = contract_detail_data[idx].getText()
        title_details = contract_detail_data[idx].getText()

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/164: 2 / 2
96/165:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data[0])
print(contract_detail_data[1])
print(contract_detail_data[2])
print(contract_detail_data[3])
print(contract_detail_data[4])
print(contract_detail_data[5])

content_details_list = {}

for idx, item in enumerate(contract_detail_data, 0):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        print(f'{idx} {title}')
    else:
        title_details = contract_detail_data[idx].getText()
        print(f'{idx} {title_details}')

    content_details_list.update({'title': title, 'details': title_details})

print(content_details_list)
96/166:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data[0])
print(contract_detail_data[1])
print(contract_detail_data[2])
print(contract_detail_data[3])
print(contract_detail_data[4])
print(contract_detail_data[5])

content_details_list = {}

for idx, item in enumerate(contract_detail_data, 0):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
        print(f'{idx} {title}')
    else:
        title_details = contract_detail_data[idx].getText()
        print(f'{idx} {title_details}')

    content_details_list.update({title: title_details})

print(content_details_list)
96/167:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

print(contract_detail_data[0])
print(contract_detail_data[1])
print(contract_detail_data[2])
print(contract_detail_data[3])
print(contract_detail_data[4])
print(contract_detail_data[5])

content_details_list = {}

for idx, item in enumerate(contract_detail_data, 0):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
#         print(f'{idx} {title}')
    else:
        title_details = contract_detail_data[idx].getText()
#         print(f'{idx} {title_details}')

    content_details_list.update({title: title_details})

print(content_details_list)
96/168:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

content_details_list = {}

for idx, item in enumerate(contract_detail_data, 0):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
#         print(f'{idx} {title}')
    else:
        title_details = contract_detail_data[idx].getText()
#         print(f'{idx} {title_details}')

    content_details_list.update({title: title_details})

print(content_details_list)
96/169:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

def get_contract_details(contract_detail_data):
    content_details_dict = {}

    for idx, item in enumerate(contract_detail_data, 0):
        if idx%2 == 0:
            title = contract_detail_data[idx].getText()
        else:
            title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})
    return content_details_dict

print(get_contract_details(contract_detail_data))
96/170:
soup = BeautifulSoup(res_details.text, 'html.parser')

table_data = soup.find('tbody')
contract_detail_data = soup.select('#home td')

# print(soup.prettify())
# print(table_data.contents)
# print(table_data[1])
# print(table_data[2])
# print(table_data[3])

content_details_dict = {}

for idx, item in enumerate(contract_detail_data, 0):
    if idx%2 == 0:
        title = contract_detail_data[idx].getText()
    else:
        title_details = contract_detail_data[idx].getText()

    content_details_dict.update({title: title_details})

print(content_details_dict)
96/171: contract_detail_url
96/172: len(contract_detail_url)
96/173:
# use regex to find all urls from a url link
contract_detail_url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
contract_detail_url
96/174:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links
96/175:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = df['tender_title']
contract_detail_links
96/176:
# Function to remove a tags from tender titles

def remove_anchor_tags(links):
    for idx, link in enumerate(links):
        modified_link = re.match('<[^<]+?>', link[idx]).group() + '</a>'
        return modified_link

remove_anchor_tags(contract_detail_links)
96/177:
# Function to remove a tags from tender titles

def remove_anchor_tags(links):
    for idx, link in enumerate(links):
        modified_link = re.match('<[^<]+?>', link[idx]) + '</a>'
        return modified_link

remove_anchor_tags(contract_detail_links)
96/178:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = df['tender_title']
contract_detail_links
96/179:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = df['tender_title', index=False]
contract_detail_links
96/180:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = df['tender_title']
contract_detail_links
96/181:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = df['tender_title']
contract_detail_links[0]
96/182:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links[0]
96/183:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/184:
# Function to remove a tags from tender titles

def remove_anchor_tags(links):
    for idx, link in enumerate(links):
        modified_link = re.match('<[^<]+?>', link[idx]).group() + '</a>'
        return modified_link

remove_anchor_tags(contract_detail_links)
96/185:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links

re.match('<[^<]+?>', contract_detail_links[0]).group() + '</a>'
96/186:
# Function to remove a tags from tender titles

def remove_anchor_tags(links):
    for idx, link in enumerate(links):
        modified_link = re.match('<[^<]+?>', link[idx]).group() + '</a>'
    return modified_link

remove_anchor_tags(contract_detail_links)
96/187:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links

rlink = re.match('<[^<]+?>', contract_detail_links[0]).group() + '</a>'
96/188:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links

rlink = re.match('<[^<]+?>', contract_detail_links[0]).group() + '</a>'
rlink
96/189:
# Function to remove a tags from tender titles

def remove_anchor_tags(links):
    modified_links = []
    for idx, link in enumerate(links):
        modified_links.append(re.match('<[^<]+?>', link[idx]).group() + '</a>')
    return modified_links

remove_anchor_tags(contract_detail_links)
96/190:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/191:
# Function to remove a tags from tender titles

def remove_anchor_tags(links):
    modified_links = []
    for idx, link in enumerate(links):
        print(link[idx)
#         modified_links.append(re.match('<[^<]+?>', link[idx]).group() + '</a>')
    return modified_links

remove_anchor_tags(contract_detail_links)
96/192:
# Function to remove a tags from tender titles

def remove_anchor_tags(links):
    modified_links = []
    for idx, link in enumerate(links):
        print(link[idx])
#         modified_links.append(re.match('<[^<]+?>', link[idx]).group() + '</a>')
    return modified_links

remove_anchor_tags(contract_detail_links)
96/193:
# Function to remove a tags from tender titles

def remove_anchor_tags(links):
    modified_links = []
    for idx, link in enumerate(links):
        print(link[idx])
#         modified_links.append(re.match('<[^<]+?>', link[idx]).group() + '</a>')
    return links

remove_anchor_tags(contract_detail_links)
96/194:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links = []
    for idx, link in enumerate(contract_detail_links):
        print(link[idx])
#         modified_links.append(re.match('<[^<]+?>', link[idx]).group() + '</a>')
    return links

remove_anchor_tags(contract_detail_links)
96/195:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links = []
    for idx, link in enumerate(contract_detail_links):
        print(link[idx])
#         modified_links.append(re.match('<[^<]+?>', link[idx]).group() + '</a>')
    return contract_detail_links

remove_anchor_tags(contract_detail_links)
96/196:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links = []
    for idx, link in enumerate(contract_detail_links):
        modified_links.append(re.match('<[^<]+?>', link[idx]).group() + '</a>')
    return contract_detail_links

remove_anchor_tags(contract_detail_links)
96/197:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links = []
    for idx, link in enumerate(contract_detail_links):
        modified_links.append(re.match('<[^<]+?>', link.group() + '</a>')
    return contract_detail_links

remove_anchor_tags(contract_detail_links)
96/198:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links = []
    for idx, link in enumerate(contract_detail_links):
        modified_links.append(re.match('<[^<]+?>', link.group() + '</a>')
    return modified_links

remove_anchor_tags(contract_detail_links)
96/199:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links = []
    for idx, link in enumerate(contract_detail_links):
        modified_links.append(re.match('<[^<]+?>', link.group() + '</a>')
    return modified_links

remove_anchor_tags(contract_detail_links)
96/200:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links = []
    for idx, link in enumerate(contract_detail_links):
        print(link)
#         modified_links.append(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links

remove_anchor_tags(contract_detail_links)
96/201:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links = []
    for idx, link in enumerate(contract_detail_links):
        modified_link = re.match('<[^<]+?>', link.group() + '</a>'
        modified_links.append(modified_link)
    return modified_links

remove_anchor_tags(contract_detail_links)
96/202:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for idx, link in enumerate(contract_detail_links):
        modified_link = re.match('<[^<]+?>', link.group() + '</a>'
        modified_links_list.append(modified_link)
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/203:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for idx, link in enumerate(contract_detail_links):
        re.match('<[^<]+?>', link.group() + '</a>'
        modified_links_list.append(modified_link)
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/204:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for idx, link in enumerate(contract_detail_links):
        print(re.match('<[^<]+?>', link.group() + '</a>')
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/205:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for idx, link in enumerate(contract_detail_links):
        print(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links_list

remove_anchor_tags(contract_detail_links)
96/206:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        print(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links_list

remove_anchor_tags(contract_detail_links)
96/207:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        print(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links_list

remove_anchor_tags(contract_detail_links)
96/208:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        print(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links_list

remove_anchor_tags(contract_detail_links)
96/209:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/210:
# Function to remove a tags from tender titles

content_details
96/211:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        print(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links_list

remove_anchor_tags(list(df['tender_title']))
96/212:
# Function to remove a tags from tender titles

def remove_anchor_tags(list(df['tender_title'])):
    modified_links_list = []
    for link in list(df['tender_title']):
        print(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links_list

remove_anchor_tags(list(df['tender_title']))
96/213:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/214:
# Function to remove a tags from tender titles

contract_detail_links
96/215:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        print(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links_list

remove_anchor_tags(contract_detail_links)
96/216:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        print(re.match('<[^<]+?>', link.group() + '</a>')
#     return modified_links_list
    pass

remove_anchor_tags(contract_detail_links)
96/217:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        print(re.match('<[^<]+?>', link.group() + '</a>')
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/218: modified_link
96/219:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', link.group() + '</a>')
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/220:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', (link).group() + '</a>')
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/221:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/222:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]) + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/223:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', link.group(0) + '</a>')
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/224:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', link.group(0))
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/225:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', link)
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/226:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', link)
    return modified_links_list.append(modified_link)

remove_anchor_tags(contract_detail_links)
96/227:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', link)
        modified_links_list.append(modified_link)
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/228:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', link)
        modified_links_list.append(modified_link.group())
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/229:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.match('<[^<]+?>', link)
        modified_links_list.append(modified_link.group() + '</a>')
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/230:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/231:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function removes the
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/232: type(modified_links_list[0])
96/233: type(modified_links_list)
96/234:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

remove_anchor_tags(contract_detail_links)
96/235:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
modified_links_list
96/236: type(modified_links_list)
96/237: type(modified_links_list[0])
96/238: type(modified_links_list[0])
96/239: Now that we have the modified list of all contract details, lets retrieve more information about the contracts
96/240: modified_links_list
96/241:
# Get the response for contract detais
for link in range(0, len(modified_links_list)):
    res_details = requests.get(link)
    print(res_details)
96/242:
# Get the response for contract detais
for link in range(0, len(modified_links_list)):
#     res_details = requests.get(link)
    print(link)
96/243:
# Get the response for contract detais
for link in (modified_links_list):
    res_details = requests.get(link)
    print(res_details)
96/244:
# Get the response for contract detais
for link in (modified_links_list):
#     res_details = requests.get(link)
    print(res_details)
96/245:
# Get the response for contract detais
for link in (modified_links_list):
#     res_details = requests.get(link)
    print(link)
96/246:
# Get the response for contract detais
for link in (modified_links_list):
#     res_details = requests.get(link)
    print(type(link))
96/247:
# Get the response for contract detais
for link in (modified_links_list):
#     res_details = requests.get(link)
    print(str(link))
96/248:
# Get the response for contract detais
for link in (modified_links_list):
#     res_details = requests.get(link)
    print(str(link))
    print
96/249:
# Get the response for contract detais
for link in (modified_links_list):
#     res_details = requests.get(link)
    print(str(link))
    print(type(str(link)))
96/250:
# Get the response for contract detais
for link in (modified_links_list):
#     res_details = requests.get(link)
    print(link[0])
96/251:
# Get the response for contract detais
for link in (modified_links_list):
    res_details = requests.get(link[0])
    print(link[0])
96/252:
# Get the response for contract detais
for link in (modified_links_list):
    res_details = requests.get(link[0])
    print(f'Hit link {link[0]}')
96/253:
# Get the response for contract detais
for link in (modified_links_list):
    res_details = requests.get(link[0])
    print(f'Hit link {link[0]} successfully')
96/254:
# Get the response for contract detais
for link in (modified_links_list):
    res_details = requests.get(link[0])
    print(f'Hit link {link[0]} successfully')
    print(res_details)
96/255:

for i in range(0, len(modified_links_list)):
    soup = BeautifulSoup(res_details.text, 'html.parser')
    table_data = soup.find('tbody')
    contract_detail_data = soup.select('#home td')

    content_details_dict = {}

    for idx, item in enumerate(contract_detail_data, 0):
        if idx%2 == 0:
            title = contract_detail_data[idx].getText()
        else:
            title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})

print(content_details_dict)
96/256:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
96/257:

for link_details in res_details_list:
    soup = BeautifulSoup(link_details.text, 'html.parser')
    table_data = soup.find('tbody')
    contract_detail_data = soup.select('#home td')

    content_details_dict = {}

    for idx, item in enumerate(contract_detail_data, 0):
        if idx%2 == 0:
            title = contract_detail_data[idx].getText()
        else:
            title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})

print(content_details_dict)
96/258:

for link_details in res_details_list:
    print(link_details.text)
    soup = BeautifulSoup(link_details.text, 'html.parser')
    table_data = soup.find('tbody')
    contract_detail_data = soup.select('#home td')

    content_details_dict = {}

    for idx, item in enumerate(contract_detail_data, 0):
        if idx%2 == 0:
            title = contract_detail_data[idx].getText()
        else:
            title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})

print(content_details_dict)
96/259:
# imports
import requests
import json
import re
import pandas as pd
96/260:
# pagination_max_original_value = 183
pagination_max_value = 1 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/261: contract_data
96/262:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/263: data
96/264:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df.head()
96/265:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/266:
# list the file
!ls
96/267:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/268: type(link), link
96/269:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/270:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
modified_links_list
96/271: type(modified_links_list[0])
96/272: modified_links_list
96/273:
from bs4 import BeautifulSoup
import pprint
96/274: len(contract_detail_url)
96/275:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
96/276:

for link_details in res_details_list:
    print(link_details.text)
    soup = BeautifulSoup(link_details.text, 'html.parser')
    table_data = soup.find('tbody')
    contract_detail_data = soup.select('#home td')

    content_details_dict = {}

    for idx, item in enumerate(contract_detail_data, 0):
        if idx%2 == 0:
            title = contract_detail_data[idx].getText()
        else:
            title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})

print(content_details_dict)
96/277: 0 / 2
96/278: 2 / 2
96/279: 1 / 2
96/280:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
modified_links_list
96/281: modified_links_list
96/282:
# imports
import requests
import json
import re
import pandas as pd
96/283:
# pagination_max_original_value = 183
pagination_max_value = 1 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/284: contract_data
96/285:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/286: data
96/287:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df.head()
96/288:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/289:
# list the file
!ls
96/290:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/291: type(link), link
96/292:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/293:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
modified_links_list
96/294: type(modified_links_list[0])
96/295: modified_links_list
96/296:
from bs4 import BeautifulSoup
import pprint
96/297: len(contract_detail_url)
96/298:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
96/299:

for link_details in res_details_list:
    soup = BeautifulSoup(link_details.text, 'html.parser')
    table_data = soup.find('tbody')
    contract_detail_data = soup.select('#home td')

    content_details_dict = {}

    for idx, item in enumerate(contract_detail_data, 0):
        if idx%2 == 0:
            title = contract_detail_data[idx].getText()
        else:
            title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})

print(content_details_dict)
96/300: 0 / 2
96/301: 2 / 2
96/302: 1 / 2
96/303:
# imports
import requests
import json
import re
import pandas as pd
96/304:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df
96/305:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df
96/306:

for link_details in res_details_list:
    soup = BeautifulSoup(link_details.text, 'html.parser')
#     table_data = soup.find('tbody')
    contract_detail_data = soup.select('#home td')

    content_details_dict = {}

    for idx, item in enumerate(contract_detail_data, 0):
        if idx%2 == 0:
            title = contract_detail_data[idx].getText()
        else:
            title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})

    print(content_details_dict)
96/307:
def retrieve_contract_details(res_details_list):
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()
            content_details_dict.update({title: title_details})
        
        return content_details_dict
    
print(retrieve_contract_details(res_details_list))
96/308:
def retrieve_contract_details(res_details_list):
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()
                content_details_dict.update({title: title_details})
        
        return content_details_dict
    
print(retrieve_contract_details(res_details_list))
96/309:
def retrieve_contract_details(res_details_list):
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()
                content_details_dict.update({title: title_details})
        
    return content_details_dict
    
print(retrieve_contract_details(res_details_list))
96/310:

for link_details in res_details_list:
    soup = BeautifulSoup(link_details.text, 'html.parser')
    contract_detail_data = soup.select('#home td')

    content_details_dict = {}

    for idx, item in enumerate(contract_detail_data, 0):
        if idx%2 == 0:
            title = contract_detail_data[idx].getText()
        else:
            title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})

    print(content_details_dict)
96/311:

def retrieve_contract_details(res_details_list):
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict.update({title: title_details})

        return content_details_dict
    return content_details_dict

print(retrieve_contract_details(res_details_list))
96/312:

def retrieve_contract_details(res_details_list):
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

        content_details_dict.update({title: title_details})

        return content_details_dict
    return content_details_dict

print(retrieve_contract_details(res_details_list))
96/313:

def retrieve_contract_details(res_details_list):
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict.update({title: title_details})

        return content_details_dict
    return content_details_dict

print(retrieve_contract_details(res_details_list))
96/314:

def retrieve_contract_details(res_details_list):
    global title, title_details
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict.update({title: title_details})

        return content_details_dict
    return content_details_dict

print(retrieve_contract_details(res_details_list))
96/315:

def retrieve_contract_details(res_details_list):
    global title, title_details
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict.update({title: title_details})
    return content_details_dict

print(retrieve_contract_details(res_details_list))
96/316:

def retrieve_contract_details(res_details_list):
    global title, title_details
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details
    return content_details_dict

print(retrieve_contract_details(res_details_list))
96/317:

def retrieve_contract_details(res_details_list):
    global title, title_details
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')
        
        print(contract_detail_data)

        content_details_dict = {}

#         for idx, item in enumerate(contract_detail_data, 0):
#             if idx%2 == 0:
#                 title = contract_detail_data[idx].getText()
#             else:
#                 title_details = contract_detail_data[idx].getText()

#             content_details_dict[title] = title_details
            
    return content_details_dict

print(retrieve_contract_details(res_details_list))
96/318:

def retrieve_contract_details(res_details_list):
    global title, title_details
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details
            
    print(content_details_dict)

(retrieve_contract_details(res_details_list))
96/319:

def retrieve_contract_details(res_details_list):
    global title, title_details
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details
            
        print(content_details_dict)

(retrieve_contract_details(res_details_list))
96/320:

def retrieve_contract_details(res_details_list):
    global title, title_details
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details
            
        return content_details_dict

print(retrieve_contract_details(res_details_list))
96/321:

def retrieve_contract_details(res_details_list):
    global title, title_details
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details
            
        return len(content_details_dict)

print(retrieve_contract_details(res_details_list))
96/322:

def retrieve_contract_details(res_details_list):
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

print(retrieve_contract_details(res_details_list))
96/323:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(all_contract_details)
print(contract_details)
96/324:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
96/325:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
96/326:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details


print(retrieve_contract_details(res_details_list))
96/327:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(all_contract_details)
print(contract_details)
96/328:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
96/329:
# Convert the list of dictionaries to json
print(json.dumps(contract_details))
96/330:
# Convert the list of dictionaries to json
print(json.dumps(contract_details, indent=2))
96/331:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, indent=2)
content_details
96/332:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, indent=2)
print(content_details)
96/333:
# Convert the list of dictionaries to json
print(json.dumps(contract_details, indent=2))
96/334:
# Convert the list of dictionaries to json
print(json.dumps(contract_details, indent=2))
96/335:
# imports
import requests
import json
import re
import pandas as pd
96/336:
# pagination_max_original_value = 183
pagination_max_value = 1 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/337: contract_data
96/338:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/339: data
96/340:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df
96/341:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/342:
# list the file
!ls
96/343:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/344: type(link), link
96/345:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/346:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
modified_links_list
96/347: type(modified_links_list[0])
96/348: modified_links_list
96/349:
from bs4 import BeautifulSoup
import pprint
96/350: len(contract_detail_url)
96/351:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
96/352:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
96/353:
# Convert the list of dictionaries to json
print(json.dumps(contract_details, indent=2))
96/354: 1 / 2
96/355:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, indent=2)
print(contract_details)
96/356: 1 / 2
96/357:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, indent=2)
print(contract_details)
96/358:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, indent=2)
print(contract_details)
96/359:
# Convert the list of dictionaries to json
np.seed.random(42)
contract_details = json.dumps(contract_details, indent=2)
print(contract_details)
96/360:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, indent=2)
print(contract_details)
96/361:
# imports
import requests
import json
import re
import pandas as pd
96/362:
# pagination_max_original_value = 183
pagination_max_value = 1 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/363: contract_data
96/364:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/365: data
96/366:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df
96/367:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/368:
# list the file
!ls
96/369:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/370: type(link), link
96/371:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/372:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
modified_links_list
96/373: type(modified_links_list[0])
96/374: modified_links_list
96/375:
from bs4 import BeautifulSoup
import pprint
96/376: len(contract_detail_url)
96/377:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
96/378:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
96/379:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, indent=2)
print(contract_details)
96/380: 1 / 2
96/381: contract_details
96/382: pprint(contract_details)
96/383: pprint.pprint(contract_details)
96/384: print(type(contract_details))
96/385:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame(content_details)
df1.head()
96/386:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame(content_details, columns=['Entity Type', 'Entity Name', 'Tender Reference Number', 'Contract Number'])
df1.head()
96/387:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame(content_details)
df1.head()
96/388:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame(content_details, index=False)
df1.head()
96/389:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame(content_details)
df1.head()
96/390:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details)
print(contract_details)
96/391:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame(content_details)
df1.head()
96/392: print(type(contract_details))
96/393:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details)
print(contract_details)
96/394: print(type(contract_details))
96/395:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details)
print(contract_details)
96/396:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, *)
print(contract_details)
96/397:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, skipkeys=True)
print(contract_details)
96/398:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, skipkeys=True)
print(contract_details)
96/399:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
96/400:
# Convert the list of dictionaries to json
contract_details = json.dumps(contract_details, skipkeys=True)
print(contract_details)
96/401:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
96/402:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame(content_details)
df1.head()
96/403:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame(content_details)
df1.head()
96/404:
# Convert the json string to a pandas dataframe
df1 = pd.io.json.json_normalize(content_details)
df1
96/405:
# Convert the json string to a pandas dataframe
df1 = pd.json_normalize(content_details)
df1
96/406:
# Convert the json string to a pandas dataframe
df1 = pd.json_normalize(content_details)
df1
96/407:
# Convert the json string to a pandas dataframe
df1 = pd.json_normalize(content_details, max_level=1)
df1
96/408:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame.from_records(contract_details)
df1
96/409: df1.to_csv('ppip_contract_details.csv')
96/410:
# Export to csv
df1.to_csv('ppip_contract_details.csv')
96/411:
# Check file
!ls
96/412:
ppip_contract_details = pd.read_csv('ppip_contracts.csv')
ppip_contract_details.head()
96/413:
ppip_contract_details = pd.read_csv('ppip_contract_details.csv.csv')
ppip_contract_details.head()
96/414:
ppip_contract_details = pd.read_csv('ppip_contract_details.csv')
ppip_contract_details.head()
96/415:
# Export to csv
df1.to_csv('ppip_contract_details.csv', index=False, header=True)
96/416:
# Check file
!ls
96/417:
ppip_contract_details = pd.read_csv('ppip_contract_details.csv')
ppip_contract_details.head()
96/418:
# imports
import requests
import json
import re
import pandas as pd
96/419:
# pagination_max_original_value = 183
pagination_max_value = 183 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/420: contract_data
96/421:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/422: data
96/423:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df
96/424:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/425:
# list the file
!ls
96/426:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/427: type(link), link
96/428:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/429:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
modified_links_list
96/430: type(modified_links_list[0])
96/431: modified_links_list
96/432: from bs4 import BeautifulSoup
96/433: len(contract_detail_url)
96/434:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
96/435:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
96/436: print(type(contract_details))
96/437:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame.from_records(contract_details)
df1
96/438:
# Export to csv
df1.to_csv('ppip_contract_details.csv', index=False, header=True)
96/439:
# Check file
!ls
96/440:
ppip_contract_details = pd.read_csv('ppip_contract_details.csv')
ppip_contract_details.head()
96/441:
# imports
import requests
import json
import re
import pandas as pd
96/442:
# pagination_max_original_value = 183
pagination_max_value = 183 

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value + 1):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/443:
# pagination_max_original_value = 183
pagination_max_value = 184

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=3)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/444:
# pagination_max_original_value = 183
pagination_max_value = 3

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=50&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1587957112095'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=184)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/445: contract_data
96/446:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/447: data
96/448:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df
96/449:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/450:
# list the file
!ls
96/451:
# Read the created csv file using pandas
ppip_contracts = pd.read_csv('ppip_contracts.csv')
ppip_contracts.head()
96/452: len(ppip_contracts)
96/453:
# Read the created csv file using pandas
ppip_contracts = pd.read_csv('ppip_contracts.csv')
ppip_contracts.head()
ppip_contracts.tail()
96/454:
# imports
import requests
import json
import re
import pandas as pd
96/455:
# pagination_max_original_value = 183
pagination_max_value = 3

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=2500&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1588036321965'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=5)

print(type(contract_data))

print(type(json.loads(contract_data)))
96/456: contract_data
96/457:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
    print(data)
96/458: data
96/459:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df
96/460:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
96/461:
# list the file
!ls
96/462:
# Read the created csv file using pandas
ppip_contracts = pd.read_csv('ppip_contracts.csv')
ppip_contracts.head()
ppip_contracts.tail()
96/463: len(ppip_contracts)
96/464:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
96/465: type(link), link
96/466:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
contract_detail_links
96/467:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
modified_links_list
96/468: type(modified_links_list[0])
96/469: modified_links_list
96/470: from bs4 import BeautifulSoup
96/471: len(contract_detail_url)
96/472:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
97/1:
# imports
import requests
import json
import re
import pandas as pd
97/2:
# pagination_max_original_value = 183
pagination_max_value = 3

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=2500&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1588036321965'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=5)

print(type(contract_data))

print(type(json.loads(contract_data)))
97/3: contract_data
97/4:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
97/5:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df.head()
97/6:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
97/7:
# list the file
!ls
97/8:
# Read the created csv file using pandas
ppip_contracts = pd.read_csv('ppip_contracts.csv')
ppip_contracts.head()
97/9: len(ppip_contracts)
97/10:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
97/11: type(link), link
97/12:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
97/13:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
97/14: type(modified_links_list[0])
97/15: from bs4 import BeautifulSoup
97/16: len(contract_detail_url)
97/17: Retrieving the json data from a given end point
97/18: Convert the retrieved data
97/19: Convert the retrieved datam
97/20:
ppip_contract_details = pd.read_csv('ppip_contract_details.csv')
ppip_contract_details.head()
97/21:
ppip_contract_details = pd.read_csv('ppip_contract_details.csv')
ppip_contract_details.head()
len(ppip_contract_details)
97/22:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
97/23:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
97/24:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
97/25: type(modified_links_list[0])
97/26: from bs4 import BeautifulSoup
97/27:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
97/28:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
97/29:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/30:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/31:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    
    all_contract_details = []
    for link_details in res_details_list:
        global title, title_details
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/32:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    
    all_contract_details = []
    for link_details in res_details_list:
        global title, title_details
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/33:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    
    all_contract_details = []
    for link_details in res_details_list:
        global title, title_details
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()
            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/34:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/35:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details=''
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/36:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title_detail
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/37:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_detail
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/38:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
#     global title, title_detail
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/39:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title
    global title_detail = ''
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/40:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/41:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, 
    global title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/42:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title 
    global title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/43:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            global title 
            global title_details
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/44:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/45:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/46:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details = ''
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/47:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details = ''
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

        content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/48:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

        content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/49: print(type(contract_details))
97/50:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame.from_records(contract_details)
df1
97/51:

def retrieve_contract_details(res_details_list):
    """
    This function retrieves the details of all contracts based on the response from the links provided for the
    details of each contract
    """
    global title, title_details
    all_contract_details = []
    for link_details in res_details_list:
        soup = BeautifulSoup(link_details.text, 'html.parser')
        contract_detail_data = soup.select('#home td')

        content_details_dict = {}

        for idx, item in enumerate(contract_detail_data, 0):
            if idx%2 == 0:
                title = contract_detail_data[idx].getText()
            else:
                title_details = contract_detail_data[idx].getText()

            content_details_dict[title] = title_details

        all_contract_details.append(content_details_dict)
    return all_contract_details

contract_details = retrieve_contract_details(res_details_list)
print(contract_details)
97/52: print(type(contract_details))
97/53:
# Convert the json string to a pandas dataframe
df1 = pd.DataFrame.from_records(contract_details)
df1
97/54:
# Export to csv
df1.to_csv('ppip_contract_details.csv', index=False, header=True)
97/55:
# Check file
!ls
97/56:
ppip_contract_details = pd.read_csv('ppip_contract_details.csv')
ppip_contract_details.head()
len(ppip_contract_details)
98/1: pprint(res)
102/1: print(rows)
109/1: import pandas as pd
109/2: pdes = pd.read_csv('pdes.csv')
109/3: pdes.head()
109/4: pdes.drop(columns=[tel])
109/5: pdes.drop(columns=['tel'])
109/6: pdes.drop(columns=['tel', 'email', 'url', 'created_at', 'updated_at'])
109/7: pdes.drop(columns=['tel', 'email', 'url', 'created_at', 'updated_at'], inplace=True)
109/8: pdes
109/9: pdes.head()
109/10: pdes[pdes['abbreviation'] == 'Masindi District Local Government']
109/11: pdes[pdes['abbreviation'] == ['Masindi District Local Government', 'Gulu District Local Government', 'Kabale District Local Government']]
109/12: pdes[pdes['abbreviation'] == ['Masindi District Local Government', 'Gulu District Local Government']]
109/13: pdes[pdes['abbreviation'] == ['Gulu District Local Government']]
109/14: pdes[pdes['abbreviation'] == 'Gulu District Local Government']
109/15: pdes[pdes['abbreviation'] == 'Gulu District Local Government']
109/16: pdes[pdes['title'] == 'Gulu District Local Government']
109/17: pdes[pdes['title'] == 'Kabale District Local Government']
109/18:
pdes = pd.read_csv('pdes.csv')
procurement_plans = pd.read_csv('procurement_plan_entries.csv')
109/19: procurement_plans.head()
109/20: procurement_plans.drop(columns=['funding_source_id'])
109/21: procurement_plans.drop(columns=['funding_source_id', 'funder_name'])
109/22:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
109/23: procurement_plans.head()
109/24:
# pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.options.display.max_seq_items = None
109/25: procurement_plans.head()
109/26:
# pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.options.display.max_seq_items = 200
109/27: procurement_plans.head()
110/1: import pandas as pd
110/2:
pdes = pd.read_csv('pdes.csv')
procurement_plans = pd.read_csv('procurement_plan_entries.csv')
110/3:
# pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.options.display.max_seq_items = 200
110/4: pdes.head()
110/5: pdes.drop(columns=['tel', 'email', 'url', 'created_at', 'updated_at'], inplace=True)
110/6: pdes[pdes['title'] == 'Kabale District Local Government']
110/7: procurement_plans.head()
110/8: procurement_plans.drop(columns=['funding_source_id', 'funder_name'])
110/9:
df1 = pd.read_csv('FY-2018-2019-total-bids-total-contracts.csv')
df1.head()
110/10:
df1 = pd.read_excel('FY-2018-2019-total-bids-total-contracts.csv')
df1.head()
110/11:
df1 = pd.read_csv('FY-2018-2019-total-bids-total-contracts.csv')
df1.head()
110/12:
df1 = pd.read_csv('FY-2018-2019-total-bids-total-contracts.csv', header=1)
df1.head()
110/13:
df1 = pd.read_csv('FY-2018-2019-total-bids-total-contracts.csv', header=2)
df1.head()
110/14:
df1 = pd.read_csv('FY-2018-2019-total-bids-total-contracts.csv', header=3)
df1.head()
110/15:
df1 = pd.read_csv('FY-2018-2019-total-bids-total-contracts.csv')
df1.head()
110/16:
df1 = pd.read_excel('FY-2018-2019-total-bids-total-contracts.xlsx')
df1.head()
110/17:
df1 = pd.read_excel('FY-2018-2019-total-bids-total-contracts.xlsx', header=0)
df1.head()
110/18:
df1 = pd.read_excel('FY-2018-2019-total-bids-total-contracts.xlsx', header=1)
df1.head()
110/19:
df1 = pd.read_csv('FY-2018-2019-total-bids-total-contracts.csv', header=1)
df1.head()
110/20:
df1 = pd.read_excel('FY-2018-2019-total-bids-total-contracts.xlsx', header=1)
df1.head()
124/1: import pandas as pd
125/1: import pandas as pd
125/2: awarded_contracts = pd.read_excel('Awarded-contracts.xls')
125/3: print(1+2)
125/4: import pandas as pd
125/5: awarded_contracts = pd.read_excel('Awarded-contracts.xls')
126/1: awarded_contracts = pd.read_excel('Awarded-contracts.xls')
126/2: import pandas as pd
126/3: awarded_contracts = pd.read_excel('Awarded-contracts.xls')
126/4: awarded_contracts = pd.read_excel('Awarded-contracts.xls', head=False)
126/5: awarded_contracts = pd.read_excel('Awarded-contracts.xls')
126/6: awarded_contracts = pd.read_xls('Awarded-contracts.xls')
126/7: awarded_contracts = pd.read_excel('Awarded-contracts.xls')
126/8: awarded_contracts = pd.read_excel('Awarded-contracts.xls', skiprows=[7],  header=None)
128/1: import pandas as pd
128/2: 3+4
128/3: awarded_contracts = pd.read_csv('awarded_contracts-FY-2019-2020.csv')
128/4: awarded_contracts.head()
128/5: awarded_contracts.tail()
128/6: awarded_contracts.head()
128/7: awarded_contracts.info()
128/8:
import pandas as pd
import datetime
128/9: awarded_contracts = pd.read_csv('awarded_contracts-FY-2019-2020.csv')
128/10: awarded_contracts.head()
128/11: awarded_contracts.info()
128/12: awarded_contracts = pd.read_csv('awarded_contracts-FY-2019-2020.csv', parse_dates=True)
128/13: awarded_contracts.head()
128/14: awarded_contracts.info()
128/15: awarded_contracts.info()
128/16:
completed = awarded_contracts['completed_contracts_value'] != ''
completed.head()
128/17:
completed = awarded_contracts['completed_contracts_value'] != ''
awarded_contracts[completed]
128/18:
completed = awarded_contracts['completed_contracts_value'] != null
awarded_contracts[completed]
128/19:
completed = awarded_contracts['completed_contracts_value'] != 'null'
awarded_contracts[completed]
128/20:
completed = awarded_contracts['completed_contracts_value'] != NaN
awarded_contracts[completed]
128/21:
completed = awarded_contracts['completed_contracts_value'] != 'NaN'
awarded_contracts[completed]
128/22:
completed = awarded_contracts['completed_contracts_value'] > 0
awarded_contracts[completed]
128/23:
completed = awarded_contracts['completed_contracts_value'] > 0
awarded_contracts[completed]
pd.to_csv('completed_contracts.csv')
128/24:
completed = awarded_contracts['completed_contracts_value'] > 0
awarded_contracts[completed]
awarded_contracts[completed].to_csv('completed_contracts.csv')
128/25:
competed_contracts = read_csv('completed_contracts.csv')
completed_contracts.head()
128/26:
competed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
128/27:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
128/28:
completed = awarded_contracts['completed_contracts_value'] > 0
awarded_contracts[completed]
awarded_contracts[completed].to_csv('completed_contracts.csv')
128/29:
completed_contracts = pd.read_csv('completed_contracts.csv', parse_dates=['initiation_date', 'signed_date', 'planned_completion_date', 'actual_completion_date'])
completed_contracts.head()
128/30:
completed_contracts = pd.read_csv('completed_contracts.csv', index=False, parse_dates=['initiation_date', 'signed_date', 'planned_completion_date', 'actual_completion_date'])
completed_contracts.head()
128/31:
completed = awarded_contracts['completed_contracts_value'] > 0
awarded_contracts[completed]
awarded_contracts[completed].to_csv('completed_contracts.csv')
128/32:
completed_contracts = pd.read_csv('completed_contracts.csv', index=False, parse_dates=['initiation_date', 'signed_date', 'planned_completion_date', 'actual_completion_date'])
completed_contracts.head()
128/33:
completed_contracts = pd.read_csv('completed_contracts.csv', index=None, parse_dates=['initiation_date', 'signed_date', 'planned_completion_date', 'actual_completion_date'])
completed_contracts.head()
128/34:
completed = awarded_contracts['completed_contracts_value'] > 0
awarded_contracts[completed]
awarded_contracts[completed].to_csv('completed_contracts.csv')
128/35:
completed_contracts = pd.read_csv('completed_contracts.csv', index=None, parse_dates=['initiation_date', 'signed_date', 'planned_completion_date', 'actual_completion_date'])
completed_contracts.head()
128/36:
completed_contracts = pd.read_csv('completed_contracts.csv', parse_dates=['initiation_date', 'signed_date', 'planned_completion_date', 'actual_completion_date'])
completed_contracts.head()
128/37:
completed = awarded_contracts['completed_contracts_value'] > 0
awarded_contracts[completed]
awarded_contracts[completed].to_csv('completed_contracts.csv', index=False)
128/38:
completed_contracts = pd.read_csv('completed_contracts.csv', parse_dates=['initiation_date', 'signed_date', 'planned_completion_date', 'actual_completion_date'])
completed_contracts.head()
128/39: completed_contracts[completed_contracts['actual_completion_date'] <= completed_contracts['planned_completion_date']]
128/40: completed_contracts[completed_contracts['actual_completion_date'] < completed_contracts['planned_completion_date']]
128/41: completed_contracts[completed_contracts['actual_completion_date']]
128/42:
completed_contracts = pd.read_csv('completed_contracts.csv', parse_dates=['initiation_date', 'signed_date', 'planned_completion_date', 'actual_completion_date'])
completed_contracts.head()
133/1: import pandas as pd
133/2:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
133/3: awarded_contracts.groupby('method')
133/4: awarded_contracts[awarded_contracts.groupby('method')]
133/5: awarded_contracts.groupby('method')
133/6:
df = awarded_contracts.groupby('method')
df
133/7:
df = awarded_contracts.groupby('method')
df()
133/8: awarded_contracts.groupby('method')
133/9: awarded_contracts.groupby('method').head()
133/10: awarded_contracts.groupby('type')
133/11: awarded_contracts.groupby('type').head()
133/12: awarded_contracts.groupby('type')
133/13: awarded_contracts.info()
133/14: awarded_contracts.tail()
133/15:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
133/16: awarded_contracts.info()
133/17:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
133/18:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
133/19:
awarded_contracts = pd.read_csv('awarded_contract.csv')
awarded_contracts.head()
133/20: import pandas as pd
133/21:
awarded_contracts = pd.read_csv('awarded_contract.csv')
awarded_contracts.head()
133/22:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
133/23: awarded_contracts.groupby('type')
133/24: import pandas as pd
133/25:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
133/26: awarded_contracts.info()
133/27: awarded_contracts.groupby('type')
133/28: awarded_contracts.tail()
133/29: awarded_contracts.groupby('type').groups
133/30: print awarded_contracts.groupby('type').groups
133/31: print awarded_contracts.groupby('type')
133/32: awarded_contracts.groupby('type')
133/33: awarded_contracts.groupby('type').groups
133/34:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.get_groups('Consultancy Services')
133/35: contracts_by_type = awarded_contracts.groupby('type')
133/36:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
133/37: awarded_contracts.filter(['Micro Procurement'])
133/38: awarded_contracts.filter(['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/39: awarded_contracts.filter(method = ['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/40: awarded_contracts.filter(method=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/41: awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/42: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/43: open_methodhods.head()
133/44: open_methodhod.head()
133/45: open_method.head()
133/46: open_method.info()
133/47:
awarded_contracts = pd.read_excel('awarded_contracts.xls')
awarded_contracts.head()
133/48: import pandas as pd
133/49:
awarded_contracts = pd.read_excel('awarded_contracts.xls')
awarded_contracts.head()
133/50: import pandas as pd
133/51:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/52: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
133/53: animals
133/54: animals.loc[3]
133/55: animals.loc[9]
133/56: awarded_contracts
133/57: awarded_contracts.loc[3]
133/58: awarded_contracts
133/59: awarded_contracts.loc[3]
133/60: awarded_contracts.iloc[3]
133/61: awarded_contracts.iloc[3]
133/62: awarded_contracts.loc[3]
133/63: awarded_contracts.iloc[:3]
133/64: awarded_contracts.loc[:3]
133/65: awarded_contracts["subject_of_procurement"]
133/66: awarded_contracts.subject_of_procurement
133/67: awarded_contracts[awarded_contracts['type'] == 'Supplies']
133/68: awarded_contracts[awarded_contract["plan_method"] != awarded_contract["method"]]
133/69: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
133/70: pd.crosstab(awarded_contracts["plan_method"], awarded_contracts["method"])
133/71: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
133/72: awarded_contracts.groupby(["type"]).mean()
133/73: awarded_contracts.groupby(["type"]).count()
133/74: awarded_contracts.groupby(["type"]).mean()
133/75: awarded_contracts.groupby(["method"])
133/76: awarded_contracts.groupby(["method"]).mean()
133/77: awarded_contracts["contract_price"].plot()
133/78:
import pandas as pd
import matplotlib
%matplotlib inline
import matplotlib.pyplot as plt
133/79:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/80:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/81:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/82: awarded_contracts["contract_price"].plot()
133/83: awarded_contracts["contract_price"].hist()
133/84: awarded_contracts["award_price"].hist()
133/85: awarded_contracts["award_price"].hist()
133/86: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_price"]
133/87: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
133/88: awarded_contracts
133/89: awarded_contracts["difference"].plot(
133/90: awarded_contracts["difference"].plot()
133/91: awarded_contracts["difference"].hist()
133/92: awarded_contracts.sample(frac=.5)
133/93: awarded_contracts.sample(frac=1)
133/94: awarded_contract.dtypes
133/95: awarded_contract.dtypes()
133/96: awarded_contracts.dtypes()
133/97: awarded_contracts.dtypes
135/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/2:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/3:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/4:
# Check the types of the dataset
timelines.dtypes()
135/5:
# Check the types of the dataset
timelines.dtypes
135/6: timelines.tail()
135/7:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/8:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/9:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/10:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/11:
# Check the types of the dataset
timelines.dtypes
135/12:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/13: timelines.tail()
135/14: timelines2020 = timelines[]
135/15: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/16: timelines2020.tail()
135/17: timelines2020.head()
135/18: timelines2020.head()
135/19: timelines2020
135/20: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/21: timelines2020
135/22: timelines2020.count()
135/23: timelines2020.info()
135/24: timelines.tail()
135/25: timelines.head()
135/26:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/27:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/28:
# Check the types of the dataset
timelines.dtypes
135/29:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/30: timelines.head()
135/31: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/32: timelines2020.info()
135/33: timelines2020
135/34:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

timelines2020.to_csv('timelines-FY-2019-2020.xlsx')
135/35: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/36: timelines2020.head()
135/37: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/38: timelines2020.head()
135/39: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/40: timelines2020
135/41: timelines2020.sample(frac=.7)
135/42: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/43: timelines2020
135/44: timelines2020.info()
135/45: timelines2020.to_datetime(timelines2020['initiation_date'], format='%Y%m%d', errors='coerce')
135/46: pd.to_datetime(timelines2020['initiation_date'], format='%Y%m%d', errors='coerce')
135/47: pd.to_datetime(timelines2020['initiation_date'])
135/48: timelines2020
135/49: timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='corece')
135/50: timelines2020.info()
135/51: timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='corece')
135/52: timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='corece')
135/53: timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')
135/54: timelines2020
135/55: timelines2020.info
135/56: timelines2020.info()
135/57:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/58: timelines2020
135/59: timelines2020.sample(frac=.4)
135/60: timelines2020
135/61: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/62: timelines2020
135/63: timelines2020.tail()
135/64: timelines2020.sample(frac=.5)
135/65: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/66: timelines2020.sample(frac=.5)
135/67: timelines2020.groupBy('method').mean()
135/68: timelines2020.groupby('method').mean()
135/69: timelines2020.groupby(['method']).mean()
135/70: timelines2020.groupBy(['method']).mean()
135/71: timelines2020.groupby(['method']).mean()
135/72: timelines2020.info()
135/73: timelines2020.groupby(['method']).mean('planning_period')
135/74: timelines2020.groupby(['method']).mean()
135/75: timelines2020ByMethod = pd.to_timedelta(timelines2020['planning_period'].groupby(['method']).mean()
135/76:
timelines2020ByMethod = pd.to_timedelta(timelines2020['planning_period'].groupby(['method'])

timelines2020ByMethod.mean()
135/77:
timelines2020ByMethod = pd.to_timedelta(timelines2020['planning_period'].groupby(['method'])

timelines2020ByMethod.mean()
135/78:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period'].groupby(['method'])

timelines2020ByMethod.mean()
135/79:
timelines2020ByMethod = pd.to_timedelta(timelines2020['planning_period']).groupby(['method'])

timelines2020ByMethod.mean()
135/80:
timelines2020ByMethod = pd.to_timedelta(timelines2020['planning_period']).groupby(timelines2020['method'])

timelines2020ByMethod.mean()
135/81:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method'])

timelines2020ByMethod.mean()
135/82:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()

# timelines2020ByMethod.mean()
135/83:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()

timelines2020ByMethod
135/84:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/85: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/86: timelines2020.sample(frac=.5)
135/87: pd.to_numeric(timelines2020['planning_period'])
135/88: pd.to_int(timelines2020['planning_period'])
135/89: pd.to_timedelta(timelines2020['planning_period'])
135/90: pd.total_seconds(timelines2020['planning_period'])
135/91: pd.to_timedelta(timelines2020['planning_period'])
135/92: timelines2020['planning_period'] = pd.to_timedelta(timelines2020['planning_period'])
135/93: timelines2020
135/94: timelines2020.sample(frac=.5)
135/95: timelines2020.sample(frac=.5)
135/96:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/97:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/98:
# Check the types of the dataset
timelines.dtypes
135/99:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/100: timelines.head()
135/101: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/102: timelines2020.info()
135/103: timelines2020
135/104:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

timelines2020.to_csv('timelines-FY-2019-2020.xlsx')
135/105: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/106: timelines2020.head()
135/107: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/108: timelines2020
135/109: timelines2020.sample(frac=.7)
135/110: timelines2020.info()
135/111:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/112: timelines2020
135/113: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/114: timelines2020.sample(frac=.5)
135/115: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/116: timelines2020.sample(frac=.5)
135/117: timelines2020.sample(frac=.5)
135/118:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/119: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/120: timelines2020.info()
135/121:
timelines2020.to_csv('timeliness-in-days.csv')

timelines2020.to_excel('timeliness-in-days.xlsx')
135/122:
timelines2020.to_csv('timeliness-in-days.csv')

timelines2020.to_excell('timeliness-in-days.xlsx')
135/123:
timelines2020.to_csv('timeliness-in-days.csv')

timelines2020.to_excel('timeliness-in-days.xlsx')
135/124:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/125:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/126:
# Check the types of the dataset
timelines.dtypes
135/127:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/128: timelines.head()
135/129: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/130: timelines2020.info()
135/131: timelines2020
135/132:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

timelines2020.to_xlsx('timelines-FY-2019-2020.xlsx')
135/133:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/134: timelines2020.info()
135/135:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/136:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/137:
# Check the types of the dataset
timelines.dtypes
135/138:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/139: timelines.head()
135/140: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/141: timelines2020.info()
135/142: timelines2020
135/143:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/144: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/145: timelines2020.head()
135/146: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/147: timelines2020
135/148: timelines2020.sample(frac=.7)
135/149: timelines2020.info()
135/150:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/151: timelines2020
135/152: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/153: timelines2020.sample(frac=.5)
135/154: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/155: timelines2020.sample(frac=.5)
135/156: timelines2020.sample(frac=.5)
135/157:
timelines2020.to_csv('timeliness-in-days.csv')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/158:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/159: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/160: timelines2020.info()
135/161: timelines2020.sample(frac=.5)
135/162: timelines2020.to_csv('timeliness-in-days.csv', index=false)
135/163: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/164:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/165: timelines2020.sample(frac=.5)
135/166: timelines2020.info()
135/167:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/168:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/169:
# Check the types of the dataset
timelines.dtypes
135/170:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/171: timelines.head()
135/172: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/173: timelines2020.info()
135/174: timelines2020
135/175:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/176: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/177: timelines2020.head()
135/178: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/179: timelines2020
135/180: timelines2020.sample(frac=.7)
135/181: timelines2020.info()
135/182:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/183: timelines2020
135/184: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/185: timelines2020.sample(frac=.5)
135/186: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/187: timelines2020.sample(frac=.5)
135/188: timelines2020.sample(frac=.5)
135/189: timelines2020.info()
135/190:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/191: timelines2020.sample(frac=.5)
135/192: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/193:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/194: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/195: timelines2020.info()
133/98:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/99:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/100:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
133/101: awarded_contracts.info()
133/102: awarded_contracts.tail()
133/103: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/104: open_method.info()
133/105: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
133/106: animals
133/107: animals.loc[3]
133/108: animals.loc[9]
133/109: awarded_contracts
133/110: awarded_contracts.loc[3]
133/111: awarded_contracts.iloc[3]
133/112: awarded_contracts.iloc[:3]
133/113: awarded_contracts.loc[:3]
133/114: awarded_contracts["subject_of_procurement"]
133/115: awarded_contracts.subject_of_procurement
133/116: awarded_contracts[awarded_contracts['type'] == 'Supplies']
133/117: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
133/118: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
133/119: awarded_contracts.groupby(["type"]).mean()
133/120: awarded_contracts.groupby(["method"]).mean()
133/121: awarded_contracts["contract_price"].plot()
133/122: awarded_contracts["award_price"].hist()
133/123: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
133/124: awarded_contracts
133/125: awarded_contracts["difference"].hist()
133/126: awarded_contracts.sample(frac=1)
133/127: awarded_contracts.dtypes
133/128:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed')
133/129:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/130:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/131:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
133/132: awarded_contracts.info()
133/133: awarded_contracts.tail()
133/134: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/135: open_method.info()
133/136: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
133/137: animals
133/138: animals.loc[3]
133/139: animals.loc[9]
133/140: awarded_contracts
133/141: awarded_contracts.loc[3]
133/142: awarded_contracts.iloc[3]
133/143: awarded_contracts.iloc[:3]
133/144: awarded_contracts.loc[:3]
133/145: awarded_contracts["subject_of_procurement"]
133/146: awarded_contracts.subject_of_procurement
133/147: awarded_contracts[awarded_contracts['type'] == 'Supplies']
133/148: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
133/149: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
133/150:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed')
133/151: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
133/152: awarded_contracts.groupby(["type"]).mean()
133/153: awarded_contracts.groupby(["method"]).mean()
133/154: awarded_contracts["contract_price"].plot()
133/155: awarded_contracts["award_price"].hist()
133/156: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
133/157: awarded_contracts
133/158: awarded_contracts["difference"].hist()
133/159: awarded_contracts.sample(frac=1)
133/160: awarded_contracts.dtypes
133/161: awarded_contracts_method_changed.sample(frac=.5)
133/162:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/163:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/164:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
133/165: awarded_contracts.info()
133/166: awarded_contracts.tail()
133/167: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/168: open_method.info()
133/169: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
133/170: animals
133/171: animals.loc[3]
133/172: animals.loc[9]
133/173: awarded_contracts
133/174: awarded_contracts.loc[3]
133/175: awarded_contracts.iloc[3]
133/176: awarded_contracts.iloc[:3]
133/177: awarded_contracts.loc[:3]
133/178: awarded_contracts["subject_of_procurement"]
133/179: awarded_contracts.subject_of_procurement
133/180: awarded_contracts[awarded_contracts['type'] == 'Supplies']
133/181: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
133/182: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
133/183:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed.csv')
133/184: awarded_contracts_method_changed.sample(frac=.5)
133/185: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
133/186: awarded_contracts.groupby(["type"]).mean()
133/187: awarded_contracts.groupby(["method"]).mean()
133/188: awarded_contracts["contract_price"].plot()
133/189: awarded_contracts["award_price"].hist()
133/190: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
133/191: awarded_contracts
133/192: awarded_contracts["difference"].hist()
133/193: awarded_contracts.sample(frac=1)
133/194: awarded_contracts.dtypes
135/196:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/197:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/198:
# Check the types of the dataset
timelines.dtypes
135/199:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/200: timelines.head()
135/201: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/202: timelines2020.info()
135/203: timelines2020
135/204:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/205: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/206: timelines2020.head()
135/207: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/208: timelines2020
135/209: timelines2020.sample(frac=.7)
135/210: timelines2020.info()
135/211:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/212: timelines2020
135/213: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/214: timelines2020.sample(frac=.5)
135/215: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/216: timelines2020.sample(frac=.5)
135/217: timelines2020.sample(frac=.5)
135/218: timelines2020.info()
135/219:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/220: timelines2020.sample(frac=.5)
135/221: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/222:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/223: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/224: timelines2020.info()
135/225: timelinesByType2020.groupBy(['method']).mean()
135/226: timelines2020.groupBy(['method']).mean()
135/227: timelines2020.groupby(['method']).mean()
135/228: timelines2020.groupby(['type']).mean()
135/229:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/230:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/231:
# Check the types of the dataset
timelines.dtypes
135/232:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/233: timelines.head()
135/234: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/235: timelines2020.info()
135/236: timelines2020
135/237:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/238: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/239: timelines2020.head()
135/240: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/241: timelines2020
135/242: timelines2020.sample(frac=.7)
135/243: timelines2020.info()
135/244:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/245: timelines2020
135/246: timelines2020['planning_period'] = pd.to_datetime(timelines2020['contract_signature_date'] - timelines2020['initiation_date'])
135/247: timelines2020.info()
135/248: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/249:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/250:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/251:
# Check the types of the dataset
timelines.dtypes
135/252:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/253: timelines.head()
135/254: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/255: timelines2020.info()
135/256: timelines2020
135/257:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/258: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/259: timelines2020.head()
135/260: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/261: timelines2020
135/262: timelines2020.sample(frac=.7)
135/263: timelines2020.info()
135/264:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/265: timelines2020
135/266: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/267: timelines2020.sample(frac=.5)
135/268: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/269: timelines2020.sample(frac=.5)
135/270: timelines2020.sample(frac=.5)
135/271: timelines2020.info()
135/272:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/273: timelines2020.sample(frac=.5)
135/274: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/275:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/276: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/277: timelines2020.info()
135/278: timelines2020.groupby(['method']).mean()
135/279: timelines2020.groupby(['type']).mean()
135/280: timelines2020['planning_period'] = timelines2020['planning_period'].dt.days.astype(integer)
135/281: timelines2020['planning_period'] = timelines2020['planning_period'].dt.days.astype('int16')
135/282: timelines2020['planning_period'] = timelines2020['planning_period'].dt.days.astype('int16')
135/283: timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')
135/284: timelines2020.info()
135/285: timelines2020.sample(frac=.5)
135/286: timelines2020.sample(frac=.5)
135/287:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/288:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

# timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/289:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

# timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/290:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/291:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/292:
# Check the types of the dataset
timelines.dtypes
135/293:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/294: timelines.head()
135/295: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/296: timelines2020.info()
135/297: timelines2020
135/298:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/299: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/300: timelines2020.head()
135/301: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/302: timelines2020
135/303: timelines2020.sample(frac=.7)
135/304: timelines2020.info()
135/305:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/306: timelines2020
135/307: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/308: timelines2020.sample(frac=.5)
135/309: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/310: timelines2020.sample(frac=.5)
135/311: timelines2020.sample(frac=.5)
135/312:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/313: timelines2020.info()
135/314:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/315: timelines2020.sample(frac=.5)
135/316: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/317:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/318: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/319: timelines2020.info()
135/320: timelines2020.groupby(['method']).mean()
135/321: timelines2020.groupby(['type']).mean()
133/195: awarded_contracts.head()
133/196: awarded_contracts.head()
133/197: awarded_contracts.head()
133/198: awarded_contracts.head()
133/199:
awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.xlxs')
133/200:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/201:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/202:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
133/203: awarded_contracts.info()
133/204: awarded_contracts.tail()
133/205: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/206: open_method.info()
133/207: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
133/208: animals
133/209: animals.loc[3]
133/210: animals.loc[9]
133/211: awarded_contracts
133/212: awarded_contracts.loc[3]
133/213: awarded_contracts.iloc[3]
133/214: awarded_contracts.iloc[:3]
133/215: awarded_contracts.loc[:3]
133/216: awarded_contracts["subject_of_procurement"]
133/217: awarded_contracts.subject_of_procurement
133/218: awarded_contracts[awarded_contracts['type'] == 'Supplies']
133/219: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
133/220: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
133/221: awarded_contracts.head()
133/222:
awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.xlxs')
133/223:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed.csv')
133/224: awarded_contracts_method_changed.sample(frac=.5)
133/225: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
133/226: awarded_contracts.groupby(["type"]).mean()
133/227: awarded_contracts.groupby(["method"]).mean()
133/228: awarded_contracts["contract_price"].plot()
133/229: awarded_contracts["award_price"].hist()
133/230: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
133/231: awarded_contracts
133/232: awarded_contracts["difference"].hist()
133/233: awarded_contracts.sample(frac=1)
133/234: awarded_contracts.dtypes
133/235:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/236:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/237:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
133/238: awarded_contracts.info()
133/239: awarded_contracts.tail()
133/240: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/241: open_method.info()
133/242: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
133/243: animals
133/244: animals.loc[3]
133/245: animals.loc[9]
133/246: awarded_contracts
133/247: awarded_contracts.loc[3]
133/248: awarded_contracts.iloc[3]
133/249: awarded_contracts.iloc[:3]
133/250: awarded_contracts.loc[:3]
133/251: awarded_contracts["subject_of_procurement"]
133/252: awarded_contracts.subject_of_procurement
133/253: awarded_contracts[awarded_contracts['type'] == 'Supplies']
133/254: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
133/255: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
133/256: awarded_contracts.head()
133/257:
awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.xlxs')
133/258:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed.csv')
133/259: awarded_contracts_method_changed.sample(frac=.5)
133/260: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
133/261: awarded_contracts.groupby(["type"]).mean()
133/262: awarded_contracts.groupby(["method"]).mean()
133/263: awarded_contracts["contract_price"].plot()
133/264: awarded_contracts["award_price"].hist()
133/265: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
133/266: awarded_contracts
133/267: awarded_contracts["difference"].hist()
133/268: awarded_contracts.sample(frac=1)
133/269: awarded_contracts.dtypes
133/270:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/271:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/272:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
133/273: awarded_contracts.info()
133/274: awarded_contracts.tail()
133/275: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/276: open_method.info()
133/277: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
133/278: animals
133/279: animals.loc[3]
133/280: animals.loc[9]
133/281: awarded_contracts
133/282: awarded_contracts.loc[3]
133/283: awarded_contracts.iloc[3]
133/284: awarded_contracts.iloc[:3]
133/285: awarded_contracts.loc[:3]
133/286: awarded_contracts["subject_of_procurement"]
133/287: awarded_contracts.subject_of_procurement
133/288: awarded_contracts[awarded_contracts['type'] == 'Supplies']
133/289: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
133/290: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
133/291: awarded_contracts.head()
133/292:
awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

awarded_contracts.to_excel('all_awarded_contracts_FY_2019-2020.xlxs')
133/293:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
133/294:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
133/295:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
133/296: awarded_contracts.info()
133/297: awarded_contracts.tail()
133/298: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
133/299: open_method.info()
133/300: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
133/301: animals
133/302: animals.loc[3]
133/303: animals.loc[9]
133/304: awarded_contracts
133/305: awarded_contracts.loc[3]
133/306: awarded_contracts.iloc[3]
133/307: awarded_contracts.iloc[:3]
133/308: awarded_contracts.loc[:3]
133/309: awarded_contracts["subject_of_procurement"]
133/310: awarded_contracts.subject_of_procurement
133/311: awarded_contracts[awarded_contracts['type'] == 'Supplies']
133/312: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
133/313: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
133/314: awarded_contracts.head()
133/315:
awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

# awarded_contracts.to_excel('all_awarded_contracts_FY_2019-2020.xlxs')
133/316:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed.csv')
133/317: awarded_contracts_method_changed.sample(frac=.5)
133/318: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
133/319: awarded_contracts.groupby(["type"]).mean()
133/320: awarded_contracts.groupby(["method"]).mean()
133/321: awarded_contracts["contract_price"].plot()
133/322: awarded_contracts["award_price"].hist()
133/323: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
133/324: awarded_contracts
133/325: awarded_contracts["difference"].hist()
133/326: awarded_contracts.sample(frac=1)
133/327: awarded_contracts.dtypes
135/322: timelines2020.dtypes
135/323:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/324:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/325:
# Check the types of the dataset
timelines.dtypes
135/326:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/327: timelines.head()
135/328: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/329: timelines2020.info()
135/330: timelines2020
135/331:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/332: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/333: timelines2020.head()
135/334: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/335: timelines2020
135/336: timelines2020.sample(frac=.7)
135/337: timelines2020.info()
135/338:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/339: timelines2020
135/340: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/341: timelines2020.sample(frac=.5)
135/342: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/343: timelines2020.sample(frac=.5)
135/344: timelines2020.sample(frac=.5)
135/345: timelines2020.dtypes
135/346:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/347: timelines2020.info()
135/348:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/349: timelines2020.sample(frac=.5)
135/350: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/351:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/352: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/353: timelines2020.info()
135/354: timelines2020.groupby(['method']).mean()
135/355: timelines2020.groupby(['type']).mean()
135/356:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/357:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/358:
# Check the types of the dataset
timelines.dtypes
135/359:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/360: timelines.head()
135/361: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/362: timelines2020.info()
135/363: timelines2020
135/364:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/365: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/366: timelines2020.head()
135/367: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/368: timelines2020
135/369: timelines2020.sample(frac=.7)
135/370: timelines2020.info()
135/371:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/372: timelines2020
135/373: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/374: timelines2020.sample(frac=.5)
135/375: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/376: timelines2020.sample(frac=.5)
135/377: timelines2020.sample(frac=.5)
135/378: timelines2020.dtypes
135/379:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/380: timelines2020.info()
135/381:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/382: timelines2020.sample(frac=.5)
135/383: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/384:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/385: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/386: timelines2020.info()
135/387: timelines2020.groupby(['method']).mean()
135/388: timelines2020.groupby(['type']).mean()
135/389:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/390:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/391:
# Check the types of the dataset
timelines.dtypes
135/392:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/393: timelines.head()
135/394: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/395: timelines2020.info()
135/396: timelines2020
135/397:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/398: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/399: timelines2020.head()
135/400: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/401: timelines2020
135/402: timelines2020.sample(frac=.7)
135/403: timelines2020.info()
135/404:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/405: timelines2020
135/406: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/407: timelines2020.sample(frac=.5)
135/408: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/409: timelines2020.sample(frac=.5)
135/410: timelines2020.sample(frac=.5)
135/411: timelines2020.dtypes
135/412:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/413: timelines2020.info()
135/414:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/415: timelines2020.sample(frac=.5)
135/416: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/417:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/418: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/419: timelines2020.info()
135/420: timelines2020.groupby(['method']).mean()
135/421: timelines2020.groupby(['type']).mean()
135/422:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/423:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/424:
# Check the types of the dataset
timelines.dtypes
135/425:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/426: timelines.head()
135/427: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/428: timelines2020.info()
135/429: timelines2020
135/430:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/431: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/432: timelines2020.head()
135/433: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/434: timelines2020
135/435: timelines2020.sample(frac=.7)
135/436: timelines2020.info()
135/437:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/438: timelines2020
135/439: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/440: timelines2020.sample(frac=.5)
135/441: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/442: timelines2020.sample(frac=.5)
135/443: timelines2020.sample(frac=.5)
135/444: timelines2020.dtypes
135/445:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/446: timelines2020.info()
135/447:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/448: timelines2020.sample(frac=.5)
135/449: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/450:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/451: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/452: timelines2020.info()
135/453: timelines2020.groupby(['method']).mean()
135/454: timelines2020.groupby(['type']).mean()
135/455:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
135/456:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
135/457:
# Check the types of the dataset
timelines.dtypes
135/458:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
135/459: timelines.head()
135/460: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
135/461: timelines2020.info()
135/462: timelines2020
135/463:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
135/464: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
135/465: timelines2020.head()
135/466: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
135/467: timelines2020
135/468: timelines2020.sample(frac=.7)
135/469: timelines2020.info()
135/470:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
135/471: timelines2020
135/472: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
135/473: timelines2020.sample(frac=.5)
135/474: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
135/475: timelines2020.sample(frac=.5)
135/476: timelines2020.sample(frac=.5)
135/477: timelines2020.dtypes
135/478:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
135/479: timelines2020.info()
135/480:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
135/481: timelines2020.sample(frac=.5)
135/482: timelines2020.to_csv('timeliness-in-days.csv', index=False)
135/483:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
135/484: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
135/485: timelines2020.info()
135/486: timelines2020.groupby(['method']).mean()
135/487: timelines2020.groupby(['type']).mean()
137/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/2:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/3:
# Check the types of the dataset
timelines.dtypes
137/4:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/5: timelines.head()
137/6: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/7: timelines2020.info()
137/8: timelines2020
137/9:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/10: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/11: timelines2020.head()
137/12: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/13: timelines2020
137/14: timelines2020.sample(frac=.7)
137/15: timelines2020.info()
137/16:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/17: timelines2020
137/18: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/19: timelines2020.sample(frac=.5)
137/20: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/21: timelines2020.sample(frac=.5)
137/22: timelines2020.sample(frac=.5)
137/23: timelines2020.dtypes
137/24:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/25: timelines2020.info()
137/26:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/27: timelines2020.sample(frac=.5)
137/28: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/29:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/30: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/31: timelines2020.info()
137/32: timelines2020.groupby(['method']).mean()
137/33: timelines2020.groupby(['type']).mean()
137/34: # Import the file
137/35:
leadtime = pd.read_csv('leadtime.csv')
leadtime.head()
137/36:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/37:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/38:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/39:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/40: leadtime.head()
137/41: leadtime.sample(frac=.3)
137/42: leadtime.sample(frac=.5)
137/43:
leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']

leadtimeOpen.head()
137/44:
# Save to csv
leadtime.to_csv('leadtimeOpen.csv')
137/45:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/46:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/47:
# Check the types of the dataset
timelines.dtypes
137/48:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/49: timelines.head()
137/50: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/51: timelines2020.info()
137/52: timelines2020
137/53:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/54: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/55: timelines2020.head()
137/56: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/57: timelines2020
137/58: timelines2020.sample(frac=.7)
137/59: timelines2020.info()
137/60:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/61: timelines2020
137/62: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/63: timelines2020.sample(frac=.5)
137/64: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/65: timelines2020.sample(frac=.5)
137/66: timelines2020.sample(frac=.5)
137/67: timelines2020.dtypes
137/68:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/69: timelines2020.info()
137/70:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/71: timelines2020.sample(frac=.5)
137/72: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/73:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/74: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/75: timelines2020.info()
137/76: timelines2020.groupby(['method']).mean()
137/77: timelines2020.groupby(['type']).mean()
137/78: # Import the file
137/79:
leadtime = pd.read_csv('leadtime.csv')
leadtime.head()
137/80:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/81:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/82:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/83:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/84: leadtime.head()
137/85: leadtime.sample(frac=.5)
137/86:
leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']

leadtimeOpen.head()
137/87:
# Save to csv
leadtime.to_csv('leadtimeOpen.csv')
137/88:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/89:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/90:
# Check the types of the dataset
timelines.dtypes
137/91:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/92: timelines.head()
137/93: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/94: timelines2020.info()
137/95: timelines2020
137/96:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/97: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/98: timelines2020.head()
137/99: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/100: timelines2020
137/101: timelines2020.sample(frac=.7)
137/102: timelines2020.info()
137/103:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/104: timelines2020
137/105: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/106: timelines2020.sample(frac=.5)
137/107: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/108: timelines2020.sample(frac=.5)
137/109: timelines2020.sample(frac=.5)
137/110: timelines2020.dtypes
137/111:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/112: timelines2020.info()
137/113:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/114: timelines2020.sample(frac=.5)
137/115: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/116:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/117: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/118: timelines2020.info()
137/119: timelines2020.groupby(['method']).mean()
137/120: timelines2020.groupby(['type']).mean()
137/121: # Import the file
137/122:
leadtime = pd.read_csv('leadtime.csv')
leadtime.head()
137/123:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/124:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/125:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/126:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/127: leadtime.head()
137/128: leadtime.sample(frac=.5)
137/129:
leadtimeOpen = leadtime[leadtime['method'] != 'Open International Bidding(OIB)']

leadtimeOpen.head()
137/130:
# Save to csv
leadtime.to_csv('leadtimeOpen.csv')
137/131:
leadtimeOpen = leadtime[leadtime['method'] != 'Open International Bidding(OIB)']

leadtimeOpen.head()
137/132: leadtimeOpen.info()
137/133: leadtimeOpen = leadtime[leadtime['method'] != 'Open International Bidding(OIB)']
137/134: leadtimeOpen.head()
137/135: leadtime[leadtime['method'] != 'Open International Bidding(OIB)']
137/136: leadtime[leadtime['method'] != 'Open International Bidding(OIB)', inplace=True]
137/137: leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
137/138: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
137/139: leadtimeOpen.head()
137/140:
# Save to csv
leadtime.to_csv('leadtimeOpen.csv')
137/141:
# Save to csv
leadtime.to_csv('leadtimeOpen.csv', index=None)
137/142: leadtimeOpen.head()
137/143:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
137/144:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/145:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/146:
# Check the types of the dataset
timelines.dtypes
137/147:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/148: timelines.head()
137/149: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/150: timelines2020.info()
137/151: timelines2020
137/152:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/153: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/154: timelines2020.head()
137/155: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/156: timelines2020
137/157: timelines2020.sample(frac=.7)
137/158: timelines2020.info()
137/159:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/160: timelines2020
137/161: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/162: timelines2020.sample(frac=.5)
137/163: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/164: timelines2020.sample(frac=.5)
137/165: timelines2020.sample(frac=.5)
137/166: timelines2020.dtypes
137/167:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/168: timelines2020.info()
137/169:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/170: timelines2020.sample(frac=.5)
137/171: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/172:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/173: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/174: timelines2020.info()
137/175: timelines2020.groupby(['method']).mean()
137/176: timelines2020.groupby(['type']).mean()
137/177: # Import the file
137/178:
leadtime = pd.read_csv('leadtime.csv')
leadtime.head()
137/179:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/180:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/181:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/182:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/183: leadtime.head()
137/184: leadtime.sample(frac=.5)
137/185: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
137/186: leadtimeOpen.info()
137/187: leadtimeOpen.head()
137/188:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
137/189: leadtimeOpen
137/190: leadtime.info
137/191: leadtime.info()
137/192:
leadtime = pd.read_csv('leadtime.csv')
leadtime.head()
137/193: leadtime.info()
137/194:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/195:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/196:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/197:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/198: leadtime.head()
137/199: leadtime.sample(frac=.5)
137/200: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
137/201: leadtimeOpen.info()
137/202: leadtimeOpen
137/203:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
137/204: leadtime.sort_values(by=['contract_price'])
137/205:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/206:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/207:
# Check the types of the dataset
timelines.dtypes
137/208:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/209: timelines.head()
137/210: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/211: timelines2020.info()
137/212: timelines2020
137/213:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/214: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/215: timelines2020.head()
137/216: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/217: timelines2020
137/218: timelines2020.sample(frac=.7)
137/219: timelines2020.info()
137/220:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/221: timelines2020
137/222: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/223: timelines2020.sample(frac=.5)
137/224: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/225: timelines2020.sample(frac=.5)
137/226: timelines2020.sample(frac=.5)
137/227: timelines2020.dtypes
137/228:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/229: timelines2020.info()
137/230:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/231: timelines2020.sample(frac=.5)
137/232: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/233:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/234: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/235: timelines2020.info()
137/236: timelines2020.groupby(['method']).mean()
137/237: timelines2020.groupby(['type']).mean()
137/238: # Import the file
137/239:
leadtime = pd.read_csv('leadtime.csv')
leadtime.head()
137/240: leadtime.info()
137/241:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/242:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/243:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/244:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/245: leadtime.head()
137/246: leadtime.sample(frac=.5)
137/247: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
137/248: leadtimeOpen.info()
137/249: leadtimeOpen
137/250:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
137/251: leadtime.sort_values(by=['contract_price'])
137/252: sortedleadtime = leadtime.sort_values(by=['contract_price'])
137/253: sortedleadtime.to_csv('sortedleadtime.csv')
137/254:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/255:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/256:
# Check the types of the dataset
timelines.dtypes
137/257:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/258: timelines.head()
137/259: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/260: timelines2020.info()
137/261: timelines2020
137/262:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/263: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/264: timelines2020.head()
137/265: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/266: timelines2020
137/267: timelines2020.sample(frac=.7)
137/268: timelines2020.info()
137/269:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/270: timelines2020
137/271: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/272: timelines2020.sample(frac=.5)
137/273: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/274: timelines2020.sample(frac=.5)
137/275: timelines2020.sample(frac=.5)
137/276: timelines2020.dtypes
137/277:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/278: timelines2020.info()
137/279:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/280: timelines2020.sample(frac=.5)
137/281: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/282:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/283: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/284: timelines2020.info()
137/285: timelines2020.groupby(['method']).mean()
137/286: timelines2020.groupby(['type']).mean()
137/287: # Import the file
137/288:
leadtime = pd.read_csv('leadtime.csv')
leadtime.head()
137/289:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/290:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/291:
# Check the types of the dataset
timelines.dtypes
137/292:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/293: timelines.head()
137/294: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/295: timelines2020.info()
137/296: timelines2020
137/297:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/298: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/299: timelines2020.head()
137/300: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/301: timelines2020
137/302: timelines2020.sample(frac=.7)
137/303: timelines2020.info()
137/304:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/305: timelines2020
137/306: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/307: timelines2020.sample(frac=.5)
137/308: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/309: timelines2020.sample(frac=.5)
137/310: timelines2020.sample(frac=.5)
137/311: timelines2020.dtypes
137/312:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/313: timelines2020.info()
137/314:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/315: timelines2020.sample(frac=.5)
137/316: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/317:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/318: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/319: timelines2020.info()
137/320: timelines2020.groupby(['method']).mean()
137/321: timelines2020.groupby(['type']).mean()
137/322: # Import the file
137/323:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
137/324: leadtime.info()
137/325:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/326:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/327:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/328:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/329: leadtime.head()
137/330: leadtime.sample(frac=.5)
137/331: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
137/332: leadtimeOpen.info()
137/333: leadtimeOpen
137/334:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
137/335: sortedleadtime = leadtime.sort_values(by=['contract_price'])
137/336: sortedleadtime.to_csv('sortedleadtime.csv')
137/337:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/338:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/339:
# Check the types of the dataset
timelines.dtypes
137/340:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/341: timelines.head()
137/342: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/343: timelines2020.info()
137/344: timelines2020
137/345:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/346: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/347: timelines2020.head()
137/348: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/349: timelines2020
137/350: timelines2020.sample(frac=.7)
137/351: timelines2020.info()
137/352:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/353: timelines2020
137/354: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/355: timelines2020.sample(frac=.5)
137/356: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/357: timelines2020.sample(frac=.5)
137/358: timelines2020.sample(frac=.5)
137/359: timelines2020.dtypes
137/360:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/361: timelines2020.info()
137/362:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/363: timelines2020.sample(frac=.5)
137/364: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/365:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/366: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/367: timelines2020.info()
137/368: timelines2020.groupby(['method']).mean()
137/369: timelines2020.groupby(['type']).mean()
137/370: # Import the file
137/371:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
137/372: leadtime.info()
137/373:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/374:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/375:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/376:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/377: leadtime.head()
137/378: leadtime.sample(frac=.5)
137/379: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
137/380: leadtimeOpen.info()
137/381: leadtimeOpen
137/382:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
137/383: sortedleadtime = leadtime.sort_values(by=['contract_price'])
137/384: sortedleadtime.to_csv('sortedleadtime.csv')
137/385:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
137/386:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
137/387:
# Check the types of the dataset
timelines.dtypes
137/388:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
137/389: timelines.head()
137/390: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
137/391: timelines2020.info()
137/392: timelines2020
137/393:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
137/394: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
137/395: timelines2020.head()
137/396: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/397: timelines2020
137/398: timelines2020.sample(frac=.7)
137/399: timelines2020.info()
137/400:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
137/401: timelines2020
137/402: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
137/403: timelines2020.sample(frac=.5)
137/404: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
137/405: timelines2020.sample(frac=.5)
137/406: timelines2020.sample(frac=.5)
137/407: timelines2020.dtypes
137/408:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
137/409: timelines2020.info()
137/410:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
137/411: timelines2020.sample(frac=.5)
137/412: timelines2020.to_csv('timeliness-in-days.csv', index=False)
137/413:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
137/414: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
137/415: timelines2020.info()
137/416: timelines2020.groupby(['method']).mean()
137/417: timelines2020.groupby(['type']).mean()
137/418: # Import the file
137/419:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
137/420: leadtime.info()
137/421:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
137/422:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
137/423:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
137/424:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
137/425: leadtime.head()
137/426: leadtime.sample(frac=.5)
137/427: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
137/428: leadtimeOpen.info()
137/429: leadtimeOpen
137/430:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
137/431: sortedleadtime = leadtime.sort_values(by=['contract_price'])
137/432: sortedleadtime.to_csv('sortedleadtime.csv')
136/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
136/2:
awarded_contracts = pd.read_excel('awarded_contracts.xlsx')
awarded_contracts.head()
136/3:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
136/4:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
136/5:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
136/6:
awarded_contracts = pd.read_csv('awarded_contract.csv')
awarded_contracts.head()
136/7: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
136/8:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
136/9:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
136/10:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
136/11:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
136/12:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
136/13: awarded_contracts.info()
136/14: awarded_contracts.tail()
136/15: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
136/16: open_method.info()
136/17: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
136/18: animals
136/19: animals.loc[3]
136/20: animals.loc[9]
136/21: awarded_contracts
136/22: awarded_contracts.loc[3]
136/23: awarded_contracts.iloc[3]
136/24: awarded_contracts.iloc[:3]
136/25: awarded_contracts.loc[:3]
136/26: awarded_contracts["subject_of_procurement"]
136/27: awarded_contracts.subject_of_procurement
136/28: awarded_contracts[awarded_contracts['type'] == 'Supplies']
136/29: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
136/30: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
136/31: awarded_contracts.head()
136/32:
awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

# awarded_contracts.to_excel('all_awarded_contracts_FY_2019-2020.xlxs')
136/33:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed.csv')
136/34: awarded_contracts_method_changed.sample(frac=.5)
136/35: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
136/36: awarded_contracts.groupby(["type"]).mean()
136/37: awarded_contracts.groupby(["method"]).mean()
136/38: awarded_contracts["contract_price"].plot()
136/39: awarded_contracts["award_price"].hist()
136/40: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
136/41: awarded_contracts
136/42: awarded_contracts["difference"].hist()
136/43: awarded_contracts.sample(frac=1)
136/44: awarded_contracts.dtypes
138/1:
# Imports
import pandas as pd
import matplotlib as pyplot
import numpy as np
138/2:
# import the file
local_content = pd.read_csv('local-content.csv')
138/3:
# view
local_content.head()
138/4:
# Sample 50%
local_content.sample(frac=.5)
138/5: nephews = ['Heuy', 'Dewey', 'Louie']
138/6: newpews
138/7: newphews
138/8: nephews
138/9: nephews[0]
138/10: nephews[3]
138/11: nephews[1]
138/12:
for i in range(3):
    nephews[i] = nephews[i] + ' Duck'
138/13: print(nephews)
138/14: mix_it_up = [1, [2, 3], 'alpha']
138/15: mix_it_up
138/16: nephews.append('April Duck')
138/17: print(nephews)
138/18: nephews.extend(['Mary Duck', 'June Duck'])
138/19: print(nephews)
138/20:
ducks = nephews + ['Donald Duck', 'Daisy Duck']
print(ducks)
138/21: ducks.insert(0, 'Scrooge McDuck')
138/22:
ducks.insert(0, 'Scrooge McDuck')
print(ducks)
138/23:
# Imports
import pandas as pd
import matplotlib as pyplot
import numpy as np
138/24:
# import the file
local_content = pd.read_csv('local-content.csv')
138/25:
# view
local_content.head()
138/26:
# Sample 50%
local_content.sample(frac=.5)
138/27: nephews = ['Heuy', 'Dewey', 'Louie']
138/28: nephews
138/29: nephews[0]
138/30: nephews[1]
138/31:
for i in range(3):
    nephews[i] = nephews[i] + ' Duck'
138/32: print(nephews)
138/33: mix_it_up = [1, [2, 3], 'alpha']
138/34: mix_it_up
138/35: nephews.append('April Duck')
138/36: print(nephews)
138/37: nephews.extend(['Mary Duck', 'June Duck'])
138/38: print(nephews)
138/39:
ducks = nephews + ['Donald Duck', 'Daisy Duck']
print(ducks)
138/40:
ducks.insert(0, 'Scrooge McDuck')
print(ducks)
138/41:
del ducks[0]
print(ducks)
138/42:
ducks.remove('Donald Duck')
print(ducks)
138/43:
ducks.sort()
print(ducks)
138/44: squares = [0, 1, 4, 9, 16, 25, 36, 49]
138/45: squares[0:2]
138/46: squares[0:2]
138/47: squares[1:3]
138/48: squares[1:]
138/49: squares[:]
138/50: squares[-1]
138/51: squares[-1:]
138/52:
squares[2:4] = ['4', '9']
print(squares)
138/53: squares.del[-2:]
138/54: del squares[-2:]
138/55:
# Imports
import pandas as pd
import matplotlib as pyplot
import numpy as np
138/56:
# import the file
local_content = pd.read_csv('local-content.csv')
138/57:
# view
local_content.head()
138/58:
# Sample 50%
local_content.sample(frac=.5)
138/59: nephews = ['Heuy', 'Dewey', 'Louie']
138/60: nephews
138/61: nephews[0]
138/62: nephews[1]
138/63:
for i in range(3):
    nephews[i] = nephews[i] + ' Duck'
138/64: print(nephews)
138/65: mix_it_up = [1, [2, 3], 'alpha']
138/66: mix_it_up
138/67: nephews.append('April Duck')
138/68: print(nephews)
138/69: nephews.extend(['Mary Duck', 'June Duck'])
138/70: print(nephews)
138/71:
ducks = nephews + ['Donald Duck', 'Daisy Duck']
print(ducks)
138/72:
ducks.insert(0, 'Scrooge McDuck')
print(ducks)
138/73:
del ducks[0]
print(ducks)
138/74:
ducks.remove('Donald Duck')
print(ducks)
138/75:
ducks.sort()
print(ducks)
138/76: squares = [0, 1, 4, 9, 16, 25, 36, 49]
138/77: squares[0:2]
138/78: squares[0:2]
138/79: squares[1:3]
138/80: squares[1:]
138/81: squares[:]
138/82: squares[-1]
138/83:
squares[2:4] = ['4', '9']
print(squares)
138/84:
del squares[-2:]
print(squares)
138/85:
for value in squares:
    print("Element:", value)
138/86:
for index, value in enumerate(squares):
    print("Element", index, "=>", value)
139/1: capitals = {'United States': 'Washington, DC', 'France': 'Paris', 'Italy': 'Rome'}
139/2: capitals['Italy']
139/3:
capitals['Spain'] = 'Madrid'
capitals
139/4: capitals['Germany']
139/5: 'Germany' in capitals
139/6: 'Italy' in capitals
139/7: morecapitals = {'Germany': 'Berlin', 'United Kingdom': 'London'}
139/8:
capitals.update(morecapitals)
capitals
139/9:
del capitals['United States']
capitals
139/10:
for key in capitals:
    print(Key, capital[key])
139/11:
for key in capitals:
    print(key, capital[key])
139/12:
for key in capitals:
    print(key, capitals[key])
139/13:
for key in capitals.keys():
    print(key)
139/14:
for value in capitals.values():
    print(value)
139/15:
for key, value in capitals.items():
    print(value)
139/16:
for key, value in capitals.items():
    print(key, value)
139/17:
squares = []

for i in range(10):
    squares.append(i**2)

squares
139/18: squares = [i**2 for i in range[10]]
139/19: squares = [i**2 for i in range(10)]
139/20: squares
139/21:
squares3 = [i**2 for i in range(30) if i % 3 == 0]
squares3
139/22:
squares3_dict = {i: i**2 for i in range(30) if i % 3 == 0}
squares3_dict
139/23: capitals_bycapital = {capitals[key]: key for key. in capitals}
139/24: capitals_bycapital = {capitals[key]: key for key in capitals}
139/25: capitals_bycapital
139/26: sum([i**2 for i in range(10)])
139/27: sum(i**2 for i in range(10))
140/1: # Creating NumPy Arrays
140/2:
import numpy as np
import matplotlib.pyplot as pp
140/3:
a = np.array([1, 2, 3, 4, 5])
a
140/4: a.dtype
140/5:
a = np.array([1, 2, 3, 4, 5], dtype=np.float64)
a
140/6: a.ndim, a.shape, a.size
140/7: b = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
140/8: b
140/9: b.type
140/10: b.dtype
140/11: b.ndim, b.shape
140/12: b.ndim, b.shape, b.size
140/13: np.zeros((3,3),'d')
140/14: np.empty((4,4), 'd')
140/15: np.linespace(0,10,5)
140/16: np.linspace(0,10,5)
140/17: np.arrange(0,10,2)
140/18: np.arange(0,10,2)
140/19: np.random.standard_normal((2,4))
140/20:
a = np.random.standard_normal((2,3))
b = np.random.standard_normal((2,3))

np.vstack([a,b])
140/21: np.hstack([a,b])
140/22: a.transpose()
140/23: np.save('example.npy', a)
140/24: a1 = np.load('example.npy')
140/25: a1
140/26:
import numpy as np
import matplotlib.pyplot as pp
%matplotlib inline
140/27: # Creating NumPy Arrays
140/28:
import numpy as np
import matplotlib.pyplot as pp
%matplotlib inline
140/29:
a = np.array([1, 2, 3, 4, 5])
a
140/30: a.dtype
140/31:
a = np.array([1, 2, 3, 4, 5], dtype=np.float64)
a
140/32: a.ndim, a.shape, a.size
140/33: b = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
140/34: b
140/35: b.dtype
140/36: b.ndim, b.shape, b.size
140/37: np.zeros((3,3),'d')
140/38: np.empty((4,4), 'd')
140/39: np.linspace(0,10,5)
140/40: np.arange(0,10,2)
140/41: np.random.standard_normal((2,4))
140/42:
a = np.random.standard_normal((2,3))
b = np.random.standard_normal((2,3))

np.vstack([a,b])
140/43: np.hstack([a,b])
140/44: a.transpose()
140/45: np.save('example.npy', a)
140/46: a1 = np.load('example.npy')
140/47: a1
140/48:
x = np.linespace(0, 10, 40)
x
140/49:
x = np.linspace(0, 10, 40)
x
140/50:
sinx = np.sin(x)
sinx
140/51: pp.plot(x, sinx)
140/52: cosx = np.cos(x)
140/53:
pp.plot(x, sinx)
pp.plot(x, cosx)
140/54:
pp.plot(x, sinx, 'x')
pp.plot(x, cosx, 'o')
140/55:
y = sinx + cosx
z = cosx**2 - sinx**2

pp.ploy(x, y)
pp.plot(x, z)

pp.legend(['sin(x) cos(X)^2', 'cos(X)^2 - sin(x)^2'])
140/56:
y = sinx + cosx
z = cosx**2 - sinx**2

pp.plot(x, y)
pp.plot(x, z)

pp.legend(['sin(x) cos(X)^2', 'cos(X)^2 - sin(x)^2'])
140/57:
y = sinx + cosx
z = cosx**2 - sinx**2

pp.plot(x, y)
pp.plot(x, z)

pp.legend(['sin(x) cos(X)', 'cos(X)^2 - sin(x)^2'])
140/58: np.dot(sinx, cosx)
140/59: mat_outer = np.outer(sinx,cosx)
140/60: mat_outer
140/61:
v = np.linspace(0, 10, 5
v + 1
140/62:
v = np.linspace(0, 10, 5)
v + 1
140/63:
vv = np.outer(v,v)
vv + v
140/64: vv + v[:, np.newaxis]
140/65: v = np.linespace(0,10,5)
140/66: v = np.linspace(0,10,5)
140/67: v[0]
140/68: v[1]
140/69: v[-1]
140/70: v[5]
140/71: vv = np.random.random((5,4))
140/72: vv
140/73: vv[0]
140/74: vv[4, 3]
140/75: 11 = [[1,2,3],[4,5,6],[7,8,9]]
140/76: ll = [[1,2,3],[4,5,6],[7,8,9]]
140/77: ll[1,2]
140/78: ll[1][2]
140/79: v[2:4]
140/80: vv[2:5,1]
140/81: vv[2:5, 1:5]
140/82: vv[2:5, 1:2]
140/83: vv[2:-1,:]
140/84: vv[:,::2]
140/85: v2 = v[2:4]
140/86: v2[0] = 0
140/87: v
140/88: v3 = v[2:4].copy()
140/89: v3[0] = 1
140/90: v
140/91: v[[1,2,3]]
140/92: bool_index = v > 0
140/93: v[bool_index]
140/94: vv[vv > 0.5] == vv[vv > 0.5] * 2
140/95: vv
140/96: reca = np.array([(1, (2.0,3.0), 'hey'),(2,(3.5,4.0), 'n')], dtype=[('x',np.init32),('y',np.float64, 2),('z',np.str,4)])
140/97: reca = np.array([(1, (2.0,3.0), 'hey'),(2,(3.5,4.0), 'n')], dtype=[('x',np.int32),('y',np.float64, 2),('z',np.str,4)])
140/98: reca
140/99: reca[0]
140/100: reca['x']
140/101: reca['x'][10]
140/102: reca['x'][0]
140/103: reca[0]['x']
140/104: np.datetime64('2015')
140/105: np.datetime64('2015-01')
140/106: np.datetime64('2015-02-03 12:00:00')
140/107: np.datetime64('2015-02-03 12:00:00+0700')
140/108: np.datetime64('2015-02-03') < np.datetime64('2015-04-03')
140/109: np.datetime64('2015-02-03') - np.datetime64('2015-01-01')
140/110: np.datetime64('2015-01-01') + np.datetime64(5, 'D')
140/111: np.datetime64('2015-01-01') + np.timedelta64(5, 'D')
140/112: np.datetime64('2015-01-01') + np.timedelta64(5, 'h')
140/113: np.datetime64('2015-01-01').astype(float)
140/114: np.datetime64('1970-01-01').astype(float)
140/115: r = np.arange(np.datetime64('2016-02-01'), np.datetime64('2016-03-01'))
140/116: r
140/117: # Creating NumPy Arrays
140/118:
import numpy as np
import matplotlib.pyplot as pp
import seaborn
%matplotlib inline
140/119: np.arange(0,10,2)
140/120: np.random.standard_normal((2,4))
140/121:
a = np.random.standard_normal((2,3))
b = np.random.standard_normal((2,3))

np.vstack([a,b])
140/122: np.hstack([a,b])
140/123: a.transpose()
140/124: np.save('example.npy', a)
140/125: a1 = np.load('example.npy')
140/126: a1
140/127:
x = np.linspace(0, 10, 40)
x
140/128:
sinx = np.sin(x)
sinx
140/129: pp.plot(x, sinx)
140/130: cosx = np.cos(x)
140/131:
pp.plot(x, sinx)
pp.plot(x, cosx)
140/132:
pp.plot(x, sinx, 'x')
pp.plot(x, cosx, 'o')
140/133:
y = sinx + cosx
z = cosx**2 - sinx**2

pp.plot(x, y)
pp.plot(x, z)

pp.legend(['sin(x) cos(X)', 'cos(X)^2 - sin(x)^2'])
140/134: np.dot(sinx, cosx)
140/135: mat_outer = np.outer(sinx,cosx)
140/136: mat_outer
140/137:
v = np.linspace(0, 10, 5)
v + 1
140/138:
vv = np.outer(v,v)
vv + v
140/139: vv + v[:, np.newaxis]
140/140: v = np.linspace(0,10,5)
140/141: v[0]
140/142: v[1]
140/143: v[-1]
140/144: v[5]
140/145: # Creating NumPy Arrays
140/146:
import numpy as np
import matplotlib.pyplot as pp
import seaborn
%matplotlib inline
140/147:
a = np.array([1, 2, 3, 4, 5])
a
140/148: a.dtype
140/149:
a = np.array([1, 2, 3, 4, 5], dtype=np.float64)
a
140/150: a.ndim, a.shape, a.size
140/151: b = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
140/152: b
140/153: b.dtype
140/154: b.ndim, b.shape, b.size
140/155: np.zeros((3,3),'d')
140/156: np.empty((4,4), 'd')
140/157: np.linspace(0,10,5)
140/158: np.arange(0,10,2)
140/159: np.random.standard_normal((2,4))
140/160:
a = np.random.standard_normal((2,3))
b = np.random.standard_normal((2,3))

np.vstack([a,b])
140/161: np.hstack([a,b])
140/162: a.transpose()
140/163: np.save('example.npy', a)
140/164: a1 = np.load('example.npy')
140/165: a1
140/166:
x = np.linspace(0, 10, 40)
x
140/167:
sinx = np.sin(x)
sinx
140/168: pp.plot(x, sinx)
140/169: cosx = np.cos(x)
140/170:
pp.plot(x, sinx)
pp.plot(x, cosx)
140/171:
pp.plot(x, sinx, 'x')
pp.plot(x, cosx, 'o')
140/172:
y = sinx + cosx
z = cosx**2 - sinx**2

pp.plot(x, y)
pp.plot(x, z)

pp.legend(['sin(x) cos(X)', 'cos(X)^2 - sin(x)^2'])
140/173: np.dot(sinx, cosx)
140/174: mat_outer = np.outer(sinx,cosx)
140/175: mat_outer
140/176:
v = np.linspace(0, 10, 5)
v + 1
140/177:
vv = np.outer(v,v)
vv + v
140/178: vv + v[:, np.newaxis]
140/179: v = np.linspace(0,10,5)
140/180: v[0]
140/181: v[1]
140/182: v[-1]
140/183: v[5]
140/184:
import numpy as np
import matplotlib.pyplot as pp
import seaborn
%matplotlib inline
import urllib.request
140/185: urllib.request.urlretrieve('ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt', 'stations.txt')
140/186: open('stations.txt', 'r').readlines()[:10]
140/187:
stations = {}

for line in open('stations.txt', 'r'):
    if 'GSN' in line:
        fields = line.split()
        
        stations[fields[0]] = ' '.join(fields[4:])
140/188: len(stations)
140/189:
def findstations(s):
    found = {code: name for code, name in stations.items() if s in name}
    print(found)
140/190: findstations('LIHUE')
140/191: findstations('SAN DIEGO')
140/192: findstations('MINNEAPOLIS')
140/193: findstations('IRKUTSK')
140/194: open('USW00022536.dly', 'r').readlines()[:10]
141/1: import pandas as pd
141/2: s = pd.Series([0, 1, 4, 9, 16, 25], name='squares')
141/3: s
141/4: s.values
141/5: s.index
141/6: s[0]
141/7: s[2]
141/8: s[2:4]
141/9: pop2014 = ps.Series([100,99.3,99.5,93.5,92.4,84.4,84.5,78.9,74.3,72.8], index=['Java', 'C', 'C++', 'Python', 'C#', 'PHP', 'JavaScript', 'Ruby', 'R', 'Matlab'])
141/10: pop2014 = pd.Series([100,99.3,99.5,93.5,92.4,84.4,84.5,78.9,74.3,72.8], index=['Java', 'C', 'C++', 'Python', 'C#', 'PHP', 'JavaScript', 'Ruby', 'R', 'Matlab'])
141/11: pop2014
141/12: pop2014.index
141/13: pop2014[0]
141/14: pop2014[0:2]
141/15: pop2014['Python']
141/16: pop2014['C++':'C#']
141/17: pop2014.iloc[0:2]
141/18: pop2014.loc['Ruby']
141/19: pop2014[pop2014 > 90]
141/20: pop2015 = pd.Series({'Java': 100, 'C': 99.9, 'C++': 99.4, 'Python': 96.5, 'C#': 91.3, 'R': 84.5, 'PHP': 84.5, 'JavaScript': 83.0, 'Ruby': 76.2, 'Matlab': 72.4})
141/21: pop2015
141/22: twoyears = pd.DataFrame({'2014': pop2014, '2015': pop2015})
141/23: twoyears
141/24: twoyears = twoyears.sort('2015', ascending=False)
141/25: twoyears = twoyears.sort('2015',ascending=False)
141/26: twoyears.values
141/27: twoyears.index
141/28: twoyears['2014']
141/29: twoyears.iloc[0:2]
141/30: twoyears.loc['C': 'Python']
141/31: twoyears['avg'] = 0.5*(twoyears['2014'] + twoyears['2015'])
141/32: twoyears
141/33:
presidents = pd.DataFrame({'name': 'Barack Obama', 'inauguration': 2009, 'birthday': 1961},
                          {'name': 'George W. Bush', 'inauguration': 2001, 'birthday': 1946},
                          {'name': 'Bill Clinton', 'inauguration': 1993, 'birthday': 1946},
                          {'name': 'George H. W. Bush', 'inauguration': 1989, 'birthday': 1924})
141/34:
presidents = pd.DataFrame([{'name': 'Barack Obama', 'inauguration': 2009, 'birthday': 1961},
                          {'name': 'George W. Bush', 'inauguration': 2001, 'birthday': 1946},
                          {'name': 'Bill Clinton', 'inauguration': 1993, 'birthday': 1946},
                          {'name': 'George H. W. Bush', 'inauguration': 1989, 'birthday': 1924}])
141/35: presidents
141/36: president_indexes = presidents.set_index('name')
141/37: president_indexes
141/38: president_indexes.loc['Bill Clinton']
141/39: president_indexes.loc['Bill Clinton']['inauguration']
141/40:
presidents_fathers = pd.DataFrame([{'son': 'Barack Obama', 'father': 'Barak Obama, Sr.'},
                                  {'son': 'George W. Bush', 'father': 'George H. W. Bush'},
                                  {'son': 'George H. W. Bush', 'father': 'Prescott Bush'}])
141/41: pd.merge(presidents, presidents_fathers, left_on='name', right_on='son')
141/42: pd.merge(presidents, presidents_fathers, left_on='name', right_on='son').drop('son',axis=1)
141/43: pd.merge(presidents, presidents_fathers, left_on='name', right_on='son',how='left').drop('son',axis=1)
141/44: pd.merge(presidents, presidents_fathers, left_on='name', right_on='son',how='right').drop('son',axis=1)
141/45: pd.merge(presidents, presidents_fathers, left_on='name', right_on='son',how='left').drop('son',axis=1)
138/87: local_content_indexed = local_content.set_index(['method'])
138/88:
local_content_indexed = local_content.set_index(['method'])
local_content_indexed.head()
138/89:
local_content_index = local_content.set_index(['type', 'method'])
local_content_index.sample(frac=.5)
138/90: local_content_index.to_csv('local_content_indexed.csv')
138/91: local_content_index.to_csv('local_content_indexed.csv')
138/92:
# Imports
import pandas as pd
import matplotlib as pyplot
import numpy as np
138/93:
# import the file
local_content = pd.read_csv('local-content.csv')
138/94:
# view
local_content.head()
138/95:
# Sample 50%
local_content.sample(frac=.5)
138/96: nephews = ['Heuy', 'Dewey', 'Louie']
138/97: nephews
138/98: nephews[0]
138/99: nephews[1]
138/100:
for i in range(3):
    nephews[i] = nephews[i] + ' Duck'
138/101: print(nephews)
138/102: mix_it_up = [1, [2, 3], 'alpha']
138/103: mix_it_up
138/104: nephews.append('April Duck')
138/105: print(nephews)
138/106: nephews.extend(['Mary Duck', 'June Duck'])
138/107: print(nephews)
138/108:
ducks = nephews + ['Donald Duck', 'Daisy Duck']
print(ducks)
138/109:
ducks.insert(0, 'Scrooge McDuck')
print(ducks)
138/110:
del ducks[0]
print(ducks)
138/111:
ducks.remove('Donald Duck')
print(ducks)
138/112:
ducks.sort()
print(ducks)
138/113: squares = [0, 1, 4, 9, 16, 25, 36, 49]
138/114: squares[0:2]
138/115: squares[0:2]
138/116: squares[1:3]
138/117: squares[1:]
138/118: squares[:]
138/119: squares[-1]
138/120:
squares[2:4] = ['4', '9']
print(squares)
138/121:
del squares[-2:]
print(squares)
138/122:
for value in squares:
    print("Element:", value)
138/123:
for index, value in enumerate(squares):
    print("Element", index, "=>", value)
138/124: local_content.mean()
138/125: local_content.describe()
138/126: local_content.groupby('method').mean()
138/127: local_content.groupby(['type', 'method']).mean()
138/128: pd.pivot_table(local_content, 'contract_price', 'type', 'method')
138/129: pd.pivot_table(local_content, 'contract_price', ['type'], ['method'])
138/130: pd.pivot_table(local_content, 'contract_price', ['method'], ['type'])
138/131: pd.pivot_table(local_content, 'contract_price', ['country_of_registration', 'method'], ['type'])
138/132: local_content.groupby('country_of_registration').mean()
138/133: local_content.groupby('country_of_registration').sum()
138/134: local_content.groupby('country_of_registration').sum().drop('maximum_indicative_time', axis=1)
138/135: local_content.groupby('method').sum()
138/136: local_content..sum()
138/137: local_content.sum()
138/138: local_content['contract_price'].sum()
138/139: local_content['award_price'].sum()
138/140: local_content_grouped = local_content.groupby('country_of_registration').sum().drop('maximum_indicative_time', axis=1)
138/141: local_content_grouped.pp
138/142: local_content_grouped['foreign']
138/143: local_content_grouped
138/144: local_content_grouped['local']
138/145: local_content_grouped[local_content_grouped['local']]
138/146: local_content_grouped.local
138/147: local_content_grouped['contract_price']
138/148: local_content_grouped['contract_price'].hist()
138/149: local_content_grouped.hist()
143/1: 3. **Completed Contracts by type and method**
143/2:
# imports

import pandas as pd
143/3:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/4:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/5:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/6: completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
143/7: completed_contracts.head()
143/8:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/9: completed_contracts.head()
143/10:
completed_contracts['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
143/11:
completed_contracts['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')
143/12:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')
143/13: completed_contracts.sample(frac=.7)
143/14: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/15: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/16:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/17:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/18:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/19:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/20:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/21: completed_contracts.head()
143/22:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')
143/23: completed_contracts.sample(frac=.7)
143/24: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/25: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/26: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['actual_contract_signature_date']
143/27: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['actual_contract_signature_date']
143/28:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/29:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/30:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/31: completed_contracts.head()
143/32:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
143/33: completed_contracts.sample(frac=.7)
143/34: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/35: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/36: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['actual_contract_signature_date']
143/37: completed_contracts.head()
143/38:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')
143/39:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
143/40:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/41:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/42:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/43: completed_contracts.head()
143/44:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
143/45: completed_contracts.sample(frac=.7)
143/46: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/47: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/48: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['actual_contract_signature_date']
143/49: completed_contracts.head()
143/50:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
143/51: completed_contracts.head()
143/52: completed_contracts['competion_price_status'] = completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'] ? 'within' : 'above'
143/53:
def check_price(contract_price, completion_price):
    if contract_price <= completion_price:
        return 'within'
    return 'above'
143/54: completed_contracts['competion_price_status'] = check_price(completed_contracts['contract_price'], completed_contracts['actual_contract_completion_date'])
143/55: completed_contracts['competion_price_status'] = check_price(completed_contracts['contract_price'], completed_contracts['completion_contract_price'])
143/56: completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/57: completed_contracts.head()
143/58: completed_contracts.sample(frac=.5)
143/59:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/60:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/61:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/62:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/63:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/64:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/65:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/66:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/67:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/68:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/69:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/70: completed_contracts.head()
143/71:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
143/72: completed_contracts.sample(frac=.7)
143/73: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/74: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/75: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['actual_contract_signature_date']
143/76: completed_contracts.head()
143/77:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
143/78: completed_contracts.head()
143/79: completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/80:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/81:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/82:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/83: completed_contracts.head()
143/84:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
143/85: completed_contracts.sample(frac=.7)
143/86: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/87: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/88: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['actual_contract_signature_date']
143/89: completed_contracts.head()
143/90:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
143/91: completed_contracts.head()
143/92: completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/93: completed_contracts['competion_time_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/94: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
143/95:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/96:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/97:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/98: completed_contracts.head()
143/99:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
143/100: completed_contracts.sample(frac=.7)
143/101: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/102: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/103: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
143/104: completed_contracts.head()
143/105:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
143/106: completed_contracts.head()
143/107: completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/108: completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/109:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/110:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/111:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/112: completed_contracts.head()
143/113:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
143/114: completed_contracts.sample(frac=.7)
143/115: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/116: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/117: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
143/118: completed_contracts.head()
143/119:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
143/120: completed_contracts.head()
143/121:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/122:

completed_contracts['competion_time_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/123:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/124:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/125:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/126: completed_contracts.head()
143/127:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
143/128: completed_contracts.sample(frac=.7)
143/129: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/130: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/131: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
143/132: completed_contracts.head()
143/133:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
143/134: completed_contracts.head()
143/135:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/136:

completed_contracts['competion_time_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/137: completed_contracts.sample(frac=.5)
143/138:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
143/139:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
143/140:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
143/141: completed_contracts.head()
143/142:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
143/143: completed_contracts.sample(frac=.7)
143/144: completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
143/145: completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
143/146: completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
143/147: completed_contracts.head()
143/148:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
143/149: completed_contracts.head()
143/150:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
143/151:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
143/152: completed_contracts.sample(frac=.5)
143/153:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
144/1:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
144/2:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
144/3:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
144/4: market_price.head()
144/5:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')
144/6:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
144/7: market_price.sample(frac=.7)
144/8:

completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']

completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
144/9:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
144/10:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')

market_price['completion_period'] = pd.to_numeric(market_price['completion_period'].dt.days, downcast='integer')
144/11:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
144/12:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
144/13:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
144/14:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
144/15: market_price.head()
144/16:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
144/17: market_price.sample(frac=.7)
144/18:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
144/19:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
144/20:

market_price['market_price_status'] = np.where(market_price['estimated_amount'] <= market_price['contract_price'], 'within', 'above')
144/21: market_price.head()
144/22: market_price.sample(frac=.7)
144/23:
# Save the file to csv and excel

market_price.to_csv('market_price_st.csv')

market_price.to_excel('market_price_st.xlsx')
144/24: market_price.describe
144/25: market_price.describe()
144/26:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
144/27: market_price.describe()
144/28:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
144/29:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
144/30:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
144/31:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
144/32: market_price.describe()
144/33:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
144/34: market_price.head()
144/35:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
144/36: market_price.sample(frac=.7)
144/37:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
144/38:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
144/39:

market_price['market_price_status'] = np.where(market_price['estimated_amount'] <= market_price['contract_price'], 'within', 'above')
144/40: market_price.sample(frac=.7)
144/41:
# Save the file to csv and excel

market_price.to_csv('market_price_st.csv')

market_price.to_excel('market_price_st.xlsx')
144/42: market_price.describe()
144/43:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
144/44:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
144/45: market_price.describe()
144/46:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
144/47: market_price.head()
144/48:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
144/49: market_price.sample(frac=.7)
144/50:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
144/51:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
144/52:

market_price['market_price_status'] = np.where(market_price['estimated_amount'] >= market_price['contract_price'], 'within', 'above')
144/53: market_price.sample(frac=.7)
144/54:
# Save the file to csv and excel

market_price.to_csv('market_price_st.csv')

market_price.to_excel('market_price_st.xlsx')
144/55: market_price.describe()
145/1: market_price.hist()
145/2:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
145/3:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
145/4: market_price.describe()
145/5:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
145/6: market_price.head()
145/7:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
145/8: market_price.sample(frac=.7)
145/9: market_price.hist()
145/10:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
145/11:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
145/12:

market_price['market_price_status'] = np.where(market_price['estimated_amount'] >= market_price['contract_price'], 'within', 'above')
145/13: market_price.sample(frac=.7)
145/14:
# Save the file to csv and excel

market_price.to_csv('market_price_st.csv')

market_price.to_excel('market_price_st.xlsx')
145/15: market_price.describe()
145/16: market_price.bar()
145/17: market_price.bar
145/18: market_price.bar(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)
145/19: market_price['country_of_registraton'].plot(kind="bar")
145/20: market_price['country_of_registraton'].value_counts().plot(kind="bar")
145/21: market_price['country_of_registration'].value_counts().plot(kind="bar")
145/22: market_price['country_of_registration'].value_counts().plot(kind="bar", colors=['salmon', 'light-blue'])
145/23: market_price['country_of_registration'].value_counts().plot(kind="bar", colors=['salmon', 'lightblue'])
145/24: market_price['country_of_registration'].value_counts().plot(kind="bar", color=['salmon', 'lightblue'])
145/25: market_price['country_of_registration'].value_counts().plot(kind="bar", color=['darkblue', 'lightblue'])
145/26: market_price['country_of_registration'].value_counts().plot(kind="hbar", color=['darkblue', 'lightblue'])
145/27: market_price['country_of_registration'].value_counts().plot(kind="bar", color=['darkblue', 'lightblue'])
145/28: market_price['country_of_registration'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
145/29:
# 1. Prepare the data

x = market_price['subject_of_procurement'].value_counts()
y = market_price['contract_price']

# 2. Setup the plot
fig, ax = plt.subplots(figsize(10, 10)) # width and height

# 3. Plot the data
ax.plot(x, y)

# 4. Customize the plot
ax.set(title = 'Market Price Within Vs Above', xlabel="x-axis", ylabel="y-axis")

# 5. Save and show
# fig.savefig('images/market_price.png')
145/30:
# 1. Prepare the data

x = market_price['subject_of_procurement'].value_counts()
y = market_price['contract_price']

# 2. Setup the plot
fig, ax = plt.subplots(figSize(10, 10)) # width and height

# 3. Plot the data
ax.plot(x, y)

# 4. Customize the plot
ax.set(title = 'Market Price Within Vs Above', xlabel="x-axis", ylabel="y-axis")

# 5. Save and show
# fig.savefig('images/market_price.png')
145/31:
# 1. Prepare the data

x = market_price['subject_of_procurement'].value_counts()
y = market_price['contract_price']

# 2. Setup the plot
fig, ax = plt.subplots(figsize(10, 10)) # width and height

# 3. Plot the data
ax.plot(x, y)

# 4. Customize the plot
ax.set(title = 'Market Price Within Vs Above', xlabel="x-axis", ylabel="y-axis")

# 5. Save and show
# fig.savefig('images/market_price.png')
152/1:
%matplotlibinline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
152/2:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
152/3: plt.plot()
152/4: plt.show()
152/5: plt.show()
152/6: plt.plot()
152/7: plt.plot([1, 2, 3, 4, 5])
152/8: plt.plot([-1, -2, -3, -4, -5])
152/9:
x = [1, 2, 3, 4, 5]
y = [-1, -2, -3, -4, -5]

plt.plot(x, y)
152/10:
x = [1, 2, 3, 4, 5]
y = [-1, -2, -3, -4, -5]

plt.plot(x, y);
152/11:
x = [1, 2, 3, 4, 5]
y = [11, 22, 32, 44, 55]

plt.plot(x, y);
152/12:
x = [1, 2, 3, 4, 5]
y = [-11, 22, 32, 44, 55]

plt.plot(x, y);
152/13:
x = [1, 2, 3, 4, 5]
y = [-11, 22, 32, -44, 55]

plt.plot(x, y);
152/14:
# 1 Method

fig = plt.figure()
ax = figure.add_subplots()
plt.show()
152/15:
# 1 Method

fig = plt.figure()
ax = fig.add_subplots()
plt.show()
152/16:
# 1 Method

fig = plt.figure()
ax = fig.add_subplots()
plt.show()
152/17:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
152/18: plt.plot()
152/19: plt.plot([1, 2, 3, 4, 5])
152/20: plt.plot([-1, -2, -3, -4, -5])
152/21:
x = [1, 2, 3, 4, 5]
y = [-11, 22, 32, -44, 55]

plt.plot(x, y);
152/22:
# 1 Method

fig = plt.figure()
ax = fig.add_subplots()
plt.show()
152/23:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
152/24: plt.plot()
152/25: plt.plot([1, 2, 3, 4, 5])
152/26: plt.plot([-1, -2, -3, -4, -5])
152/27:
x = [1, 2, 3, 4, 5]
y = [-11, 22, 32, -44, 55]

plt.plot(x, y);
152/28:
# 1 Method

fig = plt.figure()
ax = fig.add_subplots()
plt.show()
152/29:
# 1 Method

fig = plt.figure()
ax = fig.add_subplot()
plt.show()

# fig = plt.figure() # creates a figure
# ax = fig.add_subplot() # adds some axes
# plt.show()
152/30:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 2, 3, 4, 5])
ax.plot(x, y)
152/31:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 2, 3, 4, 5])
ax.plot(x, y)
plot.show()
152/32:
# 2nd Method

# fig = plt.figure()
# ax = fig.add_axes([1, 2, 3, 4, 5])
# ax.plot(x, y)
# plot.show()

fig = plt.figure() # creates a figure
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plt.show()
152/33:
# 2nd Method

# fig = plt.figure()
# ax = fig.add_axes([1, 2, 3, 4, 5])
# ax.plot(x, y)
# plot.show()

fig = plt.figure() # creates a figure
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plt.show()
152/34:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
152/35: plt.plot()
152/36: plt.plot([1, 2, 3, 4, 5])
152/37: plt.plot([-1, -2, -3, -4, -5])
152/38:
x = [1, 2, 3, 4, 5]
y = [-11, 22, 32, -44, 55]

plt.plot(x, y);
152/39:
# 1st Method

fig = plt.figure()
ax = fig.add_subplot()
plt.show()
152/40:
# 2nd Method

# fig = plt.figure()
# ax = fig.add_axes([1, 2, 3, 4, 5])
# ax.plot(x, y)
# plot.show()

fig = plt.figure() # creates a figure
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plt.show()
152/41:
# 2nd Method

# fig = plt.figure()
# ax = fig.add_axes([1, 2, 3, 4, 5])
# ax.plot(x, y)
# plot.show()

fig = plt.figure() # creates a figure
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plot.show()
152/42:
# 2nd Method

# fig = plt.figure()
# ax = fig.add_axes([1, 2, 3, 4, 5])
# ax.plot(x, y)
# plot.show()

fig = plt.figure() # creates a figure
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plt.show()
152/43:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
152/44: plt.plot()
152/45: plt.plot([1, 2, 3, 4, 5])
152/46: plt.plot([-1, -2, -3, -4, -5])
152/47:
# x = [1, 2, 3, 4, 5]
# y = [-11, 22, 32, -44, 55]

# plt.plot(x, y);
152/48:
# 1st Method

fig = plt.figure()
ax = fig.add_subplot()
plt.show()
152/49:
# 2nd Method

# fig = plt.figure()
# ax = fig.add_axes([1, 2, 3, 4, 5])
# ax.plot(x, y)
# plot.show()

fig = plt.figure() # creates a figure
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plt.show()
153/1:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
153/2: plt.plot()
153/3: plt.plot([1, 2, 3, 4, 5])
153/4: plt.plot([-1, -2, -3, -4, -5])
153/5:
# x = [1, 2, 3, 4, 5]
# y = [-11, 22, 32, -44, 55]

# plt.plot(x, y);
153/6:
# 1st Method

fig = plt.figure()
ax = fig.add_subplot()
plt.show()
153/7:
# 2nd Method

# fig = plt.figure()
# ax = fig.add_axes([1, 2, 3, 4, 5])
# ax.plot(x, y)
# plot.show()

fig = plt.figure() # creates a figure
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plt.show()
153/8:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 2, 3, 4, 5])
ax.plot(x, y)
plot.show()
153/9:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 2, 3, 4, 5])
ax.plot(x, y)
plot.show()
153/10:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 1, 1, 1, 1])
ax.plot(x, y)
plot.show()
153/11:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plot.show()
153/12:
x = [1, 2, 3, 4, 5]
y = [-11, 22, 32, -44, 55]

plt.plot(x, y);
153/13:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plot.show()
153/14:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
153/15: plt.plot()
153/16: plt.plot([1, 2, 3, 4, 5])
153/17: plt.plot([-1, -2, -3, -4, -5])
153/18:
x = [1, 2, 3, 4, 5]
y = [-11, 22, 32, -44, 55]

plt.plot(x, y);
153/19:
# 1st Method

fig = plt.figure()
ax = fig.add_subplot()
plt.show()
153/20:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plot.show()
153/21:
# 2nd Method

fig = plt.figure()
ax = fig.add_axes([1, 1, 1, 1])
ax.plot(x, y)
plt.show()
153/22: # 3rd Method recommended
153/23:
# 3rd Method recommended

fig, ax = plt.subplots()
ax.plot(x, [50, 100, 200, 250])
type(fig), type(ax)
153/24:
# 3rd Method recommended

fig, ax = plt.subplots()
ax.plot(x, [50, 100, 200, 250]);
type(fig), type(ax)
153/25:
fig, ax = plt.subplots()
ax.plot(x, [50, 100, 200, 250]); # add some data
type(fig), type(ax)
153/26:
# 3rd Method recommended

fig, ax = plt.subplots()
ax.plot(x, [50, 100, 200, 250]);
type(fig), type(ax)
153/27:
# 3rd Method recommended

fig, ax = plt.subplots()
ax.shape(2, 2)
ax.plot(x, [50, 100, 200, 250]);
type(fig), type(ax)
145/32:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
145/33:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
145/34: market_price.describe()
145/35:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
145/36: market_price.head()
145/37:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
145/38: market_price.sample(frac=.7)
145/39: market_price['country_of_registration'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
145/40:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
145/41:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
145/42:

market_price['market_price_status'] = np.where(market_price['estimated_amount'] <= market_price['contract_price'], 'within', 'above')
145/43: market_price.sample(frac=.7)
145/44:
# Save the file to csv and excel

market_price.to_csv('market_price_st.csv')

market_price.to_excel('market_price_st.xlsx')
145/45: market_price.describe()
145/46:
# 1. Prepare the data

x = market_price['subject_of_procurement'].value_counts()
y = market_price['contract_price']

# 2. Setup the plot
fig, ax = plt.subplots(figsize(10, 10)) # width and height

# 3. Plot the data
ax.plot(x, y)

# 4. Customize the plot
ax.set(title = 'Market Price Within Vs Above', xlabel="x-axis", ylabel="y-axis")

# 5. Save and show
# fig.savefig('images/market_price.png')
145/47:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
145/48:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
145/49: market_price.describe()
145/50:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
145/51: market_price.head()
145/52:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
145/53: market_price.sample(frac=.7)
145/54: market_price['country_of_registration'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
145/55:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
145/56:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
145/57:

market_price['market_price_status'] = np.where(market_price['estimated_amount'] <= market_price['contract_price'], 'within', 'above')
145/58: market_price.sample(frac=.7)
145/59:
# Save the file to csv and excel

market_price.to_csv('market_price_st.csv')

market_price.to_excel('market_price_st.xlsx')
145/60: market_price.describe()
145/61:
# 1. Prepare the data

# x = market_price['subject_of_procurement'].value_counts()
# y = market_price['contract_price']

# 2. Setup the plot
# fig, ax = plt.subplots(figsize(10, 10)) # width and height

# 3. Plot the data
# ax.plot(x, y)

# 4. Customize the plot
# ax.set(title = 'Market Price Within Vs Above', xlabel="x-axis", ylabel="y-axis")

# 5. Save and show
# fig.savefig('images/market_price.png')
154/1:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
154/2:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
154/3: bids[bids['read_out_prics'] < 1]
154/4: bids[bids['read_out_price'] < 1]
154/5: bids.describe()
154/6: (4270 / 10739) * 100
154/7:
# import the file
bids = pd.read_csv('bids.csv', index=None)

bids.head()
154/8:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
154/9: bids['no_of_bids'].sum()
154/10: total_bids = bids['no_of_bids'].sum()
154/11: no_read_out_price = bids[bids['read_out_price'] < 1]
154/12: no_read_out_price.head()
154/13:
with_read_out_price = bids[bids['read_out_price'] > 1]

with_read_out_price.head()
154/14:
with_read_out_price = bids[bids['read_out_price'] > 1]

with_read_out_price.describe()
154/15:
with_read_out_price = bids[bids['read_out_price'] > 1]

with_read_out_price.count()
154/16:
with_read_out_price = bids[bids['read_out_price'] > 1]

with_read_out_price.describe()
154/17: no_read_out_price.describe()
154/18: no_read_out_price.describe()
154/19: no_read_out_price[no_of_bids].count()
154/20: no_read_out_price['no_of_bids'].count()
154/21: (no_read_out_price['no_of_bids'].count() / total_bids) * 100
154/22: total_bids
154/23: (with_read_out_price['no_of_bids'] / total_bids) * 100
154/24: (with_read_out_price['no_of_bids'].count() / total_bids) * 100
154/25: with_read_out_price['no_of_bids'].count()
154/26: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
154/27: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
154/28: with_read_out_price['no_of_bids'].sum()
154/29: no_read_out_price['no_of_bids'].sum()
154/30: bids.plot().bar()
154/31: bids['read_out_price'].plot(kind='hist')
154/32: bids['read_out_price'].plot(kind='hist');
154/33:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
154/34:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without Zero": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
154/35:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without zero": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
154/36: no_read_out_price.groupby(['method'])
154/37: no_read_out_price.groupby(['method']).sum()
154/38: no_read_out_price.groupby(['method']).sum().hist()
154/39: no_read_out_price.groupby(['method']).sum().bar()
154/40: no_read_out_price.groupby(['method']).sum()
154/41: with_read_out_price.groupby(['method']).sum()
154/42:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
154/43:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
154/44: bids[bids['read_out_price'] < 1]
154/45: (4270 / 10739) * 100
154/46: bids.describe()
154/47: total_bids = bids['no_of_bids'].sum()
154/48: no_read_out_price = bids[bids['read_out_price'] < 1]
154/49: no_read_out_price.describe()
154/50: no_read_out_price['no_of_bids'].sum()
154/51: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
154/52:
with_read_out_price = bids[bids['read_out_price'] > 1]

with_read_out_price.describe()
154/53: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
154/54: with_read_out_price['no_of_bids'].sum()
154/55: bids['read_out_price'].plot(kind='hist');
154/56:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without zero": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
154/57: no_read_out_price.groupby(['method']).sum()
154/58: with_read_out_price.groupby(['method']).sum()
155/1:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
155/2:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
155/3: bids[bids['read_out_price'] < 1]
155/4: (4270 / 10739) * 100
155/5: bids.describe()
155/6: total_bids = bids['no_of_bids'].sum()
155/7: no_read_out_price = bids[bids['read_out_price'] < 1]
155/8: no_read_out_price.describe()
155/9: no_read_out_price['no_of_bids'].sum()
155/10: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/11:
with_read_out_price = bids[bids['read_out_price'] > 1]

with_read_out_price.describe()
155/12: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/13: with_read_out_price['no_of_bids'].sum()
155/14: bids['read_out_price'].plot(kind='hist');
155/15:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without zero": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
155/16: no_read_out_price.groupby(['method']).sum()
155/17: with_read_out_price.groupby(['method']).sum()
155/18: no_read_out_price['no_of_bids'].sum()
155/19: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/20: total_bids
155/21:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
155/22:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
155/23: bids[bids['read_out_price'] < 1]
155/24: (4270 / 10739) * 100
155/25: bids.describe()
155/26: total_bids = bids['no_of_bids'].sum()
155/27: no_read_out_price = bids[bids['read_out_price'] < 1]
155/28: no_read_out_price.describe()
155/29: no_read_out_price['no_of_bids'].sum()
155/30: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/31:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
155/32: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/33: with_read_out_price['no_of_bids'].sum()
155/34: bids['read_out_price'].plot(kind='hist');
155/35:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without zero": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
155/36: no_read_out_price.groupby(['method']).sum()
155/37: with_read_out_price.groupby(['method']).sum()
155/38: total_bids
155/39:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
155/40:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
155/41: bids[bids['read_out_price'] < 1]
155/42: (4270 / 10739) * 100
155/43: bids.describe()
155/44: total_bids = bids['no_of_bids'].sum()
155/45: no_read_out_price = bids[bids['read_out_price'] < 1]
155/46: no_read_out_price.describe()
155/47: no_read_out_price['no_of_bids'].sum()
155/48: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/49:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
155/50: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/51: with_read_out_price['no_of_bids'].sum()
155/52: bids['read_out_price'].plot(kind='hist');
155/53:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
155/54: no_read_out_price.groupby(['method']).sum()
155/55: with_read_out_price.groupby(['method']).sum()
155/56: total_bids
155/57:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
155/58:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
155/59: bids[bids['read_out_price'] < 1]
155/60: (4270 / 10739) * 100
155/61: bids.describe()
155/62: total_bids = bids['no_of_bids'].sum()
155/63: no_read_out_price = bids[bids['read_out_price'] < 1]
155/64: no_read_out_price.describe()
155/65: no_read_out_price['no_of_bids'].sum()
155/66: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/67:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
155/68: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/69: with_read_out_price['no_of_bids'].sum()
155/70: bids['read_out_price'].plot(kind='hist');
155/71:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
155/72: no_read_out_price.groupby(['method']).sum()
155/73: with_read_out_price.groupby(['method']).sum()
155/74: total_bids
155/75: bids
155/76: no_read_out_price
155/77: bids.describe()
155/78: bids.head()
155/79: bids_with_read_out_price = np.where(bids['read_out_price'] > 0)
155/80: bids_with_read_out_price.head()
155/81: bids_with_read_out_price.describe
155/82: bids_with_read_out_price
155/83: bids_with_read_out_price = bids[bids['read_out_price'] > 0]
155/84: bids_with_read_out_price.head()
155/85: bids_with_read_out_price.describe()
155/86:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
155/87:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
155/88: bids[bids['read_out_price'] < 1]
155/89: (4270 / 10739) * 100
155/90: bids.describe()
155/91: total_bids = bids['no_of_bids'].sum()
155/92: no_read_out_price = bids[bids['read_out_price'] < 1]
155/93: no_read_out_price.describe()
155/94: no_read_out_price['no_of_bids'].sum()
155/95: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/96:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
155/97: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
155/98: with_read_out_price['no_of_bids'].sum()
155/99: bids['read_out_price'].plot(kind='hist');
155/100:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
155/101: no_read_out_price.groupby(['method']).sum()
155/102: with_read_out_price.groupby(['method']).sum()
155/103: total_bids
155/104: bids
155/105: no_read_out_price
155/106: bids.head()
155/107: bids = bids[bids['read_out_price'] > 0]
155/108: bids.head()
155/109: bids.describe()
155/110: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
155/111: bids.head()
155/112: bids.to_csv('bids_average_read_out_price.csv')
155/113:
bids.to_csv('bids_average_read_out_price.csv')

bids.to_csv('bids.xlxs')
155/114:
bids.to_csv('bids_average_read_out_price.csv')

bids.to_csv('bids.xlsx')
155/115:
bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx')
155/116: bids.tail()
155/117:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
155/118: bids.groupby(['method'])
155/119: bids.groupby(['method']).sum()
160/1:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
160/2:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
160/3: bids[bids['read_out_price'] < 1]
160/4: (4270 / 10739) * 100
160/5: bids.describe()
160/6: total_bids = bids['no_of_bids'].sum()
160/7: no_read_out_price = bids[bids['read_out_price'] < 1]
160/8: no_read_out_price.describe()
160/9: no_read_out_price['no_of_bids'].sum()
160/10: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
160/11:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
160/12: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
160/13: with_read_out_price['no_of_bids'].sum()
160/14: bids['read_out_price'].plot(kind='hist');
160/15:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
160/16: no_read_out_price.groupby(['method']).sum()
160/17: with_read_out_price.groupby(['method']).sum()
160/18: total_bids
160/19: bids
160/20: no_read_out_price
160/21: bids.head()
160/22: bids = bids[bids['read_out_price'] > 0]
160/23: bids.head()
160/24: bids.describe()
160/25: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
160/26: bids.head()
160/27: bids.tail()
160/28:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
160/29: bids.groupby(['method']).sum()
160/30: bids.describe()
160/31:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
160/32:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
160/33: bids.describe()
160/34: bids[bids['read_out_price'] < 1]
160/35: (4270 / 10739) * 100
160/36: bids.describe()
160/37: total_bids = bids['no_of_bids'].sum()
160/38: no_read_out_price = bids[bids['read_out_price'] < 1]
160/39: no_read_out_price.describe()
160/40: no_read_out_price['no_of_bids'].sum()
160/41: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
160/42:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
160/43: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
160/44: with_read_out_price['no_of_bids'].sum()
160/45: bids['read_out_price'].plot(kind='hist');
160/46:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
160/47: no_read_out_price.groupby(['method']).sum()
160/48: with_read_out_price.groupby(['method']).sum()
160/49: total_bids
160/50: bids
160/51: no_read_out_price
160/52: bids.head()
160/53: bids = bids[bids['read_out_price'] > 0]
160/54: bids.head()
160/55: bids.describe()
160/56: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
160/57: bids.head()
160/58: bids.tail()
160/59:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
160/60: bids.groupby(['method']).sum()
160/61:
# import the file

av_bids = pd.read_csv('')
160/62:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
160/63:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
160/64: bids.describe()
160/65: bids[bids['read_out_price'] < 1]
160/66: (4270 / 10739) * 100
160/67: bids.describe()
160/68: total_bids = bids['no_of_bids'].sum()
160/69: no_read_out_price = bids[bids['read_out_price'] < 1]
160/70: no_read_out_price.describe()
160/71: no_read_out_price['no_of_bids'].sum()
160/72: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
160/73:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
160/74: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
160/75: with_read_out_price['no_of_bids'].sum()
160/76: bids['read_out_price'].plot(kind='hist');
160/77:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
160/78: no_read_out_price.groupby(['method']).sum()
160/79: with_read_out_price.groupby(['method']).sum()
160/80: total_bids
160/81: bids
160/82: no_read_out_price
160/83: bids.head()
160/84: bids = bids[bids['read_out_price'] > 0]
160/85: bids.head()
160/86: bids.describe()
160/87: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
160/88: bids.head()
160/89: bids.tail()
160/90:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
160/91: bids.groupby(['method']).sum()
160/92:
# import the file

av_bids = pd.read_csv('')
160/93: bids.groupby(['type']).sum()
160/94:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
160/95: av_bids.describe()
160/96: av_bids.value_counts()
160/97: av_bids.value_counts
160/98: av_bids.value_count
160/99: av_bids.value_count()
160/100: av_bids['ppdeId']value_count()
160/101: av_bids['ppdeId']value_counts()
160/102: av_bids['ppdeId']value_counts
160/103: av_bids['ppdeId']value_count
160/104: av_bids['ppdeId'].value_count
160/105: av_bids['ppeId'].value_count
160/106: av_bids['ppeId'].value_counts
160/107: av_bids['ppeId'].value_counts()
160/108: av_bids['ppeId'].sum(()
160/109: av_bids['ppeId'].sum()
160/110: av_bids['ppeId'].value_counts().sum()
160/111: av_bids.groupby(['method'])
160/112: av_bids.groupby(['method']).sum()
160/113:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method.head()
160/114:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
160/115: total_bids
162/1:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
162/2:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
162/3: bids.describe()
162/4: bids[bids['read_out_price'] < 1]
162/5: (4270 / 10739) * 100
162/6: bids.describe()
162/7: total_bids = bids['no_of_bids'].sum()
162/8: no_read_out_price = bids[bids['read_out_price'] < 1]
162/9: no_read_out_price.describe()
162/10: no_read_out_price['no_of_bids'].sum()
162/11: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/12:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
162/13: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/14: with_read_out_price['no_of_bids'].sum()
162/15: bids['read_out_price'].plot(kind='hist');
162/16:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
162/17: no_read_out_price.groupby(['method']).sum()
162/18: with_read_out_price.groupby(['method']).sum()
162/19: total_bids
162/20: bids
162/21: no_read_out_price
162/22: bids.head()
162/23: bids = bids[bids['read_out_price'] > 0]
162/24: bids.head()
162/25: bids.describe()
162/26: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
162/27: bids.head()
162/28: bids.tail()
162/29:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
162/30: bids.groupby(['method']).sum()
162/31: bids.groupby(['type']).sum()
162/32:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
162/33: av_bids.describe()
162/34: av_bids['ppeId'].value_counts().sum()
162/35: av_bids.groupby(['method']).sum()
162/36:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method.head()
162/37:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
162/38: total_bids
162/39:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
162/40:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
162/41:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
162/42: bids.describe()
162/43: bids[bids['read_out_price'] < 1]
162/44: (4270 / 10739) * 100
162/45: bids.describe()
162/46: total_bids = bids['no_of_bids'].sum()
162/47: no_read_out_price = bids[bids['read_out_price'] < 1]
162/48: no_read_out_price.describe()
162/49: no_read_out_price['no_of_bids'].sum()
162/50: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/51:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
162/52: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/53: with_read_out_price['no_of_bids'].sum()
162/54: bids['read_out_price'].plot(kind='hist');
162/55:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
162/56: no_read_out_price.groupby(['method']).sum()
162/57: with_read_out_price.groupby(['method']).sum()
162/58: total_bids
162/59: bids
162/60: no_read_out_price
162/61: bids.head()
162/62: bids = bids[bids['read_out_price'] > 0]
162/63: bids.head()
162/64: bids.describe()
162/65: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
162/66: bids.head()
162/67: bids.tail()
162/68:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
162/69: bids.groupby(['method']).sum()
162/70: bids.groupby(['type']).sum()
162/71:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
162/72: av_bids.describe()
162/73: av_bids['ppeId'].value_counts().sum()
162/74: av_bids.groupby(['method']).sum()
162/75:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
162/76:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
162/77: total_bids
162/78: total_bids['average_number_of_bids'] =
162/79:

bids_by_method['average_number_of_bids'] = ((bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts']
) * 100)
162/80: bids_by_method
162/81:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
162/82:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts']) * 100
162/83: bids_by_method
162/84:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
162/85: bids_by_method['average_number_of_bids'] = bids_by_method['average_number_of_bids'] * 100
162/86: bids_by_method
162/87: bids_by_method.describe()
162/88: bids_by_method.dtypes()
162/89: bids_by_method.['average_number_of_bids']dtypes()
162/90: bids_by_method['average_number_of_bids']dtypes()
162/91: bids_by_method['average_number_of_bids']dtypes
162/92: bids_by_method['average_number_of_bids'].dtypes
162/93: bids_by_method['average_number_of_bids'] = bids_by_method['average_number_of_bids'] / 100
162/94: bids_by_method['average_number_of_bids'].dtypes
162/95: bids_by_method
162/96:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
162/97:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
162/98: bids.describe()
162/99: bids[bids['read_out_price'] < 1]
162/100: (4270 / 10739) * 100
162/101: bids.describe()
162/102: total_bids = bids['no_of_bids'].sum()
162/103: no_read_out_price = bids[bids['read_out_price'] < 1]
162/104: no_read_out_price.describe()
162/105: no_read_out_price['no_of_bids'].sum()
162/106: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/107:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
162/108: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/109: with_read_out_price['no_of_bids'].sum()
162/110: bids['read_out_price'].plot(kind='hist');
162/111:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
162/112: no_read_out_price.groupby(['method']).sum()
162/113: with_read_out_price.groupby(['method']).sum()
162/114: total_bids
162/115: bids
162/116: no_read_out_price
162/117: bids.head()
162/118: bids = bids[bids['read_out_price'] > 0]
162/119: bids.head()
162/120: bids.describe()
162/121: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
162/122: bids.head()
162/123: bids.tail()
162/124:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
162/125: bids.groupby(['method']).sum()
162/126: bids.groupby(['type']).sum()
162/127:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
162/128: av_bids.describe()
162/129: av_bids['ppeId'].value_counts().sum()
162/130: av_bids.groupby(['method']).sum()
162/131:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
162/132:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
162/133: total_bids
162/134:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
162/135: bids_by_method['average_number_of_bids'] = bids_by_method['average_number_of_bids'] * 100
162/136: bids_by_method['average_number_of_bids'].dtypes
162/137: bids_by_method
162/138:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
162/139:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
162/140: bids.describe()
162/141: bids[bids['read_out_price'] < 1]
162/142: (4270 / 10739) * 100
162/143: bids.describe()
162/144: total_bids = bids['no_of_bids'].sum()
162/145: no_read_out_price = bids[bids['read_out_price'] < 1]
162/146: no_read_out_price.describe()
162/147: no_read_out_price['no_of_bids'].sum()
162/148: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/149:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
162/150: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/151: with_read_out_price['no_of_bids'].sum()
162/152: bids['read_out_price'].plot(kind='hist');
162/153:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
162/154: no_read_out_price.groupby(['method']).sum()
162/155: with_read_out_price.groupby(['method']).sum()
162/156: total_bids
162/157: bids
162/158: no_read_out_price
162/159: bids.head()
162/160: bids = bids[bids['read_out_price'] > 0]
162/161: bids.head()
162/162: bids.describe()
162/163: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
162/164: bids.head()
162/165: bids.tail()
162/166:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
162/167: bids.groupby(['method']).sum()
162/168: bids.groupby(['type']).sum()
162/169:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
162/170: av_bids.describe()
162/171: av_bids['ppeId'].value_counts().sum()
162/172: av_bids.groupby(['method']).sum()
162/173:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
162/174:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
162/175: total_bids
162/176:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
162/177: # bids_by_method['average_number_of_bids'] = bids_by_method['average_number_of_bids'] * 100
162/178: bids_by_method['average_number_of_bids'].dtypes
162/179: bids_by_method
162/180:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
162/181:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
162/182: bids.describe()
162/183: bids[bids['read_out_price'] < 1]
162/184: (4270 / 10739) * 100
162/185: bids.describe()
162/186: total_bids = bids['no_of_bids'].sum()
162/187: no_read_out_price = bids[bids['read_out_price'] < 1]
162/188: no_read_out_price.describe()
162/189: no_read_out_price['no_of_bids'].sum()
162/190: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/191:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
162/192: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/193: with_read_out_price['no_of_bids'].sum()
162/194: bids['read_out_price'].plot(kind='hist');
162/195:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
162/196: no_read_out_price.groupby(['method']).sum()
162/197: with_read_out_price.groupby(['method']).sum()
162/198: total_bids
162/199: bids
162/200: no_read_out_price
162/201: bids.head()
162/202: bids = bids[bids['read_out_price'] > 0]
162/203: bids.head()
162/204: bids.describe()
162/205: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
162/206: bids.head()
162/207: bids.tail()
162/208:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
162/209: bids.groupby(['method']).sum()
162/210: bids.groupby(['type']).sum()
162/211:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
162/212: av_bids.describe()
162/213: av_bids['ppeId'].value_counts().sum()
162/214: av_bids.groupby(['method']).sum()
162/215:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
162/216:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
162/217: total_bids
162/218:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
162/219: bids_by_method['average_number_of_bids'].dtypes
162/220: bids_by_method
162/221: bids_by_method.to_csv('bids_by_method.csv', index=False)
162/222:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
162/223:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
162/224: bids.describe()
162/225: bids[bids['read_out_price'] < 1]
162/226: (4270 / 10739) * 100
162/227: bids.describe()
162/228: total_bids = bids['no_of_bids'].sum()
162/229: no_read_out_price = bids[bids['read_out_price'] < 1]
162/230: no_read_out_price.describe()
162/231: no_read_out_price['no_of_bids'].sum()
162/232: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/233:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
162/234: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
162/235: with_read_out_price['no_of_bids'].sum()
162/236: bids['read_out_price'].plot(kind='hist');
162/237:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
162/238: no_read_out_price.groupby(['method']).sum()
162/239: with_read_out_price.groupby(['method']).sum()
162/240: total_bids
162/241: bids
162/242: no_read_out_price
162/243: bids.head()
162/244: bids = bids[bids['read_out_price'] > 0]
162/245: bids.head()
162/246: bids.describe()
162/247: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
162/248: bids.head()
162/249: bids.tail()
162/250:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
162/251: bids.groupby(['method']).sum()
162/252: bids.groupby(['type']).sum()
162/253:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
162/254: av_bids.describe()
162/255: av_bids['ppeId'].value_counts().sum()
162/256: av_bids.groupby(['method']).sum()
162/257:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
162/258:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
162/259: total_bids
162/260:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
162/261: bids_by_method['average_number_of_bids'].dtypes
162/262: bids_by_method.to_csv('bids_by_method.csv', index=False)
162/263: av_bids.groupby(['type']).sum()
162/264:
av_bids.groupby(['type']).sum()

av_bids.to_csv('av_bids')
162/265:
av_bids.groupby(['type']).sum()

av_bids.to_csv('av_bids.csv')
162/266:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
163/1: awarded_contracts['contract_status'] = np.where(awarded_contracts['difference'] > 0, 'Savings', 'Losses')
163/2:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
163/3:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
163/4:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
163/5: awarded_contracts.info()
163/6: awarded_contracts.tail()
163/7: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
163/8: open_method.info()
163/9: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
163/10: animals
163/11: animals.loc[3]
163/12: animals.loc[9]
163/13: awarded_contracts
163/14: awarded_contracts.loc[3]
163/15: awarded_contracts.iloc[3]
163/16: awarded_contracts.iloc[:3]
163/17: awarded_contracts.loc[:3]
163/18: awarded_contracts["subject_of_procurement"]
163/19: awarded_contracts.subject_of_procurement
163/20: awarded_contracts[awarded_contracts['type'] == 'Supplies']
163/21: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
163/22: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
163/23: awarded_contracts.head()
163/24:
awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

# awarded_contracts.to_excel('all_awarded_contracts_FY_2019-2020.xlxs')
163/25:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed.csv')
163/26: awarded_contracts_method_changed.sample(frac=.5)
163/27: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
163/28: awarded_contracts.groupby(["type"]).mean()
163/29: awarded_contracts.groupby(["method"]).mean()
163/30: awarded_contracts["contract_price"].plot()
163/31: awarded_contracts["award_price"].hist()
163/32: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
163/33: awarded_contracts
163/34: awarded_contracts["difference"].hist()
163/35: awarded_contracts.sample(frac=1)
163/36: awarded_contracts.dtypes
163/37: awarded_contracts['contract_status'] = np.where(awarded_contracts['difference'] > 0, 'Savings', 'Losses')
163/38: awarded_contracts.sample(frac=.7)
163/39: awarded_contracts.sample(frac=.8)
163/40: awarded_contracts.sample(frac=.1)
163/41: awarded_contracts['contract_status'] = np.where(awarded_contracts['difference'] >= 0, 'Savings', 'Losses')
163/42: awarded_contracts.sample(frac=.1)
163/43: awarded_contracts.to_excel('awarded_contracts.xlsx')
163/44: awarded_contracts.to_excel('awarded_contracts.xlsx', index=False)
163/45:
awarded_contracts.to_excel('awarded_contracts.xlsx', index=False)
awarded_contracts.to_csv('awarded_contracts.csv', index=False)
163/46:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
163/47:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
163/48:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
163/49: awarded_contracts.info()
163/50: awarded_contracts.tail()
163/51: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
163/52: open_method.info()
163/53: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
163/54: animals
163/55: animals.loc[3]
163/56: animals.loc[9]
163/57: awarded_contracts
163/58: awarded_contracts.loc[3]
163/59: awarded_contracts.iloc[3]
163/60: awarded_contracts.iloc[:3]
163/61: awarded_contracts.loc[:3]
163/62: awarded_contracts["subject_of_procurement"]
163/63: awarded_contracts.subject_of_procurement
163/64: awarded_contracts[awarded_contracts['type'] == 'Supplies']
163/65: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
163/66: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
163/67: awarded_contracts.head()
163/68:
awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

# awarded_contracts.to_excel('all_awarded_contracts_FY_2019-2020.xlxs')
163/69:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed.csv')
163/70: awarded_contracts_method_changed.sample(frac=.5)
163/71: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
163/72: awarded_contracts.groupby(["type"]).mean()
163/73: awarded_contracts.groupby(["method"]).mean()
163/74: awarded_contracts["contract_price"].plot()
163/75: awarded_contracts["award_price"].hist()
163/76: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
163/77: awarded_contracts
163/78: awarded_contracts["difference"].hist()
163/79: awarded_contracts.sample(frac=1)
163/80: awarded_contracts.dtypes
163/81: awarded_contracts['contract_status'] = np.where(awarded_contracts['difference'] >= 0, 'Savings', 'Losses')
163/82: awarded_contracts.sample(frac=.1)
163/83:
# awarded_contracts.to_excel('awarded_contracts.xlsx', index=False)
# awarded_contracts.to_csv('awarded_contracts.csv', index=False)
167/1: pip list | grep "matplotlib*\|numpy*"
167/2: pip3 list | grep "matplotlib*\|numpy*"
167/3: pip list | grep "matplotlib*\|numpy*"
167/4: pip list | grep "matplotlib*\|numpy*|openpyxl"
167/5: pip list | grep "matplotlib*\|numpy\*|openpyxl"
167/6: pip list | grep "matplotlib*\|numpy\*|openpyxl*"
167/7: pip list | grep "matplotlib*\|numpy*"
169/1:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/2:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/3: bids.describe()
169/4: bids[bids['read_out_price'] < 1]
169/5: (4270 / 10739) * 100
169/6: bids.describe()
169/7: total_bids = bids['no_of_bids'].sum()
169/8: no_read_out_price = bids[bids['read_out_price'] < 1]
169/9: no_read_out_price.describe()
169/10: no_read_out_price['no_of_bids'].sum()
169/11: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/12:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/13: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/14: with_read_out_price['no_of_bids'].sum()
169/15: bids['read_out_price'].plot(kind='hist');
169/16:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/17: no_read_out_price.groupby(['method']).sum()
169/18: with_read_out_price.groupby(['method']).sum()
169/19: total_bids
169/20: bids
169/21: no_read_out_price
169/22: bids.head()
169/23: bids = bids[bids['read_out_price'] > 0]
169/24: bids.head()
169/25: bids.describe()
169/26: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/27: bids.head()
169/28: bids.tail()
169/29:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/30: bids.groupby(['method']).sum()
169/31: bids.groupby(['type']).sum()
169/32:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/33: av_bids.describe()
169/34: av_bids['ppeId'].value_counts().sum()
169/35: av_bids.groupby(['method']).sum()
169/36:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/37:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/38:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/39: total_bids
169/40:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/41: bids_by_method['average_number_of_bids'].dtypes
169/42: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/43:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/44: bids_only.fillna(0)
169/45:     bids_only.sample(frac=.7)
169/46: bids_only.fillna(0, inplace=True)
169/47:     bids_only.sample(frac=.7)
169/48: bids_only.sample(frac=.7)
169/49: bids_only.drop(index, labels="ppeId")
169/50: bids_only.drop(labels="ppeId")
169/51: bids_only.drop(labels=["ppeId"])
169/52: bids_only.drop(labels=["ppeId"], axis=0)
169/53: bids_only.tail()
169/54: bids_only.value_counts(['ppeId'])
169/55: bids_only.value_count(['ppeId'])
169/56: bids_only.values_count(['ppeId'])
169/57: bids_only.values_count
169/58: bids_only.groupBy(['method']).sum()
169/59: bids_only.groupby(['method']).sum()
169/60: bids_only.descibe()
169/61: bids_only.descibe
169/62: bids_only.describe
169/63: bids_only.describe()
169/64: bids_only.groupby(['ppeId']).sum()
169/65: bids_only['totalBids'] = bids_only.groupby(['ppeId']).sum()
169/66: bids_only.groupby(['ppeId']).sum()
169/67: bids_only.groupby(['ppeId'])
169/68: bids_only.groupby(['ppeId']).sum()
169/69: bids_only[bids_only['method'] == 'Open Domestic Bidding']
169/70:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/71: bids_only.groupby(['ppeId']).count()
169/72: bids_only.groupby(['subject_of_procurement']).count()
169/73:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/74: bids_by_subject.tail()
169/75:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/76:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/77: bids.describe()
169/78: bids[bids['read_out_price'] < 1]
169/79: (4270 / 10739) * 100
169/80: bids.describe()
169/81: total_bids = bids['no_of_bids'].sum()
169/82: no_read_out_price = bids[bids['read_out_price'] < 1]
169/83: no_read_out_price.describe()
169/84: no_read_out_price['no_of_bids'].sum()
169/85: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/86:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/87: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/88: with_read_out_price['no_of_bids'].sum()
169/89: bids['read_out_price'].plot(kind='hist');
169/90:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/91: no_read_out_price.groupby(['method']).sum()
169/92: with_read_out_price.groupby(['method']).sum()
169/93: total_bids
169/94: bids
169/95: no_read_out_price
169/96: bids.head()
169/97: bids = bids[bids['read_out_price'] > 0]
169/98: bids.head()
169/99: bids.describe()
169/100: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/101: bids.head()
169/102: bids.tail()
169/103:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/104: bids.groupby(['method']).sum()
169/105: bids.groupby(['type']).sum()
169/106:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/107: av_bids.describe()
169/108: av_bids['ppeId'].value_counts().sum()
169/109: av_bids.groupby(['method']).sum()
169/110:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/111:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/112:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/113: total_bids
169/114:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/115: bids_by_method['average_number_of_bids'].dtypes
169/116: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/117:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/118: bids_only.fillna(0, inplace=True)
169/119: bids_only.sample(frac=.7)
169/120: bids_only.describe()
169/121:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/122: bids_by_subject.tail()
169/123:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/124: framework_bids = bids_only[bids_only['is_framework'] == 1]
169/125: framework_bids.head()
169/126: lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]
169/127: lotted_framework_bids.head()
169/128: framework_bids.describe()
169/129: lotted_framework_bids.describe()
169/130: all_framework_bids = framework_bids.merge(lotted_framework_bids)
169/131: all_framework_bids.describe()
169/132: all_framework_bids = framework_bids.merge(lotted_framework_bids, how='inner')
169/133: all_framework_bids.describe()
169/134: all_framework_bids = framework_bids.merge(lotted_framework_bids, left_on="ppeId")
169/135:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/136:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/137: bids.describe()
169/138: bids[bids['read_out_price'] < 1]
169/139: (4270 / 10739) * 100
169/140: bids.describe()
169/141: total_bids = bids['no_of_bids'].sum()
169/142: no_read_out_price = bids[bids['read_out_price'] < 1]
169/143: no_read_out_price.describe()
169/144: no_read_out_price['no_of_bids'].sum()
169/145: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/146:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/147: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/148: with_read_out_price['no_of_bids'].sum()
169/149: bids['read_out_price'].plot(kind='hist');
169/150:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/151: no_read_out_price.groupby(['method']).sum()
169/152: with_read_out_price.groupby(['method']).sum()
169/153: total_bids
169/154: bids
169/155: no_read_out_price
169/156: bids.head()
169/157: bids = bids[bids['read_out_price'] > 0]
169/158: bids.head()
169/159: bids.describe()
169/160: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/161: bids.head()
169/162: bids.tail()
169/163:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/164: bids.groupby(['method']).sum()
169/165: bids.groupby(['type']).sum()
169/166:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/167: av_bids.describe()
169/168: av_bids['ppeId'].value_counts().sum()
169/169: av_bids.groupby(['method']).sum()
169/170:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/171:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/172:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/173: total_bids
169/174:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/175: bids_by_method['average_number_of_bids'].dtypes
169/176: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/177:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/178: bids_only.fillna(0, inplace=True)
169/179: bids_only.sample(frac=.7)
169/180: framework_bids = bids_only[bids_only['is_framework'] == 1]
169/181: framework_bids.head()
169/182: lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]
169/183: lotted_framework_bids.head()
169/184: all_framework_bids = framework_bids.merge(lotted_framework_bids, left_on="ppeId")
169/185:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/186:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/187: bids.describe()
169/188: bids[bids['read_out_price'] < 1]
169/189: (4270 / 10739) * 100
169/190: bids.describe()
169/191: total_bids = bids['no_of_bids'].sum()
169/192: no_read_out_price = bids[bids['read_out_price'] < 1]
169/193: no_read_out_price.describe()
169/194: no_read_out_price['no_of_bids'].sum()
169/195: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/196:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/197: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/198: with_read_out_price['no_of_bids'].sum()
169/199: bids['read_out_price'].plot(kind='hist');
169/200:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/201: no_read_out_price.groupby(['method']).sum()
169/202: with_read_out_price.groupby(['method']).sum()
169/203: total_bids
169/204: bids
169/205: no_read_out_price
169/206: bids.head()
169/207: bids = bids[bids['read_out_price'] > 0]
169/208: bids.head()
169/209: bids.describe()
169/210: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/211: bids.head()
169/212: bids.tail()
169/213:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/214: bids.groupby(['method']).sum()
169/215: bids.groupby(['type']).sum()
169/216:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/217: av_bids.describe()
169/218: av_bids['ppeId'].value_counts().sum()
169/219: av_bids.groupby(['method']).sum()
169/220:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/221:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/222:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/223: total_bids
169/224:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/225: bids_by_method['average_number_of_bids'].dtypes
169/226: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/227:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/228: bids_only.fillna(0, inplace=True)
169/229: bids_only.sample(frac=.7)
169/230: framework_bids = bids_only[bids_only['is_framework'] == 1]
169/231: framework_bids.head()
169/232: lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]
169/233: lotted_framework_bids.head()
169/234: all_framework_bids.describe()
169/235: bids_only.describe()
169/236:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/237: bids_by_subject.tail()
169/238:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/239:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/240:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/241:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/242:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/243: bids.describe()
169/244: bids[bids['read_out_price'] < 1]
169/245: (4270 / 10739) * 100
169/246: bids.describe()
169/247: total_bids = bids['no_of_bids'].sum()
169/248: no_read_out_price = bids[bids['read_out_price'] < 1]
169/249: no_read_out_price.describe()
169/250: no_read_out_price['no_of_bids'].sum()
169/251: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/252:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/253: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/254: with_read_out_price['no_of_bids'].sum()
169/255: bids['read_out_price'].plot(kind='hist');
169/256:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/257: no_read_out_price.groupby(['method']).sum()
169/258: with_read_out_price.groupby(['method']).sum()
169/259: total_bids
169/260: bids
169/261: no_read_out_price
169/262: bids.head()
169/263: bids = bids[bids['read_out_price'] > 0]
169/264: bids.head()
169/265: bids.describe()
169/266: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/267: bids.head()
169/268: bids.tail()
169/269:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/270: bids.groupby(['method']).sum()
169/271: bids.groupby(['type']).sum()
169/272:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/273: av_bids.describe()
169/274: av_bids['ppeId'].value_counts().sum()
169/275: av_bids.groupby(['method']).sum()
169/276:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/277:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/278:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/279: total_bids
169/280:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/281: bids_by_method['average_number_of_bids'].dtypes
169/282: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/283:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/284: bids_only.fillna(0, inplace=True)
169/285: bids_only.sample(frac=.7)
169/286:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/287: framework_bids.head()
169/288:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/289: lotted_framework_bids.head()
169/290: all_framework_bids.describe()
169/291: bids_only.describe()
169/292:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/293: bids_by_subject.tail()
169/294:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/295:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 1 or bids_only['is_lotted_framework'] == 1]

bids_without_frameworks.head()
169/296:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 1 | bids_only['is_lotted_framework'] == 1]

bids_without_frameworks.head()
169/297:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 1 & bids_only['is_lotted_framework'] == 1]

bids_without_frameworks.head()
169/298:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 1] | bids_only[bids_only['is_lotted_framework'] == 1]

bids_without_frameworks.head()
169/299:
bids_without_frameworks = (bids_only[bids_only['is_framework'] == 1] | bids_only[bids_only['is_lotted_framework'] == 1])

bids_without_frameworks.head()
169/300:
bids_without_frameworks = bids_only.query('is_framework' == 1 | 'is_lotted_framework' == 1)

bids_without_frameworks.head()
169/301: bids_only.describe()
169/302: bids_only.dtypes()
169/303: bids_only.dtypes
169/304:
bids_without_frameworks = bids_only.query('is_framework' == 1 & 'is_lotted_framework' == 1)

bids_without_frameworks.head()
169/305:
bids_without_frameworks = bids_only.query('is_framework' == 1)

bids_without_frameworks.head()
169/306:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 1]

bids_without_frameworks.head()
169/307:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/308:
bids_without_lotted_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework' == 0]]

bids_without_lotted_frameworks.head()
169/309:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework' == 0]]

bids_without_frameworks.head()
169/310:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/311:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/312: bids.describe()
169/313: bids[bids['read_out_price'] < 1]
169/314: (4270 / 10739) * 100
169/315: bids.describe()
169/316: total_bids = bids['no_of_bids'].sum()
169/317: no_read_out_price = bids[bids['read_out_price'] < 1]
169/318: no_read_out_price.describe()
169/319: no_read_out_price['no_of_bids'].sum()
169/320: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/321:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/322: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/323: with_read_out_price['no_of_bids'].sum()
169/324: bids['read_out_price'].plot(kind='hist');
169/325:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/326: no_read_out_price.groupby(['method']).sum()
169/327: with_read_out_price.groupby(['method']).sum()
169/328: total_bids
169/329: bids
169/330: no_read_out_price
169/331: bids.head()
169/332: bids = bids[bids['read_out_price'] > 0]
169/333: bids.head()
169/334: bids.describe()
169/335: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/336: bids.head()
169/337: bids.tail()
169/338:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/339: bids.groupby(['method']).sum()
169/340: bids.groupby(['type']).sum()
169/341:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/342: av_bids.describe()
169/343: av_bids['ppeId'].value_counts().sum()
169/344: av_bids.groupby(['method']).sum()
169/345:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/346:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/347:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/348: total_bids
169/349:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/350: bids_by_method['average_number_of_bids'].dtypes
169/351: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/352:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/353: bids_only.fillna(0, inplace=True)
169/354: bids_only.sample(frac=.7)
169/355:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/356: framework_bids.head()
169/357:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/358: lotted_framework_bids.head()
169/359: all_framework_bids.describe()
169/360: bids_only.describe()
169/361:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/362: bids_only.dtypes
169/363:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/364:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework' == 0]]

bids_without_frameworks.head()
169/365:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/366:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/367:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/368:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/369: bids.describe()
169/370: bids[bids['read_out_price'] < 1]
169/371: (4270 / 10739) * 100
169/372: bids.describe()
169/373: total_bids = bids['no_of_bids'].sum()
169/374: no_read_out_price = bids[bids['read_out_price'] < 1]
169/375: no_read_out_price.describe()
169/376: no_read_out_price['no_of_bids'].sum()
169/377: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/378:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/379: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/380: with_read_out_price['no_of_bids'].sum()
169/381: bids['read_out_price'].plot(kind='hist');
169/382:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/383: no_read_out_price.groupby(['method']).sum()
169/384: with_read_out_price.groupby(['method']).sum()
169/385: total_bids
169/386: bids
169/387: no_read_out_price
169/388: bids.head()
169/389: bids = bids[bids['read_out_price'] > 0]
169/390: bids.head()
169/391: bids.describe()
169/392: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/393: bids.head()
169/394: bids.tail()
169/395:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/396: bids.groupby(['method']).sum()
169/397: bids.groupby(['type']).sum()
169/398:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/399: av_bids.describe()
169/400: av_bids['ppeId'].value_counts().sum()
169/401: av_bids.groupby(['method']).sum()
169/402:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/403:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/404:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/405: total_bids
169/406:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/407: bids_by_method['average_number_of_bids'].dtypes
169/408: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/409:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/410: bids_only.fillna(0, inplace=True)
169/411: bids_only.sample(frac=.7)
169/412:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/413: framework_bids.head()
169/414:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/415: lotted_framework_bids.head()
169/416: all_framework_bids.describe()
169/417: bids_only.describe()
169/418:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/419: bids_only.dtypes
169/420:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/421:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/422: bids_by_subject.tail()
169/423:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/424:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/425:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/426: bids.describe()
169/427: bids[bids['read_out_price'] < 1]
169/428: (4270 / 10739) * 100
169/429: bids.describe()
169/430: total_bids = bids['no_of_bids'].sum()
169/431: no_read_out_price = bids[bids['read_out_price'] < 1]
169/432: no_read_out_price.describe()
169/433: no_read_out_price['no_of_bids'].sum()
169/434: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/435:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/436: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/437: with_read_out_price['no_of_bids'].sum()
169/438: bids['read_out_price'].plot(kind='hist');
169/439:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/440: no_read_out_price.groupby(['method']).sum()
169/441: with_read_out_price.groupby(['method']).sum()
169/442: total_bids
169/443: bids
169/444: no_read_out_price
169/445: bids.head()
169/446: bids = bids[bids['read_out_price'] > 0]
169/447: bids.head()
169/448: bids.describe()
169/449: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/450: bids.head()
169/451: bids.tail()
169/452:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/453: bids.groupby(['method']).sum()
169/454: bids.groupby(['type']).sum()
169/455:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/456: av_bids.describe()
169/457: av_bids['ppeId'].value_counts().sum()
169/458: av_bids.groupby(['method']).sum()
169/459:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/460:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/461:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/462: total_bids
169/463:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/464: bids_by_method['average_number_of_bids'].dtypes
169/465: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/466:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/467: bids_only.fillna(0, inplace=True)
169/468: bids_only.sample(frac=.7)
169/469:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/470: framework_bids.head()
169/471:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/472: lotted_framework_bids.head()
169/473: all_framework_bids.describe()
169/474: bids_only.describe()
169/475:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/476: bids_only.dtypes
169/477:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/478:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/479: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/480: bids_by_subject.tail()
169/481:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/482: bids_only.groupby(['method']).sum()
169/483: bids_without_frameworks.groupby('ppeId').count()
169/484: bids_without_frameworks.groupby('ppeId').count('ppeId')
169/485: bids_without_frameworks.groupby('ppeId').count()
169/486: bids_without_frameworks.groupby(['ppeId', 'method']).count()
169/487: bids_without_frameworks.groupby(['method']).count()
169/488: bids_without_frameworks.groupby(['method']).sum()
169/489:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/490:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/491: bids.describe()
169/492: bids[bids['read_out_price'] < 1]
169/493: (4270 / 10739) * 100
169/494: bids.describe()
169/495: total_bids = bids['no_of_bids'].sum()
169/496: no_read_out_price = bids[bids['read_out_price'] < 1]
169/497: no_read_out_price.describe()
169/498: no_read_out_price['no_of_bids'].sum()
169/499: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/500:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/501: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/502: with_read_out_price['no_of_bids'].sum()
169/503: bids['read_out_price'].plot(kind='hist');
169/504:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/505: no_read_out_price.groupby(['method']).sum()
169/506: with_read_out_price.groupby(['method']).sum()
169/507: total_bids
169/508: bids
169/509: no_read_out_price
169/510: bids.head()
169/511: bids = bids[bids['read_out_price'] > 0]
169/512: bids.head()
169/513: bids.describe()
169/514: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/515: bids.head()
169/516: bids.tail()
169/517:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/518: bids.groupby(['method']).sum()
169/519: bids.groupby(['type']).sum()
169/520:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/521: av_bids.describe()
169/522: av_bids['ppeId'].value_counts().sum()
169/523: av_bids.groupby(['method']).sum()
169/524:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/525:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/526:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/527: total_bids
169/528:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/529: bids_by_method['average_number_of_bids'].dtypes
169/530: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/531:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/532: bids_only.fillna(0, inplace=True)
169/533: bids_only.sample(frac=.7)
169/534: bids_only.groupby(['method']).sum()
169/535:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/536: framework_bids.head()
169/537:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/538: lotted_framework_bids.head()
169/539: all_framework_bids.describe()
169/540: bids_only.describe()
169/541:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/542: bids_only.dtypes
169/543:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/544:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/545: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/546: bids_by_count = bids_without_frameworks.groupby(['method']).count()
169/547: bids_by_sum = bids_without_frameworks.groupby(['method']).sum()
169/548: bids_by_subject.tail()
169/549:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/550: bids_by_count.merge(bids_by_sum, left_on="method")
169/551:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/552:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/553: bids.describe()
169/554: bids[bids['read_out_price'] < 1]
169/555: (4270 / 10739) * 100
169/556: bids.describe()
169/557: total_bids = bids['no_of_bids'].sum()
169/558: no_read_out_price = bids[bids['read_out_price'] < 1]
169/559: no_read_out_price.describe()
169/560: no_read_out_price['no_of_bids'].sum()
169/561: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/562:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/563: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/564: with_read_out_price['no_of_bids'].sum()
169/565: bids['read_out_price'].plot(kind='hist');
169/566:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/567: no_read_out_price.groupby(['method']).sum()
169/568: with_read_out_price.groupby(['method']).sum()
169/569: total_bids
169/570: bids
169/571: no_read_out_price
169/572: bids.head()
169/573: bids = bids[bids['read_out_price'] > 0]
169/574: bids.head()
169/575: bids.describe()
169/576: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/577: bids.head()
169/578: bids.tail()
169/579:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/580: bids.groupby(['method']).sum()
169/581: bids.groupby(['type']).sum()
169/582:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/583: av_bids.describe()
169/584: av_bids['ppeId'].value_counts().sum()
169/585: av_bids.groupby(['method']).sum()
169/586:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/587:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/588:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/589: total_bids
169/590:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/591: bids_by_method['average_number_of_bids'].dtypes
169/592: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/593:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/594: bids_only.fillna(0, inplace=True)
169/595: bids_only.sample(frac=.7)
169/596: bids_only.groupby(['method']).sum()
169/597:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/598: framework_bids.head()
169/599:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/600: lotted_framework_bids.head()
169/601: all_framework_bids.describe()
169/602: bids_only.describe()
169/603:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/604: bids_only.dtypes
169/605:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/606:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/607: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/608: bids_by_count = bids_without_frameworks.groupby(['method']).count()
169/609: bids_by_count
169/610: bids_by_sum = bids_without_frameworks.groupby(['method']).sum()
169/611: bids_by_sum
169/612: bids_by_count.merge(bids_by_sum, left_on="method")
169/613:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/614:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/615: bids.describe()
169/616: bids[bids['read_out_price'] < 1]
169/617: (4270 / 10739) * 100
169/618: bids.describe()
169/619: total_bids = bids['no_of_bids'].sum()
169/620: no_read_out_price = bids[bids['read_out_price'] < 1]
169/621: no_read_out_price.describe()
169/622: no_read_out_price['no_of_bids'].sum()
169/623: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/624:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/625: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/626: with_read_out_price['no_of_bids'].sum()
169/627: bids['read_out_price'].plot(kind='hist');
169/628:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/629: no_read_out_price.groupby(['method']).sum()
169/630: with_read_out_price.groupby(['method']).sum()
169/631: total_bids
169/632: bids
169/633: no_read_out_price
169/634: bids.head()
169/635: bids = bids[bids['read_out_price'] > 0]
169/636: bids.head()
169/637: bids.describe()
169/638: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/639: bids.head()
169/640: bids.tail()
169/641:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/642: bids.groupby(['method']).sum()
169/643: bids.groupby(['type']).sum()
169/644:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/645: av_bids.describe()
169/646: av_bids['ppeId'].value_counts().sum()
169/647: av_bids.groupby(['method']).sum()
169/648:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/649:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/650:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/651: total_bids
169/652:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/653: bids_by_method['average_number_of_bids'].dtypes
169/654: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/655:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/656: bids_only.fillna(0, inplace=True)
169/657: bids_only.sample(frac=.7)
169/658: bids_only.groupby(['method']).sum()
169/659:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/660: framework_bids.head()
169/661:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/662: lotted_framework_bids.head()
169/663: all_framework_bids.describe()
169/664: bids_only.describe()
169/665:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/666: bids_only.dtypes
169/667:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/668:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/669: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/670:
bids_by_count = bids_without_frameworks.groupby(['method']).count()

bids_by_count.to_csv('bids_by_count.csv')
169/671: bids_by_count
169/672:
bids_by_sum = bids_without_frameworks.groupby(['method']).sum()

bids_by_sum.to_csv('bids_by_sum.csv')
169/673: bids_by_sum
169/674: bids_by_count.merge(bids_by_sum, left_on="method")
169/675:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/676:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/677: bids.describe()
169/678: bids[bids['read_out_price'] < 1]
169/679: (4270 / 10739) * 100
169/680: bids.describe()
169/681: total_bids = bids['no_of_bids'].sum()
169/682: no_read_out_price = bids[bids['read_out_price'] < 1]
169/683: no_read_out_price.describe()
169/684: no_read_out_price['no_of_bids'].sum()
169/685: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/686:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/687: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/688: with_read_out_price['no_of_bids'].sum()
169/689: bids['read_out_price'].plot(kind='hist');
169/690:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/691: no_read_out_price.groupby(['method']).sum()
169/692: with_read_out_price.groupby(['method']).sum()
169/693: total_bids
169/694: bids
169/695: no_read_out_price
169/696: bids.head()
169/697: bids = bids[bids['read_out_price'] > 0]
169/698: bids.head()
169/699: bids.describe()
169/700: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/701: bids.head()
169/702: bids.tail()
169/703:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/704: bids.groupby(['method']).sum()
169/705: bids.groupby(['type']).sum()
169/706:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/707: av_bids.describe()
169/708: av_bids['ppeId'].value_counts().sum()
169/709: av_bids.groupby(['method']).sum()
169/710:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/711:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/712:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/713: total_bids
169/714:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/715: bids_by_method['average_number_of_bids'].dtypes
169/716: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/717:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/718: bids_only.fillna(0, inplace=True)
169/719: bids_only.sample(frac=.7)
169/720: bids_only.groupby(['method']).sum()
169/721:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/722:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/723:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/724: bids.describe()
169/725: bids[bids['read_out_price'] < 1]
169/726: (4270 / 10739) * 100
169/727: bids.describe()
169/728: total_bids = bids['no_of_bids'].sum()
169/729: no_read_out_price = bids[bids['read_out_price'] < 1]
169/730: no_read_out_price.describe()
169/731: no_read_out_price['no_of_bids'].sum()
169/732: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/733:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/734: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/735: with_read_out_price['no_of_bids'].sum()
169/736: bids['read_out_price'].plot(kind='hist');
169/737:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/738: no_read_out_price.groupby(['method']).sum()
169/739: with_read_out_price.groupby(['method']).sum()
169/740: total_bids
169/741: bids
169/742: no_read_out_price
169/743: bids.head()
169/744: bids = bids[bids['read_out_price'] > 0]
169/745: bids.head()
169/746: bids.describe()
169/747: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/748: bids.head()
169/749: bids.tail()
169/750:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/751: bids.groupby(['method']).sum()
169/752: bids.groupby(['type']).sum()
169/753:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/754: av_bids.describe()
169/755: av_bids['ppeId'].value_counts().sum()
169/756: av_bids.groupby(['method']).sum()
169/757:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/758:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/759:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/760: total_bids
169/761:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/762: bids_by_method['average_number_of_bids'].dtypes
169/763: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/764:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/765: bids_only.fillna(0, inplace=True)
169/766: bids_only.sample(frac=.7)
169/767: bids_only.groupby(['method']).sum()
169/768:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/769:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/770:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/771:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/772: bids.describe()
169/773: bids[bids['read_out_price'] < 1]
169/774: (4270 / 10739) * 100
169/775: bids.describe()
169/776: total_bids = bids['no_of_bids'].sum()
169/777: no_read_out_price = bids[bids['read_out_price'] < 1]
169/778: no_read_out_price.describe()
169/779: no_read_out_price['no_of_bids'].sum()
169/780: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/781:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/782: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/783: with_read_out_price['no_of_bids'].sum()
169/784: bids['read_out_price'].plot(kind='hist');
169/785:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/786: no_read_out_price.groupby(['method']).sum()
169/787: with_read_out_price.groupby(['method']).sum()
169/788: total_bids
169/789: bids
169/790: no_read_out_price
169/791: bids.head()
169/792: bids = bids[bids['read_out_price'] > 0]
169/793: bids.head()
169/794: bids.describe()
169/795: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/796: bids.head()
169/797: bids.tail()
169/798:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/799: bids.groupby(['method']).sum()
169/800: bids.groupby(['type']).sum()
169/801:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/802: av_bids.describe()
169/803: av_bids['ppeId'].value_counts().sum()
169/804: av_bids.groupby(['method']).sum()
169/805:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/806:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/807:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/808: total_bids
169/809:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/810: bids_by_method['average_number_of_bids'].dtypes
169/811: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/812:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/813: bids_only.fillna(0, inplace=True)
169/814: bids_only.sample(frac=.7)
169/815: bids_only.groupby(['method']).sum()
169/816:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/817: framework_bids.head()
169/818:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/819: lotted_framework_bids.head()
169/820: all_framework_bids.describe()
169/821: bids_only.describe()
169/822:
bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

bids_by_subject.to_csv('bids_by_subject.csv')

bids_by_subject = pd.read_csv('bids_by_subject.csv')

bids_by_subject.head()
169/823: framework_bids.merge(lotted_framework, how="left" on=method)
169/824: framework_bids.merge(lotted_framework, how="left" on="method")
169/825: framework_bids.merge(lotted_framework, how="left" on="method")
169/826: framework_bids.merge(lotted_framework, how="left" on=["method"])
169/827: framework_bids.merge(lotted_framework, how="left")
169/828: framework_bids.merge(lotted_framework_bids, how="left")
169/829: framework_bids.merge(lotted_framework_bids, how="left").groupby(['method'])
169/830: framework_bids.merge(lotted_framework_bids, how="left").groupby(['method']).sum()
169/831:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/832:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/833: bids.describe()
169/834: bids[bids['read_out_price'] < 1]
169/835: (4270 / 10739) * 100
169/836: bids.describe()
169/837: total_bids = bids['no_of_bids'].sum()
169/838: no_read_out_price = bids[bids['read_out_price'] < 1]
169/839: no_read_out_price.describe()
169/840: no_read_out_price['no_of_bids'].sum()
169/841: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/842:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/843: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/844: with_read_out_price['no_of_bids'].sum()
169/845: bids['read_out_price'].plot(kind='hist');
169/846:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/847: no_read_out_price.groupby(['method']).sum()
169/848: with_read_out_price.groupby(['method']).sum()
169/849: total_bids
169/850: bids
169/851: no_read_out_price
169/852: bids.head()
169/853: bids = bids[bids['read_out_price'] > 0]
169/854: bids.head()
169/855: bids.describe()
169/856: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/857: bids.head()
169/858: bids.tail()
169/859:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/860: bids.groupby(['method']).sum()
169/861: bids.groupby(['type']).sum()
169/862:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/863: av_bids.describe()
169/864: av_bids['ppeId'].value_counts().sum()
169/865: av_bids.groupby(['method']).sum()
169/866:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/867:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/868:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/869: total_bids
169/870:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/871: bids_by_method['average_number_of_bids'].dtypes
169/872: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/873:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/874: bids_only.fillna(0, inplace=True)
169/875: bids_only.sample(frac=.7)
169/876: bids_only.groupby(['method']).sum()
169/877:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/878: framework_bids.head()
169/879:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/880: lotted_framework_bids.head()
169/881: combined_framework = framework_bids.merge(lotted_framework_bids, how="left")
169/882: bids_only.describe()
169/883:
# bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

# bids_by_subject.to_csv('bids_by_subject.csv')

# bids_by_subject = pd.read_csv('bids_by_subject.csv')

# bids_by_subject.head()
169/884: bids_only.dtypes
169/885:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/886:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/887: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/888:
bids_by_count = bids_without_frameworks.groupby(['method']).count()

bids_by_count.to_csv('bids_by_count.csv')
169/889: bids_by_count
169/890:
bids_by_sum = bids_without_frameworks.groupby(['method']).sum()

bids_by_sum.to_csv('bids_by_sum.csv')
169/891: bids_by_sum
169/892: bids_by_count.merge(bids_by_sum, left_on="method")
169/893:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/894:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/895: bids.describe()
169/896: bids[bids['read_out_price'] < 1]
169/897: (4270 / 10739) * 100
169/898: bids.describe()
169/899: total_bids = bids['no_of_bids'].sum()
169/900: no_read_out_price = bids[bids['read_out_price'] < 1]
169/901: no_read_out_price.describe()
169/902: no_read_out_price['no_of_bids'].sum()
169/903: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/904:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/905: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/906: with_read_out_price['no_of_bids'].sum()
169/907: bids['read_out_price'].plot(kind='hist');
169/908:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/909: no_read_out_price.groupby(['method']).sum()
169/910: with_read_out_price.groupby(['method']).sum()
169/911: total_bids
169/912: bids
169/913: no_read_out_price
169/914: bids.head()
169/915: bids = bids[bids['read_out_price'] > 0]
169/916: bids.head()
169/917: bids.describe()
169/918: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/919: bids.head()
169/920: bids.tail()
169/921:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/922: bids.groupby(['method']).sum()
169/923: bids.groupby(['type']).sum()
169/924:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/925: av_bids.describe()
169/926: av_bids['ppeId'].value_counts().sum()
169/927: av_bids.groupby(['method']).sum()
169/928:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/929:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/930:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/931: total_bids
169/932:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/933: bids_by_method['average_number_of_bids'].dtypes
169/934: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/935:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/936: bids_only.fillna(0, inplace=True)
169/937: bids_only.sample(frac=.7)
169/938: bids_only.groupby(['method']).sum()
169/939:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/940: framework_bids.head()
169/941:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/942: lotted_framework_bids.head()
169/943: combined_framework = framework_bids.merge(lotted_framework_bids, how="left")
169/944: bids_only.describe()
169/945:
# bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

# bids_by_subject.to_csv('bids_by_subject.csv')

# bids_by_subject = pd.read_csv('bids_by_subject.csv')

# bids_by_subject.head()
169/946: bids_only.dtypes
169/947:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/948:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/949: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/950:
bids_by_count = bids_without_frameworks.groupby(['method']).count()

bids_by_count.to_csv('bids_by_count.csv')
169/951: bids_by_count
169/952:
bids_by_sum = bids_without_frameworks.groupby(['method']).sum()

bids_by_sum.to_csv('bids_by_sum.csv')
169/953: bids_by_sum
169/954: # bids_by_count.merge(bids_by_sum, left_on="method")
169/955: bids_by_subject.tail()
169/956:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/957:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/958:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/959: bids.describe()
169/960: bids[bids['read_out_price'] < 1]
169/961: (4270 / 10739) * 100
169/962: bids.describe()
169/963: total_bids = bids['no_of_bids'].sum()
169/964: no_read_out_price = bids[bids['read_out_price'] < 1]
169/965: no_read_out_price.describe()
169/966: no_read_out_price['no_of_bids'].sum()
169/967: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/968:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/969: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/970: with_read_out_price['no_of_bids'].sum()
169/971: bids['read_out_price'].plot(kind='hist');
169/972:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/973: no_read_out_price.groupby(['method']).sum()
169/974: with_read_out_price.groupby(['method']).sum()
169/975: total_bids
169/976: bids
169/977: no_read_out_price
169/978: bids.head()
169/979: bids = bids[bids['read_out_price'] > 0]
169/980: bids.head()
169/981: bids.describe()
169/982: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/983: bids.head()
169/984: bids.tail()
169/985:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/986: bids.groupby(['method']).sum()
169/987: bids.groupby(['type']).sum()
169/988:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/989: av_bids.describe()
169/990: av_bids['ppeId'].value_counts().sum()
169/991: av_bids.groupby(['method']).sum()
169/992:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/993:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/994:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/995: total_bids
169/996:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/997: bids_by_method['average_number_of_bids'].dtypes
169/998: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/999:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/1000: bids_only.fillna(0, inplace=True)
169/1001: bids_only.sample(frac=.7)
169/1002: bids_only.groupby(['method']).sum()
169/1003:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/1004: framework_bids.head()
169/1005:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/1006: lotted_framework_bids.head()
169/1007: combined_framework = framework_bids.merge(lotted_framework_bids, how="left")
169/1008: bids_only.describe()
169/1009:
# bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

# bids_by_subject.to_csv('bids_by_subject.csv')

# bids_by_subject = pd.read_csv('bids_by_subject.csv')

# bids_by_subject.head()
169/1010: bids_only.dtypes
169/1011:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/1012:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/1013: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/1014:
bids_by_count = bids_without_frameworks.groupby(['method']).count()

bids_by_count.to_csv('bids_by_count.csv')
169/1015: bids_by_count
169/1016:
bids_by_sum = bids_without_frameworks.groupby(['method']).sum()

bids_by_sum.to_csv('bids_by_sum.csv')
169/1017: bids_by_sum
169/1018: # bids_by_count.merge(bids_by_sum, left_on="method")
169/1019: bids_by_subject.tail()
169/1020:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
169/1021:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/1022:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/1023: bids.describe()
169/1024: bids[bids['read_out_price'] < 1]
169/1025: (4270 / 10739) * 100
169/1026: bids.describe()
169/1027: total_bids = bids['no_of_bids'].sum()
169/1028: no_read_out_price = bids[bids['read_out_price'] < 1]
169/1029: no_read_out_price.describe()
169/1030: no_read_out_price['no_of_bids'].sum()
169/1031: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/1032:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/1033: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/1034: with_read_out_price['no_of_bids'].sum()
169/1035: bids['read_out_price'].plot(kind='hist');
169/1036:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/1037: no_read_out_price.groupby(['method']).sum()
169/1038: with_read_out_price.groupby(['method']).sum()
169/1039: total_bids
169/1040: bids
169/1041: no_read_out_price
169/1042: bids.head()
169/1043: bids = bids[bids['read_out_price'] > 0]
169/1044: bids.head()
169/1045: bids.describe()
169/1046: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/1047: bids.head()
169/1048: bids.tail()
169/1049:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/1050: bids.groupby(['method']).sum()
169/1051: bids.groupby(['type']).sum()
169/1052:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/1053: av_bids.describe()
169/1054: av_bids['ppeId'].value_counts().sum()
169/1055: av_bids.groupby(['method']).sum()
169/1056:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/1057:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/1058:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/1059: total_bids
169/1060:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/1061: bids_by_method['average_number_of_bids'].dtypes
169/1062: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/1063:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/1064: bids_only.fillna(0, inplace=True)
169/1065: bids_only.sample(frac=.7)
169/1066: bids_only.groupby(['method']).sum()
169/1067:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/1068: framework_bids.head()
169/1069:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/1070: lotted_framework_bids.head()
169/1071: combined_framework = framework_bids.merge(lotted_framework_bids, how="left")
169/1072: bids_only.describe()
169/1073:
# bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

# bids_by_subject.to_csv('bids_by_subject.csv')

# bids_by_subject = pd.read_csv('bids_by_subject.csv')

# bids_by_subject.head()
169/1074: bids_only.dtypes
169/1075:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/1076:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/1077: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/1078:
bids_by_count = bids_without_frameworks.groupby(['method']).count()

bids_by_count.to_csv('bids_by_count.csv')
169/1079: bids_by_count
169/1080:
bids_by_sum = bids_without_frameworks.groupby(['method']).sum()

bids_by_sum.to_csv('bids_by_sum.csv')
169/1081: bids_by_sum
169/1082: bids_by_count.merge(bids_by_sum, left_on="method")
169/1083:
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
169/1084:
# import the file
bids = pd.read_csv('bids.csv')

bids.head()
169/1085: bids.describe()
169/1086: bids[bids['read_out_price'] < 1]
169/1087: (4270 / 10739) * 100
169/1088: bids.describe()
169/1089: total_bids = bids['no_of_bids'].sum()
169/1090: no_read_out_price = bids[bids['read_out_price'] < 1]
169/1091: no_read_out_price.describe()
169/1092: no_read_out_price['no_of_bids'].sum()
169/1093: (no_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/1094:
with_read_out_price = bids[bids['read_out_price'] >= 1]

with_read_out_price.describe()
169/1095: (with_read_out_price['no_of_bids'].sum() / total_bids) * 100
169/1096: with_read_out_price['no_of_bids'].sum()
169/1097: bids['read_out_price'].plot(kind='hist');
169/1098:
read_out_prices = {
    "with": with_read_out_price['no_of_bids'].sum(),
    "without read out price": no_read_out_price['no_of_bids'].sum()
}

fig, ax = plt.subplots()
ax.bar(read_out_prices.keys(), height=read_out_prices.values())
ax.set(title="Read Out Prices",
       ylabel="Prices");
169/1099: no_read_out_price.groupby(['method']).sum()
169/1100: with_read_out_price.groupby(['method']).sum()
169/1101: total_bids
169/1102: bids
169/1103: no_read_out_price
169/1104: bids.head()
169/1105: bids = bids[bids['read_out_price'] > 0]
169/1106: bids.head()
169/1107: bids.describe()
169/1108: bids['average_read_out_price'] = bids['read_out_price'] / bids['no_of_bids']
169/1109: bids.head()
169/1110: bids.tail()
169/1111:

bids.to_csv('bids_average_read_out_price.csv')

bids.to_excel('bids.xlsx', index=False)
169/1112: bids.groupby(['method']).sum()
169/1113: bids.groupby(['type']).sum()
169/1114:
# import the file

av_bids = pd.read_csv('av_bids.csv')

av_bids.head()
169/1115: av_bids.describe()
169/1116: av_bids['ppeId'].value_counts().sum()
169/1117: av_bids.groupby(['method']).sum()
169/1118:
bids_by_type = av_bids.groupby(['type']).sum()

bids_by_type.to_csv('bids_by_type.csv')
169/1119:
# Import the file for bids by method

bids_by_method = pd.read_csv('bids_by_method.csv')

bids_by_method
169/1120:
# Calculate the average number of bids

total_bids = bids_by_method['totalBidsRecieved'].sum()
169/1121: total_bids
169/1122:

bids_by_method['average_number_of_bids'] = (bids_by_method['totalBidsRecieved'] / bids_by_method['awardedContracts'])
169/1123: bids_by_method['average_number_of_bids'].dtypes
169/1124: bids_by_method.to_csv('bids_by_method.csv', index=False)
169/1125:
bids_only = pd.read_csv('bids_only.csv')

bids_only.head()
169/1126: bids_only.fillna(0, inplace=True)
169/1127: bids_only.sample(frac=.7)
169/1128: bids_only.groupby(['method']).sum()
169/1129:
framework_bids = bids_only[bids_only['is_framework'] == 1]

framework_bids.to_csv('framework_bids.csv')
169/1130: framework_bids.head()
169/1131:
lotted_framework_bids = bids_only[bids_only['is_lotted_framework'] == 1]

lotted_framework_bids.to_csv('lotted_framework_bids.csv')
169/1132: lotted_framework_bids.head()
169/1133: combined_framework = framework_bids.merge(lotted_framework_bids, how="left")
169/1134: bids_only.describe()
169/1135:
# bids_by_subject = bids_only.groupby(['subject_of_procurement']).count()

# bids_by_subject.to_csv('bids_by_subject.csv')

# bids_by_subject = pd.read_csv('bids_by_subject.csv')

# bids_by_subject.head()
169/1136: bids_only.dtypes
169/1137:
bids_without_frameworks = bids_only[bids_only['is_framework'] == 0]

bids_without_frameworks.head()
169/1138:
bids_without_frameworks = bids_without_frameworks[bids_without_frameworks['is_lotted_framework'] == 0]

bids_without_frameworks.head()
169/1139: bids_without_frameworks.to_csv('bids_without_frameworks.csv')
169/1140:
bids_by_count = bids_without_frameworks.groupby(['method']).count()

bids_by_count.to_csv('bids_by_count.csv')
169/1141: bids_by_count
169/1142:
bids_by_sum = bids_without_frameworks.groupby(['method']).sum()

bids_by_sum.to_csv('bids_by_sum.csv')
169/1143: bids_by_sum
169/1144: # bids_by_count.merge(bids_by_sum, left_on="method")
169/1145: bids_by_subject.tail()
169/1146:
open_bids = bids_only[bids_only['method'] == 'Open Domestic Bidding']

open_bids.to_csv('open_bids.csv')
168/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
168/2:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabular
168/3:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
168/4:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
168/5:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
168/6:
ed1996 = pd.read_pdf('Const_Res-tab_1996.pdf')

ed1996.head()
168/7:
pd.test
# ed1996 = pd.read_pdf('Const_Res-tab_1996.pdf')

# ed1996.head()
168/8:
ed1996 = pd.read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/9:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
from tabula import wrapper
168/10:
ed1996 = wrapper.read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/11:
ed1996 = wrapper.read_pdf('Const_Res-tab_1996')

ed1996
168/12:
ed1996 = wrapper.read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/13:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
168/14:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/15:
ed1996 = read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/16:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
# import tabula
from tabula import read_pdf
168/17:
ed1996 = read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/18:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
168/19:
from tabula import read_pdf

ed1996 = read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/20:
from tabula import read_pdf

ed1996 = read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/21:
ed1996 = tabula.read_pdf_with_template('Const_Res-tab_1996.pdf')

ed1996
168/22: ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf')
168/23:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/24:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
168/25:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/26:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf')

ed1996.tail()
168/27:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf')

ed1996
168/28:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', index=None)

ed1996
168/29:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', header=False)

ed1996
168/30:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', header=None)

ed1996
168/31:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True)

ed1996
168/32:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True)

ed1996.tail()
168/33:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True)

ed1996.tail()
168/34:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages=11)

ed1996.tail()
168/35:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages=11)

ed1996
168/36:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages=11)

ed1996.head()
168/37:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages=11)

ed1996
168/38:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages=11)

ed1996[0]
168/39:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages=11)

ed1996
168/40:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages=11)

len(ed1996)
168/41: ed1996
168/42:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages='1-11')

len(ed1996)
168/43: ed1996
171/1: timelines2020.head()
171/2: - **Procurements Where the actual implementation period is greater than the maximun indicative time**
171/3:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
171/4:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
171/5:
# Check the types of the dataset
timelines.dtypes
171/6:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
171/7: timelines.head()
171/8: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
171/9: timelines2020.info()
171/10: timelines2020
171/11:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
171/12: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
171/13: timelines2020.head()
171/14: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
171/15: timelines2020
171/16: timelines2020.sample(frac=.7)
171/17: timelines2020.info()
171/18:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
171/19: timelines2020
171/20: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
171/21: timelines2020.sample(frac=.5)
171/22: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
171/23: timelines2020.sample(frac=.5)
171/24: timelines2020.sample(frac=.5)
171/25: timelines2020.dtypes
171/26:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
171/27: timelines2020.info()
171/28:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
171/29: timelines2020.sample(frac=.5)
171/30: timelines2020.to_csv('timeliness-in-days.csv', index=False)
171/31:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
171/32: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
171/33: timelines2020.info()
171/34: timelines2020.groupby(['method']).mean()
171/35: timelines2020.groupby(['type']).mean()
171/36: # Import the file
171/37:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
171/38: leadtime.info()
171/39:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
171/40:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
171/41:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
171/42:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
171/43: leadtime.head()
171/44: leadtime.sample(frac=.5)
171/45: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
171/46: leadtimeOpen.info()
171/47: leadtimeOpen
171/48:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
171/49: sortedleadtime = leadtime.sort_values(by=['contract_price'])
171/50: sortedleadtime.to_csv('sortedleadtime.csv')
171/51: timelines2020.head()
171/52: timelines2020.sample(frac=.5)
171/53: timelines2020.sample(frac=.7)
171/54: timelines2020.describe()
171/55:

timelines2020Exceeded = timelines2020[timelines2020['implementation_period'] >= timelines2020['maximum_indicative_time']]

timelines2020Exceeded.head()
171/56: timelines2020Exceeded.groupby(['method']).sum()
171/57: timelines2020Exceeded.groupby(['method']).sum()
171/58: timelines2020Exceeded.groupby(['method']).count()
171/59:
timelines2020Exceeded.groupby(['method']).agg(
    total_contracts = pd.NamedAgg(column='subject_of_procurement', aggfunc=count)
)
171/60:
timelines2020Exceeded.groupby(['method']).agg(
    total_contracts = pd.NamedAgg(column='subject_of_procurement', aggfunc=sum)
)
171/61: timelines2020Exceeded.groupby(['method']).count()
171/62: timelines2020Exceeded.groupby(['method']).sum()
171/63:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
171/64:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
171/65:
# Check the types of the dataset
timelines.dtypes
171/66:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
171/67: timelines.head()
171/68: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
171/69: timelines2020.info()
171/70: timelines2020
171/71:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
171/72: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
171/73: timelines2020.head()
171/74: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
171/75: timelines2020
171/76: timelines2020.sample(frac=.7)
171/77: timelines2020.info()
171/78:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
171/79: timelines2020
171/80: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
171/81: timelines2020.sample(frac=.5)
171/82: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
171/83: timelines2020.sample(frac=.5)
171/84: timelines2020.sample(frac=.5)
171/85: timelines2020.dtypes
171/86:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
171/87: timelines2020.info()
171/88:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
171/89: timelines2020.sample(frac=.5)
171/90: timelines2020.to_csv('timeliness-in-days.csv', index=False)
171/91:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
171/92: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
171/93: timelines2020.info()
171/94: timelines2020.groupby(['method']).mean()
171/95: timelines2020.groupby(['type']).mean()
171/96: # Import the file
171/97:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
171/98: leadtime.info()
171/99:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
171/100:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
171/101:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
171/102:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
171/103: leadtime.head()
171/104: leadtime.sample(frac=.5)
171/105: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
171/106: leadtimeOpen.info()
171/107: leadtimeOpen
171/108:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
171/109: sortedleadtime = leadtime.sort_values(by=['contract_price'])
171/110: sortedleadtime.to_csv('sortedleadtime.csv')
171/111: timelines2020.head()
171/112: timelines2020.describe()
171/113:

timelines2020Exceeded = timelines2020[timelines2020['implementation_period'] >= timelines2020['maximum_indicative_time']]

timelines2020Exceeded.head()
171/114:
no_of_contracts = timelines2020Exceeded.groupby(['method']).count()
no_of_contracts.to_csv('no_of_contracts.csv')
171/115:
value_Of_Contracts = timelines2020Exceeded.groupby(['method']).sum()
value_Of_Contracts.to_csv('value_Of_Contracts.to_csv')
171/116:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
171/117:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
171/118:
# Check the types of the dataset
timelines.dtypes
171/119:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
171/120: timelines.head()
171/121: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
171/122: timelines2020.info()
171/123: timelines2020
171/124:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
171/125: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
171/126: timelines2020.head()
171/127: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
171/128: timelines2020
171/129: timelines2020.sample(frac=.7)
171/130: timelines2020.info()
171/131:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
171/132: timelines2020
171/133: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
171/134: timelines2020.sample(frac=.5)
171/135: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
171/136: timelines2020.sample(frac=.5)
171/137: timelines2020.sample(frac=.5)
171/138: timelines2020.dtypes
171/139:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
171/140: timelines2020.info()
171/141:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
171/142: timelines2020.sample(frac=.5)
171/143: timelines2020.to_csv('timeliness-in-days.csv', index=False)
171/144:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
171/145: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
171/146: timelines2020.info()
171/147: timelines2020.groupby(['method']).mean()
171/148: timelines2020.groupby(['type']).mean()
171/149: # Import the file
171/150:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
171/151: leadtime.info()
171/152:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
171/153:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
171/154:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
171/155:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
171/156: leadtime.head()
171/157: leadtime.sample(frac=.5)
171/158: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
171/159: leadtimeOpen.info()
171/160: leadtimeOpen
171/161:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
171/162: sortedleadtime = leadtime.sort_values(by=['contract_price'])
171/163: sortedleadtime.to_csv('sortedleadtime.csv')
171/164: timelines2020.head()
171/165: timelines2020.describe()
171/166:

timelines2020Exceeded = timelines2020[timelines2020['implementation_period'] >= timelines2020['maximum_indicative_time']]

timelines2020Exceeded.head()
171/167:
no_of_contracts = timelines2020Exceeded.groupby(['method']).count()
no_of_contracts.to_csv('no_of_contracts.csv')
171/168:
value_Of_Contracts = timelines2020Exceeded.groupby(['method']).sum()
value_Of_Contracts.to_csv('value_Of_Contracts.csv')
171/169:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
171/170:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
171/171:
# Check the types of the dataset
timelines.dtypes
171/172:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
171/173: timelines.head()
171/174: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
171/175: timelines2020.info()
171/176: timelines2020
171/177:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
171/178: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
171/179: timelines2020.head()
171/180: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
171/181: timelines2020
171/182: timelines2020.sample(frac=.7)
171/183: timelines2020.info()
171/184:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
171/185: timelines2020
171/186: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
171/187: timelines2020.sample(frac=.5)
171/188: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
171/189: timelines2020.sample(frac=.5)
171/190: timelines2020.sample(frac=.5)
171/191: timelines2020.dtypes
171/192:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
171/193: timelines2020.info()
171/194:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
171/195: timelines2020.sample(frac=.5)
171/196: timelines2020.to_csv('timeliness-in-days.csv', index=False)
171/197:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
171/198: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
171/199: timelines2020.info()
171/200: timelines2020.groupby(['method']).mean()
171/201: timelines2020.groupby(['type']).mean()
171/202: # Import the file
171/203:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
171/204: leadtime.info()
171/205:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
171/206:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
171/207:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
171/208:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
171/209: leadtime.head()
171/210: leadtime.sample(frac=.5)
171/211: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
171/212: leadtimeOpen.info()
171/213: leadtimeOpen
171/214:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
171/215: sortedleadtime = leadtime.sort_values(by=['contract_price'])
171/216: sortedleadtime.to_csv('sortedleadtime.csv')
171/217: timelines2020.head()
171/218: timelines2020.describe()
171/219:

timelines2020Exceeded = timelines2020[timelines2020['implementation_period'] >= timelines2020['maximum_indicative_time']]

timelines2020Exceeded.head()
171/220: timelines2020Exceeded.to_csv('timelines2020Exceeded.csv')
171/221:
no_of_contracts = timelines2020Exceeded.groupby(['method']).count()
no_of_contracts.to_csv('no_of_contracts.csv')
171/222:
value_Of_Contracts = timelines2020Exceeded.groupby(['method']).sum()
value_Of_Contracts.to_csv('value_Of_Contracts.csv')
171/223:
# Load file for those that exceeded timelines

exceededTimelines = pd.read_csv('timelines2020Exceeded.csv')

exceededTimelines.head()
171/224: exceededTimelines.describe()
171/225: exceededTimelines.groupby(['method']).count()
171/226: exceededTimelines[exceededTimelines['method'] == 'Direct Procurement']
172/1: ed1996.sample(frac=.78)
172/2:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/3:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages='1-11')

len(ed1996)
172/4: ed1996.sample(frac=.78)
172/5:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages='1-11', header=None)

len(ed1996)
172/6: ed1996.sample(frac=.78)
172/7:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/8:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, stream=True, pages='1-11', header=None)

len(ed1996)
172/9: ed1996.sample(frac=.78)
172/10:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options='{'header': None}')

len(ed1996)
172/11:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/12:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/13: ed1996.sample(frac=.78)
172/14: ed1996.header()
172/15: ed1996.head()
172/16:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/17:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/18: ed1996.head()
172/19: ed1996.sample(frac=.78)
172/20: ed1996.to_csv('ed1996.csv')
172/21:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/22:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/23: ed1996.head()
172/24: ed1996.sample(frac=.78)
172/25: ed1996.to_csv('ed1996.csv', index=None)
172/26: ed1996.to_csv('ed1996.csv', index=None, header=False)
172/27:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/28:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/29: ed1996.head()
172/30: ed1996.sample(frac=.78)
172/31: ed1996.to_csv('ed1996.csv', index=None, header=False)
172/32:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/33:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/34: ed1996.head()
172/35: ed1996.sample(frac=.78)
172/36: ed1996.to_csv('ed1996.csv', index=None, header=False)
172/37:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', multipla_tables=True, 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/38: distSumed2001.head()
172/39: distSumed2001.to_csv('distSumed2001.csv', index=None, header=False)
172/40:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/41:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/42: ed1996.head()
172/43: ed1996.sample(frac=.78)
172/44: ed1996.to_csv('ed1996.csv', index=None, header=False)
172/45:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', multipla_tables=True, 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/46: distSumed2001.head()
172/47: distSumed2001.to_csv('distSumed2001.csv', index=None, header=False)
172/48:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multipla_tables=True, 
                         stream=True, pages='1-62',
                         pandas_options={'header': None})

len(constSumed2001)
172/49:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multipla_tables=True, 
                         stream=True, pages='1-62',
                         pandas_options={'header': None})

len(constSumed2001)
172/50:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multipla_tables=True, 
                         stream=True, pages='1-62')

len(constSumed2001)
172/51:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf')

len(constSumed2001)
172/52:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/53:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-62')

len(constSumed2001)
172/54: constSumed2001.head()
172/55: constSumed2001
172/56: constSumed2001.tail()
172/57:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-62', pandas_options={'header': None})

len(constSumed2001)
172/58: constSumed2001.tail()
172/59: constSumed2001
172/60: constSumed2001.to_csv('constSumed2001.csv')
172/61:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/62:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/63: ed1996.head()
172/64: ed1996.sample(frac=.78)
172/65: ed1996.to_csv('ed1996.csv', index=None, header=False)
172/66:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/67: distSumed2001.head()
174/1:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/2:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/3:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/4: completed_contracts.head()
174/5:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/6: completed_contracts.sample(frac=.7)
174/7:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/8:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/9:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/10: completed_contracts.head()
174/11:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/12: completed_contracts.head()
174/13:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/14:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/15: completed_contracts.sample(frac=.5)
174/16:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/17: completed_contracts.describe()
174/18: completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/19:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/20:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/21:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/22: completed_contracts.head()
174/23: completed_contracts.describe()
174/24:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/25: completed_contracts.sample(frac=.7)
174/26:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/27:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/28:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/29: completed_contracts.head()
174/30:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/31: completed_contracts.head()
174/32:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/33:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/34: completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/35: completed_contracts.sample(frac=.5)
174/36:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/37:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/38:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/39:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/40: completed_contracts.head()
174/41: completed_contracts.describe()
174/42:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/43: completed_contracts.sample(frac=.7)
174/44:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/45:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/46:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/47: completed_contracts.head()
174/48:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/49: completed_contracts.head()
174/50:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/51:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/52: completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/53: completed_contracts.sample(frac=.5)
174/54:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/55: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/56: above.head()
174/57: above.describe()
174/58: above['contract_price'].sum()
174/59: above['completion_contract_price'].sum()
174/60: above[above['completion_period'] == 0]
174/61: above[above['completion_period'] > 0]
174/62: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/63: within[within['completion_period'] == 0]
174/64:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/65:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/66:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/67: completed_contracts.head()
174/68: completed_contracts.describe()
174/69:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/70: completed_contracts.sample(frac=.7)
174/71:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/72:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/73:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/74: completed_contracts.head()
174/75:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/76: completed_contracts.head()
174/77:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/78:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/79:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/80: completed_contracts.sample(frac=.5)
174/81:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/82: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/83: above['completion_contract_price'].sum()
174/84: above[above['completion_period'] > 0]
174/85: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/86: within[within['completion_period'] == 0]
174/87: completed_contracts[completed_contracts['completeionStatusPeriod'] == ' ']
174/88: completed_contracts[completed_contracts['completeionStatusPeriod'] == '']
174/89: completed_contracts[completed_contracts['completion_period'] == ' ']
174/90: completed_contracts[completed_contracts['completion_period'] == '']
174/91: completed_contracts[completed_contracts['completion_period'] == ' ']
174/92: completed_contracts[completed_contracts['completion_period'] == 0]
174/93: completed_contracts['completion_period'].fillna(0, inplace=True)
174/94:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/95:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/96:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/97: completed_contracts.head()
174/98: completed_contracts.describe()
174/99:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/100: completed_contracts.sample(frac=.7)
174/101:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/102:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/103:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/104: completed_contracts.head()
174/105:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/106: completed_contracts['completion_period'].fillna(0.0, inplace=True)
174/107: completed_contracts.head()
174/108:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/109:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/110:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/111: completed_contracts.sample(frac=.5)
174/112:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/113: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/114: above['completion_contract_price'].sum()
174/115: above[above['completion_period'] > 0]
174/116: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/117: within[within['completion_period'] == 0]
174/118:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/119:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/120:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/121: completed_contracts.head()
174/122: completed_contracts.describe()
174/123:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/124: completed_contracts.sample(frac=.7)
174/125:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/126:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/127:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/128: completed_contracts.head()
174/129:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/130: completed_contracts['completion_period'].not_null()
174/131:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/132:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/133:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/134: completed_contracts.head()
174/135: completed_contracts.describe()
174/136:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/137: completed_contracts.sample(frac=.7)
174/138:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/139:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/140:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/141: completed_contracts.head()
174/142:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/143: completed_contracts['completion_period'].notnull()
174/144: completed_contracts.head()
174/145:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/146:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/147:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/148: completed_contracts.sample(frac=.5)
174/149:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/150: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/151: above['completion_contract_price'].sum()
174/152: above[above['completion_period'] > 0]
174/153: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/154: within[within['completion_period'] == 0]
174/155: completed_contracts['completion_period'].isnull()
174/156: completed_contracts[completed_contracts['completion_period'] == '']
174/157: completed_contracts[completed_contracts['completion_period'] == ' ']
174/158:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/159:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/160:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/161: completed_contracts.head()
174/162: completed_contracts.describe()
174/163:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/164: completed_contracts.sample(frac=.7)
174/165:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/166:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/167:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/168: completed_contracts.head()
174/169:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/170: completed_contracts['completion_period'].fillna('NULL', inplace=True)
174/171: completed_contracts['completion_period'].isnull()
174/172: completed_contracts.head()
174/173:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/174:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/175:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/176:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/177:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/178:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/179: completed_contracts.head()
174/180: completed_contracts.describe()
174/181:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/182: completed_contracts.sample(frac=.7)
174/183:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/184:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/185:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/186: completed_contracts.head()
174/187:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/188: completed_contracts['completion_period'].fillna(1, inplace=True)
174/189: completed_contracts['completion_period'].isnull()
174/190: completed_contracts.head()
174/191:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/192:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/193:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/194: completed_contracts.sample(frac=.5)
174/195:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/196: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/197: above['completion_contract_price'].sum()
174/198: above[above['completion_period'] > 0]
174/199: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/200: within[within['completion_period'] == 0]
174/201:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/202:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/203:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/204: completed_contracts.head()
174/205: completed_contracts.describe()
174/206:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/207: completed_contracts.sample(frac=.7)
174/208:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/209:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/210:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/211: completed_contracts.head()
174/212:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/213: completed_contracts['completion_period'].fillna(0.0, inplace=True)
174/214: completed_contracts['completion_period'].isnull()
174/215: completed_contracts.head()
174/216:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/217:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/218:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/219: completed_contracts.sample(frac=.5)
174/220:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/221: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/222: above['completion_contract_price'].sum()
174/223: above[above['completion_period'] > 0]
174/224: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/225: within[within['completion_period'] == 0]
174/226:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/227:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/228:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/229: completed_contracts.head()
174/230: completed_contracts.describe()
174/231:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/232: completed_contracts.sample(frac=.7)
174/233:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/234:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/235:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/236: completed_contracts.head()
174/237:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/238: completed_contracts['completion_period'].fillna(-1, inplace=True)
174/239: completed_contracts['completion_period'].isnull()
174/240: completed_contracts.head()
174/241:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/242:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/243:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/244: completed_contracts.sample(frac=.5)
174/245:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/246: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/247: above['completion_contract_price'].sum()
174/248: above[above['completion_period'] > 0]
174/249: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/250: within[within['completion_period'] == 0]
174/251:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/252:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/253:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/254: completed_contracts.head()
174/255: completed_contracts.describe()
174/256:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/257: completed_contracts.sample(frac=.7)
174/258:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/259:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/260:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/261: completed_contracts.head()
174/262:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/263: completed_contracts['completion_period'].fillna(1, inplace=True)
174/264: completed_contracts['completion_period'].isnull()
174/265: completed_contracts.head()
174/266:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/267:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/268:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/269: completed_contracts.sample(frac=.5)
174/270:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/271: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/272: above['completion_contract_price'].sum()
174/273: above[above['completion_period'] > 0]
174/274: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/275: within[within['completion_period'] == 0]
174/276:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/277:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/278:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/279: completed_contracts.head()
174/280: completed_contracts.describe()
174/281:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/282: completed_contracts.sample(frac=.7)
174/283:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/284:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/285:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/286: completed_contracts.head()
174/287:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/288: completed_contracts['completion_period'].fillna(0.0, inplace=True)
174/289: completed_contracts['completion_period'].isnull()
174/290: completed_contracts.head()
174/291:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] <= completed_contracts['completion_contract_price'], 'within', 'above')
174/292:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/293:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/294: completed_contracts.sample(frac=.5)
174/295:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/296: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/297: above['completion_contract_price'].sum()
174/298: above[above['completion_period'] > 0]
174/299: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/300: within[within['completion_period'] == 0]
174/301:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/302:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/303:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/304: completed_contracts.head()
174/305: completed_contracts.describe()
174/306:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/307: completed_contracts.sample(frac=.7)
174/308:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/309:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/310:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/311: completed_contracts.head()
174/312:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/313: completed_contracts['completion_period'].fillna(0.0, inplace=True)
174/314: completed_contracts['completion_period'].isnull()
174/315: completed_contracts.head()
174/316:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] >= completed_contracts['completion_contract_price'], 'within', 'above')
174/317:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/318:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/319: completed_contracts.sample(frac=.5)
174/320:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/321: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/322: above['completion_contract_price'].sum()
174/323: above[above['completion_period'] > 0]
174/324: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/325: within[within['completion_period'] == 0]
174/326: completed_contracts.sample(frac=.8)
174/327:


completed_contracts['completion_price_difference'] = completed_contracts['completion_contract_price'] - completed_contracts['contract_price']
174/328:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/329:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/330:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/331: completed_contracts.head()
174/332: completed_contracts.describe()
174/333:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/334: completed_contracts.sample(frac=.7)
174/335:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/336:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/337:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/338:


completed_contracts['completion_price_difference'] = completed_contracts['completion_contract_price'] - completed_contracts['contract_price']
174/339: completed_contracts.head()
174/340:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/341: completed_contracts['completion_period'].fillna(0.0, inplace=True)
174/342: completed_contracts['completion_period'].isnull()
174/343: completed_contracts.head()
174/344:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] >= completed_contracts['completion_contract_price'], 'within', 'above')
174/345:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/346:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/347: completed_contracts.sample(frac=.8)
174/348:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/349: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/350: above['completion_contract_price'].sum()
174/351: above[above['completion_period'] > 0]
174/352: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/353: within[within['completion_period'] == 0]
174/354:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
174/355:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
174/356:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
174/357: completed_contracts.head()
174/358: completed_contracts.describe()
174/359:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
174/360: completed_contracts.sample(frac=.7)
174/361:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
174/362:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
174/363:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
174/364:


completed_contracts['completion_price_difference'] = completed_contracts['completion_contract_price'] - completed_contracts['contract_price']
174/365: completed_contracts.head()
174/366:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
174/367: completed_contracts['completion_period'].fillna(0.0, inplace=True)
174/368: completed_contracts['completion_period'].isnull()
174/369: completed_contracts.head()
174/370:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] >= completed_contracts['completion_contract_price'], 'within', 'above')
174/371:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
174/372:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
174/373:


completed_contracts['completeionPriceStatus'] = np.where(completed_contracts['completion_price_difference'] <= 0, 'within', 'above')
174/374: completed_contracts.sample(frac=.8)
174/375:
# Write the results to a csv and excel file

completed_contracts.to_csv('completed_contracts_tm.csv')

completed_contracts.to_excel('completed_contracts_tm.xlsx')
174/376: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
174/377: above['completion_contract_price'].sum()
174/378: above[above['completion_period'] > 0]
174/379: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
174/380: within[within['completion_period'] == 0]
172/68: distSumed2001
172/69: distSumed2001.describe()
172/70: distSumed2001 = pd.DataFrame(distSumed2001)
172/71: distSumed2001.head()
172/72: distSumed2001.tail()
172/73: distSumed2001.describe()
172/74:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/75:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/76: ed1996.head()
172/77: ed1996.sample(frac=.78)
172/78: ed1996.to_csv('ed1996.csv', index=None, header=False)
172/79:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/80: distSumed2001.describe()
172/81: distSumed2001
172/82:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-62', pandas_options={'header': None})

len(constSumed2001)
172/83:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-62', pandas_options={'header': None})

len(constSumed2001)
172/84:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-5')

len(distSumed2001)
172/85: distSumed2001
172/86: distSumed2001[0]
172/87:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/88: distSumed2001[0]
172/89: distSumed2001[1]
172/90:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/91: distSumed2001.head()
172/92: distSumed2001.tail()
172/93:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/94:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/95: ed1996.head()
172/96: ed1996.sample(frac=.78)
172/97: ed1996.to_csv('ed1996.csv', index=None, header=False)
172/98:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/99: distSumed2001.tail()
172/100: distSumed2001.to_csv('distSumed2001.csv', index=None, header=False)
172/101:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-62', pandas_options={'header': None})

len(constSumed2001)
172/102: constSumed2001.to_csv('constSumed2001.csv')
172/103:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', 
                         stream=True, pages='1-62', pandas_options={'header': None})

len(constSumed2001)
172/104:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
172/105:
ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
172/106: ed1996.head()
172/107: ed1996.sample(frac=.78)
172/108: ed1996.to_csv('ed1996.csv', index=None, header=False)
172/109:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
172/110: distSumed2001.tail()
172/111: distSumed2001.to_csv('distSumed2001.csv', index=None, header=False)
172/112:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', 
                         stream=True, pages='1-62', pandas_options={'header': None})

len(constSumed2001)
172/113:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-62', pandas_options={'header': None})

len(constSumed2001)
172/114: constSumed2001.head()
172/115: constSumed2001[0]
172/116: constSumed2001[1]
175/1:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/2:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/3:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/4:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/5:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/6:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/7:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/8:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv', index_col="None")

all_awarded_contracts.head()
175/9:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/10:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv', index_col="False")

all_awarded_contracts.head()
175/11:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv', index="None")

all_awarded_contracts.head()
175/12:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv', index="False")

all_awarded_contracts.head()
175/13:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/14:

all_awarded_contracts['completion_status'] = np.where(pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price']))
175/15:

all_awarded_contracts['completion_status'] = np.where(all_awarded_contracts['actual_contract_completion_date'].isnull())
175/16:

all_awarded_contracts['completion_status'] = np.where(all_awarded_contracts['actual_contract_completion_date'].isnull(), 'Awarded', 'Awarded and Completed')
175/17: all_awarded_contracts.sample(frac=.5)
175/18: all_awarded_contracts.sample(frac=.7)
175/19: all_awarded_contracts.head()
175/20:

all_awarded_contracts['completion_status'] = np.where(pd.isnull(all_awarded_contracts['actual_contract_completion_date']), 'Awarded', 'Awarded and Completed')
175/21: all_awarded_contracts.head()
175/22: all_awarded_contracts.describe()
175/23:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/24:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/25:

all_awarded_contracts['completion_status'] = np.where(pd.isnull(all_awarded_contracts['actual_contract_completion_date']), 'Awarded', 'Awarded and Completed')
175/26: all_awarded_contracts.describe()
175/27: all_awarded_contracts.sample(frac=.7)
175/28:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/29:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/30:

all_awarded_contracts['completion_status'] = np.where(pd.isnull(all_awarded_contracts['actual_contract_completion_date']), 'Awarded', 'Awarded and Completed')
175/31: all_awarded_contracts.sample(frac=.7)
175/32:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
175/33: all_awarded_contracts.groupby(['completion_status'])
175/34: all_awarded_contracts.groupby(['completion_status']).sum()
175/35: all_awarded_contracts.groupby(['completion_status']).value_counts()
175/36: all_awarded_contracts.groupby(['completion_status']).values_counts()
175/37: all_awarded_contracts.groupby(['completion_status']).values_count()
175/38: all_awarded_contracts.groupby(['completion_status']).hist()
175/39: all_awarded_contracts.groupby(by='completion_status').hist()
175/40: all_awarded_contracts.groupby(by='completion_status')
175/41: all_awarded_contracts.groupby(by='completion_status').count()
175/42:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/43:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/44:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
175/45: all_awarded_contracts.sample(frac=.7)
175/46: all_awarded_contracts.groupby(by='completion_status').count()
175/47: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
175/48: awarded_contracts_not_completed.head()
175/49: awarded_contracts_not_completed.describe()
175/50: awarded_contracts_not_completed['provisional_actual_completion_date'] == '2020-08-01'
175/51: awarded_contracts_not_completed['provisional_actual_completion_date'] = '2020-08-01'
175/52: awarded_contracts_not_completed.head()
175/53: awarded_contracts_not_completed.describe()
175/54: awarded_contracts_not_completed.dtypes
175/55:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')
175/56: awarded_contracts_not_completed.describe()
175/57: awarded_contracts_not_completed.dtypes
175/58: awarded_contracts_not_completed.head()
175/59: awarded_contracts_not_completed.sample(frac=.7)
175/60: awarded_contracts_not_completed.head()
175/61:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
175/62:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/63:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/64:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
175/65: all_awarded_contracts.sample(frac=.7)
175/66: all_awarded_contracts.groupby(by='completion_status').count()
175/67: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
175/68: awarded_contracts_not_completed.head()
175/69: awarded_contracts_not_completed.describe()
175/70:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
175/71: awarded_contracts_not_completed.dtypes
175/72:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

# awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
#                                                                     errors='coerce')
175/73: awarded_contracts_not_completed.dtypes
175/74: awarded_contracts_not_completed.head()
175/75: awarded_contracts_not_completed['provisional_completion_status'] =
175/76:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/77:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/78:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
175/79: all_awarded_contracts.sample(frac=.7)
175/80: all_awarded_contracts.groupby(by='completion_status').count()
175/81: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
175/82: awarded_contracts_not_completed.head()
175/83: awarded_contracts_not_completed.describe()
175/84:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
175/85: awarded_contracts_not_completed.dtypes
175/86:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

# awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
#                                                                     errors='coerce')
175/87: awarded_contracts_not_completed.dtypes
175/88: awarded_contracts_not_completed.head()
175/89:

awarded_contracts_not_completed['provisional_completion_period'] = awarded_contracts_not_completed['provisional_actual_completion_date'] - awarded_contracts_not_completed['planned_completion_date']
175/90:

# awarded_contracts_not_completed['provisional_completion_status'] = np.where(awarded_contracts_not_completed[''])
175/91: awarded_contracts_not_completed.sample(frac=.7)
175/92:

awarded_contracts_not_completed['provisional_completion_period'] = pd.to_numeric(awarded_contracts_not_completed['provisional_completion_period'].dt.days, downcast="integer")
175/93: awarded_contracts_not_completed.head()
175/94: awarded_contracts_not_completed.sample(frac=.7)
175/95:

awarded_contracts_not_completed['provisional_completion_status'] = np.where(awarded_contracts_not_completed['provisional_completion_period'] <= 0, 'In Time', 'Late')
175/96: awarded_contracts_not_completed.head()
175/97: awarded_contracts_not_completed.tail()
175/98: awarded_contracts_not_completed.group(by='type')
175/99: awarded_contracts_not_completed.groupby(by='type')
175/100: awarded_contracts_not_completed.groupby(by='type').count()
175/101:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/102:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/103:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
175/104: all_awarded_contracts.sample(frac=.7)
175/105: all_awarded_contracts.groupby(by='completion_status').count()
175/106: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
175/107: awarded_contracts_not_completed.head()
175/108: awarded_contracts_not_completed.describe()
175/109:

awarded_contracts_not_completed_type_num = awarded_contracts_not_completed.groupby(by='type').count()

awarded_contracts_not_completed_type_num.to_csv('awarded_contracts_not_completed_type_num.csv')

awarded_contracts_not_completed_type_val = awarded_contracts_not_completed.groupby(by='type').sum()

awarded_contracts_not_completed_type_val.to_csv('awarded_contracts_not_completed_type_val.csv')
175/110:
awarded_contracts_not_completed_method_num = awarded_contracts_not_completed.groupby(by='method').count()

awarded_contracts_not_completed_method_num.to_csv('awarded_contracts_not_completed_method_num.csv')

awarded_contracts_not_completed_method_val = awarded_contracts_not_completed.groupby(by='method').sum()

awarded_contracts_not_completed_method_val.to_csv('awarded_contracts_not_completed_method_val.csv')
175/111:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
175/112: awarded_contracts_not_completed.dtypes
175/113:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

# awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
#                                                                     errors='coerce')
175/114: awarded_contracts_not_completed.dtypes
175/115: awarded_contracts_not_completed.head()
175/116:

awarded_contracts_not_completed['provisional_completion_period'] = awarded_contracts_not_completed['provisional_actual_completion_date'] - awarded_contracts_not_completed['planned_completion_date']
175/117:

awarded_contracts_not_completed['provisional_completion_period'] = pd.to_numeric(awarded_contracts_not_completed['provisional_completion_period'].dt.days, downcast="integer")
175/118: awarded_contracts_not_completed.sample(frac=.7)
175/119: awarded_contracts_not_completed.sample(frac=.7)
175/120:

awarded_contracts_not_completed['provisional_completion_status'] = np.where(awarded_contracts_not_completed['provisional_completion_period'] <= 0, 'In Time', 'Late')
175/121: awarded_contracts_not_completed.tail()
177/1:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
177/2:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
177/3: market_price.describe()
177/4:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
177/5: market_price.head()
177/6:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
177/7: market_price.sample(frac=.7)
177/8: market_price['country_of_registration'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
177/9:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
177/10:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
177/11:

market_price['market_price_status'] = np.where(market_price['estimated_amount'] <= market_price['contract_price'], 'within', 'above')
177/12: market_price.sample(frac=.7)
177/13:
# Save the file to csv and excel

market_price.to_csv('market_price_st.csv')

market_price.to_excel('market_price_st.xlsx')
177/14: market_price.describe()
177/15:
# 1. Prepare the data

# x = market_price['subject_of_procurement'].value_counts()
# y = market_price['contract_price']

# 2. Setup the plot
# fig, ax = plt.subplots(figsize(10, 10)) # width and height

# 3. Plot the data
# ax.plot(x, y)

# 4. Customize the plot
# ax.set(title = 'Market Price Within Vs Above', xlabel="x-axis", ylabel="y-axis")

# 5. Save and show
# fig.savefig('images/market_price.png')
177/16:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
177/17:
# Import the csv file
market_price = pd.read_csv('market_price.csv')
market_price.head()
177/18: market_price.describe()
177/19:

market_price['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
market_price['initiation_date'].fillna(value="0000-00-00", inplace=True)
177/20: market_price.head()
177/21:
market_price['initiation_date'] = pd.to_datetime(market_price['initiation_date'], errors='coerce')

market_price['contract_signature_date'] = pd.to_datetime(market_price['contract_signature_date'], errors='coerce')

market_price['actual_contract_signature_date'] = pd.to_datetime(market_price['actual_contract_signature_date'], errors='coerce')

market_price['planned_completion_date'] = pd.to_datetime(market_price['planned_completion_date'], errors='coerce')
177/22: market_price.sample(frac=.7)
177/23: market_price['country_of_registration'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
177/24:

market_price['planning_period'] = market_price['contract_signature_date'] - market_price['initiation_date']

market_price['implementation_period'] = market_price['actual_contract_signature_date'] - market_price['initiation_date']
177/25:

market_price['planning_period'] = pd.to_numeric(market_price['planning_period'].dt.days, downcast='integer')

market_price['implementation_period'] = pd.to_numeric(market_price['implementation_period'].dt.days, downcast='integer')
177/26:

market_price['market_price_status'] = np.where(market_price['contract_price'] <= market_price['estimated_amount'], 'within', 'above')
177/27: market_price.sample(frac=.7)
177/28:
# Save the file to csv and excel

market_price.to_csv('market_price_st.csv')

market_price.to_excel('market_price_st.xlsx')
177/29: market_price.describe()
177/30:
# 1. Prepare the data

# x = market_price['subject_of_procurement'].value_counts()
# y = market_price['contract_price']

# 2. Setup the plot
# fig, ax = plt.subplots(figsize(10, 10)) # width and height

# 3. Plot the data
# ax.plot(x, y)

# 4. Customize the plot
# ax.set(title = 'Market Price Within Vs Above', xlabel="x-axis", ylabel="y-axis")

# 5. Save and show
# fig.savefig('images/market_price.png')
175/122:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/123:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/124:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
175/125: all_awarded_contracts.sample(frac=.7)
175/126: all_awarded_contracts.groupby(by='completion_status').count()
175/127: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
175/128: awarded_contracts_not_completed.head()
175/129: awarded_contracts_not_completed.describe()
175/130:

# awarded_contracts_not_completed_type_num = awarded_contracts_not_completed.groupby(by='type').count()

# awarded_contracts_not_completed_type_num.to_csv('awarded_contracts_not_completed_type_num.csv')

# awarded_contracts_not_completed_type_val = awarded_contracts_not_completed.groupby(by='type').sum()

# awarded_contracts_not_completed_type_val.to_csv('awarded_contracts_not_completed_type_val.csv')
175/131:
# awarded_contracts_not_completed_method_num = awarded_contracts_not_completed.groupby(by='method').count()

# awarded_contracts_not_completed_method_num.to_csv('awarded_contracts_not_completed_method_num.csv')

# awarded_contracts_not_completed_method_val = awarded_contracts_not_completed.groupby(by='method').sum()

# awarded_contracts_not_completed_method_val.to_csv('awarded_contracts_not_completed_method_val.csv')
175/132:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
175/133: awarded_contracts_not_completed.dtypes
175/134:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

# awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
#                                                                     errors='coerce')
175/135: awarded_contracts_not_completed.dtypes
175/136: awarded_contracts_not_completed.head()
175/137:

awarded_contracts_not_completed['provisional_completion_period'] = awarded_contracts_not_completed['provisional_actual_completion_date'] - awarded_contracts_not_completed['planned_completion_date']
175/138:

awarded_contracts_not_completed['provisional_completion_period'] = pd.to_numeric(awarded_contracts_not_completed['provisional_completion_period'].dt.days, downcast="integer")
175/139: awarded_contracts_not_completed.sample(frac=.7)
175/140: awarded_contracts_not_completed.sample(frac=.7)
175/141:

awarded_contracts_not_completed['provisional_completion_status'] = np.where(awarded_contracts_not_completed['provisional_completion_period'] <= 0, 'In Time', 'Late')
175/142: awarded_contracts_not_completed.tail()
175/143:

awarded_contracts_not_completed_late = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'Late']

awarded_contracts_not_completed_late.to_csv('awarded_contracts_not_completed_late.csv')
175/144:

awarded_contracts_not_completed_in_time = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'In Time']

awarded_contracts_not_completed_in_time.to_csv('awarded_contracts_not_completed_in_time.csv')
175/145:

awarded_contracts_not_completed_late_type_num = awarded_contracts_not_completed_late.groupby(by='type').count()

awarded_contracts_not_completed_late_type_num.to_csv('awarded_contracts_not_completed_late_type_num.csv')

awarded_contracts_not_completed_late_type_val = awarded_contracts_not_completed_late.groupby(by='type').sum()

awarded_contracts_not_completed_late_type_val.to_csv('awarded_contracts_not_completed_late_type_val.csv')
175/146:

awarded_contracts_not_completed_late_method_num = awarded_contracts_not_completed_late.groupby(by='method').count()

awarded_contracts_not_completed_late_method_num.to_csv('awarded_contracts_not_completed_late_method_num.csv')

awarded_contracts_not_completed_late_method_val = awarded_contracts_not_completed_late.groupby(by='method').sum()

awarded_contracts_not_completed_late_method_val.to_csv('awarded_contracts_not_completed_late_method_val.csv')
175/147:


awarded_contracts_not_completed_in_time_type_num = awarded_contracts_not_completed_in_time.groupby(by='type').count()

awarded_contracts_not_completed_in_time_type_num.to_csv('awarded_contracts_not_completed_in_time_type_num.csv')

awarded_contracts_not_completed_in_time_type_val = awarded_contracts_not_completed_in_time.groupby(by='type').sum()

awarded_contracts_not_completed_in_time_type_val.to_csv('awarded_contracts_not_completed_in_time_type_val.csv')
175/148:


awarded_contracts_not_completed_in_time_method_num = awarded_contracts_not_completed_in_time.groupby(by='method').count()

awarded_contracts_not_completed_in_time_method_num.to_csv('awarded_contracts_not_completed_in_time_method_num.csv')

awarded_contracts_not_completed_in_time_method_val = awarded_contracts_not_completed_in_time.groupby(by='method').sum()

awarded_contracts_not_completed_in_time_method_val.to_csv('awarded_contracts_not_completed_in_time_method_val.csv')
175/149:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/150:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/151:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
175/152: all_awarded_contracts.sample(frac=.7)
175/153: all_awarded_contracts.groupby(by='completion_status').count()
175/154: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
175/155: awarded_contracts_not_completed.head()
175/156: awarded_contracts_not_completed.describe()
175/157:

# awarded_contracts_not_completed_type_num = awarded_contracts_not_completed.groupby(by='type').count()

# awarded_contracts_not_completed_type_num.to_csv('awarded_contracts_not_completed_type_num.csv')

# awarded_contracts_not_completed_type_val = awarded_contracts_not_completed.groupby(by='type').sum()

# awarded_contracts_not_completed_type_val.to_csv('awarded_contracts_not_completed_type_val.csv')
175/158:
# awarded_contracts_not_completed_method_num = awarded_contracts_not_completed.groupby(by='method').count()

# awarded_contracts_not_completed_method_num.to_csv('awarded_contracts_not_completed_method_num.csv')

# awarded_contracts_not_completed_method_val = awarded_contracts_not_completed.groupby(by='method').sum()

# awarded_contracts_not_completed_method_val.to_csv('awarded_contracts_not_completed_method_val.csv')
175/159:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
175/160: awarded_contracts_not_completed.dtypes
175/161:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

# awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
#                                                                     errors='coerce')
175/162: awarded_contracts_not_completed.dtypes
175/163: awarded_contracts_not_completed.head()
175/164:

awarded_contracts_not_completed['provisional_completion_period'] = awarded_contracts_not_completed['provisional_actual_completion_date'] - awarded_contracts_not_completed['planned_completion_date']
175/165:

awarded_contracts_not_completed['provisional_completion_period'] = pd.to_numeric(awarded_contracts_not_completed['provisional_completion_period'].dt.days, downcast="integer")
175/166: awarded_contracts_not_completed.sample(frac=.7)
175/167: awarded_contracts_not_completed.sample(frac=.7)
175/168:

awarded_contracts_not_completed['provisional_completion_status'] = np.where(awarded_contracts_not_completed['provisional_completion_period'] <= 0, 'In Time', 'Late')
175/169: awarded_contracts_not_completed.tail()
175/170:

awarded_contracts_not_completed_late = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'Late']

awarded_contracts_not_completed_late.to_csv('awarded_contracts_not_completed_late.csv')
175/171:

awarded_contracts_not_completed_in_time = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'In Time']

awarded_contracts_not_completed_in_time.to_csv('awarded_contracts_not_completed_in_time.csv')
175/172:

awarded_contracts_not_completed_late_type_num = awarded_contracts_not_completed_late.groupby(by='type').count()

awarded_contracts_not_completed_late_type_num.to_csv('awarded_contracts_not_completed_late_type_num.csv')

awarded_contracts_not_completed_late_type_val = awarded_contracts_not_completed_late.groupby(by='type').sum()

awarded_contracts_not_completed_late_type_val.to_csv('awarded_contracts_not_completed_late_type_val.csv')
175/173:

awarded_contracts_not_completed_late_method_num = awarded_contracts_not_completed_late.groupby(by='method').count()

awarded_contracts_not_completed_late_method_num.to_csv('awarded_contracts_not_completed_late_method_num.csv')

awarded_contracts_not_completed_late_method_val = awarded_contracts_not_completed_late.groupby(by='method').sum()

awarded_contracts_not_completed_late_method_val.to_csv('awarded_contracts_not_completed_late_method_val.csv')
175/174:


awarded_contracts_not_completed_in_time_type_num = awarded_contracts_not_completed_in_time.groupby(by='type').count()

awarded_contracts_not_completed_in_time_type_num.to_csv('awarded_contracts_not_completed_in_time_type_num.csv')

awarded_contracts_not_completed_in_time_type_val = awarded_contracts_not_completed_in_time.groupby(by='type').sum()

awarded_contracts_not_completed_in_time_type_val.to_csv('awarded_contracts_not_completed_in_time_type_val.csv')
175/175:


awarded_contracts_not_completed_in_time_method_num = awarded_contracts_not_completed_in_time.groupby(by='method').count()

awarded_contracts_not_completed_in_time_method_num.to_csv('awarded_contracts_not_completed_in_time_method_num.csv')

awarded_contracts_not_completed_in_time_method_val = awarded_contracts_not_completed_in_time.groupby(by='method').sum()

awarded_contracts_not_completed_in_time_method_val.to_csv('awarded_contracts_not_completed_in_time_method_val.csv')
175/176:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
175/177:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
175/178:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
175/179: all_awarded_contracts.sample(frac=.7)
175/180: all_awarded_contracts.groupby(by='completion_status').count()
175/181: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
175/182: awarded_contracts_not_completed.head()
175/183: awarded_contracts_not_completed.describe()
175/184:

# awarded_contracts_not_completed_type_num = awarded_contracts_not_completed.groupby(by='type').count()

# awarded_contracts_not_completed_type_num.to_csv('awarded_contracts_not_completed_type_num.csv')

# awarded_contracts_not_completed_type_val = awarded_contracts_not_completed.groupby(by='type').sum()

# awarded_contracts_not_completed_type_val.to_csv('awarded_contracts_not_completed_type_val.csv')
175/185:
# awarded_contracts_not_completed_method_num = awarded_contracts_not_completed.groupby(by='method').count()

# awarded_contracts_not_completed_method_num.to_csv('awarded_contracts_not_completed_method_num.csv')

# awarded_contracts_not_completed_method_val = awarded_contracts_not_completed.groupby(by='method').sum()

# awarded_contracts_not_completed_method_val.to_csv('awarded_contracts_not_completed_method_val.csv')
175/186:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
175/187: awarded_contracts_not_completed.dtypes
175/188:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

# awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
#                                                                     errors='coerce')
175/189: awarded_contracts_not_completed.dtypes
175/190: awarded_contracts_not_completed.head()
175/191:

awarded_contracts_not_completed['provisional_completion_period'] = awarded_contracts_not_completed['provisional_actual_completion_date'] - awarded_contracts_not_completed['planned_completion_date']
175/192:

awarded_contracts_not_completed['provisional_completion_period'] = pd.to_numeric(awarded_contracts_not_completed['provisional_completion_period'].dt.days, downcast="integer")
175/193: awarded_contracts_not_completed.sample(frac=.7)
175/194: awarded_contracts_not_completed.sample(frac=.7)
175/195:

awarded_contracts_not_completed['provisional_completion_status'] = np.where(awarded_contracts_not_completed['provisional_completion_period'] <= 0, 'In Time', 'Late')
175/196: awarded_contracts_not_completed.tail()
175/197: awarded_contracts_not_completed.to_csv('all_awarded_contracts_not_completed.csv')
175/198:

awarded_contracts_not_completed_late = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'Late']

awarded_contracts_not_completed_late.to_csv('awarded_contracts_not_completed_late.csv')
175/199:

awarded_contracts_not_completed_in_time = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'In Time']

awarded_contracts_not_completed_in_time.to_csv('awarded_contracts_not_completed_in_time.csv')
175/200:

awarded_contracts_not_completed_late_type_num = awarded_contracts_not_completed_late.groupby(by='type').count()

awarded_contracts_not_completed_late_type_num.to_csv('awarded_contracts_not_completed_late_type_num.csv')

awarded_contracts_not_completed_late_type_val = awarded_contracts_not_completed_late.groupby(by='type').sum()

awarded_contracts_not_completed_late_type_val.to_csv('awarded_contracts_not_completed_late_type_val.csv')
175/201:

awarded_contracts_not_completed_late_method_num = awarded_contracts_not_completed_late.groupby(by='method').count()

awarded_contracts_not_completed_late_method_num.to_csv('awarded_contracts_not_completed_late_method_num.csv')

awarded_contracts_not_completed_late_method_val = awarded_contracts_not_completed_late.groupby(by='method').sum()

awarded_contracts_not_completed_late_method_val.to_csv('awarded_contracts_not_completed_late_method_val.csv')
175/202:


awarded_contracts_not_completed_in_time_type_num = awarded_contracts_not_completed_in_time.groupby(by='type').count()

awarded_contracts_not_completed_in_time_type_num.to_csv('awarded_contracts_not_completed_in_time_type_num.csv')

awarded_contracts_not_completed_in_time_type_val = awarded_contracts_not_completed_in_time.groupby(by='type').sum()

awarded_contracts_not_completed_in_time_type_val.to_csv('awarded_contracts_not_completed_in_time_type_val.csv')
175/203:


awarded_contracts_not_completed_in_time_method_num = awarded_contracts_not_completed_in_time.groupby(by='method').count()

awarded_contracts_not_completed_in_time_method_num.to_csv('awarded_contracts_not_completed_in_time_method_num.csv')

awarded_contracts_not_completed_in_time_method_val = awarded_contracts_not_completed_in_time.groupby(by='method').sum()

awarded_contracts_not_completed_in_time_method_val.to_csv('awarded_contracts_not_completed_in_time_method_val.csv')
180/1:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/2:
contracts = pd.read_csv('estimate_read_out_contract_price.csv')

contracts.head()
180/3: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/4: contracts.head()
180/5: contracts['no_of_bids'].fillna(1)
180/6:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/7:
contracts = pd.read_csv('estimate_read_out_contract_price.csv')

contracts.head()
180/8: contracts['no_of_bids'].fillna(1)
180/9: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/10: contracts.head()
180/11: contracts['no_of_bids'] = contracts['no_of_bids'].where(contracts['no_of_bids'] == 0)
180/12: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/13: contracts.head()
180/14:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/15:
contracts = pd.read_csv('estimate_read_out_contract_price.csv')

contracts.head()
180/16: contracts['no_of_bids'] = contracts['no_of_bids'].where(contracts['no_of_bids'] == 0, 1)
180/17: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/18: contracts.head()
180/19: contracts.head()
180/20:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/21:
contracts = pd.read_csv('estimate_read_out_contract_price.csv')

contracts.head()
180/22: # contracts['no_of_bids'] = contracts['no_of_bids'].where(contracts['no_of_bids'] == 0, 1)
180/23: # contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/24: contracts.head()
180/25: contracts.dtypes()
180/26: contracts.dtypes
180/27:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/28:
contracts = pd.read_csv('estimate_read_out_contract_price.csv')

contracts.head()
180/29: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/30: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/31: contracts.head()
180/32: contracts.dtypes
180/33: contracts.tail()
180/34: contracts.head()
180/35: contracts.to_csv('estimate_avg_read_out_price.csv')
180/36:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/37:
contracts = pd.read_csv('estimate_read_out_contract_price.csv')

contracts.head()
180/38: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/39: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/40: contracts.head()
180/41: contracts.dtypes
180/42: contracts.to_csv('estimate_avg_read_out_price.csv')
180/43:
contracts.groupby(by="type").agg(
    {'Type': ['sum', 'count']}
)
180/44:
contracts.groupby(by="type").agg(
    {'avg_read_out_price': ['sum', 'count']}
)
180/45:
contracts.groupby(by="type").agg(
    {'avg_read_out_price': ['number', 'count']}
)
180/46:
contracts.groupby(by="type").agg(
    {'avg_read_out_price': ['sum', 'count']}
)
180/47:
contracts.groupby(by="type").agg(
    {'estimated_amount': ['sum', 'count']}
    {'avg_read_out_price': ['sum', 'count']}
    {'contract_price': ['sum', 'count']}
)
180/48:
contracts.groupby(by="type").agg(
    {'estimated_amount': ['sum', 'count']}
    {'avg_read_out_price': ['sum', 'count']}
#     {'contract_price': ['sum', 'count']}
)
180/49:
contracts.groupby(by="type").agg(
    {'estimated_amount': ['sum', 'count']},
    {'avg_read_out_price': ['sum', 'count']},
    {'contract_price': ['sum', 'count']}
)
180/50:
contracts.groupby(by="type").agg(
#     {'estimated_amount': ['sum', 'count']},
#     {'avg_read_out_price': ['sum', 'count']},
    {'contract_price': ['sum', 'count']}
)
180/51:
contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum', 'count'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum', 'count']
    }
)
180/52:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum', 'count'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum', 'count']
    }
)
180/53:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum', 'count'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum', 'count']
    }
)

contracts_by_type
180/54:
contracts_by_method = contracts.agg(
    {
        'estimated_amount': ['sum', 'count'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum', 'count']
    }
)

contracts_by_method
180/55:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum', 'count'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum', 'count']
    }
)

contracts_by_method
180/56:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)

contracts_by_method
180/57:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/58:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/59:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/60:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/61: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/62: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/63: contracts.head()
180/64: contracts.dtypes
180/65: contracts.to_csv('estimate_avg_read_out_price.csv')
180/66:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum', 'count'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum', 'count']
    }
)

contracts_by_type
180/67:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)

contracts_by_method
180/68:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

contracts_by_type
180/69:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum', 'count'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum', 'count']
    }
)

contracts_by_method
180/70:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum', 'count'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

contracts_by_method
180/71:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

contracts_by_method
180/72:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/73:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/74: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/75: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/76: contracts.head()
180/77: contracts.dtypes
180/78: contracts.to_csv('estimate_avg_read_out_price.csv')
180/79:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
180/80:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
180/81: contracts.to_csv('contracts.xlsx')
180/82: contracts.to_excel('contracts.xlsx')
180/83:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/84:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/85: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/86: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/87: contracts.head()
180/88: contracts.to_excel('contracts.xlsx')
180/89: contracts.dtypes
180/90: contracts.to_csv('estimate_avg_read_out_price.csv')
180/91:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
180/92:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
180/93:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/94:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/95: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/96: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/97: contracts.head()
180/98: contracts.to_excel('contracts.csv')
180/99: contracts.to_csv('contracts.csv')
180/100:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/101:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/102: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/103: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/104: contracts.head()
180/105: contracts.to_csv('contracts.csv')
180/106: contracts.dtypes
180/107: contracts.to_csv('estimate_avg_read_out_price.csv')
180/108:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
180/109:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
180/110:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/111:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/112: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/113: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/114: contracts.head()
180/115: contracts.to_csv('contracts.csv')
180/116: contracts.dtypes
180/117: contracts.to_csv('estimate_avg_read_out_price.csv')
180/118:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
180/119:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
180/120:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
180/121:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/122:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/123: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/124: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/125: contracts.head()
180/126: contracts.to_csv('contracts.csv')
180/127:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
180/128:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <=, 'within', 'above')
180/129:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
180/130:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/131:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/132: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/133: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/134: contracts.head()
180/135: contracts.to_csv('contracts.csv')
180/136:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
180/137:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
180/138: contracts.dtypes
180/139: contracts.to_csv('estimate_avg_read_out_price.csv')
180/140:
contracts_by_type = contracts.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
180/141:
contracts_by_method = contracts.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
180/142: contracts.head()
180/143: contracts_above_estimate.head()
180/144: contracts_above_estimate = contracts[contracts['price_status'] == 'Above']
180/145: contracts_above_estimate.head()
180/146: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
180/147: contracts_above_estimate.head()
180/148:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
180/149:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
180/150: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
180/151: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
180/152: contracts.head()
180/153: contracts.to_csv('contracts.csv')
180/154:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
180/155:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
180/156: contracts.head()
180/157: contracts.dtypes
180/158: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
180/159: contracts_above_estimate.head()
180/160:

contracts_above_estimate.to_csv('contracts_above_estimate.csv')
180/161:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
180/162:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
181/1: %pwd
181/2: a = [1, 2, 3, 4, 5]
181/3: a?
182/1: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 1000000000000]
182/2: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 1000000000000]]
182/3: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 1000000]
182/4:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/5:
contracts = pd.read_csv('estimate_read_out_contract_price_2.csv')

contracts.head()
182/6: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/7: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/8: contracts.head()
182/9: contracts.to_csv('contracts.csv')
182/10:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/11:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/12: contracts.head()
182/13: contracts.dtypes
182/14: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/15: contracts_above_estimate.head()
182/16: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 1000000000000]
182/17:

contracts_above_estimate.to_csv('contracts_above_estimate.csv')
182/18:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/19:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/20: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/21: contracts_above_estimate[contracts_above_estimate['award_price'] < 1]
182/22:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/23:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/24:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/25: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/26: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/27: contracts.head()
182/28: contracts.to_csv('contracts.csv')
182/29:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/30:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/31: contracts.head()
182/32: contracts.dtypes
182/33: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/34: contracts_above_estimate.head()
182/35: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/36: contracts_above_estimate[contracts_above_estimate['award_price'] < 1]
182/37:

contracts_above_estimate.to_csv('contracts_above_estimate.csv')
182/38:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/39:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/40: contracts_above_estimate[contracts_above_estimate['is_frame_work'] == True | contracts_above_estimate['is_frame_work'] == True]
182/41: contracts_above_estimate[contracts_above_estimate['is_framework'] == True | contracts_above_estimate['is_frame_work'] == True]
182/42:

contracts_above_estimate[contracts_above_estimate['is_framework'] == True | contracts_above_estimate['is_lotted_framework'] == True]
182/43:

contracts_above_estimate[contracts_above_estimate['is_framework'] == True or contracts_above_estimate['is_lotted_framework'] == True]
182/44:

contracts_above_estimate[contracts_above_estimate['is_framework'] == True]
182/45:

contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]
182/46:

framework_contracts = contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]

framework_contracts
182/47:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] != True) | (contracts_above_estimate['is_lotted_framework'] != True)]

contracts_above_estimate_without_frameworks.head()
182/48:

contracts_above_estimate.to_csv('contracts_above_estimate.csv')
182/49:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/50:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/51:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/52: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/53: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/54: contracts.head()
182/55: contracts.to_csv('contracts.csv')
182/56:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/57:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/58: contracts.head()
182/59: contracts.dtypes
182/60: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/61: contracts_above_estimate.head()
182/62: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/63:

framework_contracts = contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]

framework_contracts
182/64:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] != True) | (contracts_above_estimate['is_lotted_framework'] != True)]

contracts_above_estimate_without_frameworks.head()
182/65:

contracts_above_estimate.to_csv('contracts_above_estimate.csv')
182/66:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/67:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/68:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/69:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/70: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/71: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/72: contracts.head()
182/73: contracts.to_csv('contracts.csv')
182/74:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/75:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/76: contracts.head()
182/77: contracts.dtypes
182/78: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/79: contracts_above_estimate.head()
182/80: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/81:

framework_contracts = contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]

framework_contracts
182/82:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] != True) | (contracts_above_estimate['is_lotted_framework'] != True)]

contracts_above_estimate_without_frameworks.head()
182/83:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/84:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/85:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/86:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/87:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/88: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/89: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/90: contracts.head()
182/91: contracts.to_csv('contracts.csv')
182/92:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/93:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/94: contracts.head()
182/95: contracts.dtypes
182/96: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/97: contracts_above_estimate.head()
182/98: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/99:

framework_contracts = contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]

framework_contracts
182/100:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] != True) | (contracts_above_estimate['is_lotted_framework'] != True)]

contracts_above_estimate_without_frameworks.head()
182/101:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/102:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/103:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/104:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/105:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/106: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/107: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/108: contracts.head()
182/109: contracts.to_csv('contracts.csv')
182/110:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/111:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/112: contracts.head()
182/113: contracts.dtypes
182/114: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/115: contracts_above_estimate.head()
182/116: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/117:

framework_contracts = contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]

framework_contracts
182/118:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] != True) | (contracts_above_estimate['is_lotted_framework'] != True)]

contracts_above_estimate_without_frameworks.head()
182/119:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/120:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/121:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/122:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/123:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/124: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/125: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/126: contracts.head()
182/127: contracts.to_csv('contracts.csv')
182/128:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/129:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/130: contracts.head()
182/131: contracts.dtypes
182/132: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/133: contracts_above_estimate.head()
182/134: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/135:

framework_contracts = contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]

framework_contracts
182/136:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] != True) | (contracts_above_estimate['is_lotted_framework'] != True)]

contracts_above_estimate_without_frameworks.head()
182/137:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/138:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/139:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/140:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] != True) | (contracts_above_estimate['is_lotted_framework'] != True)]

contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['is_framework'] == True]
182/141:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] == False) & (contracts_above_estimate['is_lotted_framework'] != False)]

contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['is_framework'] == True]
182/142:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/143:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/144: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/145: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/146: contracts.head()
182/147: contracts.to_csv('contracts.csv')
182/148:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/149:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/150: contracts.head()
182/151: contracts.dtypes
182/152: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/153: contracts_above_estimate.head()
182/154: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/155:

framework_contracts = contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]

framework_contracts
182/156:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] == False) & (contracts_above_estimate['is_lotted_framework'] != False)]

contracts_above_estimate_without_frameworks.head()
182/157:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/158:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/159:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/160:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/161:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/162: contracts['no_of_bids'] = np.where(contracts['no_of_bids'] == 0, 1, contracts['no_of_bids'])
182/163: contracts['avg_read_out_price'] = contracts['total_read_out_price'] / contracts['no_of_bids']
182/164: contracts.head()
182/165: contracts.to_csv('contracts.csv')
182/166:

contracts['difference_contract_price_estimate'] = contracts['contract_price'] - contracts['estimated_amount']
182/167:

contracts['price_status'] = np.where(contracts['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/168: contracts.head()
182/169: contracts.dtypes
182/170: contracts_above_estimate = contracts[contracts['price_status'] == 'above']
182/171: contracts_above_estimate.head()
182/172: contracts_above_estimate[contracts_above_estimate['estimated_amount'] > 100000000000]
182/173:

framework_contracts = contracts_above_estimate[(contracts_above_estimate['is_framework'] == True) | (contracts_above_estimate['is_lotted_framework'] == True)]

framework_contracts
182/174:

contracts_above_estimate_without_frameworks = contracts_above_estimate[(contracts_above_estimate['is_framework'] == False) & (contracts_above_estimate['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
182/175:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/176:
contracts_by_type = contracts_above_estimate.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/177:
contracts_by_method = contracts_above_estimate.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/178:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
182/179:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/180:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/181:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
182/182:

contracts_above_estimate_without_frameworks['no_of_bids'] = np.where(contracts_above_estimate_without_frameworks['no_of_bids'] == 0, 1, contracts_above_estimate_without_frameworks['no_of_bids'])
182/183:

contracts_above_estimate_without_frameworks['avg_read_out_price'] = contracts_above_estimate_without_frameworks['total_read_out_price'] / contracts_above_estimate_without_frameworks['no_of_bids']
182/184:

contracts_above_estimate_without_frameworks.head()
182/185:

contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] = contracts_above_estimate_without_frameworks['contract_price'] - contracts_above_estimate_without_frameworks['estimated_amount']
182/186:

contracts_above_estimate_without_frameworks['price_status'] = np.where(contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/187:

contracts_above_estimate_without_frameworks.head()
182/188:

contracts_above_estimate_without_frameworks.dtypes
182/189:

contracts_above_estimate_without_frameworks = contracts[contracts['price_status'] == 'above']
182/190:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/191:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/192:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
182/193:

contracts_above_estimate_without_frameworks['no_of_bids'] = np.where(contracts_above_estimate_without_frameworks['no_of_bids'] == 0, 1, contracts_above_estimate_without_frameworks['no_of_bids'])
182/194:

contracts_above_estimate_without_frameworks['avg_read_out_price'] = contracts_above_estimate_without_frameworks['total_read_out_price'] / contracts_above_estimate_without_frameworks['no_of_bids']
182/195:

contracts_above_estimate_without_frameworks.head()
182/196:

contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] = contracts_above_estimate_without_frameworks['contract_price'] - contracts_above_estimate_without_frameworks['estimated_amount']
182/197:

contracts_above_estimate_without_frameworks['price_status'] = np.where(contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/198:

contracts_above_estimate_without_frameworks.head()
182/199:

contracts_above_estimate_without_frameworks.dtypes
182/200:

contracts_above_estimate_without_frameworks = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['price_status'] == 'above']
182/201:

contracts_above_estimate_without_frameworks.head()
182/202:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/203:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/204:
contracts_by_method = contracts_above_estimate_without_frameworks.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/205:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/206:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/207:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
182/208:

contracts_above_estimate_without_frameworks['no_of_bids'] = np.where(contracts_above_estimate_without_frameworks['no_of_bids'] == 0, 1, contracts_above_estimate_without_frameworks['no_of_bids'])
182/209:

contracts_above_estimate_without_frameworks['avg_read_out_price'] = contracts_above_estimate_without_frameworks['total_read_out_price'] / contracts_above_estimate_without_frameworks['no_of_bids']
182/210:

contracts_above_estimate_without_frameworks.head()
182/211:

contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] = contracts_above_estimate_without_frameworks['contract_price'] - contracts_above_estimate_without_frameworks['estimated_amount']
182/212:

contracts_above_estimate_without_frameworks['price_status'] = np.where(contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/213:

contracts_above_estimate_without_frameworks.head()
182/214:

contracts_above_estimate_without_frameworks.dtypes
182/215:

# contracts_above_estimate_without_frameworks = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['price_status'] == 'above']
182/216:

# contracts_above_estimate_without_frameworks.head()
182/217:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/218:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/219:
contracts_by_method = contracts_above_estimate_without_frameworks.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/220:

contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['contract_price'] < 1]
182/221:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/222:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/223:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
182/224:

contracts_above_estimate_without_frameworks['no_of_bids'] = np.where(contracts_above_estimate_without_frameworks['no_of_bids'] == 0, 1, contracts_above_estimate_without_frameworks['no_of_bids'])
182/225:

contracts_above_estimate_without_frameworks['avg_read_out_price'] = contracts_above_estimate_without_frameworks['total_read_out_price'] / contracts_above_estimate_without_frameworks['no_of_bids']
182/226:

contracts_above_estimate_without_frameworks.head()
182/227:

contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] = contracts_above_estimate_without_frameworks['contract_price'] - contracts_above_estimate_without_frameworks['estimated_amount']
182/228:

contracts_above_estimate_without_frameworks['price_status'] = np.where(contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/229:

contracts_above_estimate_without_frameworks.head()
182/230:

contracts_above_estimate_without_frameworks.dtypes
182/231:

# contracts_above_estimate_without_frameworks = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['price_status'] == 'above']
182/232:

# contracts_above_estimate_without_frameworks.head()
182/233:

contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['contract_price'] < 1]
182/234:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/235:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/236:
contracts_by_method = contracts_above_estimate_without_frameworks.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
182/237:

contracts_above_estimate_without_frameworks.value_counts
182/238:

contracts_above_estimate_without_frameworks.value_counts()
182/239:

contracts_above_estimate_without_frameworks.values_count()
182/240:

contracts_above_estimate_without_frameworks.values_count
182/241:

contracts_above_estimate_without_frameworks.value_count
182/242:

contracts_above_estimate_without_frameworks.descibe()
182/243:

contracts_above_estimate_without_frameworks.describe()
182/244:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
182/245:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
182/246:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
182/247:

contracts_above_estimate_without_frameworks['no_of_bids'] = np.where(contracts_above_estimate_without_frameworks['no_of_bids'] == 0, 1, contracts_above_estimate_without_frameworks['no_of_bids'])
182/248:

contracts_above_estimate_without_frameworks['avg_read_out_price'] = contracts_above_estimate_without_frameworks['total_read_out_price'] / contracts_above_estimate_without_frameworks['no_of_bids']
182/249:

contracts_above_estimate_without_frameworks.head()
182/250:

contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] = contracts_above_estimate_without_frameworks['contract_price'] - contracts_above_estimate_without_frameworks['estimated_amount']
182/251:

contracts_above_estimate_without_frameworks['price_status'] = np.where(contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] <= 0, 'within', 'above')
182/252:

contracts_above_estimate_without_frameworks.head()
182/253:

contracts_above_estimate_without_frameworks.dtypes
182/254:

# contracts_above_estimate_without_frameworks = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['price_status'] == 'above']
182/255:

# contracts_above_estimate_without_frameworks.head()
182/256:

contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['contract_price'] < 1]
182/257:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
182/258:

contracts_above_estimate_without_frameworks.describe()
182/259:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
182/260:
contracts_by_method = contracts_above_estimate_without_frameworks.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
183/1:

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tabula
183/2:

ed1996 = tabula.read_pdf('Const_Res-tab_1996.pdf', multipla_tables=True, 
                         stream=True, pages='1-11',
                         pandas_options={'header': None})

len(ed1996)
183/3: ed1996.head()
183/4: ed1996.sample(frac=.78)
183/5:

ed1996.to_csv('ed1996.csv', index=None, header=False)
183/6:
distSumed2001 = tabula.read_pdf('Dist_Sum_2001.pdf', 
                         stream=True, pages='1-5',
                         pandas_options={'header': None})

len(distSumed2001)
183/7: distSumed2001.tail()
183/8:


distSumed2001.to_csv('distSumed2001.csv', index=None, header=False)
183/9:
constSumed2001 = tabula.read_pdf('sum_const_2001.pdf', multiple_tables=True, 
                         stream=True, pages='1-62', pandas_options={'header': None})

len(constSumed2001)
183/10: constSumed2001[1]
183/11: constSumed2001.to_csv('constSumed2001.csv', index=None, header=False)
185/1: # strings
185/2: ' single quotes'
185/3: "this is a string"
185/4: "I can'nt go"
185/5: "I can't go"
185/6: print("my number is {} and name is {}".format(num, name))
185/7:
num =12
name = "John"
185/8: print("my number is {} and name is {}".format(num, name))
185/9: s = 'Hello'
185/10: s[0]
185/11: s[4]
185/12: s[:]
185/13: s = 'abcdefghijk'
185/14: s[0]
185/15: s[0:5]
185/16: s[5:]
185/17: [1, 2, 3]
185/18: ['a, b, c']
185/19: my_list = ['a', 'b', 'c', 'd']
185/20: my_list.append('e')
185/21: my_list
185/22: my_list[0]
185/23: my_list[0] = 'New'
185/24: my_list
185/25: # Dictionaries
185/26: d = { 'Key1': 'Value1', 'Key2': 123 }
185/27: d['Key2']
185/28: d = { 'Key3': [1, 2, 3] }
185/29: d
185/30: d['key3']
185/31: d['Key3']
185/32: [num for num in range(5)]
185/33:
def my_func(name):
    print('Hello {}'.format(name))
185/34: my_func('John Doe')
185/35: times2 = lambda num: num*2
185/36: times2(4)
185/37: list(map(times2, range(5)))
185/38: list(filter(lambda num: num%2 == 0, range(5)))
186/1: 7**4
186/2: list(s)
186/3: list(s)
186/4: list(s)
186/5:
s = "Hi there Sam!"
list(s)
186/6:
s = "Hi there Sam!"
list(s.spli(' '))
186/7:
s = "Hi there Sam!"
list(s.split(' '))
186/8:
planet = "Earth"
diameter = 12742
186/9: "The diameter of {} is {}".format(planet, diameter)
186/10: lst = [1,2,[3,4],[5,[100,200,['hello']],23,11],1,7]
186/11: lst[3][1][2]
186/12:
x = lst[3][1][2]

print(x)
186/13: lst[3][1][2].toString()
186/14: lst[3][1][2]
186/15: d = {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]}
186/16: d = {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]}
186/17: d['k1']
186/18: d['k1']['tricky']
186/19: d['k1'].tricky
186/20: d['k1']
186/21: d['k1'].values
186/22: d['k1'].values()
186/23: d.values()
186/24: d['k1']
186/25: x = d['k1']
186/26:
x = d['k1']
x
186/27: d['k1'][3]
186/28: d['k1'][3]['tricky']
186/29: d['k1'][3]['tricky'][3]
186/30: d['k1'][3]['tricky'][3]['target']
186/31: d['k1'][3]['tricky'][3]['target'][3]
186/32: # Tuple is immutable
186/33: # List id mutable
186/34:
def getEmail(webdomain):
    return webdomain.split('@')
186/35: getEmail(user@domain.com)
186/36:
def getEmail(webdomain):
    return webdomain.split('@')
186/37: getEmail('user@domain.com')
186/38:
def getEmail(webdomain):
    return webdomain.split('@')[0]
186/39: getEmail('user@domain.com')
186/40:
def getDomain(webdomain):
    return webdomain.split('@')[1]
186/41: getEmail('user@domain.com')
186/42: getEmail('user@domain.com')
186/43:
def getDomain(webdomain):
    return webdomain.split('@')
186/44: getEmail('user@domain.com')
186/45:
def getDomain(webdomain):
    return webdomain.split('@')
186/46:
def getDomain(webdomain):
    return webdomain.split('@')[1]
186/47: getDomain('user@domain.com')
186/48:
def findDog(word_list):
    if 'dog' in word_list:
        return True
186/49: findDog('Is there a dog here?')
186/50: countDog('This dog runs faster than the other dog dude!')
186/51:
def countDog(word_list):
    counter = 0;
    for word in word_list:
        if 'dog' == word:
            counter++
    return counter
186/52:
def countDog(word_list):
    counter = 0;
    for word in word_list:
        if 'dog' == word:
            counter += 1
    return counter
186/53: countDog('This dog runs faster than the other dog dude!')
186/54:
def countDog(word_list):
    counter = 0;
    for word in word_list:
        if 'dog' == word:
            counter += 1
    return word_list
186/55: countDog('This dog runs faster than the other dog dude!')
186/56:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
        if 'dog' == word:
            counter += 1
    return counter
186/57: countDog('This dog runs faster than the other dog dude!')
186/58:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
        if 'dog' == word:
            counter += 1
    return list(word_list)
186/59: countDog('This dog runs faster than the other dog dude!')
186/60:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
        if 'dog' == word:
            counter += 1
    return list(word_list).split(' ')
186/61: countDog('This dog runs faster than the other dog dude!')
186/62:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
        if 'dog' == word:
            counter += 1
    return word_list

countDog('This dog runs faster than the other dog dude!')
186/63:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
        if 'dog' in list(word_list):
            counter += 1
    return word_list

countDog('This dog runs faster than the other dog dude!')
186/64:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
        if 'dog' in list(word_list):
            counter += 1
    return word_list

countDog('This dog runs faster than the other dog dude!')
186/65:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
        if 'dog' in list(word_list):
            counter += 1
    return counter

countDog('This dog runs faster than the other dog dude!')
186/66:
def countDog(word_list):
    counter = 0;
    if 'dog' in list(word_list):
        counter += 1
    return counter

countDog('This dog runs faster than the other dog dude!')
186/67:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
        print(word)
        if 'dog' in list(word_list):
            counter += 1
#     return counter

countDog('This dog runs faster than the other dog dude!')
186/68:
def countDog(word_list):
    counter = 0;
    for word in list(word_list):
#         print(word)
        if 'dog' in list(word_list):
            counter += 1
    return word_list.split(' ')

countDog('This dog runs faster than the other dog dude!')
186/69:
def countDog(word_list):
    counter = 0;
    for word in word_list.split(' '):
#         print(word)
        if 'dog' in list(word_list):
            counter += 1
    return word_list.split(' ')

countDog('This dog runs faster than the other dog dude!')
186/70:
def countDog(word_list):
    counter = 0;
    for word in word_list.split(' '):
        print(word)
        if 'dog' in list(word_list):
            counter += 1
    return word_list.split(' ')

countDog('This dog runs faster than the other dog dude!')
186/71:
def countDog(word_list):
    counter = 0;
    for word in word_list.split(' '):
        if 'dog' in list(word_list):
            counter += 1
    return counter

countDog('This dog runs faster than the other dog dude!')
186/72:
def countDog(word_list):
    counter = 0;
    for word in word_list.split(' '):
        if 'dog' == word:
            counter += 1
    return counter

countDog('This dog runs faster than the other dog dude!')
186/73: countDog('This dog runs faster than the other dog dude!')
186/74:
def countDog(word_list):
    counter = 0;
    for word in word_list.split(' '):
        if 'dog' == word:
            counter += 1
    return counter
186/75: countDog('This dog runs faster than the other dog dude!')
186/76: countDog('This dog runs faster than the other dog dude!')
186/77: list(filter(lambda seq: seq[0] == 's'))
186/78: list(filter(lambda seq: seq[0] == 's', seq))
186/79: list(filter(lambda x: x[0] == 's', seq))
186/80: list(filter(lambda x: x[0] == 's', for x in seq))
186/81: list(filter(lambda x: x[0] == 's', seq))
186/82: seq = ['soup','dog','salad','cat','great']
186/83: list(filter(lambda x: x[0] == 's', seq))
186/84:
def caught_speeding(speed, is_birthday):
    if speed <= 60:
        return 'Small Ticket'
    elif speed >= 61 and speed <= 80:
        return 'No Ticket'
    elif speed >= 81:
        return 'Big Ticket'
    pass
186/85:
def caught_speeding(speed, is_birthday):
    if speed <= 60:
        return 'No Ticket'
    elif speed >= 61 and speed <= 80:
        return 'Small Ticket'
    elif speed >= 81:
        return 'Big Ticket'
186/86: caught_speeding(81,True)
186/87:
def caught_speeding(speed, is_birthday):
    if speed <= 60:
        return 'No Ticket'
    elif speed >= 61 and speed <= 80:
        return 'Small Ticket'
    elif speed >= 81:
        return 'Big Ticket'
186/88:
def caught_speeding(speed, is_birthday):
    if speed <= 60 and is_birthday == False:
        return 'No Ticket'
    elif speed >= 61 and speed <= 80:
        return 'Small Ticket'
    elif speed >= 81:
        return 'Big Ticket'
186/89:
def caught_speeding(speed, is_birthday):
    if is_birthday == True:
        speed = speed * 5
    if speed <= 60:
        return 'No Ticket'
    elif speed >= 61 and speed <= 80:
        return 'Small Ticket'
    elif speed >= 81:
        return 'Big Ticket'
186/90: caught_speeding(81,True)
186/91: caught_speeding(81,False)
186/92:
def caught_speeding(speed, is_birthday):
    if is_birthday == True:
        speed *= 5
    if speed <= 60:
        return 'No Ticket'
    elif speed >= 61 and speed <= 80:
        return 'Small Ticket'
    elif speed >= 81:
        return 'Big Ticket'
186/93: caught_speeding(81,False)
186/94: caught_speeding(81, True)
186/95:
def caught_speeding(speed, is_birthday):
    if is_birthday == True:
        speed += 5
    else:
        speed -= 5
        
    if speed <= 60:
        return 'No Ticket'
    elif speed >= 61 and speed <= 80:
        return 'Small Ticket'
    elif speed >= 81:
        return 'Big Ticket'
186/96: caught_speeding(81, True)
186/97:
def caught_speeding(speed, is_birthday):
    if is_birthday == True:
        speed -= 5
    else:
        speed = speed
        
    if speed <= 60:
        return 'No Ticket'
    elif speed >= 61 and speed <= 80:
        return 'Small Ticket'
    elif speed >= 81:
        return 'Big Ticket'
186/98: caught_speeding(81, True)
186/99: caught_speeding(81,True)
186/100: caught_speeding(81,False)
188/1: my_list = [1, 2, 3]
188/2: import numpy as np
188/3: my_list = [1, 2, 3]
188/4: my_list = [1, 2, 3]
188/5: my_list
188/6: np.array(my_list)
188/7: arr = np.array(my_list)
188/8: arr
188/9: my_mat = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
188/10: np.array(my_mat)
188/11:  np.arange(5)
188/12:  np.arange(0, 10)
188/13:  np.arange(0, 12, 2)
188/14: np.zeros
188/15: np.zeros(3)
188/16: np.zeros(2, 3)
188/17: np.zeros((2, 3))
188/18: np.ones(4)
188/19: np.ones((2, 4))
188/20: np.ones((3, 4))
188/21: np.linspace(0, 5, 1000)
188/22: np.eyes(4)
188/23: np.eye(4)
188/24: np.random.rand(5)
188/25: np.random.rand((2,5))
188/26: np.random.rand(2,5)
188/27: np.random.randn(2)
188/28: np.random.randn(4, 4)
188/29: np.random.randint(1, 100)
188/30: np.random.randint(1, 100)
188/31: np.random.randint(1, 100, 10)
188/32: arr = np.random.rand(2,5)
188/33: arr.reshape(5, 5)
188/34: arr = np.random.randint(1, 100, 10)
188/35: arr.reshape(5, 5)
188/36: np.random.randint(1, 100, 10)
188/37: arr.reshape(5, 5)
188/38: my_list = [1, 2, 3]
188/39: my_list
188/40: import numpy as np
188/41: arr = np.array(my_list)
188/42: arr
188/43: my_mat = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
188/44: np.array(my_mat)
188/45:  np.arange(0, 12, 2)
188/46: np.zeros(3)
188/47: np.zeros((2, 3))
188/48: np.ones(4)
188/49: np.ones((3, 4))
188/50: np.linspace(0, 5, 1000)
188/51: np.eye(4)
188/52: arr = np.random.rand(2,5)
188/53: np.random.randn(4, 4)
188/54: np.random.randint(1, 100, 10)
188/55: arr.reshape(5, 5)
188/56: arr = np.arange(25)
188/57: arr.reshape(5, 5)
188/58: rarr = np.random.randint(1, 100, 10)
188/59: rarr
188/60: rarr.max
188/61: rarr
188/62: rarr.max()
188/63: rarr.min()
188/64: rarr.argmin()
188/65: rarr.argmax()
188/66: rarr.shape
188/67: rarr.reshape(5, 5)
188/68: arr.reshape(5, 5)
188/69: arr = np.arange(0, 11)
188/70: arr
188/71: my_list = [1, 2, 3]
188/72: my_list
188/73: import numpy as np
188/74: arr = np.array(my_list)
188/75: arr
188/76: my_mat = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
188/77: np.array(my_mat)
188/78:  np.arange(0, 12, 2)
188/79: np.zeros(3)
188/80: np.zeros((2, 3))
188/81: np.ones(4)
188/82: np.ones((3, 4))
188/83: np.linspace(0, 5, 1000)
188/84: np.eye(4)
188/85: arr = np.random.rand(2,5)
188/86: np.random.randn(4, 4)
188/87: rarr = np.random.randint(1, 100, 10)
188/88: arr = np.arange(25)
188/89: arr.reshape(5, 5)
188/90: rarr
188/91: rarr.max()
188/92: rarr.min()
188/93: rarr.argmin()
188/94: rarr.argmax()
188/95: rarr.shape
188/96: arr.reshape(5, 5)
188/97: arr = np.arange(0, 11)
188/98: arr
189/1: pip list | grep "matplotlib*\|numpy*"
189/2: %pwd
189/3: a = [1, 2, 3, 4, 5]
189/4: a?
189/5: a?
189/6: import numpy as np
189/7: data = {i: np.random.randn() for i in range(7)}
189/8: data
189/9: data = {i: np.random.randn() for i in range(7)}
189/10: data
189/11: import datetime
189/12: datetime?
189/13: datetime
189/14: datetime.
189/15: datetime
189/16: b = [1, 2, 3]
189/17: b?
189/18: print?
189/19:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
189/20: add_numbers?
189/21: add_numbers??
189/22:
# A single question mark shows you the doc string

add_numbers?
189/23:
# Double question marks show you the source code

add_numbers??
189/24: np.*load*?
189/25:
x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
189/26: %cpaste
189/27:
%paste
x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
189/28: %cpaste
189/29: %paste
189/30:

x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
189/31: %cpaste
189/32:
a = np.random.randn(100, 100)
%timeit np.dot(a, a)
189/33:
a = np.random.randn(100, 100)
%timeit np.dot(a, a)
189/34:
a = np.random.randn(100, 100)
%timeit np.dot(a, a)
189/35: %debug?
189/36: %quickref
189/37: %magic
189/38: %matplotlib
189/39:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50), cumsum())
189/40:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
189/41:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
189/42: a = [1, 2, 3]
189/43: b = a
189/44: a.append(4)
189/45: a
189/46: b
189/47: a = 5
189/48: isinstance(a, int)
189/49: isinstance(a, float)
189/50: isinstance(a, str)
189/51: isinstance(a, int)
189/52: a = 'foo'
189/53: a.
189/54: 5 / 2
189/55: 5 // 2
189/56: from datetime import datetime, date, time
189/57: dt = datetime(2019, 10, 29, 20, 30, 21)
189/58: dt.days
189/59: dt.day
189/60: dt.minute
189/61: dt.date()
189/62: dt.time()
189/63: dt.strftime('%m/%d/%Y %H:%M')
189/64: dt.strftime('%m/%d/%y %H:%M')
189/65: dt.strftime('%m/%d/%Y %H:%M')
189/66:
# Formate datetime as a string

dt.strftime('%m/%d/%Y %H:%M')
189/67:
# Convert string to datetime object

dt.strptime('20191031', '%Y%m%d')
189/68:
# Turples

tup = 4, 5, 6

tup
189/69: nested_turples = (4, 5, 6), (7, 8)
189/70: nested_turples
189/71:
# Tuples

tup = 4, 5, 6

tup
189/72: nested_tuples = (4, 5, 6), (7, 8)
189/73: nested_tuples
189/74:
# sort

a = [7, 5, 3, 2, 1]

a.sort()
189/75:
# sort

a = [7, 5, 3, 2, 1]
189/76: a.sort()
189/77: a
189/78: a.rsort()
189/79: a
189/80:

b = ['saw', 'small', 'He', 'foxes', six]
189/81:

b = ['saw', 'small', 'He', 'foxes', 'six']
189/82: b.sort(key=len)
189/83: b
189/84: import bisect
189/85:
c = [1, 2, 2, 2, 3, 4, 7]

bisect.bisect(c, 2)
189/86: bisect.bisect(c, 5)
189/87: bisect.insort(c, 5)
189/88: c
189/89: bisect.insort(c, 6)
189/90: c
189/91:
# Slicing

seq = [7, 2, 3, 7, 5, 6, 0, 1]

seq[-6:-2]
189/92: seq[::2]
189/93: seq[::1]
189/94: seq[::2]
189/95: seq[::-1]
189/96:
# enumerate

some_list = ['foo', 'bar', 'baz']

mapping = {}

for i, v in enumerate(some_list):
    mapping[v] = i
189/97: mapping
191/1:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
191/2: import numpy as np
191/3:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
191/4: z = np.sqrt(xs ** 2 + ys ** 2)
191/5: z
191/6: plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()
191/7: pip list | grep "matplotlib*\|numpy*"
191/8: %pwd
191/9: a = [1, 2, 3, 4, 5]
191/10: a?
191/11: import numpy as np
191/12: data = {i: np.random.randn() for i in range(7)}
191/13: data
191/14: import datetime
191/15: datetime
191/16: b = [1, 2, 3]
191/17: b?
191/18: print?
191/19:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
191/20:
# A single question mark shows you the doc string

add_numbers?
191/21:
# Double question marks show you the source code

add_numbers??
191/22: np.*load*?
191/23: %paste
191/24: plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()
191/25:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
191/26: pip list | grep "matplotlib*\|numpy*"
191/27: %pwd
191/28: a = [1, 2, 3, 4, 5]
191/29: a?
191/30: import numpy as np
191/31: data = {i: np.random.randn() for i in range(7)}
191/32: data
191/33: import datetime
191/34: datetime
191/35: b = [1, 2, 3]
191/36: b?
191/37: print?
191/38:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
191/39:
# A single question mark shows you the doc string

add_numbers?
191/40:
# Double question marks show you the source code

add_numbers??
191/41: np.*load*?
191/42: %paste
191/43: pip list | grep "matplotlib*\|numpy*"
191/44: %pwd
191/45: a = [1, 2, 3, 4, 5]
191/46: a?
191/47: import numpy as np
191/48: data = {i: np.random.randn() for i in range(7)}
191/49: data
191/50: import datetime
191/51: datetime
191/52: b = [1, 2, 3]
191/53: b?
191/54: print?
191/55:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
191/56:
# A single question mark shows you the doc string

add_numbers
191/57:
# Double question marks show you the source code

add_numbers
191/58: np.*load*
191/59: pip list | grep "matplotlib*\|numpy*"
191/60: %pwd
191/61: a = [1, 2, 3, 4, 5]
191/62: a?
191/63: import numpy as np
191/64: data = {i: np.random.randn() for i in range(7)}
191/65: data
191/66: import datetime
191/67: datetime
191/68: b = [1, 2, 3]
191/69: # b?
191/70: # print?
191/71:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
191/72:
# A single question mark shows you the doc string

add_numbers
191/73:
# Double question marks show you the source code

add_numbers
191/74: # np.*load*?
191/75: %paste
191/76: pip list | grep "matplotlib*\|numpy*"
191/77: %pwd
191/78: a = [1, 2, 3, 4, 5]
191/79: a?
191/80: import numpy as np
191/81: data = {i: np.random.randn() for i in range(7)}
191/82: data
191/83: import datetime
191/84: datetime
191/85: b = [1, 2, 3]
191/86: # b?
191/87: # print?
191/88:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
191/89:
# A single question mark shows you the doc string

add_numbers
191/90:
# Double question marks show you the source code

add_numbers
191/91: # np.*load*?
191/92: # %paste
191/93:

x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
191/94: # %cpaste
191/95:
a = np.random.randn(100, 100)
# %timeit np.dot(a, a)
191/96: # %debug
191/97: # %quickref
191/98: # %magic
191/99: %matplotlib
191/100:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
191/101: a = [1, 2, 3]
191/102: b = a
191/103: a.append(4)
191/104: a
191/105: b
191/106: a = 5
191/107: isinstance(a, int)
191/108: a = 'foo'
191/109: 5 / 2
191/110: 5 // 2
191/111: from datetime import datetime, date, time
191/112: dt = datetime(2019, 10, 29, 20, 30, 21)
191/113: dt.day
191/114: dt.minute
191/115: dt.date()
191/116: dt.time()
191/117:
# Formate datetime as a string

dt.strftime('%m/%d/%Y %H:%M')
191/118:
# Convert string to datetime object

dt.strptime('20191031', '%Y%m%d')
191/119:
# Tuples

tup = 4, 5, 6

tup
191/120: nested_tuples = (4, 5, 6), (7, 8)
191/121: nested_tuples
191/122:
# sort

a = [7, 5, 3, 2, 1]
191/123: a.sort()
191/124: a
191/125:

b = ['saw', 'small', 'He', 'foxes', 'six']
191/126: b.sort(key=len)
191/127: b
191/128: import bisect
191/129:
c = [1, 2, 2, 2, 3, 4, 7]

bisect.bisect(c, 2)
191/130: bisect.bisect(c, 5)
191/131: bisect.insort(c, 6)
191/132: c
191/133:
# Slicing

seq = [7, 2, 3, 7, 5, 6, 0, 1]

seq[-6:-2]
191/134: seq[::2]
191/135: seq[::-1]
191/136:
# enumerate

some_list = ['foo', 'bar', 'baz']

mapping = {}

for i, v in enumerate(some_list):
    mapping[v] = i
191/137: mapping
191/138: import numpy as np
191/139:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
191/140: z = np.sqrt(xs ** 2 + ys ** 2)
191/141: z
191/142: plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()
191/143:
plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()

plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
192/1:
plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()

plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
192/2: pip list | grep "matplotlib*\|numpy*"
192/3: %pwd
192/4: a = [1, 2, 3, 4, 5]
192/5: a?
192/6: import numpy as np
192/7: data = {i: np.random.randn() for i in range(7)}
192/8: data
192/9: import datetime
192/10: datetime
192/11: b = [1, 2, 3]
192/12: # b?
192/13: # print?
192/14:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
192/15:
# A single question mark shows you the doc string

add_numbers
192/16:
# Double question marks show you the source code

add_numbers
192/17: # np.*load*?
192/18: # %paste
192/19:

x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
192/20: # %cpaste
192/21:
a = np.random.randn(100, 100)
# %timeit np.dot(a, a)
192/22: # %debug
192/23: # %quickref
192/24: # %magic
192/25: %matplotlib
192/26:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
192/27: a = [1, 2, 3]
192/28: b = a
192/29: a.append(4)
192/30: a
192/31: b
192/32: a = 5
192/33: isinstance(a, int)
192/34: a = 'foo'
192/35: 5 / 2
192/36: 5 // 2
192/37: from datetime import datetime, date, time
192/38: dt = datetime(2019, 10, 29, 20, 30, 21)
192/39: dt.day
192/40: dt.minute
192/41: dt.date()
192/42: dt.time()
192/43:
# Formate datetime as a string

dt.strftime('%m/%d/%Y %H:%M')
192/44:
# Convert string to datetime object

dt.strptime('20191031', '%Y%m%d')
192/45:
# Tuples

tup = 4, 5, 6

tup
192/46: nested_tuples = (4, 5, 6), (7, 8)
192/47: nested_tuples
192/48:
# sort

a = [7, 5, 3, 2, 1]
192/49: a.sort()
192/50: a
192/51:

b = ['saw', 'small', 'He', 'foxes', 'six']
192/52: b.sort(key=len)
192/53: b
192/54: import bisect
192/55:
c = [1, 2, 2, 2, 3, 4, 7]

bisect.bisect(c, 2)
192/56: bisect.bisect(c, 5)
192/57: bisect.insort(c, 6)
192/58: c
192/59:
# Slicing

seq = [7, 2, 3, 7, 5, 6, 0, 1]

seq[-6:-2]
192/60: seq[::2]
192/61: seq[::-1]
192/62:
# enumerate

some_list = ['foo', 'bar', 'baz']

mapping = {}

for i, v in enumerate(some_list):
    mapping[v] = i
192/63: mapping
192/64: import numpy as np
192/65:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
192/66: z = np.sqrt(xs ** 2 + ys ** 2)
192/67: z
192/68:
plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()

plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
192/69: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
192/70: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
192/71: cond = np.where([True, False, True, True, False])
192/72:
# Using List Comprehension

result = [(x if x else y) for x, y, z in zip(xarr, yarr, cond)]
192/73: result
192/74:
# Using List Comprehension

result = [(x if x else y) for x, y, z in zip(xarr, yarr, cond)]
192/75: result
192/76:
# Using List Comprehension

result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]
192/77: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
192/78: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
192/79: cond = np.where([True, False, True, True, False])
192/80:
# Using List Comprehension

result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]
192/81: result
192/82: result = np.where(cond, xarr, yarr)
192/83: pip list | grep "matplotlib*\|numpy*"
192/84: %pwd
192/85: a = [1, 2, 3, 4, 5]
192/86: a?
192/87: import numpy as np
192/88: data = {i: np.random.randn() for i in range(7)}
192/89: data
192/90: import datetime
192/91: datetime
192/92: b = [1, 2, 3]
192/93: # b?
192/94: # print?
192/95:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
192/96:
# A single question mark shows you the doc string

add_numbers
192/97:
# Double question marks show you the source code

add_numbers
192/98: # np.*load*?
192/99: # %paste
192/100:

x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
192/101: # %cpaste
192/102:
a = np.random.randn(100, 100)
# %timeit np.dot(a, a)
192/103: # %debug
192/104: # %quickref
192/105: # %magic
192/106: %matplotlib
192/107:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
192/108: a = [1, 2, 3]
192/109: b = a
192/110: a.append(4)
192/111: a
192/112: b
192/113: a = 5
192/114: isinstance(a, int)
192/115: a = 'foo'
192/116: 5 / 2
192/117: 5 // 2
192/118: from datetime import datetime, date, time
192/119: dt = datetime(2019, 10, 29, 20, 30, 21)
192/120: dt.day
192/121: dt.minute
192/122: dt.date()
192/123: dt.time()
192/124:
# Formate datetime as a string

dt.strftime('%m/%d/%Y %H:%M')
192/125:
# Convert string to datetime object

dt.strptime('20191031', '%Y%m%d')
192/126:
# Tuples

tup = 4, 5, 6

tup
192/127: nested_tuples = (4, 5, 6), (7, 8)
192/128: nested_tuples
192/129:
# sort

a = [7, 5, 3, 2, 1]
192/130: a.sort()
192/131: a
192/132:

b = ['saw', 'small', 'He', 'foxes', 'six']
192/133: b.sort(key=len)
192/134: b
192/135: import bisect
192/136:
c = [1, 2, 2, 2, 3, 4, 7]

bisect.bisect(c, 2)
192/137: bisect.bisect(c, 5)
192/138: bisect.insort(c, 6)
192/139: c
192/140:
# Slicing

seq = [7, 2, 3, 7, 5, 6, 0, 1]

seq[-6:-2]
192/141: seq[::2]
192/142: seq[::-1]
192/143:
# enumerate

some_list = ['foo', 'bar', 'baz']

mapping = {}

for i, v in enumerate(some_list):
    mapping[v] = i
192/144: mapping
192/145: import numpy as np
192/146:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
192/147: z = np.sqrt(xs ** 2 + ys ** 2)
192/148: z
192/149:
plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()

plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
192/150: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
192/151: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
192/152: cond = np.where([True, False, True, True, False])
192/153:
# Using List Comprehension

result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]
192/154: result
192/155: result = np.where(cond, xarr, yarr)
192/156: pip list | grep "matplotlib*\|numpy*"
192/157: %pwd
192/158: a = [1, 2, 3, 4, 5]
192/159: a?
192/160: import numpy as np
192/161: data = {i: np.random.randn() for i in range(7)}
192/162: data
192/163: import datetime
192/164: datetime
192/165: b = [1, 2, 3]
192/166: # b?
192/167: # print?
192/168:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
192/169:
# A single question mark shows you the doc string

add_numbers
192/170:
# Double question marks show you the source code

add_numbers
192/171: # np.*load*?
192/172: # %paste
192/173:

x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
192/174: # %cpaste
192/175:
a = np.random.randn(100, 100)
# %timeit np.dot(a, a)
192/176: # %debug
192/177: # %quickref
192/178: # %magic
192/179: %matplotlib
192/180:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
192/181: a = [1, 2, 3]
192/182: b = a
192/183: a.append(4)
192/184: a
192/185: b
192/186: a = 5
192/187: isinstance(a, int)
192/188: a = 'foo'
192/189: 5 / 2
192/190: 5 // 2
192/191: from datetime import datetime, date, time
192/192: dt = datetime(2019, 10, 29, 20, 30, 21)
192/193: dt.day
192/194: dt.minute
192/195: dt.date()
192/196: dt.time()
192/197:
# Formate datetime as a string

dt.strftime('%m/%d/%Y %H:%M')
192/198:
# Convert string to datetime object

dt.strptime('20191031', '%Y%m%d')
192/199:
# Tuples

tup = 4, 5, 6

tup
192/200: nested_tuples = (4, 5, 6), (7, 8)
192/201: nested_tuples
192/202:
# sort

a = [7, 5, 3, 2, 1]
192/203: a.sort()
192/204: a
192/205:

b = ['saw', 'small', 'He', 'foxes', 'six']
192/206: b.sort(key=len)
192/207: b
192/208: import bisect
192/209:
c = [1, 2, 2, 2, 3, 4, 7]

bisect.bisect(c, 2)
192/210: bisect.bisect(c, 5)
192/211: bisect.insort(c, 6)
192/212: c
192/213:
# Slicing

seq = [7, 2, 3, 7, 5, 6, 0, 1]

seq[-6:-2]
192/214: seq[::2]
192/215: seq[::-1]
192/216:
# enumerate

some_list = ['foo', 'bar', 'baz']

mapping = {}

for i, v in enumerate(some_list):
    mapping[v] = i
192/217: mapping
192/218: import numpy as np
192/219:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
192/220: z = np.sqrt(xs ** 2 + ys ** 2)
192/221: z
192/222:
plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()

plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
192/223: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
192/224: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
192/225: cond = np.array([True, False, True, True, False])
192/226:
# Using List Comprehension

result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]
192/227: result
192/228: result = np.where(cond, xarr, yarr)
192/229: result
192/230: arr = np.random.randn(4, 4)
192/231: arr
192/232: arr > 0
192/233: np.where(arr > 0, 2, -2)
192/234: np.where(arr > 0, 2, arr)
192/235: np.where(arr > 0, 2, 1)
192/236: np.where(arr > 0, 2, arr)
192/237: arr = np.random.randn(5, 4)
192/238: arr
192/239: arr.mean()
192/240: np.mean()
192/241: np.arr.mean()
192/242:

np.mean(arr)
192/243: arr.sum()
192/244: np.sum(arr)
192/245: arr.sum()
192/246: arr.mean(axis=1)
192/247: arr.mean(axis=0)
192/248:
# Sorting

arr = np.random.randn(6)
192/249: arr
192/250: arr.sort()
192/251: arr
192/252: arr = np.random.randn(5, 3)
192/253: arr
192/254: arr.sort(1)
192/255: arr
192/256: large_arr = np.random.randn(1000)
192/257: large_arr.sort()
192/258: large_arr[int(0.05 * len(large_arr))]
192/259: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])
192/260: np.unique(names)
192/261: ints  = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])
192/262: np.unique(ints)
192/263: sorted(set(names))
192/264:
values = np.array([6, 0, 0, 3, 2, 5, 6])

np.in1d(values, [2, 3, 6])
192/265: arr = np.arange(10)
192/266: arr
192/267: np.save('some_array', arr)
192/268: np.load('some_array.npy')
192/269: np.savez('array_archive.npz', a=arr, b=arr)
192/270:
arch = np.load('array_archive.npz')

arch['b']
192/271: np.saves_compressed('arrays_compressed.npz', a=arr, b=arr)
192/272: np.savez_compressed('arrays_compressed.npz', a=arr, b=arr)
192/273: # Linear Algebra
192/274: x = np.array([1., 2., 3.], [4., 5., 6.])
192/275: x = np.array([[1., 2., 3.], [4., 5., 6.]])
192/276: y = np.array([[6., 23.], [-1, 7], [8, 9]])
192/277: x
192/278: y
192/279: x.dot(y)
192/280: np.dot(x, y)
192/281: np.dot(x, np.ones(3))
192/282: x @ np.ones(3)
192/283: from numpy.linalg import inv, qr
192/284: X = np.random.randn(5, 5)
192/285: mat = X.T.dot(X)
192/286: inv(mat)
192/287: mat.dot(inv(mat))
192/288: q, r = qr(mat)
192/289: r
192/290: pip list | grep "matplotlib*\|numpy*"
192/291: %pwd
192/292: a = [1, 2, 3, 4, 5]
192/293: a?
192/294: import numpy as np
192/295: data = {i: np.random.randn() for i in range(7)}
192/296: data
192/297: import datetime
192/298: datetime
192/299: b = [1, 2, 3]
192/300: # b?
192/301: # print?
192/302:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
192/303:
# A single question mark shows you the doc string

add_numbers
192/304:
# Double question marks show you the source code

add_numbers
192/305: # np.*load*?
192/306: # %paste
192/307:

x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
192/308: # %cpaste
192/309:
a = np.random.randn(100, 100)
# %timeit np.dot(a, a)
192/310: # %debug
192/311: # %quickref
192/312: # %magic
192/313: %matplotlib
192/314:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
192/315: a = [1, 2, 3]
192/316: b = a
192/317: a.append(4)
192/318: a
192/319: b
192/320: a = 5
192/321: isinstance(a, int)
192/322: a = 'foo'
192/323: 5 / 2
192/324: 5 // 2
192/325: from datetime import datetime, date, time
192/326: dt = datetime(2019, 10, 29, 20, 30, 21)
192/327: dt.day
192/328: dt.minute
192/329: dt.date()
192/330: dt.time()
192/331:
# Formate datetime as a string

dt.strftime('%m/%d/%Y %H:%M')
192/332:
# Convert string to datetime object

dt.strptime('20191031', '%Y%m%d')
192/333:
# Tuples

tup = 4, 5, 6

tup
192/334: nested_tuples = (4, 5, 6), (7, 8)
192/335: nested_tuples
192/336:
# sort

a = [7, 5, 3, 2, 1]
192/337: a.sort()
192/338: a
192/339:

b = ['saw', 'small', 'He', 'foxes', 'six']
192/340: b.sort(key=len)
192/341: b
192/342: import bisect
192/343:
c = [1, 2, 2, 2, 3, 4, 7]

bisect.bisect(c, 2)
192/344: bisect.bisect(c, 5)
192/345: bisect.insort(c, 6)
192/346: c
192/347:
# Slicing

seq = [7, 2, 3, 7, 5, 6, 0, 1]

seq[-6:-2]
192/348: seq[::2]
192/349: seq[::-1]
192/350:
# enumerate

some_list = ['foo', 'bar', 'baz']

mapping = {}

for i, v in enumerate(some_list):
    mapping[v] = i
192/351: mapping
192/352: import numpy as np
192/353:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
192/354: z = np.sqrt(xs ** 2 + ys ** 2)
192/355: z
192/356:
plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()

plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
192/357: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
192/358: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
192/359: cond = np.array([True, False, True, True, False])
192/360:
# Using List Comprehension

result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]
192/361: result
192/362: result = np.where(cond, xarr, yarr)
192/363: result
192/364: arr = np.random.randn(4, 4)
192/365: arr
192/366: arr > 0
192/367: np.where(arr > 0, 2, -2)
192/368: np.where(arr > 0, 2, arr)
192/369: arr = np.random.randn(5, 4)
192/370: arr
192/371: arr.mean()
192/372:

np.mean(arr)
192/373: arr.sum()
192/374: arr.mean(axis=1)
192/375: arr.mean(axis=0)
192/376:
# Sorting

arr = np.random.randn(6)
192/377: arr
192/378: arr.sort()
192/379: arr
192/380: arr = np.random.randn(5, 3)
192/381: arr
192/382: arr.sort(1)
192/383: arr
192/384: large_arr = np.random.randn(1000)
192/385: large_arr.sort()
192/386: large_arr[int(0.05 * len(large_arr))]
192/387: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])
192/388: np.unique(names)
192/389: ints  = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])
192/390: np.unique(ints)
192/391: sorted(set(names))
192/392:
values = np.array([6, 0, 0, 3, 2, 5, 6])

np.in1d(values, [2, 3, 6])
192/393: arr = np.arange(10)
192/394: arr
192/395: np.save('some_array', arr)
192/396: np.load('some_array.npy')
192/397: np.savez('array_archive.npz', a=arr, b=arr)
192/398:
arch = np.load('array_archive.npz')

arch['b']
192/399: np.savez_compressed('arrays_compressed.npz', a=arr, b=arr)
192/400: # Linear Algebra
192/401: x = np.array([[1., 2., 3.], [4., 5., 6.]])
192/402: y = np.array([[6., 23.], [-1, 7], [8, 9]])
192/403: x
192/404: y
192/405: x.dot(y)
192/406: np.dot(x, y)
192/407: np.dot(x, np.ones(3))
192/408: x @ np.ones(3)
192/409: from numpy.linalg import inv, qr
192/410: X = np.random.randn(5, 5)
192/411: mat = X.T.dot(X)
192/412: inv(mat)
192/413: mat.dot(inv(mat))
192/414: q, r = qr(mat)
192/415: r
192/416:
import random

position = 0
walk = [position]
steps = 1000
for i in range(steps):
    step 1 if random.randint(0, 1) else -1
    position += step
    walk.append(position)
192/417:
import random

position = 0
walk = [position]
steps = 1000
for i in range(steps):
    step = 1 if random.randint(0, 1) else -1
    position += step
    walk.append(position)
192/418: plt.plot(walk[:100])
193/1: plt.plot(walk[:100])
193/2: pip list | grep "matplotlib*\|numpy*"
193/3: %pwd
193/4: a = [1, 2, 3, 4, 5]
193/5: a?
193/6: import numpy as np
193/7: data = {i: np.random.randn() for i in range(7)}
193/8: data
193/9: import datetime
193/10: datetime
193/11: b = [1, 2, 3]
193/12: # b?
193/13: # print?
193/14:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
193/15:
# A single question mark shows you the doc string

add_numbers
193/16:
# Double question marks show you the source code

add_numbers
193/17: # np.*load*?
193/18: # %paste
193/19:

x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
193/20: # %cpaste
193/21:
a = np.random.randn(100, 100)
# %timeit np.dot(a, a)
193/22: # %debug
193/23: # %quickref
193/24: # %magic
193/25: %matplotlib
193/26:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
193/27: a = [1, 2, 3]
193/28: b = a
193/29: a.append(4)
193/30: a
193/31: b
193/32: a = 5
193/33: isinstance(a, int)
193/34: a = 'foo'
193/35: 5 / 2
193/36: 5 // 2
193/37: from datetime import datetime, date, time
193/38: dt = datetime(2019, 10, 29, 20, 30, 21)
193/39: dt.day
193/40: dt.minute
193/41: dt.date()
193/42: dt.time()
193/43:
# Formate datetime as a string

dt.strftime('%m/%d/%Y %H:%M')
193/44:
# Convert string to datetime object

dt.strptime('20191031', '%Y%m%d')
193/45:
# Tuples

tup = 4, 5, 6

tup
193/46: nested_tuples = (4, 5, 6), (7, 8)
193/47: nested_tuples
193/48:
# sort

a = [7, 5, 3, 2, 1]
193/49: a.sort()
193/50: a
193/51:

b = ['saw', 'small', 'He', 'foxes', 'six']
193/52: b.sort(key=len)
193/53: b
193/54: import bisect
193/55:
c = [1, 2, 2, 2, 3, 4, 7]

bisect.bisect(c, 2)
193/56: bisect.bisect(c, 5)
193/57: bisect.insort(c, 6)
193/58: c
193/59:
# Slicing

seq = [7, 2, 3, 7, 5, 6, 0, 1]

seq[-6:-2]
193/60: seq[::2]
193/61: seq[::-1]
193/62:
# enumerate

some_list = ['foo', 'bar', 'baz']

mapping = {}

for i, v in enumerate(some_list):
    mapping[v] = i
193/63: mapping
193/64: import numpy as np
193/65:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
193/66: z = np.sqrt(xs ** 2 + ys ** 2)
193/67: z
193/68:
plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()

plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
193/69: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
193/70: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
193/71: cond = np.array([True, False, True, True, False])
193/72:
# Using List Comprehension

result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]
193/73: result
193/74: result = np.where(cond, xarr, yarr)
193/75: result
193/76: arr = np.random.randn(4, 4)
193/77: arr
193/78: arr > 0
193/79: np.where(arr > 0, 2, -2)
193/80: np.where(arr > 0, 2, arr)
193/81: arr = np.random.randn(5, 4)
193/82: arr
193/83: arr.mean()
193/84:

np.mean(arr)
193/85: arr.sum()
193/86: arr.mean(axis=1)
193/87: arr.mean(axis=0)
193/88:
# Sorting

arr = np.random.randn(6)
193/89: arr
193/90: arr.sort()
193/91: arr
193/92: arr = np.random.randn(5, 3)
193/93: arr
193/94: arr.sort(1)
193/95: arr
193/96: large_arr = np.random.randn(1000)
193/97: large_arr.sort()
193/98: large_arr[int(0.05 * len(large_arr))]
193/99: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])
193/100: np.unique(names)
193/101: ints  = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])
193/102: np.unique(ints)
193/103: sorted(set(names))
193/104:
values = np.array([6, 0, 0, 3, 2, 5, 6])

np.in1d(values, [2, 3, 6])
193/105: arr = np.arange(10)
193/106: arr
193/107: np.save('some_array', arr)
193/108: np.load('some_array.npy')
193/109: np.savez('array_archive.npz', a=arr, b=arr)
193/110:
arch = np.load('array_archive.npz')

arch['b']
193/111: np.savez_compressed('arrays_compressed.npz', a=arr, b=arr)
193/112: # Linear Algebra
193/113: x = np.array([[1., 2., 3.], [4., 5., 6.]])
193/114: y = np.array([[6., 23.], [-1, 7], [8, 9]])
193/115: x
193/116: y
193/117: x.dot(y)
193/118: np.dot(x, y)
193/119: np.dot(x, np.ones(3))
193/120: x @ np.ones(3)
193/121: from numpy.linalg import inv, qr
193/122: X = np.random.randn(5, 5)
193/123: mat = X.T.dot(X)
193/124: inv(mat)
193/125: mat.dot(inv(mat))
193/126: q, r = qr(mat)
193/127: r
193/128:
import random

position = 0
walk = [position]
steps = 1000
for i in range(steps):
    step = 1 if random.randint(0, 1) else -1
    position += step
    walk.append(position)
193/129: plt.plot(walk[:100])
193/130: plt.plot(walk[:100])
193/131: plt.plot(walk[:1000])
193/132: plt.plot(walk[:100])
193/133: plt.plot(walk[:100])
194/1: import pandas as pd
194/2: from pandas import Series, DataFrame
194/3: # Series
194/4: obj = pd.Series([4, 7, -5, 3])
194/5: obj
194/6: obj.values
194/7: obj.index
194/8: obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
194/9: obj2
194/10: obj2.index
194/11: obj2['a']
194/12: obj2['d']
194/13: obj2['c', 'a', 'd']
194/14: obj2[['c', 'a', 'd']]
194/15: obj2[obj2 > 0]
194/16: import pandas as pd
194/17: from pandas import Series, DataFrame
194/18: # Series
194/19: obj = pd.Series([4, 7, -5, 3])
194/20: obj
194/21: obj.values
194/22: obj.index
194/23: obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
194/24: obj2
194/25: obj2.index
194/26: obj2['a']
194/27: obj2['d']
194/28: obj2[['c', 'a', 'd']]
194/29: obj2[obj2 > 0]
194/30: obj2 * 2
194/31: np.exp(obj2)
194/32:
import pandas as pd
import numpy as np
194/33: from pandas import Series, DataFrame
194/34: # Series
194/35: obj = pd.Series([4, 7, -5, 3])
194/36: obj
194/37: obj.values
194/38: obj.index
194/39: obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
194/40: obj2
194/41: obj2.index
194/42: obj2['a']
194/43: obj2['d']
194/44: obj2[['c', 'a', 'd']]
194/45: obj2[obj2 > 0]
194/46: obj2 * 2
194/47: np.exp(obj2)
194/48: 'b' in obj2
194/49: 'e' in obj2
194/50: sdata = {'ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
194/51: obj3 = pd.Series(sdata)
194/52: obj3
194/53: states = ['California', 'Ohio', 'Oregon', 'Texas']
194/54: obj4 = pd.Series(sdata, index=states)
194/55: obj4
194/56:
import pandas as pd
import numpy as np
194/57: from pandas import Series, DataFrame
194/58: # Series
194/59: obj = pd.Series([4, 7, -5, 3])
194/60: obj
194/61: obj.values
194/62: obj.index
194/63: obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
194/64: obj2
194/65: obj2.index
194/66: obj2['a']
194/67: obj2['d']
194/68: obj2[['c', 'a', 'd']]
194/69: obj2[obj2 > 0]
194/70: obj2 * 2
194/71: np.exp(obj2)
194/72: 'b' in obj2
194/73: 'e' in obj2
194/74: sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
194/75: obj3 = pd.Series(sdata)
194/76: obj3
194/77: states = ['California', 'Ohio', 'Oregon', 'Texas']
194/78: obj4 = pd.Series(sdata, index=states)
194/79: obj4
194/80: pd.isnull(obj4)
194/81: pd.notnull(obj4)
194/82: obj4.isnull()
194/83: obj3
194/84: obj4
194/85: obj3 + obj4
194/86: obj4.name = 'population'
194/87: obj4.index.name = 'state'
194/88: obj4
194/89: obj
194/90: obj.index
194/91: obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']
194/92: obj
194/93: # DataFrame
194/94:
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        'years': [2000, 2001, 2002, 2001, 2002, 2003],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
194/95: frame = pd.DataFrame(data)
194/96: frame
194/97: frame.head()
194/98: pd.DataFrame(data, columns=[year, state, pop])
194/99: pd.DataFrame(data, columns=['year', 'state', 'pop'])
194/100:
import pandas as pd
import numpy as np
194/101: from pandas import Series, DataFrame
194/102: # Series
194/103: obj = pd.Series([4, 7, -5, 3])
194/104: obj
194/105: obj.values
194/106: obj.index
194/107: obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
194/108: obj2
194/109: obj2.index
194/110: obj2['a']
194/111: obj2['d']
194/112: obj2[['c', 'a', 'd']]
194/113: obj2[obj2 > 0]
194/114: obj2 * 2
194/115: np.exp(obj2)
194/116: 'b' in obj2
194/117: 'e' in obj2
194/118: sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
194/119: obj3 = pd.Series(sdata)
194/120: obj3
194/121: states = ['California', 'Ohio', 'Oregon', 'Texas']
194/122: obj4 = pd.Series(sdata, index=states)
194/123: obj4
194/124: pd.isnull(obj4)
194/125: pd.notnull(obj4)
194/126: obj4.isnull()
194/127: obj3
194/128: obj4
194/129: obj3 + obj4
194/130: obj4.name = 'population'
194/131: obj4.index.name = 'state'
194/132: obj4
194/133: obj
194/134: obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']
194/135: obj
194/136: # DataFrame
194/137:
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002, 2003],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
194/138: frame = pd.DataFrame(data)
194/139: frame
194/140: frame.head()
194/141: pd.DataFrame(data, columns=['year', 'state', 'pop'])
194/142:
frame2 = pd.DataFrame(data, columns = ['year', 'state', 'pop', 'debt'], index=['one', 'two', 'three', 'four', 'five', 'six'])

frame2
194/143: frame['state']
194/144: frame2.columns
194/145: frame2['state']
194/146: frame2.year
194/147: frame2.loc['three']
194/148: frame2['debt'] = 16.5
194/149: frame2
194/150: frame2['debt'] = np.arange(6.)
194/151: frame2
194/152: frame2['debt'] = np.range(6.)
194/153: frame2['debt'] = np.arange(6.)
194/154: frame2
194/155: val = pd.Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])
194/156: frame2['debt'] = val
194/157: frame2
194/158: frame2['eastern'] = frame2.state == 'Ohio'
194/159: frame2
194/160: del frame2['eastern']
194/161: frame2
194/162: frame2.columns
194/163:
# Nested Data

pop = {'Nevada': { 2001: 2.4, 2002: 2.9 }, 'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}
194/164: frame3 = pd.DataFrame(pop)
194/165: frame3
194/166: frame3.T
194/167: pd.DataFrame(pop, index=[2001, 2002, 2003])
194/168: #Dict of Series
194/169: pdata = {'Ohio': frame3['Ohio'][:-1], 'Nevada': frame3['Nevada'][:3]}
194/170: pd.DataFrame(pdata)
194/171: pdata = {'Ohio': frame3['Ohio'][:-1], 'Nevada': frame3['Nevada'][:2]}
194/172: pd.DataFrame(pdata)
194/173: frame3.index.name = 'year'; frame3.columns.name = 'state'
194/174: frame3
194/175: frame3.values
194/176: frame2.values
194/177: # Index Objects
194/178: obj = pd.Series(range(3), index=['a', 'b', 'c'])
194/179: index = obj.index
194/180: index
194/181: index[1:]
194/182: index[1] = 'd'
194/183: labels = pd.Index(np.arange(3))
194/184: labels
194/185: obj2 = pd.Series([1.5, -2.5, 0], index=['labels'])
194/186: obj2 = pd.Series([1.5, -2.5, 0], index=labels)
194/187: obj2
194/188: obj2 = pd.Series([1.5, -2.5, 0], index=[labels])
194/189: obj2
194/190: obj2 = pd.Series([1.5, -2.5, 0], index=labels)
194/191: obj2
194/192: obj2.index is labels
194/193: frame3
194/194: frame3.rows
194/195: frame3.row
194/196: frame3.columns
194/197: 'Ohio' in frame3.columns
194/198: 2003 in frame3.index
194/199: dup_labels = pd.Index(['foo', 'foo', 'bar', 'bar'])
194/200: dup_labels
194/201: # Essential Functionality
194/202: # Reindexing
194/203: obj = pd.Series([4.5, 7.2, -5.3, 2.6], index=['d', 'b', 'a', 'c'])
194/204: obj
194/205: obj2= obj.reindex(['a', 'b', 'c', 'd', 'e'])
194/206: obj2
194/207: obj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])
194/208: obj3
194/209: obj3.reindex(range(6), method='ffill')
194/210:
frame = pd.DataFrame(np.arange(9).reshape((3, 3)),
                     index=['a', 'c', 'd'],
                     columns=['Ohio', 'Texas', 'California'])
194/211: frame
194/212: frame2 = frame.reindex(['a', 'b', 'c', 'd'])
194/213: frame2
194/214: frame.loc[[]'a', 'b', 'c', 'd'], states]
194/215: frame.loc[['a', 'b', 'c', 'd'], states]
194/216: # frame.loc[['a', 'b', 'c', 'd'], states]
194/217: # Dropping entries from an axis
194/218: obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])
194/219: obj
194/220: new_obj = obj.drop('c')
194/221: new_obj
194/222: obj.drop(['d', 'c'])
194/223:
data = pd.DataFrame(np.arange(16).reshape((4, 4)),
                   index=['Ohio', 'Colorado', 'Utal', 'New York'],
                   columns=['one', 'two', 'three', 'four'])
194/224: data
194/225: data.drop(['Colorado', 'Ohio'])
194/226: data.drop('two', axis=1)
194/227: data.drop(['two', 'four'], axis='columns')
194/228: obj.drop('c', inplace=True)
194/229: obj
194/230: # Indexing, Selection and Filtering
194/231: obj = pd.Series(np.arange(4.), index=['a', 'b', 'c', 'd'])
194/232: obj
194/233: obj['b']
194/234: obj[1]
194/235: obj[2:4]
194/236: obj[['b', 'a', 'd']]
194/237: obj[[1, 3]]
194/238: obj[obj > 2]
194/239: obj[obj < 2]
194/240: obj['b': 'c']
194/241: obj['b': 'c'] = 5
194/242: obj
194/243:
data = pd.DataFrame(np.arange(16).reshape((4, 4)),
                   index=['Ohio', 'Colorado', 'Utah', 'New York'],
                   columns=['one', 'two', 'three', 'four'])
194/244: data
194/245: data['two']
194/246: data['three', 'one']
194/247: data[['three', 'one']]
194/248: data[:2]
194/249: data[data['three'] > 5]
194/250: data < 5
194/251: data < 5
194/252: data[data < 5] = 0
194/253: data
194/254: # Selection with loc and iloc
194/255: data.loc['Colorado', ['two', 'three']]
194/256: data.iloc[2, [3, 0, 1]]
194/257: data.iloc[2]
194/258: data.iloc[[1, 2], [3, 0, 1]]
194/259: data.iloc[2]
194/260: data.loc['Utah', 'two']
194/261: data.loc[:'Utah', 'two']
194/262: data.iloc[:, :3][data.three > 5]
196/1: pip list | grep "matplotlib*\|numpy*"
196/2: %pwd
196/3: a = [1, 2, 3, 4, 5]
196/4: a?
196/5: import numpy as np
196/6: data = {i: np.random.randn() for i in range(7)}
196/7: data
196/8: import datetime
196/9: datetime
196/10: b = [1, 2, 3]
196/11: # b?
196/12: # print?
196/13:
def add_numbers(a, b):
    """
    Add two numbers together
    
    Returns
    -------
    the_sum: type of arguments
    """
    
    return a + b
196/14:
# A single question mark shows you the doc string

add_numbers
196/15:
# Double question marks show you the source code

add_numbers
196/16: # np.*load*?
196/17: # %paste
196/18:

x = 5
y = 7

if x > 5:
    x += 1
    
    y = 8
196/19: # %cpaste
196/20:
a = np.random.randn(100, 100)
# %timeit np.dot(a, a)
196/21: # %debug
196/22: # %quickref
196/23: # %magic
196/24: %matplotlib
196/25:
import matplotlib.pyplot as plt
plt.plot(np.random.randn(50).cumsum())
196/26: a = [1, 2, 3]
196/27: b = a
196/28: a.append(4)
196/29: a
196/30: b
196/31: a = 5
196/32: isinstance(a, int)
196/33: a = 'foo'
196/34: 5 / 2
196/35: 5 // 2
196/36: from datetime import datetime, date, time
196/37: dt = datetime(2019, 10, 29, 20, 30, 21)
196/38: dt.day
196/39: dt.minute
196/40: dt.date()
196/41: dt.time()
196/42:
# Formate datetime as a string

dt.strftime('%m/%d/%Y %H:%M')
196/43:
# Convert string to datetime object

dt.strptime('20191031', '%Y%m%d')
196/44:
# Tuples

tup = 4, 5, 6

tup
196/45: nested_tuples = (4, 5, 6), (7, 8)
196/46: nested_tuples
196/47:
# sort

a = [7, 5, 3, 2, 1]
196/48: a.sort()
196/49: a
196/50:

b = ['saw', 'small', 'He', 'foxes', 'six']
196/51: b.sort(key=len)
196/52: b
196/53: import bisect
196/54:
c = [1, 2, 2, 2, 3, 4, 7]

bisect.bisect(c, 2)
196/55: bisect.bisect(c, 5)
196/56: bisect.insort(c, 6)
196/57: c
196/58:
# Slicing

seq = [7, 2, 3, 7, 5, 6, 0, 1]

seq[-6:-2]
196/59: seq[::2]
196/60: seq[::-1]
196/61:
# enumerate

some_list = ['foo', 'bar', 'baz']

mapping = {}

for i, v in enumerate(some_list):
    mapping[v] = i
196/62: mapping
196/63: import numpy as np
196/64:
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

ys
196/65: z = np.sqrt(xs ** 2 + ys ** 2)
196/66: z
196/67:
plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()

plt.title("Image plot of $\sqrt{x^2 + y^2}$ for a grid of values")
196/68: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
196/69: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
196/70: cond = np.array([True, False, True, True, False])
196/71:
# Using List Comprehension

result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]
196/72: result
196/73: result = np.where(cond, xarr, yarr)
196/74: result
196/75: arr = np.random.randn(4, 4)
196/76: arr
196/77: arr > 0
196/78: np.where(arr > 0, 2, -2)
196/79: np.where(arr > 0, 2, arr)
196/80: arr = np.random.randn(5, 4)
196/81: arr
196/82: arr.mean()
196/83:

np.mean(arr)
196/84: arr.sum()
196/85: arr.mean(axis=1)
196/86: arr.mean(axis=0)
196/87:
# Sorting

arr = np.random.randn(6)
196/88: arr
196/89: arr.sort()
196/90: arr
196/91: arr = np.random.randn(5, 3)
196/92: arr
196/93: arr.sort(1)
196/94: arr
196/95: large_arr = np.random.randn(1000)
196/96: large_arr.sort()
196/97: large_arr[int(0.05 * len(large_arr))]
196/98: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])
196/99: np.unique(names)
196/100: ints  = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])
196/101: np.unique(ints)
196/102: sorted(set(names))
196/103:
values = np.array([6, 0, 0, 3, 2, 5, 6])

np.in1d(values, [2, 3, 6])
196/104: arr = np.arange(10)
196/105: arr
196/106: np.save('some_array', arr)
196/107: np.load('some_array.npy')
196/108: np.savez('array_archive.npz', a=arr, b=arr)
196/109:
arch = np.load('array_archive.npz')

arch['b']
196/110: np.savez_compressed('arrays_compressed.npz', a=arr, b=arr)
196/111: # Linear Algebra
196/112: x = np.array([[1., 2., 3.], [4., 5., 6.]])
196/113: y = np.array([[6., 23.], [-1, 7], [8, 9]])
196/114: x
196/115: y
196/116: x.dot(y)
196/117: np.dot(x, y)
196/118: np.dot(x, np.ones(3))
196/119: x @ np.ones(3)
196/120: from numpy.linalg import inv, qr
196/121: X = np.random.randn(5, 5)
196/122: mat = X.T.dot(X)
196/123: inv(mat)
196/124: mat.dot(inv(mat))
196/125: q, r = qr(mat)
196/126: r
196/127:
import random

position = 0
walk = [position]
steps = 1000
for i in range(steps):
    step = 1 if random.randint(0, 1) else -1
    position += step
    walk.append(position)
196/128: plt.plot(walk[:100])
197/1:
import pandas as pd
import numpy as np
197/2: from pandas import Series, DataFrame
197/3: # Series
197/4: obj = pd.Series([4, 7, -5, 3])
197/5: obj
197/6: obj.values
197/7: obj.index
197/8: obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
197/9: obj2
197/10: obj2.index
197/11: obj2['a']
197/12: obj2['d']
197/13: obj2[['c', 'a', 'd']]
197/14: obj2[obj2 > 0]
197/15: obj2 * 2
197/16: np.exp(obj2)
197/17: 'b' in obj2
197/18: 'e' in obj2
197/19: sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
197/20: obj3 = pd.Series(sdata)
197/21: obj3
197/22: states = ['California', 'Ohio', 'Oregon', 'Texas']
197/23: obj4 = pd.Series(sdata, index=states)
197/24: obj4
197/25: pd.isnull(obj4)
197/26: pd.notnull(obj4)
197/27: obj4.isnull()
197/28: obj3
197/29: obj4
197/30: obj3 + obj4
197/31: obj4.name = 'population'
197/32: obj4.index.name = 'state'
197/33: obj4
197/34: obj
197/35: obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']
197/36: obj
197/37: # DataFrame
197/38:
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002, 2003],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
197/39: frame = pd.DataFrame(data)
197/40: frame
197/41: frame.head()
197/42: pd.DataFrame(data, columns=['year', 'state', 'pop'])
197/43:
frame2 = pd.DataFrame(data, columns = ['year', 'state', 'pop', 'debt'], index=['one', 'two', 'three', 'four', 'five', 'six'])

frame2
197/44: frame2.columns
197/45: frame2['state']
197/46: frame2.year
197/47: frame2.loc['three']
197/48: frame2['debt'] = 16.5
197/49: frame2
197/50: frame2['debt'] = np.arange(6.)
197/51: frame2
197/52: val = pd.Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])
197/53: frame2['debt'] = val
197/54: frame2
197/55: frame2['eastern'] = frame2.state == 'Ohio'
197/56: frame2
197/57: del frame2['eastern']
197/58: frame2.columns
197/59:
# Nested Data

pop = {'Nevada': { 2001: 2.4, 2002: 2.9 }, 'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}
197/60: frame3 = pd.DataFrame(pop)
197/61: frame3
197/62: frame3.T
197/63: pd.DataFrame(pop, index=[2001, 2002, 2003])
197/64: #Dict of Series
197/65: pdata = {'Ohio': frame3['Ohio'][:-1], 'Nevada': frame3['Nevada'][:2]}
197/66: pd.DataFrame(pdata)
197/67: frame3.index.name = 'year'; frame3.columns.name = 'state'
197/68: frame3
197/69: frame3.values
197/70: frame2.values
197/71: # Index Objects
197/72: obj = pd.Series(range(3), index=['a', 'b', 'c'])
197/73: index = obj.index
197/74: index
197/75: index[1:]
197/76: labels = pd.Index(np.arange(3))
197/77: labels
197/78: obj2 = pd.Series([1.5, -2.5, 0], index=labels)
197/79: obj2
197/80: obj2.index is labels
197/81: frame3
197/82: frame3.columns
197/83: 'Ohio' in frame3.columns
197/84: 2003 in frame3.index
197/85: dup_labels = pd.Index(['foo', 'foo', 'bar', 'bar'])
197/86: dup_labels
197/87: # Essential Functionality
197/88: # Reindexing
197/89: obj = pd.Series([4.5, 7.2, -5.3, 2.6], index=['d', 'b', 'a', 'c'])
197/90: obj
197/91: obj2= obj.reindex(['a', 'b', 'c', 'd', 'e'])
197/92: obj2
197/93: obj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])
197/94: obj3
197/95: obj3.reindex(range(6), method='ffill')
197/96:
frame = pd.DataFrame(np.arange(9).reshape((3, 3)),
                     index=['a', 'c', 'd'],
                     columns=['Ohio', 'Texas', 'California'])
197/97: frame
197/98: frame2 = frame.reindex(['a', 'b', 'c', 'd'])
197/99: frame2
197/100: # frame.loc[['a', 'b', 'c', 'd'], states]
197/101: # Dropping entries from an axis
197/102: obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])
197/103: obj
197/104: new_obj = obj.drop('c')
197/105: new_obj
197/106: obj.drop(['d', 'c'])
197/107:
data = pd.DataFrame(np.arange(16).reshape((4, 4)),
                   index=['Ohio', 'Colorado', 'Utal', 'New York'],
                   columns=['one', 'two', 'three', 'four'])
197/108: data
197/109: data.drop(['Colorado', 'Ohio'])
197/110: data.drop('two', axis=1)
197/111: data.drop(['two', 'four'], axis='columns')
197/112: obj.drop('c', inplace=True)
197/113: obj
197/114: # Indexing, Selection and Filtering
197/115: obj = pd.Series(np.arange(4.), index=['a', 'b', 'c', 'd'])
197/116: obj
197/117: obj['b']
197/118: obj[1]
197/119: obj[2:4]
197/120: obj[['b', 'a', 'd']]
197/121: obj[[1, 3]]
197/122: obj[obj < 2]
197/123: obj['b': 'c']
197/124: obj['b': 'c'] = 5
197/125: obj
197/126:
data = pd.DataFrame(np.arange(16).reshape((4, 4)),
                   index=['Ohio', 'Colorado', 'Utah', 'New York'],
                   columns=['one', 'two', 'three', 'four'])
197/127: data
197/128: data['two']
197/129: data[['three', 'one']]
197/130: data[:2]
197/131: data[data['three'] > 5]
197/132: data < 5
197/133: data[data < 5] = 0
197/134: data
197/135: # Selection with loc and iloc
197/136: data.loc['Colorado', ['two', 'three']]
197/137: data.iloc[2, [3, 0, 1]]
197/138: data.iloc[2]
197/139: data.iloc[[1, 2], [3, 0, 1]]
197/140: data.iloc[2]
197/141: data.loc[:'Utah', 'two']
197/142: data.iloc[:, :3][data.three > 5]
197/143: # Integer Indexer
197/144: ser = pd.Series(np.arange(3.))
197/145: ser
197/146: ser[-1]
197/147: ser2 = pd.Series(np.arange(3.), index=['a', 'b', 'c'])
197/148: ser2[-1]
197/149: ser[:1]
197/150: ser.loc[:1]
197/151: ser.iloc[:1]
197/152: s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'b', 'c', 'd', 'e'])
197/153: s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])
197/154: s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])
197/155: s1
197/156: s2
197/157: s1 + s2
197/158: df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])
197/159: df2 = pd.DataFrame(np.arange(12).reshape((4, 2)), columns=list('bde'), index=['Utal', 'Ohio', 'Texas', 'Oregon'])
197/160: df2 = pd.DataFrame(np.arange(12).reshape((4, 3)), columns=list('bde'), index=['Utal', 'Ohio', 'Texas', 'Oregon'])
197/161: df1
197/162: df2
197/163: df1 + df2
197/164: df1 = pd.DataFrame({'A': [1, 2]})
197/165: df2 = pd.DataFrame({'B': [3, 4]})
197/166: df1
197/167: df1
197/168: df1 - df2
197/169: df1 = pd.DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))
197/170: df2 = pd.DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))
197/171: df2.loc[1, 'b'] = np.nan
197/172: df1
197/173: df2
197/174: df1 + df2
197/175: df1.add(df2, fill_value=0)
197/176: 1 / df1
197/177: df1.rdiv(1)
197/178: df1.reindex(columns=df2.columns, fill_value=0)
197/179: arr = np.arange(12.).reshape((3, 4))
197/180: arr
197/181: arr[0]
197/182: arr - arr[0]
197/183: frame = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])
197/184: series = frame.iloc[0]
197/185: frame
197/186: series
197/187: series = frame.loc[0]
197/188: series = frame.iloc[0]
197/189: frame
197/190: series
197/191: frame - series
197/192: series2 = pd.Series(range(3), index=['b', 'e', 'f'])
197/193: frame + series2
197/194: series3 = frame['d']
197/195: frame
197/196: series3
197/197: frame.sub(series3, axis='index')
197/198: # Function Application and Mapping
197/199: frame = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])
197/200: frame
197/201: np.abs(frame)
197/202: f = lambda x: x.max() - x.min()
197/203: frame.apply(x)
197/204: frame.apply(f)
197/205: frame.apply(f, axis='columns')
197/206:
def f(x):
    return pd.Series([x.min(), x.max()], index=['min', 'max'])
197/207: frame.apply(f)
197/208: format = lambda x: '%.2f' % x
197/209: frame.applymap(format)
197/210: frame['e'].map(format)
197/211: obj = pd.Series(range(4), index=['d', 'a', 'b', 'c'])
197/212: obj.sort_index()
197/213:
frame = pd.DataFrame(np.arange(8).reshape((2, 4)),
                     index=['three', 'one'],
                     columns=['d', 'a', 'b', 'c'])
197/214: frame.sort_index
197/215: frame.sort_index()
197/216: obj = pd.Series([4, np.nan, 7, np.nan, -3, 2])
197/217: obj.sort_values
197/218: obj.sort_values()
197/219: frame = pd.DataFrame({'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]})
197/220: frame
197/221: frame.sort_value(by='b')
197/222: frame.sort_values(by='b')
197/223: frame.sort_valus(by=['a', 'b'])
197/224: frame.sort_values(by=['a', 'b'])
197/225: frame.sort_values(by=['a', 'b'])
197/226: obj = pd.Series([7, -5, 7, 4, 2, 0, 4])
197/227: obj.rank()
197/228: obj.rank(method='first')
197/229: obj.rank(ascending=False, method='max')
197/230: frame = pd.DataFrame({'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1], 'c': [-2, 5, 8, -2.5]})
197/231: frame
197/232: frame.rank(axis='columns')
197/233: obj = pd.Series(range(5), index=['a', 'a', 'b', 'b', 'c'])
197/234: obj
197/235: obj.index.is_unique
197/236: obj['a']
197/237: obj['c']
197/238: df = pd.DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])
197/239: df
197/240: df.loc['b']
197/241:
df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]],
                 index=['a', 'b', 'c', 'd'],
                 columns=['one', 'two'])
197/242: df
197/243: df.sum()
197/244: df.sum(axis='columns')
197/245: df.mean(axis='columns', skipna=False)
197/246: df.idxmax()
197/247: df.cumsum()
197/248: df.describe()
197/249: obj = pd.Series(['a', 'a', 'b', 'c'] * 4)
197/250: obj.describe()
197/251: import pandas_datareader.data as web
197/252:
all_data = {ticker: web.get_data_yahoo(ticker)}
    for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']

price = pd.DataFrame({ticker: data['Adj Close'] for ticker, data in all_data.items()})

volume = pd.DataFrame({ticker: data['Volumne'] for ticker, data in all_data.items()})
197/253:
all_data = {ticker: web.get_data_yahoo(ticker) for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']}

price = pd.DataFrame({ticker: data['Adj Close'] for ticker, data in all_data.items()})

volume = pd.DataFrame({ticker: data['Volumne'] for ticker, data in all_data.items()})
197/254:
all_data = {ticker: web.get_data_yahoo(ticker) for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']}

price = pd.DataFrame({ticker: data['Adj Close'] for ticker, data in all_data.items()})

volume = pd.DataFrame({ticker: data['Volumne'] for ticker, data in all_data.items()})
197/255:
all_data = {ticker: web.get_data_yahoo(ticker) for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']}

price = pd.DataFrame({ticker: data['Adj Close'] for ticker, data in all_data.items()})

volume = pd.DataFrame({ticker: data['Volume'] for ticker, data in all_data.items()})
197/256: returns = price.pct_change()
197/257: returns.tail()
197/258: returns['MSFT'].corr(returns['IBM'])
197/259: returns['MSFT'].cov(returns['IBM'])
197/260: returns.cor
197/261: returns.corr()
197/262: returns.cov
197/263: returns.cov()
199/1: import requests as req
199/2: req
199/3:
import requests as req
from bs4 import BeautifulSoup

soup = BeautifulSoup(html_doc, 'html.parser')
199/4:
import requests as req
from bs4 import BeautifulSoup

req = url('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(html_doc, 'html.parser')
199/5:
import requests as req
from bs4 import BeautifulSoup
199/6:
import requests as req
from bs4 import BeautifulSoup
199/7:
palmembers = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(palmembers, 'html.parser')
199/8:
palmembers = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
# soup = BeautifulSoup(palmembers, 'html.parser')
199/9:
palmembers = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
# soup = BeautifulSoup(palmembers, 'html.parser')
199/10: palmembers
199/11: palmembers
199/12: print(palmembers)
199/13: print(palmembers.text)
199/14:
palmembers = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(palmembers, 'html.parser')
199/15: palmembers.len
199/16:
import requests as req
from bs4 import BeautifulSoup
199/17:
res = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(res.text, 'html.parser')
199/18:
for i in range(700):
    print(i)
199/19:
for i in range(1, 700):
    print(i)
199/20:
for i in range(1, 701):
    print(i)
199/21:
for i in range(1, 5):
    link = 'https://www.parliament.go.ug/mp_database/profile.php?mid=' + i
    print('Hit Link ', link)
#     res = req.get(link)
199/22:
for i in range(1, 5):
    link = 'https://www.parliament.go.ug/mp_database/profile.php?mid={i}'.format(i)
    print('Hit Link ', link)
#     res = req.get(link)
199/23:
for i in range(1, 5):
    link = 'https://www.parliament.go.ug/mp_database/profile.php?mid={i}'.format(i)
    print('Hit Link ', link)
199/24:
for i in range(1, 5):
    link = 'https://www.parliament.go.ug/mp_database/profile.php?mid={}'.format(i)
    print('Hit Link ', link)
200/1:
# imports
import requests
import json
import re
import pandas as pd
200/2:
# pagination_max_original_value = 183
pagination_max_value = 3

def retrieve_json_data(pagination_max_value):
    """
    This function returns the pretty json data from the PPIP website end point
    pagination_max_vale = The pagination maximum value in the data table
    """
    for pagination_value in range(1, pagination_max_value):
        end_point = f'https://www.tenders.go.ke/website/contracts/advancedSearch?draw={pagination_value}&columns%5B0%5D%5Bdata%5D=type&columns%5B0%5D%5Bname%5D=organizations.type&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=org_name&columns%5B1%5D%5Bname%5D=organizations.name&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=tender_ref_no&columns%5B2%5D%5Bname%5D=tender_notices.tender_ref_no&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=tender_title&columns%5B3%5D%5Bname%5D=tender_notices.tender_title&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=supplier_name&columns%5B4%5D%5Bname%5D=suppliers.supplier_name&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=pin_number&columns%5B5%5D%5Bname%5D=suppliers.pin_number&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=contract_amount&columns%5B6%5D%5Bname%5D=contracts.contract_amount&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=tender_award_date&columns%5B7%5D%5Bname%5D=contracts.tender_award_date&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=true&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=updated_at&columns%5B8%5D%5Bname%5D=contracts.updated_at&columns%5B8%5D%5Bsearchable%5D=false&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=0&order%5B0%5D%5Bdir%5D=asc&start=0&length=2500&search%5Bvalue%5D=&search%5Bregex%5D=false&month=&type=&name=&tender_number=&year=&status=&_=1588036321965'
        res = requests.get(url=end_point)
        print(f'Hit pagination page = {pagination_value}')
    return json.dumps(res.json(), indent=2)

contract_data = retrieve_json_data(pagination_max_value=5)

print(type(contract_data))

print(type(json.loads(contract_data)))
200/3: contract_data
200/4:
# Convert the contract_data json string to python dictionary

contract_data_dict = json.loads(contract_data)

data = {}

for item in contract_data_dict:
    data = contract_data_dict['data']
200/5:
# Convert the data to a pandas dataframe
df = pd.DataFrame(data)
df.head()
200/6:
# Save the data to a CSV file
df.to_csv('ppip_contracts.csv', index=False, header=True)
200/7:
# list the file
!ls
200/8:
# Read the created csv file using pandas
ppip_contracts = pd.read_csv('ppip_contracts.csv')
ppip_contracts.head()
200/9: len(ppip_contracts)
200/10:
# Remove html tags from the tender title for first link
link = re.match('<[^<]+?>', ppip_contracts['tender_title'][0]).group() + '</a>'
re.sub('<[^<]+?>', '', ppip_contracts['tender_title'][0])
200/11: type(link), link
200/12:
# Retrieve all the contract details links from the ppip_contracts.csv dataset
contract_detail_links = list(df['tender_title'])
200/13:
# Function to remove a tags from tender titles

def remove_anchor_tags(contract_detail_links):
    """
    This function retrieves the href attribute with its value and 
    removes the opening and closing anchor tags <a> and </a>
    """
    modified_links_list = []
    for link in contract_detail_links:
        modified_link = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
        modified_links_list.append(modified_link)
    return modified_links_list

modified_links_list = remove_anchor_tags(contract_detail_links)
200/14: type(modified_links_list[0])
200/15: from bs4 import BeautifulSoup
200/16:
# Get the response for contract detais
res_details_list = []
for link in (modified_links_list):
    res_details = requests.get(link[0])
    res_details_list.append(res_details)
    print(f'Hit link {link[0]} successfully')
    print(res_details)
199/25:
member_list = []
for i in range(1, 5):
    link = f'https://www.parliament.go.ug/mp_database/profile.php?mid={i}'
    print('Hit Link ', link)
    res = req.get(link)
    member_list.append()
199/26:
res = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(res.text, 'html.parser')
199/27:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')
199/28: res.text()
199/29: res.text
199/30: print(soup.pretify())
199/31: print(soup.pretify())
199/32:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')
199/33: print(soup.pretify())
199/34: print(res.text)
199/35:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')
199/36: print(soup.tr['img-polaroid'])
199/37: print(soup.tr['.img-polaroid'])
199/38: print(soup.tr)
199/39: print(soup.td)
199/40: print(soup.td.string)
199/41: print(soup.td)
199/42: print(soup.td[0])
199/43: print(soup.td)
199/44:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'lxml')
199/45: table = soup.find_all('table')[0]
199/46: table = soup.find_all('table')[0]
199/47: table
199/48:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/49:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'lxml')
199/50: table = soup.find_all('table')[0]
199/51: table
199/52: table = soup.find_all('table')[0]
199/53: table = pd.DataFrame(index=[0])
199/54: table
199/55: table = soup.find_all('table')[0]
199/56:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'lxml')
199/57: table = soup.find_all('table')[0]
199/58: table
199/59: table = pd.DataFrame(columns=range(0,2), index=[0])
199/60: table
199/61:
row_marker = 0
    for row in table.find_all('tr'):
        column_marker = 0
        columns = row.find_all('td')
        for column in columns:
            new_table.iat[row_marker,column_marker] = column.get_text()
            column_marker += 1
    
    new_table
199/62:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'lxml')
199/63: table = soup.find_all('table')[0]
199/64: new_table = pd.DataFrame(columns=range(0,2), index=[0])
199/65:
row_marker = 0
    for row in table.find_all('tr'):
        column_marker = 0
        columns = row.find_all('td')
        for column in columns:
            new_table.iat[row_marker,column_marker] = column.get_text()
            column_marker += 1
    
    new_table
199/66:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
199/67:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
199/68: table = soup.find_all('table')[0]
199/69: table
199/70: table = soup.find_all('table')[0]
199/71:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = table.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
199/72:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'lxml')
199/73: table = soup.find_all('table')[0]
199/74:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = table.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
199/75:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html_parser')
199/76:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')
199/77: table = soup.find_all('table')[0]
199/78:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = table.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
199/79:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
199/80: data
199/81: table = soup.find_all('table')
199/82: table
199/83: table.len
199/84: len(table)
199/85: table[0]
199/86: table[2]
199/87: table[2]
199/88:

def retrieve_data_from_table():
    data = []
    table_body = table[2].find('tbody')
    
    rows = table_body.find_all('tr')
    
    for row in rows:
        cols = row.find_all('td')
        cols = [item.text.strip() for item in cols]
        data.append([item for item in cols if item])
    
    return data

retrieve_data_from_table()
199/89:

def retrieve_data_from_table():
    data = []
    table_body = soup.find('tbody')
    
    rows = table_body.find_all('tr')
    
    for row in rows:
        cols = row.find_all('td')
        cols = [item.text.strip() for item in cols]
        data.append([item for item in cols if item])
    
    return data

retrieve_data_from_table()
199/90:
# Print the list of tables

for i in range(0, len(table)):
    print(i)
199/91:
# Print the list of tables

for i in range(1, len(table)):
    print(i)
199/92:
# Print the list of tables

for i in range(0, len(table)):
    print(i)
199/93:
# Print the list of tables

for i in range(0, len(table)):
    print(i)
199/94:
# Print the list of tables

for i in range(0, len(table)):
    print(table[i])
199/95:
# Print the list of tables

for i in range(0, len(table)):
    print(i)
199/96:
# Print the list of tables

for i in range(0, len(table)):
    print(i)
199/97: table[2][0]
199/98: table[2]
199/99: table[2].find_all('tr')
199/100: table[2].find_all('tr').string
199/101: table[2].find_all('tr')
199/102: table[2].find_all('tr')[0]
199/103: table[2].find_all('tr')[0]['td']
199/104: table[2].find_all('tr')[0]
199/105: table[2].find_all('td')[0]
199/106: table[2].find_all('td')
199/107:
table[2].find_all('td')[0]

table[2].find_all('td')[1]
199/108:
print(table[2].find_all('td')[0])

print(table[2].find_all('td')[1])
199/109:
print(table[2].find_all('td')[0])

print(table[2].find_all('td')[1])

len(table[2].find_all('td'))
199/110:
print(table[2].find_all('td')[0])

print(table[2].find_all('td')[1])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/111:
print(table[3].find_all('td')[0])

print(table[2].find_all('td')[1])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/112:
print(table[3].find_all('td'))

print(table[2].find_all('td')[1])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/113:
print(table[].find_all('td')[0])

print(table[2].find_all('td')[0])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/114:
print(table[2].find_all('td')[0])

print(table[2].find_all('td')[0])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/115:
print(table[2].find_all('td')[0])

print(table[2].find_all('td')[2])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/116:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
199/117:
print(table[1].find_all('td')[0])

print(table[2].find_all('td')[2])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/118:
print(table[0].find_all('td')[0])

print(table[2].find_all('td')[2])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/119:
print(table[0].find_all('td')[0][0])

print(table[2].find_all('td')[2])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/120:
print(table[0].find_all('td')[0])

print(table[2].find_all('td')[2])

len(table[2].find_all('td'))

len(table[3].find_all('td'))
199/121:
print(table[0].find_all('td')[0])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[3].find_all('td'))
199/122:
print(table[0].find_all('td')[1])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[3].find_all('td'))
199/123:
print(table[0].find_all('td')[2])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[3].find_all('td'))
199/124:
print(table[0].find_all('td')[2])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[3].find_all('td'))
199/125:
print(table[0].find_all('td')[2])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/126:
print(table[0].find_all('td')[2])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td')[0])
199/127:
print(table[0].find_all('td')[2])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td')[2])
199/128:
print(table[0].find_all('td')[2])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/129:
print(table[0].find_all('td')[1])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/130:
print(table[0].find_all('td')[1])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/131:
for i in range(0, len(table[0].find_all('td'))):
    print(table[0].find_all('td')[1])
199/132:
for i in range(0, len(table[0].find_all('td'))):
    print(table[0].find_all('td')[i])
199/133:
print(table[0].find_all('td')[1])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('tr'))
199/134:
print(table[0].find_all('td')[1])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/135:
print(table[0].find_all('td')[1].get_text)

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/136:
print(table[0].find_all('td')[1].get_text())

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/137:
print(table[0].find_all('td')[1].get_text())

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/138: len(table)
199/139:
print(table[0].find_all('td'))

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/140:
print(table[0].find_all('td')[0])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/141:
print(table[0].find_all('td')[1])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/142:
print(table[0].find_all('td')[2])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/143:
print(table[0].find_all('td')[3])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/144:
print(table[0].find_all('td')[4])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/145:
print(table[0].find_all('td')[5])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/146:
print(table[0].find_all('td')[7])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/147:
for i in range(0, len(table[0].find_all('td'))):
    print(len(table[i]))
    print(table[0].find_all('td')[i])
199/148:
for i in range(0, len(table[0].find_all('td'))):
    print(len(table[i]))
#     print(table[0].find_all('td')[i])
199/149:
print(table[0].find_all('td')[0])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

len(table[0].find_all('td'))
199/150:
for i in range(0, len(table[0].find_all('td'))):
    print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
199/151:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
199/152:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')
199/153: profile_table = soup.find('table', {'class': 'table-striped'})
199/154: profile_table = soup.find('table', {'class': 'table-striped'})
199/155: profile_table
199/156: profile_table
199/157:
profile_table = soup.find('table', {'class': 'table-striped'})

backgroun_table = soup.find('table', {'class': 'dataTable'})
199/158:
profile_table = soup.find('table', {'class': 'table-striped'})

backgroun_table = soup.find('table', {'class': 'dataTable'})
199/159:
profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find('table', {'class': 'dataTable'})
199/160: background_table
199/161: background_table
199/162:
profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find('table', {'class': 'dataTable'})
199/163: background_table
199/164:
profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find('table', {'class': 'dataTable'})
199/165:
# profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find('table', {'class': 'dataTable'})
199/166: background_table
199/167: profile_table
199/168: background_table
199/169: background_table
199/170:
# profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find_all(id='example')
199/171: background_table
199/172:
# profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find_all(id='example').get_text()
199/173:
# profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find(id='example').get_text()
199/174: background_table
199/175:
# profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find(id='example')
199/176: background_table
199/177:
# profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find_all(id='example')
199/178: background_table
199/179:
# profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find_all(id='example')[0]
199/180: background_table
199/181:
# profile_table = soup.find('table', {'class': 'table-striped'})

background_table = soup.find_all(id='example')
199/182: len(background_table)
199/183:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

experience_table = soup.find_all(id='example')[1]
199/184: experience_table
199/185: experience_table.get_text()
199/186:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

experience_table = soup.find_all(id='example')[1].get_text()
199/187: experience_table
199/188:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

experience_table = soup.find_all(id='example')[1]
199/189: experience_table
199/190: experience_table[0]
199/191: experience_table
199/192:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

hobbies = soup.find_all(id='example')[2]
199/193:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

hobbies_table = soup.find_all(id='example')[2]
199/194: hobbies_table
199/195:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

membership_table = soup.find_all(id='example')[3]
199/196:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

membership_table = soup.find_all(id='example')[4]
199/197:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example')[4]

len(soup.find_all(id='example'))
199/198:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

membership_table = soup.find_all(id='example5')

len(soup.find_all(id='example5'))
199/199:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

membership_table = soup.find_all(id='example5')[0]

len(soup.find_all(id='example5'))
199/200: membership_table
199/201:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all(id='example5')[1]

len(soup.find_all(id='example5'))
199/202: committee_table
199/203:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all(id='example5')[1][0]

len(soup.find_all(id='example5'))
199/204:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all(id='example5')

len(soup.find_all(id='example5'))
199/205: committee_table
199/206:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find(id='example5')

len(soup.find_all(id='example5'))
199/207:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find(id='example5')[1]

len(soup.find_all(id='example5'))
199/208:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find(id='example5')[0]

len(soup.find_all(id='example5'))
199/209:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find(id='example5')

len(soup.find_all(id='example5'))
199/210: committee_table
199/211:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')
199/212:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find(id='example5')

len(soup.find_all(id='example5'))
199/213: committee_table
199/214:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find(id='example5')[1]

len(soup.find_all(id='example5'))
199/215:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all(id='example5')[1]

len(soup.find_all(id='example5'))
199/216: committee_table
199/217:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all(id='example5')[1]

member_table = soup.select('odd.gradeX')

len(soup.find_all(id='example5'))
199/218: member_table
199/219:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all(id='example5')[1]

member_table = soup.select('.odd.gradeX')

len(soup.find_all(id='example5'))
199/220: member_table
199/221:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all(id='example5')[1]
199/222: committee_table
199/223:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
199/224: table = soup.find_all('table')
199/225: table[7]
199/226: table[8]
199/227: table[7]
199/228:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all('tbody')[-1]
199/229: committee_table
199/230:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all('tbody')[-2]
199/231: committee_table
199/232:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all(id='example')[2]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all('tbody')[-3]
199/233: committee_table
199/234:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all('tbody')[-2]
199/235: committee_table
199/236:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table = soup.find_all(id='example5')[0]

committee_table = soup.find_all('tbody')[-1]
199/237: committee_table
199/238: membership_table_title
199/239:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]
# hobbies_table = soup.find_all('tbody')[-3]

membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

committee_table = soup.find_all('tbody')[-1]
199/240: membership_table_title
199/241:
# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]
199/242: membership_table_title
199/243: hobbies_table_title
199/244: hobbies_table_title
199/245:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]
199/246: profile_table
199/247:
profile_table = soup.find('table', {'class': 'table-striped'}).get_text()

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]
199/248: profile_table
199/249:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]
199/250: list(profile_table.get_text()]
199/251: list(profile_table.get_text())
199/252: profile_table
199/253:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')
199/254:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')
199/255: rows
199/256:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/257: rows
199/258:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/259: data
199/260:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols])
199/261: data
199/262:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/263: data
199/264:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
#     cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/265: data
199/266:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/267: data
199/268: data
199/269: pd.DataFrame(data, index=None)
199/270:

pd.DataFrame(data, columns=['DISTRICT', 'CONSTITUENCY', 'POLITICAL PARTY', 'PROFESSION', 'MARITAL STATUS', 'PHONE NUMBER', 'DATE OF BIRTH', 'EMAIL'])
199/271: data[0]
199/272: data[1]
199/273: data[2]
199/274: data
199/275: data[2] = []
199/276: data[2] = []
199/277: data
199/278:

pd.DataFrame(data, columns=['DISTRICT', 'CONSTITUENCY', 'POLITICAL PARTY', 'PROFESSION', 'MARITAL STATUS', 'PHONE NUMBER', 'DATE OF BIRTH', 'EMAIL'])
199/279: data
199/280:

pd.DataFrame(data)
199/281:

df = pd.DataFrame(data)
199/282: rows
199/283:

df = pd.DataFrame(data)
199/284: df
199/285: df.stack()
199/286: df
199/287:

df = pd.DataFrame(data, index=None)
199/288: df
199/289:

df = pd.DataFrame(data, index=None, columns=None)
199/290: df
199/291:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/292:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')
199/293:
profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/294: data[2] = []
199/295: data
199/296:

df = pd.DataFrame(data, index=None, columns=None)
199/297: df
199/298: rows
199/299: profile_table
199/300: table = soup.find_all('table')
199/301: table[7]
199/302: len(table)
199/303:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
199/304:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
199/305:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
199/306:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
199/307: data
199/308: table
199/309: new_table = pd.DataFrame(columns=range(0,2), index=[0])
199/310:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
202/1:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
202/2: res = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
202/3: res.get_text()
202/4: res.text
202/5: res.text()
202/6: res.text
202/7:
res = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(res.text, 'html.parser')
202/8: res.text
202/9:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
202/10:
res = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(res.text, 'html.parser')
202/11: so
202/12: member_list = soup.find('table', {'class': 'table-striped'})
202/13: member_list
202/14:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
202/15:
res = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(res.text, 'html.parser')
202/16: member_list = soup.find('table', {'class': 'table-striped'})
202/17:
rows = member_list.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
202/18: data
202/19: df = pd.DataFrame(data, index=None, columns=None)
202/20: df
202/21: df = pd.DataFrame(data, index=None, columns=['Name', 'District', 'Constituency', 'Political Party', 'Religion', ''])
202/22: df
202/23: df = pd.DataFrame(data, index=None, columns=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])
202/24: df = pd.DataFrame(data, index=None, columns=['Name', 'District', 'Constituency', 'Political Party', 'Religion', ''])
202/25: df
202/26: menber_list = pd.DataFrame(data, index=None, columns=['Name', 'District', 'Constituency', 'Political Party', 'Religion', ''])
202/27: member_list = pd.DataFrame(data, index=None, columns=['Name', 'District', 'Constituency', 'Political Party', 'Religion', ''])
202/28: member_list
202/29: member_list.to_csv('member_list.csv')
199/311:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/312:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/313: data[2] = []
199/314: data
199/315:

df = pd.DataFrame(data, index=None, columns=None)
199/316: df
199/317: rows
199/318: profile_table
199/319: table = soup.find_all('table')
199/320: table[7]
199/321: len(table)
199/322:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
199/323:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
199/324:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
199/325:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
199/326: data
199/327: table
199/328: new_table = pd.DataFrame(columns=range(0,2), index=[0])
199/329:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
199/330: df
199/331:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/332:
url = 'https://www.parliament.go.ug/mp_database/profile.php?mid=1'

# Get request to get raw html content
html_content = req.get(url).text()

# Parse the html content
soup = BeautifulSoup(html_content, 'lxml')

print(soup.prettify())
199/333:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/334:
url = 'https://www.parliament.go.ug/mp_database/profile.php?mid=1'

# Get request to get raw html content
html_content = req.get(url).text()

# Parse the html content
soup = BeautifulSoup(html_content, 'lxml')

print(soup.prettify())
199/335:
url = 'https://www.parliament.go.ug/mp_database/profile.php?mid=1'

# Get request to get raw html content
html_content = req.get(url).text()

# Parse the html content
soup = BeautifulSoup(html_content, 'lxml')

print(soup.prettify())

# importing the libraries
from bs4 import BeautifulSoup
import requests

url="https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)"

# Make a GET request to fetch the raw HTML content
html_content = requests.get(url).text

# Parse the html content
soup = BeautifulSoup(html_content, "lxml")
print(soup.prettify()) # print the parsed data of html
199/336:
# importing the libraries
from bs4 import BeautifulSoup
import requests

url="https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)"

# Make a GET request to fetch the raw HTML content
html_content = requests.get(url).text

# Parse the html content
soup = BeautifulSoup(html_content, "lxml")
print(soup.prettify()) # print the parsed data of html
199/337:
url = 'https://www.parliament.go.ug/mp_database/profile.php?mid=1'

# Get request to get raw html content
html_content = req.get(url).text()

# Parse the html content
soup = BeautifulSoup(html_content, "lxml")

print(soup.prettify())
199/338:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/339:
url = 'https://www.parliament.go.ug/mp_database/profile.php?mid=1'

# Get request to get raw html content
html_content = req.get(url).text()

# Parse the html content
soup = BeautifulSoup(html_content, "lxml")

print(soup.prettify())
199/340:
url = 'https://www.parliament.go.ug/mp_database/profile.php?mid=1'

# Get request to get raw html content
html_content = req.get(url).text

# Parse the html content
soup = BeautifulSoup(html_content, "lxml")

print(soup.prettify())
199/341:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/342:
url = 'https://www.parliament.go.ug/mp_database/profile.php?mid=1'

# Get request to get raw html content
html_content = req.get(url).text

# Parse the html content
soup = BeautifulSoup(html_content, "lxml")

print(soup.prettify())
199/343: print(soup.title)
199/344: print(soup.title.text)
199/345:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table
199/346:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table_data = profile_table.tbody.find_all('tr')

profile_table_data
199/347:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table_data = profile_table.tbody.find_all('tr')

profile_table_data
199/348:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table_data = profile_table.tbody.find('tr')

profile_table_data
199/349:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table_data = profile_table.tbody.find_all('tr')

profile_table_data
199/350:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table
199/351:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table.tr
199/352:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table.tr[0]
199/353:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table
199/354:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table.td
199/355:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table.td.get_text()
199/356:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table.td
199/357:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table
199/358:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/359: data[2] = []
199/360: data
199/361:

df = pd.DataFrame(data, index=None, columns=None)
199/362: df
199/363:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

# data = []

# for row in rows:
#     cols = row.find_all('td')
#     cols = [item.text.strip() for item in cols]
#     data.append([item for item in cols if item])
199/364:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

# data = []

# for row in rows:
#     cols = row.find_all('td')
#     cols = [item.text.strip() for item in cols]
#     data.append([item for item in cols if item])
199/365: rows
199/366: rows.find('td', attrs={'class': 'img-polaroid'})
199/367: rows.find('td', attrs={'class': 'img-polaroid'})
199/368: rows.find_all('td', attrs={'class': 'img-polaroid'})
199/369: profile_table('td', attrs={'class': 'img-polaroid'})
199/370: profile_table('tr', attrs={'class': 'img-polaroid'})
199/371: profile_table.find('td', attrs={'class': 'img-polaroid'})
199/372: profile_table.find_all('td', attrs={'class': 'img-polaroid'})
199/373:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

# data = []

# for row in rows:
#     cols = row.find_all('td')
#     cols = [item.text.strip() for item in cols]
#     data.append([item for item in cols if item])
199/374: profile_table.find_all('td', attrs={'class': 'img-polaroid'})
199/375: rows
199/376:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

# data = []

# for row in rows:
#     cols = row.find_all('td')
#     cols = [item.text.strip() for item in cols]
#     data.append([item for item in cols if item])
199/377:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

# data = []

# for row in rows:
#     cols = row.find_all('td')
#     cols = [item.text.strip() for item in cols]
#     data.append([item for item in cols if item])
199/378: rows
199/379: rows.get_text()
199/380:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/381:
url = 'https://www.parliament.go.ug/mp_database/profile.php?mid=1'

# Get request to get raw html content
html_content = req.get(url).text

# Parse the html content
soup = BeautifulSoup(html_content, "lxml")

print(soup.prettify())
199/382: print(soup.title)
199/383: print(soup.title.text)
199/384:
profile_table = soup.find('table', attrs={'class': 'table-striped'})

profile_table
199/385:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

# data = []

# for row in rows:
#     cols = row.find_all('td')
#     cols = [item.text.strip() for item in cols]
#     data.append([item for item in cols if item])
199/386: profile_table.find_all('td', attrs={'class': 'img-polaroid'})
199/387: rows.get_text()
199/388: rows
199/389:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
199/390: rows
199/391: data
199/392: data
199/393:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    data.append([item for item in cols if item])
199/394:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    data.append([item for item in cols if item])
199/395: data
199/396: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
199/397: rows
199/398:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/399:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    data.append([item for item in cols if item])
199/400: rows
199/401:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/402:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    data.append([item for item in cols if item])
199/403: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
199/404: rows
199/405: data
199/406: data[2] = []
199/407: data
199/408:

df = pd.DataFrame(data, index=None, columns=None)
199/409: df
199/410: rows
199/411: profile_table
199/412: table = soup.find_all('table')
199/413: table[7]
199/414: len(table)
199/415:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
199/416:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
199/417:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
199/418:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
199/419: data
199/420: table
199/421: new_table = pd.DataFrame(columns=range(0,2), index=[0])
199/422:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
199/423: rows[0]
199/424: rows[1]
199/425: rows[-1]
199/426:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
#     data.append([item for item in cols if item])
199/427:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print('=========')
#     data.append([item for item in cols if item])
199/428:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols[0])
    print('=========')
#     profile_dict.append({ title: })
#     data.append([item for item in cols if item])
199/429:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print('=========')
    profile_dict.append({ title: cols[0], details: cols[1]})
#     data.append([item for item in cols if item])
199/430:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print('=========')
    profile_dict = { title: cols[0], details: cols[1]}
#     data.append([item for item in cols if item])
199/431:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print('=========')
    profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/432:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for row, index enumerate rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/433:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for row, index in enumerate rows:
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/434:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for row, index in enumerate(rows):
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/435:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/436:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index == 0 && index > 2:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/437:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index == 0 & index > 2:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/438:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index == 0 & index > 2:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/439:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index == 0 & index > 2:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/440:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
199/441:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index == 0 & index > 2:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/442: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
199/443: rows
199/444: data
199/445: data[2] = []
199/446:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/447:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index[0])
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/448:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/449:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print(row[index])
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/450:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print(row[0])
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/451:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    cols = row.find_all('td')
    cols = [item.text.replace('\n', ' ').strip() for item in cols]
    print(cols)
    print(index)
    print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/452:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/453:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index == 0 and index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/454:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index == 0 | index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/455:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index < 1 & index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/456:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/457:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols[0])
        print(index)
        print('=========')
#     profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/458:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
        profile_dict = { 'title': cols[0], 'details': cols[1]}
#     data.append([item for item in cols if item])
199/459:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
        profile_dict.push({ 'title': cols[0], 'details': cols[1]})
#     data.append([item for item in cols if item])
199/460:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
        profile_dict.update({ 'title': cols[0], 'details': cols[1]})
#     data.append([item for item in cols if item])
199/461:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
    
    profile_dict.update({ 'title': cols[0], 'details': cols[1]})
#     data.append([item for item in cols if item])
199/462:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
199/463:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
199/464: profile_dict
199/465:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
199/466:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
199/467:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
199/468:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if 3 > index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
199/469:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
199/470:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
#         profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
203/1:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
203/2:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
#         profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
203/3:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
#         print(cols[0])
        print('=========')
#         profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
203/4:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
203/5:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
#         print(cols[0])
        print('=========')
#         profile_dict.update({ 'title': cols[0] })
#     data.append([item for item in cols if item])
203/6:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
203/7:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
# print(cols[0])
        print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
203/8:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
# print(cols[0])
        print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
203/9:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
#         print(index)
# print(cols[0])
        print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
203/10:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

# for index, row in enumerate(rows):
#     if index > 3:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
# print(cols[0])
        print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
203/11:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

# for index, row in enumerate(rows):
#     if index > 3:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
# print(cols[0])
#         print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/1:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
204/2:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

# for index, row in enumerate(rows):
#     if index > 3:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
# print(cols[0])
#         print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/3:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

# rows = profile_table.find_all('tr')

# data = []

# profile_dict = {}

# for index, row in enumerate(rows):
#     if index > 3:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
# print(cols[0])
#         print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/4:
# res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
# soup = BeautifulSoup(res.text, 'html.parser')

# profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

# rows = profile_table.find_all('tr')

# data = []

# profile_dict = {}

# for index, row in enumerate(rows):
#     if index > 3:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
# print(cols[0])
#         print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/5:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
# print(cols[0])
        print('=========')
#   profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/6:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
# print(cols[0])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/7: profile_dict
204/8:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/9:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[index])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/10:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if index > 3:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/11:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
#         print(cols[1])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/12:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/13:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_dict.update({ 'title': cols[0] })
#   data.append([item for item in cols if item])
204/14: profile_dict
204/15:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#   data.append([item for item in cols if item])
204/16: profile_dict
204/17:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_dict.update({ 'title': cols[0][index], 'details': cols[1][index] })
#   data.append([item for item in cols if item])
204/18:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_dict = {}

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_dict.update({ 'title': cols[0], 'details': cols[1] })
        data.append([item for item in cols if item])
204/19: data
204/20:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
        profile_data.append([item for item in cols if item])
204/21: profile_data
204/22:

df = pd.DataFrame(profile_data, index=None, columns=None)
204/23: df
204/24:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
#     if 3 < index < 11:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
#         print(cols[1])
#         print('=========')
#         profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/25:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
#     if 3 < index < 11:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
#         print(cols[1])
#         print('=========')
#         profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/26:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
#     if 3 < index < 11:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
#         print(cols[1])
#         print('=========')
#         profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
        profile_name = cols[1]
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/27:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
#     if 3 < index < 11:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
#         print(cols[1])
#         print('=========')
#         profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
        profile_name = cols[1]
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/28: profile_name
204/29:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
#     if 3 < index < 11:
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         print(cols)
#         print(index)
#         print(cols[1])
#         print('=========')
#         profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
        profile_name = cols[0]
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/30: profile_name
204/31: profile_data.append([profile_name])
204/32: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
204/33: profile_data
204/34:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
        profile_name = cols[0]
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/35: profile_name
204/36: profile_data.append([profile_name])
204/37: profile_data
204/38:

df = pd.DataFrame(profile_data, index=None, columns=None)
204/39: df
204/40:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
204/41:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
        profile_name = cols[0]
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/42: profile_name
204/43: profile_data.append(['NAME', profile_name])
204/44: profile_data
204/45: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
204/46: rows
204/47: data
204/48: data[2] = []
204/49:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
204/50:
res = req.get('https://www.parliament.go.ug/mp_database/profile.php?mid=1')
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
        profile_name = cols[0]
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/51: profile_name
204/52: profile_data.append(['NAME', profile_name])
204/53: profile_data
204/54: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
204/55: rows
204/56:

df = pd.DataFrame(profile_data, index=None, columns=None)
204/57: df
204/58: rows
204/59: profile_table
204/60: table = soup.find_all('table')
204/61: table[7]
204/62: len(table)
204/63:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
204/64:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
204/65:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
204/66:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
204/67: data
204/68: table
204/69: new_table = pd.DataFrame(columns=range(0,2), index=[0])
204/70:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
204/71: df.T
204/72:
pagination_num = 1
url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
res = req.get(pagination_num)
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
        profile_name = cols[0]
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/73:
pagination_num = 1
url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
res = req.get(url)
soup = BeautifulSoup(res.text, 'html.parser')

profile_table = soup.find('table', {'class': 'table-striped'})

# background_table = soup.find_all(id='example')[0]

# experience_table = soup.find_all(id='example')[1]

# hobbies_table_title = soup.find_all(id='example5')[0]

# hobbies_table = soup.find_all('tbody')[-3]

# membership_table_title = soup.find_all(id='example5')[1]

# membership_table = soup.find_all('tbody')[-1]

rows = profile_table.find_all('tr')

data = []

profile_data = []

for index, row in enumerate(rows):
    if 3 < index < 11:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[1])
        print('=========')
        profile_data.append([item for item in cols if item])
    
    if index == 0:
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        print(cols)
        print(index)
        print(cols[0])
        print(cols[1])
        print('=========')
        profile_name = cols[0]
#         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
#         profile_data.append([item for item in cols if item])
204/74:
pagination_max = 5

for pagination_num in range(1, 5)
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    data = []

    profile_data = []

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('=========')
            profile_name = cols[0]
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/75:
pagination_max = 5

for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    data = []

    profile_data = []

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('=========')
            profile_name = cols[0]
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/76: profile_name
204/77: profile_data.append(['NAME', profile_name])
204/78: profile_data
204/79:
pagination_max = 5

for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    data = []

    profile_data = []
    
    profile_name = []

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('=========')
            profile_name.push[{'NAME': cols[0]}]
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/80:
pagination_max = 5

for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    data = []

    profile_data = []
    
    profile_name = []

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('=========')
            profile_name = [{'NAME': cols[0]}]
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/81: profile_name
204/82:
pagination_max = 5

for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    data = []

    profile_data = []
    
    profile_name = []

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_name = [{'NAME': cols[0]}]
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/83:
pagination_max = 5

for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    data = []

    profile_data = []
    
    profile_name = []

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_name.append([{'NAME': cols[0]}])
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/84: profile_name
204/85: profile_data
204/86:
pagination_max = 5

data = []

    profile_data = []
    
    profile_name = []


for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_name.append([{'NAME': cols[0]}])
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/87:
pagination_max = 5

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_name.append([{'NAME': cols[0]}])
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/88: profile_data
204/89: profile_data.append(['NAME', profile_name])
204/90: profile_data
204/91:

df = pd.DataFrame(profile_data, index=None, columns=None)
204/92: df.T
204/93: df
204/94: df.T
204/95:
pagination_max = 5

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_name = cols[0]
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/96: profile_data
204/97: profile_data.append(['NAME', profile_name])
204/98: profile_data
204/99:
pagination_max = 5

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_name.append(['NAME', cols[0]])
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/100: profile_data
204/101: profile_data.append(profile_name)
204/102:
pagination_max = 5

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_name.append(['NAME': cols[0]])
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/103:
pagination_max = 5

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_name.append({'NAME': cols[0]})
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/104: profile_data
204/105: profile_data.append(profile_name)
204/106:
pagination_max = 5

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_data.append(cols[0])
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/107: profile_data
204/108:
pagination_max = 5

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, 5):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_data.append(['NAME', cols[0]])
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/109: profile_data
204/110: profile_data
204/111:

df = pd.DataFrame(profile_data, index=None, columns=None)
204/112: df.T
204/113: df
204/114:
pagination_max = 500

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    # background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[1])
            print('=========')
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            print(cols)
            print(index)
            print(cols[0])
            print(cols[1])
            print('==|==|==|==|=')
            profile_data.append(['NAME', cols[0]])
    #         profile_dict.update({ 'title': cols[0], 'details': cols[1] })
    #         profile_data.append([item for item in cols if item])
204/115: profile_data
204/116:

df = pd.DataFrame(profile_data, index=None, columns=None)
204/117: df
204/118: df.tail()
204/119: df.sample(frac=.5)
204/120: df.sample(frac=.3)
204/121: df.tp_csv('profile_data.csv')
204/122: df.to_csv('profile_data.csv', index=False, index_label=False)
204/123: df.T
204/124:
pagination_max = 2

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
204/125:
pagination_max = 2

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
204/126: background_table
204/127:
pagination_max = 2

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
204/128:
pagination_max = 2

data = []

profile_data = []

profile_name = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
204/129: bg_data_rows
204/130:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([item for item in cols if item])
204/131: background_data
204/132:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        heading = row.find_all('th')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([heading])
        background_data.append([item for item in cols if item])
204/133: background_data
204/134:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        heading = row.find_all('th').get_text()
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([heading])
        background_data.append([item for item in cols if item])
204/135: background_data
204/136:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        heading = row.find_all('th')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([heading.get_text()])
        background_data.append([item for item in cols if item])
204/137:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        heading = row.find_all('th')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([heading.text()])
        background_data.append([item for item in cols if item])
204/138:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        heading = row.find_all('th')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([heading.text])
        background_data.append([item for item in cols if item])
204/139:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        heading = row.find_all('th').text
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([heading])
        background_data.append([item for item in cols if item])
204/140:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([heading])
        background_data.append([item for item in cols if item])
204/141: background_data
204/142:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

#     rows = profile_table.find_all('tr')

#     for index, row in enumerate(rows):
#         if 3 < index < 11:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
#             profile_data.append([item for item in cols if item])

#         if index == 0:
#             cols = row.find_all('td')
#             cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
#             profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
204/143: background_data
204/144: background_data
204/145:
df = pd.DataFrame(background_data, index=None, columns=None)
 df
204/146:
df = pd.DataFrame(background_data, index=None, columns=None)
df
204/147:
df = pd.DataFrame(background_data, index=None, columns=['Year Attained', 'Qualification', 'Type', 'Institution'])
df
204/148:
df = pd.DataFrame(background_data, index=None, columns=['Year Attained', 'Qualification', 'Type', 'Institution'])
df.tail()
204/149:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
204/150: background_data
204/151:
df = pd.DataFrame(background_data, index=None, columns=['Year Attained', 'Qualification', 'Type', 'Institution'])
df
204/152:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name.append(['NAME', cols[0]])
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append(profile_data)
204/153: background_data
204/154:
df = pd.DataFrame(background_data, index=None, columns=['NAME','Year Attained', 'Qualification', 'Type', 'Institution'])
df
204/155:
df = pd.DataFrame(background_data, index=None, columns=['Year Attained', 'Qualification', 'Type', 'Institution'])
df
204/156:
df = pd.DataFrame(background_data, index=None)
df
204/157:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name.append(['NAME', cols[0]])
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append(profile_name)
204/158: background_data
204/159:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name.append(['NAME', cols[0]])
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append(['Name', headings])
        background_data.append([item for item in cols if item])
        background_data.append(profile_name)
204/160: background_data
204/161:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name.append(['NAME', cols[0]])
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings.push('NAME')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append(profile_name)
204/162:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name.append(['NAME', cols[0]])
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings.append('NAME')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append(profile_name)
204/163:
pagination_max = 2

data = []

profile_data = []
profile_name = []

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name.append(['NAME', cols[0]])
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings.append(['NAME'])
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append(profile_name)
204/164:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append(['NAME', profile_name])
204/165: background_data
204/166:
df = pd.DataFrame(background_data, index=None)
df
204/167:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/168: background_data
204/169:
df = pd.DataFrame(background_data, index=None)
df
204/170:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/171: background_data
204/172:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings[4] = 'NAME'
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/173:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings.append('NAME')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/174:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings.append(['NAME'])
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/175:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings.push(['NAME'])
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/176:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings.inser(4, 'NAME')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/177:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.inser(4, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/178:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(4, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/179: background_data
204/180:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.append([profile_name])
204/181: background_data
204/182:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        background_data.append([headings])
        background_data.append([item for item in cols if item])
        background_data.insert(0, [profile_name])
204/183: background_data
204/184:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.inser(0, [profile_name])
        background_data.append([headings])
        background_data.append([item for item in cols if item])
#         background_data.insert(0, [profile_name])
204/185:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, [profile_name])
        background_data.append([headings])
        background_data.append([item for item in cols if item])
#         background_data.insert(0, [profile_name])
204/186: background_data
204/187:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
204/188: background_data
204/189:
df = pd.DataFrame(background_data, index=None)
df
204/190:
df = pd.DataFrame(background_data, index=None)
df.tail
204/191:
df = pd.DataFrame(background_data, index=None)
df.tail()
204/192:
profile_data

# background_data
204/193:
profile_data

# background_data
204/194:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
204/195:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
204/196:
profile_data

# background_data
204/197:
df = pd.DataFrame(background_data, index=None)
df.tail()
204/198: bg_data_rows
204/199: background_table
204/200: profile_data
204/201: profile_data
204/202: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
204/203: rows
204/204:

df = pd.DataFrame(profile_data, index=None, columns=None)
204/205: df.to_csv('profile_data.csv', index=False, index_label=False)
204/206: df.sample(frac=.3)
204/207: df.T
204/208: rows
204/209: profile_table
204/210: table = soup.find_all('table')
204/211: table[7]
204/212: len(table)
204/213:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
204/214:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
204/215:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
204/216:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
204/217: data
204/218: table
204/219: new_table = pd.DataFrame(columns=range(0,2), index=[0])
204/220:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
204/221:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'NAME')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
204/222:
profile_data

# background_data
204/223:
# profile_data

background_data
204/224:
df = pd.DataFrame(background_data, index=None)
df.tail()
204/225:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
204/226:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    # experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
204/227:
# profile_data

background_data
204/228:
df = pd.DataFrame(background_data, index=None, columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df.tail()
204/229: bg_data_rows
204/230: background_table
204/231: profile_data
204/232: profile_data
204/233: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
204/234: rows
204/235:

df = pd.DataFrame(profile_data, index=None, columns=None)
204/236: df.to_csv('profile_data.csv', index=False, index_label=False)
204/237: df.sample(frac=.3)
204/238: df.T
204/239: rows
204/240: profile_table
204/241: table = soup.find_all('table')
204/242: table[7]
204/243: len(table)
204/244:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
204/245:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
204/246:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
204/247:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
204/248: data
204/249: table
204/250: new_table = pd.DataFrame(columns=range(0,2), index=[0])
204/251:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
204/252:
df = pd.DataFrame(background_data, index=None, columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df
204/253:
df = pd.DataFrame(background_data, index=None, index_label=False, columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df
204/254:
df = pd.DataFrame(background_data, index=None, index_label=None, columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df
204/255:
df = pd.DataFrame(background_data, index=None, columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df
204/256:
df = pd.DataFrame(background_data, index=None, columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df.dropna(axis=0, inplace=True)
204/257:
df = pd.DataFrame(background_data, index=None, columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df.dropna(axis=0, inplace=True)
df
204/258:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []

experience_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        background_data.append([item for item in cols if item])
204/259:
# profile_data

# background_data

experience_data
204/260:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []

experience_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        background_data.append([item for item in cols if item])
204/261:
# profile_data

# background_data

experience_data
204/262:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []

experience_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        background_data.append([item for item in cols if item])
204/263: experience_data_rows
204/264:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []

experience_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
204/265:
# profile_data

# background_data

experience_data
204/266:
df = pd.DataFrame(background_data, index=None, columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df.dropna(axis=0, inplace=True)
df
204/267:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []

experience_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
204/268:
# profile_data

# background_data

experience_data
204/269:
df = pd.DataFrame(background_data, index=None, columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df.dropna(axis=0, inplace=True)
df
204/270:
df = pd.DataFrame(experience_data, index=None, columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df.dropna(axis=0, inplace=True)
df
204/271:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []

experience_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    # hobbies_table_title = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
204/272:
df = pd.DataFrame(experience_data, index=None, columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df.dropna(axis=0, inplace=True)
df
204/273:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table_title = soup.find_all(id='example5')[0]

    hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
#     for index, row in enumerate(hobbies_data_rows):
#         heading = row.find_all('th')
#         headings = [title]
204/274:
# profile_data

# background_data

# experience_data

hobbies_data_rows
204/275:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table_title = soup.find_all(id='example5')[0]

    hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
#     for index, row in enumerate(hobbies_data_rows):
#         heading = row.find_all('th')
#         headings = [title]
204/276:
# profile_data

# background_data

# experience_data

hobbies_data_rows
204/277:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example5')[0]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
#     for index, row in enumerate(hobbies_data_rows):
#         heading = row.find_all('th')
#         headings = [title]
204/278:
# profile_data

# background_data

# experience_data

hobbies_data_rows
204/279:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example5')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
#     for index, row in enumerate(hobbies_data_rows):
#         heading = row.find_all('th')
#         headings = [title]
204/280:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
#     for index, row in enumerate(hobbies_data_rows):
#         heading = row.find_all('th')
#         headings = [title]
204/281:
# profile_data

# background_data

# experience_data

hobbies_data_rows
204/282:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        heading = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for iten in cols if item])
204/283:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        heading = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('li')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for iten in cols if item])
204/284:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        heading = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for iten in cols if item])
204/285:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
#         heading = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for iten in cols if item])
204/286:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        heading = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
205/1:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
205/2:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        heading = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
205/3:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        heading = row.find_all('th')
        headings = [title.strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
205/4:
# profile_data

# background_data

# experience_data

hobbies_data_rows
205/5:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        heading = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
205/6:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
205/7:
# profile_data

# background_data

# experience_data

hobbies_data_rows
205/8:
# profile_data

# background_data

# experience_data

hobbies_data
205/9:
df = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests', 'Any Other Information'])
df.dropna(axis=0, inplace=True)
df
205/10:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests', 'Any Other Information'])
df.dropna(axis=0, inplace=True)
df
205/11:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  )
df.dropna(axis=0, inplace=True)
df
205/12:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df
205/13:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df.T
205/14:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df
205/15:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ', ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
205/16:
# profile_data

# background_data

# experience_data

hobbies_data
205/17:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df
205/18:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df.T
205/19:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    # hobbies_table = soup.find_all('tbody')[-3]

    # membership_table_title = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
205/20:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df
205/21:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_table
205/22:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
205/23:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_table
205/24:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
205/25:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_data
205/26:
df = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df
205/27:
df = pd.DataFrame(membership_data,
                  index=None,
                  )
df.dropna(axis=0, inplace=True)
df
205/28:
df = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Committee Membership', 'Membership'])
df.dropna(axis=0, inplace=True)
df
205/29:
pagination_max = 500

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[1]

    # membership_table = soup.find_all('tbody')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
205/30:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_data
205/31:
df = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Committee Membership', 'Membership'])
df.dropna(axis=0, inplace=True)
df
205/32:
df = pd.DataFrame(membership_data,
                  index=None)
df.dropna(axis=0, inplace=True)
df
205/33:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
205/34:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_data
205/35:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_table_rows
205/36:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
205/37:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
205/38:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_table_rows
205/39:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[2]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
205/40:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
205/41:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_table_rows
205/42:
# profile_data

# background_data

# experience_data

# hobbies_data

membership_data
205/43:
df = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df.dropna(axis=0, inplace=True)
df
205/44:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/45:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/46:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_rows
205/47:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

membership_table_rows
205/48:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_rows
205/49:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/50:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_rows
205/51:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/52:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_rows
205/53:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/54:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/55:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_rows
205/56:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tbody')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/57:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_rows
205/58:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
205/59:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tbody')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/60:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
205/61:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tbody')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/62:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
205/63:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tbody')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/64:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
205/65:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/66:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
205/67:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/68:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
205/69:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('td')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
205/70:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/1:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
206/2:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('td')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/3:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/4:
df = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df.dropna(axis=0, inplace=True)
df
206/5:
df = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df.dropna(axis=0, inplace=True)
df
206/6:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df
206/7:
df = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df.dropna(axis=0, inplace=True)
df
206/8: bg_data_rows
206/9: background_table
206/10: profile_data
206/11: profile_data
206/12: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
206/13: rows
206/14:

df = pd.DataFrame(profile_data, index=None, columns=None)
206/15: df.to_csv('profile_data.csv', index=False, index_label=False)
206/16: df.sample(frac=.3)
206/17: df.T
206/18: rows
206/19: profile_table
206/20: table = soup.find_all('table')
206/21: table[7]
206/22: len(table)
206/23:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
206/24:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
206/25:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
206/26:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
206/27: data
206/28: table
206/29: new_table = pd.DataFrame(columns=range(0,2), index=[0])
206/30:
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_table.iat[row_marker,column_marker] = column.get_text()
        column_marker += 1

new_table
206/31:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('table')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/32:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/33:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/34:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/35:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find('table')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/36:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/37:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/38:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/39:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/40:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/41:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/42:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find('class': 'odd gradeX')
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/43:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find(attrs: {'class': 'odd gradeX'})
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/44:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find(attrs={'class': 'odd gradeX'})
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/45:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/46:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find(attrs={'class': 'odd gradeX'})
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/47:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/48:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find_all('tr', class_='odd gradeX')
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/49:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/50:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_date
206/51:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find_all('tr', class_='odd gradeX')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/52:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_date
206/53:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find_all('tr', class_='odd gradeX')[-1]
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/54:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find_all(id="example5") 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/55:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_date
206/56:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/57:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_date
206/58:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all(id='example5')[1]
    
    committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/59:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_date
206/60:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', id='example5')[1]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/61:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_date
206/62:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/63:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', id='example5')[1]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/64:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/65:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', id='example5')[1]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/66:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_rows
206/67:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', id='example5')[1]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/68:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/69:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', id='example5')[1]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/70:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/71:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/72:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/73:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/74:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find('table', class_='table table-striped table-bordered')[4]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/75:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find('table', class_='table table-striped table-bordered')
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/76:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/77:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[0]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/78:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/79:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[1]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/80:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/81:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[2]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/82:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/83:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[3]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/84:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/85:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/86:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/87:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/88:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/89:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/90:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/91:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/92:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/93:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/94:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[5]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/95:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
#     committe_table_date = soup.find_all('tr', class_='odd gradeX') 
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/96:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/97:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-1:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/98:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table
206/99:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_data
206/100:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-10:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/101:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_data
206/102:
pagination_max = 20

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-10:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/103:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_data
206/104:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
206/105:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_data
207/1: !cat examples/ex1.csv
207/2: import pandas as pd
207/3: !cat examples/ex1.csv
207/4: pd.options.display.max_rows = 10
207/5: result = pd.read_csv('member_list.csv')
207/6: result
207/7: result = pd.read_csv('member_list.csv', index_col=False)
207/8: result
207/9: result = pd.read_csv('member_list.csv', index_col=None)
207/10: result
207/11: result = pd.read_csv('member_list.csv', skip_rows=[0])
207/12: result
207/13: result = pd.read_csv('member_list.csv', skiprows=[0])
207/14: result
207/15: result = pd.read_csv('member_list.csv', skiprows=[0], index_col='')
207/16: result = pd.read_csv('member_list.csv', skiprows=[0], nrows=4)
207/17: result
207/18: result = pd.read_csv('member_list.csv', skiprows=[0], index=None)
207/19: result = pd.read_csv('member_list.csv', skiprows=[0], header=None)
207/20: result
207/21: result = pd.read_csv('member_list.csv', skiprows=[0])
207/22: result
207/23: result.drop_index()
207/24: result
207/25: result = pd.read_csv('member_list.csv', nrows=4)
207/26: result
207/27: result
207/28: result = pd.read_csv('member_list.csv', nrows=4, skiprows=[0])
207/29: result
207/30: result = pd.read_csv('member_list.csv', nrows=4, skiprows=[0], columns=['Name', 'District', 'Constituency', 'Party', 'Religion', 'Details'])
207/31: result = pd.read_csv('member_list.csv', nrows=4, skiprows=[0])
207/32: result
207/33: result = pd.read_csv('member_list.csv', chuncksize=300)
207/34: result = pd.read_csv('member_list.csv', chunksize=300)
207/35: result
207/36: chunker = pd.read_csv('member_list.csv', chunksize=300)
207/37: chunker
207/38: data = pd.read_csv('member_list.csv', nrows=10, skiprows=[0])
207/39: data
207/40: data.to_csv('out.csv')
207/41: !cat out.csv
207/42: %cat out.csv
207/43: !%cat out.csv
207/44: !cat out.csv
207/45: import sys
207/46: data.to_csv(sys.stdout, sep='|')
207/47: data.to_csv(sys.stdout, na_rep='NULL')
207/48: data.to_csv(sys.stdout, index=False, header=False)
207/49: data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])
207/50: data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c', 'd', 'e', 'f', 'g'])
207/51: data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c', 'd', 'e', 'f'])
207/52: data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])
207/53: data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c', 'd', 'e'])
207/54: data.to_csv(sys.stdout, index=False)
207/55: dates = pd.date_range('1/1/2000', periods=7)
207/56: ts = pd.Series(np.arange(7), index=dates)
207/57:
import pandas as pd
import numpy as np
207/58: ts = pd.Series(np.arange(7), index=dates)
207/59: ts.to_csv(sys.stdout)
207/60: ts.to_csv(sys.stdout, index=False)
207/61: ts.to_csv(sys.stdout)
207/62: ts.to_csv('tseries.csv')
207/63: !cat tseries.csv
207/64: !cat tseries.csv
207/65: ### Working with delimeted formats
207/66: import csv
207/67: f = open('member_list.csv')
207/68: reader = csv.reader(f)
207/69:
for line in reader:
    print(line)
207/70:
with open('member_list.csv') as f:
    lines = list(csv.reader(f))
207/71: header, values = lines[''], lines[1:]
207/72: header, values = lines[0], lines[1:]
207/73: header, values = lines[0], lines[1:]
207/74: data_dict = {h: v for h, v in zip(header, zip(*values))}
207/75: data_dict
207/76:
obj = """
    {"name": "Wes",
     "places_lived": ["United States", "Spain", "Germany"],
     "pet": null,
     "siblings": [{"name": "Scott", "age": 30, "pets": ["Zeus", "Zuko"]},
                  {"name": "Katie", "age": 38,
                   "pets": ["Sixes", "Stache", "Cisco"]}]
} """
207/77: import json
207/78: results = json.loads(obj)
207/79: results
207/80: asjson = json.dumps(results)
207/81: siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])
207/82: siblings = pd.DataFrame(results['siblings'], columns=['name', 'age'])
207/83: siblings
208/1:
import pandas as pd
import numpy as np
212/1:
import pandas as pd
import numpy as np
212/2: string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])
212/3: string_data
212/4: string_dat.isnull()
212/5: string_dat.is_null()
212/6: string_data.isnull()
212/7: string_data[0] = None
212/8: string_data.isnull()
212/9: from numpy import nan as NA
212/10: data = pd.Series([1, NA, 3.5, NA, 7])
212/11: data.dropna()
212/12: data[data.notnull()]
212/13: data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])
212/14: cleaned = data.dropna()
212/15: data
212/16: cleaned
212/17: data.dropna(how="all")
212/18: data[4] = NA
212/19: data
212/20: data.dropna(axis=1, how='all')
212/21: data.dropna(axis=0, how='all')
212/22: data.dropna(axis=1, how='all')
212/23: data.dropna(axis=0, how='all')
212/24: data.dropna(axis=1, how='all')
212/25: df = pd.DataFrame(np.random.randn(7, 3))
212/26: df.iloc[:4, 1] = NA
212/27: df.ilco[:2, 2] = NA
212/28: df.iloc[:2, 2] = NA
212/29: df
212/30: df.loc[:2, 2] = NA
212/31: df
212/32: df = pd.DataFrame(np.random.randn(7, 3))
212/33: df.iloc[:4, 1] = NA
212/34: df.iloc[:2, 2] = NA
212/35: df.dropna()
212/36: df.dropna(thresh=2)
212/37: df.dropna(thresh=3)
212/38: df.dropna(thresh=4)
212/39: df.dropna(thresh=1)
212/40: df.dropna(thresh=2)
212/41: df.dropna(thresh=0)
212/42: df.dropna(thresh=1)
212/43: df.dropna(thresh=2)
212/44: df.dropna(thresh=1)
212/45: df.dropna(thresh=2)
212/46: df.dropna(thresh=3)
212/47: df.dropna(thresh=2)
212/48: df.dropna(thresh)
212/49: df.dropna(thresh=2)
212/50: df.fillna(0)
212/51: df
212/52: df.fillna(0)
212/53: df.fillna({1: 0.5, 2: 0})
212/54: df.fillna({1: df[0], 2: df[0]})
212/55: df.fillna({1: 0.5, 2: 0})
212/56: _ = df.fillna(0, inplace=True)
212/57: f
212/58: df
212/59: df = pd.DataFrame(np.random.randn(6, 3))
212/60: df.iloc[2:, 1] = NA
212/61: df.iloc[4:, 2] = NA
212/62: df
212/63: df.fillna(method='ffill')
212/64: df.fillna(method='ffill', limit=2)
212/65: data = pd.Series([1., NA, 3.5, NA, 7])
212/66: data
212/67: data.fillna(data.mean())
212/68: #### Removing Duplicates
212/69:
data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],
                     'k2': [1, 1, 2, 3, 3, 4, 4]})
212/70: data
212/71: data.duplicated()
212/72: data.drop_duplicates()
212/73: data['v1'] = range(7)
212/74: data
212/75: data.drop_duplicates(['k1'])
212/76: data['v1'] = range(7)
212/77: data['v1'] = range(7)
212/78: data
212/79: data.drop_duplicates(['k1'])
212/80: data.drop_duplicates(['k1'])
212/81: data.drop_duplicates(['k1', 'k2'], keep='last')
212/82: data.drop_duplicates(['k1'], keep='last')
212/83: data.drop_duplicates(['k1'])
212/84: data.drop_duplicates(['k1', 'k2'])
212/85: data.drop_duplicates(['k1', 'k2'],  keep='last')
212/86: ### Transforming Data Using a Function or Mapping
212/87:
data = pd.DataFrame({'food': ['bacon', 'pulled pock', 'bacon',
                             'Pastrami', 'corned beef', 'Bacon',
                             'pastrami', 'honey ham', 'nova lox'],
                    'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
212/88: data
212/89:
meat_to_animal = {
    'bacon': 'pig',
    'pulled pork': 'pig',
    'pastrami': 'cow',
    'corned beef': 'cow',
    'honey ham': 'pig',
    'nova lox': 'salmon'
}
212/90: lowercases = data['food'].str.lower()
212/91: lowercased = data['food'].str.lower()
212/92: lowercased = data['food'].str.lower()
212/93: lowercased
212/94: data['animal'] = lowercased.map(meat_to_animal)
212/95: data
212/96:
import pandas as pd
import numpy as np
212/97: string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])
212/98: string_data
212/99: string_data.isnull()
212/100: string_data[0] = None
212/101: string_data.isnull()
212/102: from numpy import nan as NA
212/103: data = pd.Series([1, NA, 3.5, NA, 7])
212/104: data.dropna()
212/105: data[data.notnull()]
212/106: data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])
212/107: cleaned = data.dropna()
212/108: data
212/109: cleaned
212/110: data.dropna(how="all")
212/111: data[4] = NA
212/112: data
212/113: data.dropna(axis=1, how='all')
212/114: df = pd.DataFrame(np.random.randn(7, 3))
212/115: df.iloc[:4, 1] = NA
212/116: df.iloc[:2, 2] = NA
212/117: df
212/118: df.dropna()
212/119: df.dropna(thresh=2)
212/120: df.fillna(0)
212/121: df.fillna({1: 0.5, 2: 0})
212/122: _ = df.fillna(0, inplace=True)
212/123: df
212/124: df = pd.DataFrame(np.random.randn(6, 3))
212/125: df.iloc[2:, 1] = NA
212/126: df.iloc[4:, 2] = NA
212/127: df
212/128: df.fillna(method='ffill')
212/129: df.fillna(method='ffill', limit=2)
212/130: data = pd.Series([1., NA, 3.5, NA, 7])
212/131: data.fillna(data.mean())
212/132:
data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],
                     'k2': [1, 1, 2, 3, 3, 4, 4]})
212/133: data
212/134: data.duplicated()
212/135: data.drop_duplicates()
212/136: data['v1'] = range(7)
212/137: data
212/138: data.drop_duplicates(['k1'])
212/139: data.drop_duplicates(['k1', 'k2'],  keep='last')
212/140:
data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',
                             'Pastrami', 'corned beef', 'Bacon',
                             'pastrami', 'honey ham', 'nova lox'],
                    'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
212/141: data
212/142:
meat_to_animal = {
    'bacon': 'pig',
    'pulled pork': 'pig',
    'pastrami': 'cow',
    'corned beef': 'cow',
    'honey ham': 'pig',
    'nova lox': 'salmon'
}
212/143: lowercased = data['food'].str.lower()
212/144: lowercased
212/145: data['animal'] = lowercased.map(meat_to_animal)
212/146: data
212/147: data['food'].map(lambda x: meat_to_animal[x.lower()])
212/148: data = pd.Series([1., -999, 2., -999., -1000, 3.])
212/149: data
212/150: data.replace(-999, np.nan)
212/151: data.replace([-999, -999], np.nan)
212/152: data.replace([-999, -1000], np.nan)
212/153: data.replace([-999, -1000], [np.nan, 0])
212/154: data.replace({-999, -1000}, {np.nan, 0})
212/155: data.replace({-999: np.nan, -1000: 0})
212/156:
data = pd.DataFrame(np.arange(12).reshape((3, 4)),
                   index=['Ohio', 'Colorado', 'New York'],
                   columns=['one', 'two', 'three', 'four'])
212/157: transform = lambda x: x[:4].upper()
212/158: data.index.map(transform)
212/159: data.index.map(transform)
212/160: data.index
212/161: data.index = data.index.map(transform)
212/162: data
212/163: data.raname(index=str.title, columns=str.upper)
212/164: data.rename(index=str.title, columns=str.upper)
212/165: data.rename(index=str.upper, columns=str.upper)
212/166: data.rename(index=str.title, columns=str.upper)
212/167:
data.rename(index={'OHIO': 'INDIANA'},
           columns={'three': 'peekaboo'})
212/168: data.rename(columns={'four': 4})
212/169:
data.rename(index={'OHIO': 'INDIANA'},
           columns={'three': 'peekaboo'})
212/170: data.rename(index={'OHIO': 'INDIANA'}, inplace=True)
212/171: data
212/172: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
212/173: bins = [18, 25, 35, 60, 100]
212/174: cats = pd.cut(ages, bins)
212/175: cats
212/176: cats.codes
212/177: cats.categories
212/178: pd.value_counts(cats)
212/179: pd.cut([ages, [18, 26, 36, 61, 100]], right=False)
212/180: pd.cut(ages, [18, 26, 36, 61, 100]], right=False)
212/181: pd.cut(ages, [18, 26, 36, 61, 100], right=False)
212/182: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']
212/183: pd.cut(ages, bins, labels=group_names)
212/184: data = np.random.rand(20)
212/185: pd.cut(data, 4, precision=2)
212/186: data = np.random.randn(1000)
212/187: cats = qcut(data, 4)
212/188: cats = pd.qcut(data, 4)
212/189: cats
212/190: pd.value_counts()
212/191: pd.value_counts(cats)
212/192: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])
212/193: data = pd.DataFrame(np.random.randn(1000, 4))
212/194: data.describe()
212/195: col = data[2]
212/196: col[np.abs(col) > 3]
212/197: data[(np.abs(data) > 3).any(1)]
212/198: data[(np.abs(data) > 3).any(1)]
212/199: data[np.abs(data) > 3] = np.sign(data) * 3
212/200: data.describe
212/201: data.describe()
212/202: np.sign(data).head()
212/203: df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))
212/204: sampler = np.random.permutation(5)
212/205: sampler
212/206: df
212/207: df.take(sampler)
212/208: df.sample(n=3)
212/209: choices = pd.Series([5, 7, -1, 6, 4])
212/210: draws = choices.sample(n=10, replace=True)
212/211: draws
212/212: df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})
212/213: pd.get_dummies(df['key'])
212/214: dummies = pd.get_dummies(df['key'], prefix='key')
212/215: df_with_dummy = df[['data']].join(dummies)
212/216: df_with_dummy = df[['data1']].join(dummies)
212/217: df_with_dummy
212/218: df_with_dummy
212/219: mnames = ['movie_id', 'title', 'genres']
212/220: val = 'a,b, guido'
212/221: val.split(',')
212/222: pieces = [x.strip() for x in val.split(',')]
212/223: pieces
212/224: first + second + third = pieces
212/225: first, second, third = pieces
212/226: pieces
212/227: first + '::' + second + '::' third
212/228: first + '::' + second + '::' + third
212/229: 'guido' in val
212/230: val.index('')
212/231: val.index(',')
212/232: val.find('::')
212/233: val.find(':')
212/234: val.find(':')
212/235: val.index('::')
212/236: val.index(':')
212/237: val.index('::')
212/238: val.index(':')
212/239: val.index(':')
212/240: val.count(',')
212/241: val.replace(',', '::')
212/242: val.replace(',', '')
212/243: val.replace(',', ' ')
212/244: val.replace(',', '')
212/245: import re
212/246: text = "foo bar\t baz \tqux"
212/247: re.split('\s+', text)
212/248: regex = re.compile('\s+')
212/249: regex.split(text)
212/250: regex.findall(text)
212/251:
import pandas as pd
import numpy as np
212/252: string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])
212/253: string_data
212/254: string_data.isnull()
212/255: string_data[0] = None
212/256: string_data.isnull()
212/257: from numpy import nan as NA
212/258: data = pd.Series([1, NA, 3.5, NA, 7])
212/259: data.dropna()
212/260: data[data.notnull()]
212/261: data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])
212/262: cleaned = data.dropna()
212/263: data
212/264: cleaned
212/265: data.dropna(how="all")
212/266: data[4] = NA
212/267: data
212/268: data.dropna(axis=1, how='all')
212/269: df = pd.DataFrame(np.random.randn(7, 3))
212/270: df.iloc[:4, 1] = NA
212/271: df.iloc[:2, 2] = NA
212/272: df
212/273: df.dropna()
212/274: df.dropna(thresh=2)
212/275: df.fillna(0)
212/276: df.fillna({1: 0.5, 2: 0})
212/277: _ = df.fillna(0, inplace=True)
212/278: df
212/279: df = pd.DataFrame(np.random.randn(6, 3))
212/280: df.iloc[2:, 1] = NA
212/281: df.iloc[4:, 2] = NA
212/282: df
212/283: df.fillna(method='ffill')
212/284: df.fillna(method='ffill', limit=2)
212/285: data = pd.Series([1., NA, 3.5, NA, 7])
212/286: data.fillna(data.mean())
212/287:
data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],
                     'k2': [1, 1, 2, 3, 3, 4, 4]})
212/288: data
212/289: data.duplicated()
212/290: data.drop_duplicates()
212/291: data['v1'] = range(7)
212/292: data
212/293: data.drop_duplicates(['k1'])
212/294: data.drop_duplicates(['k1', 'k2'],  keep='last')
212/295:
data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',
                             'Pastrami', 'corned beef', 'Bacon',
                             'pastrami', 'honey ham', 'nova lox'],
                    'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
212/296: data
212/297:
meat_to_animal = {
    'bacon': 'pig',
    'pulled pork': 'pig',
    'pastrami': 'cow',
    'corned beef': 'cow',
    'honey ham': 'pig',
    'nova lox': 'salmon'
}
212/298: lowercased = data['food'].str.lower()
212/299: lowercased
212/300: data['animal'] = lowercased.map(meat_to_animal)
212/301: data
212/302: data['food'].map(lambda x: meat_to_animal[x.lower()])
212/303: data = pd.Series([1., -999, 2., -999., -1000, 3.])
212/304: data
212/305: data.replace(-999, np.nan)
212/306: data.replace([-999, -1000], np.nan)
212/307: data.replace([-999, -1000], [np.nan, 0])
212/308: data.replace({-999: np.nan, -1000: 0})
212/309:
data = pd.DataFrame(np.arange(12).reshape((3, 4)),
                   index=['Ohio', 'Colorado', 'New York'],
                   columns=['one', 'two', 'three', 'four'])
212/310: transform = lambda x: x[:4].upper()
212/311: data.index.map(transform)
212/312: data.index = data.index.map(transform)
212/313: data
212/314: data.rename(index=str.title, columns=str.upper)
212/315:
data.rename(index={'OHIO': 'INDIANA'},
           columns={'three': 'peekaboo'})
212/316: data.rename(index={'OHIO': 'INDIANA'}, inplace=True)
212/317: data
212/318: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
212/319: bins = [18, 25, 35, 60, 100]
212/320: cats = pd.cut(ages, bins)
212/321: cats
212/322: cats.codes
212/323: cats.categories
212/324: pd.value_counts(cats)
212/325: pd.cut(ages, [18, 26, 36, 61, 100], right=False)
212/326: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']
212/327: pd.cut(ages, bins, labels=group_names)
212/328: data = np.random.rand(20)
212/329: pd.cut(data, 4, precision=2)
212/330: data = np.random.randn(1000)
212/331: cats = pd.qcut(data, 4)
212/332: cats
212/333: pd.value_counts(cats)
212/334: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])
212/335: data = pd.DataFrame(np.random.randn(1000, 4))
212/336: data.describe()
212/337: col = data[2]
212/338: col[np.abs(col) > 3]
212/339: data[(np.abs(data) > 3).any(1)]
212/340: data[np.abs(data) > 3] = np.sign(data) * 3
212/341: data.describe()
212/342: np.sign(data).head()
212/343: df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))
212/344: sampler = np.random.permutation(5)
212/345: sampler
212/346: df
212/347: df.take(sampler)
212/348: df.sample(n=3)
212/349: choices = pd.Series([5, 7, -1, 6, 4])
212/350: draws = choices.sample(n=10, replace=True)
212/351: draws
212/352: df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})
212/353: pd.get_dummies(df['key'])
212/354: dummies = pd.get_dummies(df['key'], prefix='key')
212/355: df_with_dummy = df[['data1']].join(dummies)
212/356: df_with_dummy
212/357: val = 'a,b, guido'
212/358: val.split(',')
212/359: pieces = [x.strip() for x in val.split(',')]
212/360: pieces
212/361: first, second, third = pieces
212/362: pieces
212/363: first + '::' + second + '::' + third
212/364: 'guido' in val
212/365: val.index(',')
212/366: val.find(':')
212/367: val.index(':')
212/368: text = "foo bar\t baz \tqux"
212/369: re.split('\s+', text)
212/370: regex = re.compile('\s+')
212/371: regex.split(text)
212/372: regex.findall(text)
214/1:
import pandas as pd
import numpy as np
214/2:
data = pd.Series(np.random.randn(9), index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],
                                            [1, 2, 3, 1, 3, 1, 2, 2, 3]])
214/3: data
214/4: data.index
214/5:
MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]],
          labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1, 1, 2]])
214/6:
MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]],
labels=[[0,0,0,1,1,2,2,3,3],[0,1,2,0,2,0,1,1,2]])
214/7: data.index
214/8: data['b']
214/9: data['b':'c']
214/10: data.loc['b', 'd']
214/11: data.loc[['b', 'd']]
214/12: data.loc[:, 2]
214/13: data.unstack()
214/14: data.unstack().stack()
215/1:
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
215/2:
ri = pd.read_csv('police.csv')
ted = pd.read_csv('ted.csv')
ri.driver_age.plot()
215/3: ri.head()
215/4: ri.shape()
215/5: ri.shape
215/6: ri.dtypes
215/7: ri.isnull().sum()
215/8: ri.isnull()
215/9: ri.isnull().sum()
215/10: ri.isnull().sum()
215/11: ## 1. Remove the column that only contains missing values
215/12: ri.shape()
215/13: ri.shape
215/14: ri.drop('county_name', axis='columns', inplace=True)
215/15: ri.shape
215/16: ri.columns
215/17: ri.dropna(axis='columns', how='all').shape
215/18: ri[ri.violation == 'Speeding'].driver_gender.value_counts()
215/19: ri[ri.violation == 'Speeding'].driver_gender.value_counts(normalize=True)
215/20: ri[ri['violation'] == 'Speeding'].driver_gender.value_counts(normalize=True)
215/21: ri[ri['driver_gender'] == 'M'].violation.value_counts(normalize=True)
215/22: ri[ri['driver_gender'] == 'M']['violation'].value_counts(normalize=True)
215/23: ri[ri['violation'] == 'Speeding']['driver_gender'].value_counts(normalize=True)
215/24: ri[ri['driver_gender'] == 'M']['violation'].value_counts(normalize=True)
215/25: ri[ri['driver_gender'] == 'F']['violation'].value_counts(normalize=True)
215/26: ri.groupby('driver_gender')['violation'].value_counts(normalize=True)
215/27: ri.search_conducted.value_counts()
215/28: ri.search_conducted.value_counts(normalize=True)
215/29: ri.search_conducted.value_counts(normalize=True).unstack()
215/30: ri.search_conducted.value_counts(normalize=True).stack()
215/31: ri.search_conducted.value_counts(normalize=True)
215/32: ri['search_conducted'].value_counts(normalize=True)
215/33: ri['search_conducted'].mean()
215/34: ri.groupby('driver_gender')['search_conducted'].mean()
215/35: ri.groupby(['violation', 'driver_gender'])['search_conducted'].mean()
215/36: ri.isnull().sum()
215/37: ri['search_conducted'].value_counts()
215/38: ri[ri['search_conducted'] == False]['search_type'].value_counts()
215/39: ri[ri['search_conducted'] == False]['search_type'].value_counts(dropna=False)
215/40: ri['search_stype'].str.contains('frisked')
215/41: ri['search_type'].str.contains('frisked')
215/42: ri['search_type'].str.contains('Protective Frisk')
215/43: ri['frisk'] = ri['search_type'].str.contains('Protective Frisk')
215/44: ri['frisk'].value_counts()
215/45: ri['frisk'].value_counts(dropna=False)
215/46: ri['frisk'].sum()
215/47: ri['frisk'].mean()
215/48: ri['stop_date'].str.slice(0, 4)
215/49: ri['stop_date'].str.slice(0, 4).value_counts()
215/50: combined = ri['stop_date'].str.cat(ri['stop_time'], sep=' ')
215/51: combined
215/52: ri['stop_datetime'] = pd.to_datetime(combined)
215/53: ri.dtypes
215/54: ri['stop_datetime'].dt.days
215/55: ri['stop_datetime'].dt.day
215/56: ri['stop_datetime'].dt.month
215/57: ri['stop_datetime'].dt.weekday
215/58: ri['stop_datetime'].dt.year
215/59: ri['stop_datetime'].dt.year.value_counts()
215/60: ri['stop_datetime'].dt.year.value_counts().sort_vales()
215/61: ri['stop_datetime'].dt.year.value_counts().sort_values()
215/62: ri['stop_datetime'].dt.year.value_counts().sort_values()[0]
215/63: ri['stop_datetime'].dt.year.value_counts().sort_values().index(0)
215/64: ri['stop_datetime'].dt.year.value_counts().sort_values().index[0]
215/65: ri['drug_related_stop'].mean()
215/66: ri['drugs_related_stop'].mean()
215/67: ri.groupby(ri['stop_date'])['drug_related_stop'].mean()
215/68: ri.groupby(ri['stop_date'])['drugs_related_stop'].mean()
215/69: ri.groupby(ri['stop_datetime'].dt.hours)['drugs_related_stop'].mean()
215/70: ri.groupby(ri['stop_datetime'].dt.hour)['drugs_related_stop'].mean()
215/71: ri.groupby(ri['stop_datetime'].dt.hour)['drugs_related_stop'].mean().plot()
215/72: ri.['stop_datetime'].dt.hour
215/73: ri['stop_datetime'].dt.hour
215/74: ri['stop_datetime'].dt.hour.value_counts()
215/75: ri['stop_datetime'].dt.hour.value_counts().sort_values()
215/76: ri['stop_datetime'].dt.hour.value_counts().sort_index()
215/77: ri['stop_datetime'].dt.hour.value_counts().sort_index().plot()
215/78: ri['stop_duration'].value_counts()
215/79: ri['stop_duration'].value_counts(dropna=False)
215/80: ri.loc[() ]
215/81: ri.loc[(ri['stop_duration'] == '1') | (ri['stop_duration'] == '2'), 'stop_duration'] = 'NaN'
215/82: ri['stop_duration'].value_counts()
215/83: ri['stop_duration'].value_counts(dropna=False)
215/84: import numpy as np
215/85: ri.loc[ri['stop_duration'] == 'NaN', 'stop_duration'] = np.nan
215/86: ri['stop_duration'].value_counts(dropna=False)
216/1:
import pandas as pd
import numpy as np
216/2:
data = pd.Series(np.random.randn(9), index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],
                                            [1, 2, 3, 1, 3, 1, 2, 2, 3]])
216/3: data
216/4: data.index
216/5: data['b']
216/6: data['b':'c']
216/7: data.loc[['b', 'd']]
216/8: data.loc[:, 2]
216/9: data.unstack()
216/10: data.unstack().stack()
216/11:
frame = pd.DataFrame(np.arange(12).reshape((4, 3)),
                    index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],
                    columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])
216/12: frame
216/13: frame.index.names
216/14: frame.index.names = ['Key1', 'Key2']
216/15: frame.columns.names = ['state', 'color']
216/16: color
216/17: frame
216/18: frame['Ohio']
218/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
218/2:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
218/3:
contracts_by_type = awarded_contracts.groupby('type')
contracts_by_type.groups
218/4: awarded_contracts.info()
218/5: awarded_contracts.tail()
218/6: open_method = awarded_contracts.filter(items=['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest'])
218/7: open_method.info()
218/8: animals = pd.Series(["cat", "dog", "bird", "panda", "snake"], index=[0, 3, 9, 8, 3])
218/9: animals
218/10: animals.loc[3]
218/11: animals.loc[9]
218/12: awarded_contracts
218/13: awarded_contracts.loc[3]
218/14: awarded_contracts.iloc[3]
218/15: awarded_contracts.iloc[:3]
218/16: awarded_contracts.loc[:3]
218/17: awarded_contracts["subject_of_procurement"]
218/18: awarded_contracts.subject_of_procurement
218/19: awarded_contracts[awarded_contracts['type'] == 'Supplies']
218/20: awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]
218/21: awarded_contracts['difference_in_contract_price'] = awarded_contracts['contract_price'] - awarded_contracts['estimated_amount']
218/22: awarded_contracts.head()
218/23:
# awarded_contracts.to_csv('all_awarded_contracts_FY_2019-2020.csv')

# awarded_contracts.to_excel('all_awarded_contracts_FY_2019-2020.xlxs')
218/24:
awarded_contracts_method_changed = awarded_contracts[awarded_contracts["plan_method"] != awarded_contracts["method"]]

awarded_contracts_method_changed.to_csv('awarded_contracts_method_changed.csv')
218/25: awarded_contracts_method_changed.sample(frac=.5)
218/26: pd.crosstab(awarded_contracts["method"], awarded_contracts["plan_method"])
218/27: awarded_contracts.groupby(["type"]).mean()
218/28: awarded_contracts.groupby(["method"]).mean()
218/29: awarded_contracts["contract_price"].plot()
218/30: awarded_contracts["award_price"].hist()
218/31: awarded_contracts["difference"] = awarded_contracts["contract_price"] - awarded_contracts["estimated_amount"]
218/32: awarded_contracts
218/33: awarded_contracts["difference"].hist()
218/34: awarded_contracts.sample(frac=1)
218/35: awarded_contracts.dtypes
218/36: awarded_contracts['contract_status'] = np.where(awarded_contracts['difference'] >= 0, 'Savings', 'Losses')
218/37: awarded_contracts.sample(frac=.1)
218/38:
# awarded_contracts.to_excel('awarded_contracts.xlsx', index=False)
# awarded_contracts.to_csv('awarded_contracts.csv', index=False)
218/39: open_method.info()
218/40: open_method.head()
218/41:

open_methods = awarded_contracts[awarded_contracts['method'] == ['Open Domestic Bidding', 'Open International Bidding(OIB)', 'Expression Of Interest']]
218/42:

open_methods = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']

open_methods
218/43:

open_methods = np.where(awarded_contracts['method'] == 'Open Domestic Bidding' || awarded_contracts['method'] == 'Open International Bidding(OIB)' || awarded_contracts['method'] == 'Expression Of Interest')

open_methods
218/44:

open_methods = np.where(awarded_contracts['method'] == 'Open Domestic Bidding' | awarded_contracts['method'] == 'Open International Bidding(OIB)' | awarded_contracts['method'] == 'Expression Of Interest')

open_methods
218/45:

open_methods = np.where(awarded_contracts['method'] == 'Open Domestic Bidding' & awarded_contracts['method'] == 'Open International Bidding(OIB)' & awarded_contracts['method'] == 'Expression Of Interest')

open_methods
218/46:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods_3
218/47:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.merge(open_methods_1, open_methods_2, open_methods_3)

open_methods
218/48:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.merge(open_methods_1, open_methods_2)

open_methods
218/49:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.merge(open_methods_1, open_methods_2)

open_methods
218/50:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.merge(open_methods_1, open_methods_2, on="entity")

open_methods
218/51:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = open_methods_1 + open_methods_2

open_methods
218/52:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.concat(open_methods_1, open_methods_2)

open_methods
218/53:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.concat([open_methods_1, open_methods_2])

open_methods
218/54:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.concat([open_methods_1, open_methods_2, open_methods_3])

open_methods.tail()
218/55:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.concat([open_methods_1, open_methods_2, open_methods_3])

open_methods.value_counts()
218/56:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.concat([open_methods_1, open_methods_2, open_methods_3])

open_methods.value_counts
218/57:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.concat([open_methods_1, open_methods_2, open_methods_3])

open_methods
218/58:
awarded_contracts = pd.read_csv('awarded_contracts.csv')
awarded_contracts.head()
218/59: awarded_contracts.describe()
218/60: open_methods.value_counts()
218/61: open_methods['entity'].value_counts()
218/62: open_methods['entity'].value_counts()
218/63: open_methods['entity'].value_counts(normalize=True)
218/64: open_methods['entity'].value_counts(normalize=True * 10)
218/65: open_methods['entity'].value_counts(normalize=True * 100)
218/66: open_methods['entity'].value_counts(normalize=True) * 100
218/67: open_methods['index'].value_counts()
218/68: open_methods.describe()
218/69: open_methods['entity'].value_counts(normalize=True)
218/70: open_methods['entity'].value_counts(normalize=True)
218/71: open_methods.groupby('method')['entity'].value_counts(normalize=True)
218/72: open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)
218/73: open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)
218/74:
open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)

open_methods_grouped
218/75:
open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)

pd.DataFrame(open_methods_grouped)
218/76:
open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)

open_methods_grouped = pd.DataFrame(open_methods_grouped)

open_methods_grouped['Expression Of Interest']

open_methods_grouped['Open International Bidding(OIB)']

open_methods_grouped['Open Domestic Bidding']
218/77:
open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)

open_methods_grouped = pd.DataFrame(open_methods_grouped)

open_methods_grouped['Expression Of Interest']
218/78:
open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)

open_methods_grouped = pd.DataFrame(open_methods_grouped)

open_methods_grouped
218/79:
open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)

open_methods_grouped = pd.DataFrame(open_methods_grouped)

open_methods_grouped['method']
218/80:
open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)

open_methods_grouped = pd.DataFrame(open_methods_grouped)

open_methods_grouped
218/81:
open_methods_grouped = open_methods.groupby('method')['entity'].value_counts(normalize=True)

open_methods_grouped
218/82:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.concat([open_methods_1, open_methods_2, open_methods_3])

open_methods

open_methods.to_csv('open_methods_only_analysis.csv')
218/83:

open_methods_1 = awarded_contracts[awarded_contracts['method'] == 'Open Domestic Bidding']
open_methods_2 = awarded_contracts[awarded_contracts['method'] == 'Open International Bidding(OIB)']
open_methods_3 = awarded_contracts[awarded_contracts['method'] == 'Expression Of Interest']

open_methods = pd.concat([open_methods_1, open_methods_2, open_methods_3])

open_methods

# open_methods.to_csv('open_methods_only_analysis.csv')
218/84:
open_methods.groupby('entity').agg(
    'min', 'mean'
)
218/85:
open_methods.groupby('entity').agg(
    ['min', 'mean']
)
218/86:
open_methods.groupby('entity')['contract_price'].agg(
    ['min', 'mean']
)
218/87:
open_methods.groupby('entity')['contract_price'].agg(
    ['min', 'max']
)
220/1:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.tail()
220/2:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
220/3:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.tail()
220/4:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
220/5:

contracts_above_estimate_without_frameworks['no_of_bids'] = np.where(contracts_above_estimate_without_frameworks['no_of_bids'] == 0, 1, contracts_above_estimate_without_frameworks['no_of_bids'])
220/6:

contracts_above_estimate_without_frameworks['avg_read_out_price'] = contracts_above_estimate_without_frameworks['total_read_out_price'] / contracts_above_estimate_without_frameworks['no_of_bids']
220/7:

contracts_above_estimate_without_frameworks.head()
220/8:

contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] = contracts_above_estimate_without_frameworks['contract_price'] - contracts_above_estimate_without_frameworks['estimated_amount']
220/9:

contracts_above_estimate_without_frameworks['price_status'] = np.where(contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] <= 0, 'within', 'above')
220/10:

contracts_above_estimate_without_frameworks.head()
220/11:

contracts_above_estimate_without_frameworks.dtypes
220/12:

# contracts_above_estimate_without_frameworks = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['price_status'] == 'above']
220/13:

# contracts_above_estimate_without_frameworks.head()
220/14:

contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['contract_price'] < 1]
220/15:

# contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
220/16:

# contracts_above_estimate_without_frameworks.describe()
220/17:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
# contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
220/18:
contracts_by_method = contracts_above_estimate_without_frameworks.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
# contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
220/19:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)
220/20:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

np.where(contracts['ppeId'] == 159153)
220/21:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

np.where(contracts[contracts['ppeId'] == 159153])
220/22:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = np.where(contracts[contracts['ppeId'] == 159153])
220/23:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts
220/24:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts.goupby(ppeId)
220/25:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts.goupby('ppeId')
220/26:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts.groupby('ppeId')
220/27:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts.groupby('ppeId')['total_read_out_price'].max()
220/28:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts.groupby('ppeId')['total_read_out_price'].sum()
220/29:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts.groupby('ppeId')['total_read_out_price'].value_counts()
220/30:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts.groupby('ppeId')['entity']['total_read_out_price'].value_counts()
220/31:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts.groupby('ppeId')['total_read_out_price'].value_counts()
220/32:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]
220/33:
contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.sample(frac=.8)

contracts = contracts[contracts['ppeId'] == 159153]

contracts
220/34:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
220/35:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
220/36:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
220/37: #### Read the CSV Files
220/38:
competetive = pd.read_csv('estimated_highest_readtout-competitive.csv')
#competetive = competetive[competetive['method'] != 'Micro Procurements']
#competetive = competetive[competetive['method'] != 'Direct Procurements']

competetive.shape()
220/39:
competetive = pd.read_csv('estimated_highest_readtout-competitive.csv')
#competetive = competetive[competetive['method'] != 'Micro Procurements']
#competetive = competetive[competetive['method'] != 'Direct Procurements']

competetive.shape
220/40:
competetive = pd.read_csv('estimated_highest_readtout-competitive.csv')
competetive = competetive[competetive['method'] != 'Micro Procurements']
#competetive = competetive[competetive['method'] != 'Direct Procurements']

competetive.shape
220/41:
competetive = pd.read_csv('estimated_highest_readtout-competitive.csv')
competetive = competetive[competetive['method'] != 'Micro Procurements']
competetive = competetive[competetive['method'] != 'Direct Procurements']

competetive.shape
220/42:
competetive = pd.read_csv('estimated_highest_readtout-competitive.csv')
competetive = competetive[competetive['method'] != 'Micro Procurements']
competetive = competetive[competetive['method'] != 'Direct Procurements']

competetive.sample(frac=.8)
220/43:
competetive = pd.read_csv('estimated_highest_readtout-competitive.csv')
competetive = competetive[competetive['method'] != 'Micro Procurement']
competetive = competetive[competetive['method'] != 'Direct Procurement']

competetive.shape
220/44:
non_competetive = pd.read_csv('estimated_highest_readtout-non-competitive.csv')

non_competetive.shape
220/45:
non_competetive = pd.read_csv('estimated_highest_readtout-non-competitive.csv')
competetive_m = competetive[competetive['method'] == 'Micro Procurement']
competetive_d = competetive[competetive['method'] == 'Direct Procurement']
non_competetive = pd.concat([competetive_m, competetive_d])
non_competetive.shape
220/46:
non_competetive = pd.read_csv('estimated_highest_readtout-non-competitive.csv')
competetive_m = competetive[competetive['method'] == 'Micro Procurement']
competetive_d = competetive[competetive['method'] == 'Direct Procurement']
non_competetive = pd.concat([competetive_m, competetive_d])
non_competetive.shape
competetive_d
220/47:
non_competetive = pd.read_csv('estimated_highest_readtout-non-competitive.csv')
non_competetive_1 = non_competetive[non_competetive['method'] == 'Micro Procurement']
non_competetive_2 = non_competetive[non_competetive['method'] == 'Direct Procurement']
non_competetive = pd.concat([non_competetive_1, non_competetive_2])
non_competetive.shape
220/48:
non_competetive = pd.read_csv('estimated_highest_readtout-non-competitive.csv')
non_competetive_1 = non_competetive[non_competetive['method'] == 'Micro Procurement']
non_competetive_2 = non_competetive[non_competetive['method'] == 'Direct Procurement']
non_competetive = pd.concat([non_competetive_1, non_competetive_2])
non_competetive.shape
non_competetive
220/49:
non_competetive = pd.read_csv('estimated_highest_readtout-non-competitive.csv')
non_competetive_1 = non_competetive[non_competetive['method'] == 'Micro Procurement']
non_competetive_2 = non_competetive[non_competetive['method'] == 'Direct Procurement']
non_competetive = pd.concat([non_competetive_1, non_competetive_2])
non_competetive.shape
220/50:
contracts = pd.concat([competetive, non_competetive])

contracts.shape()
220/51:
contracts = pd.concat([competetive, non_competetive])

contracts.shape
220/52:
contracts = pd.concat([competetive, non_competetive])

contracts
220/53:
# contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
220/54:
contracts = pd.concat([competetive, non_competetive])

contracts.head()
220/55:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
220/56:
competetive = pd.read_csv('estimated_highest_readtout-competitive.csv')
competetive = competetive[competetive['method'] != 'Micro Procurement']
competetive = competetive[competetive['method'] != 'Direct Procurement']
competetive.shape
220/57:
non_competetive = pd.read_csv('estimated_highest_readtout-non-competitive.csv')
non_competetive_1 = non_competetive[non_competetive['method'] == 'Micro Procurement']
non_competetive_2 = non_competetive[non_competetive['method'] == 'Direct Procurement']
non_competetive = pd.concat([non_competetive_1, non_competetive_2])
non_competetive.shape
220/58:
contracts = pd.concat([competetive, non_competetive])

contracts.head()
220/59:
# contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
220/60:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
220/61:

contracts_above_estimate_without_frameworks['no_of_bids'] = np.where(contracts_above_estimate_without_frameworks['no_of_bids'] == 0, 1, contracts_above_estimate_without_frameworks['no_of_bids'])
220/62:

contracts_above_estimate_without_frameworks['avg_read_out_price'] = contracts_above_estimate_without_frameworks['total_read_out_price'] / contracts_above_estimate_without_frameworks['no_of_bids']
220/63:

contracts_above_estimate_without_frameworks.head()
220/64:

contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] = contracts_above_estimate_without_frameworks['contract_price'] - contracts_above_estimate_without_frameworks['estimated_amount']
220/65:

contracts_above_estimate_without_frameworks['price_status'] = np.where(contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] <= 0, 'within', 'above')
220/66:

contracts_above_estimate_without_frameworks.head()
220/67:

contracts_above_estimate_without_frameworks.dtypes
220/68:

contracts_above_estimate_without_frameworks = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['price_status'] == 'above']
220/69:

contracts_above_estimate_without_frameworks.head()
220/70:

contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['contract_price'] < 1]
220/71:

contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
220/72:

contracts_above_estimate_without_frameworks.describe()
220/73:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
220/74:
contracts_by_method = contracts_above_estimate_without_frameworks.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
220/75:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
220/76:
competetive = pd.read_csv('estimated_highest_readtout-competitive.csv')
competetive = competetive[competetive['method'] != 'Micro Procurement']
competetive = competetive[competetive['method'] != 'Direct Procurement']
competetive.shape
220/77:
non_competetive = pd.read_csv('estimated_highest_readtout-non-competitive.csv')
non_competetive_1 = non_competetive[non_competetive['method'] == 'Micro Procurement']
non_competetive_2 = non_competetive[non_competetive['method'] == 'Direct Procurement']
non_competetive = pd.concat([non_competetive_1, non_competetive_2])
non_competetive.shape
220/78:
contracts = pd.concat([competetive, non_competetive])

contracts.head()
220/79:
# contracts = pd.read_csv('estimate_read_out_contract_price_3.csv')

contracts.head()
220/80:

contracts_above_estimate_without_frameworks = contracts[(contracts['is_framework'] == False) & (contracts['is_lotted_framework'] == False)]

contracts_above_estimate_without_frameworks.head()
220/81:

contracts_above_estimate_without_frameworks['no_of_bids'] = np.where(contracts_above_estimate_without_frameworks['no_of_bids'] == 0, 1, contracts_above_estimate_without_frameworks['no_of_bids'])
220/82:

contracts_above_estimate_without_frameworks['avg_read_out_price'] = contracts_above_estimate_without_frameworks['total_read_out_price'] / contracts_above_estimate_without_frameworks['no_of_bids']
220/83:

contracts_above_estimate_without_frameworks.head()
220/84:

contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] = contracts_above_estimate_without_frameworks['contract_price'] - contracts_above_estimate_without_frameworks['estimated_amount']
220/85:

contracts_above_estimate_without_frameworks['price_status'] = np.where(contracts_above_estimate_without_frameworks['difference_contract_price_estimate'] <= 0, 'within', 'above')
220/86:

contracts_above_estimate_without_frameworks.head()
220/87:

contracts_above_estimate_without_frameworks.dtypes
220/88:

contracts_above_estimate_without_frameworks = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['price_status'] == 'above']
220/89:

contracts_above_estimate_without_frameworks.head()
220/90:

contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['contract_price'] < 1]
220/91:

# contracts_above_estimate_without_frameworks.to_csv('contracts_above_estimate.csv')
220/92:

contracts_above_estimate_without_frameworks.describe()
220/93:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
# contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
220/94:
contracts_by_method = contracts_above_estimate_without_frameworks.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum']
    }
)

# Convert to csv
# contracts_by_method.to_csv('contracts_by_method.csv')

contracts_by_method
220/95:

contracts_above_estimate_without_frameworks.describe()
220/96: contracts_above_estimate_without_frameworks.shape()
220/97: contracts_above_estimate_without_frameworks.shape
220/98:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum', 'count'],
        'contract_price': ['sum', 'count']
    }
)

# Convert to csv
# contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
220/99:
contracts_by_type = contracts_above_estimate_without_frameworks.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)

# Convert to csv
# contracts_by_type.to_csv('contracts_by_type.csv')

contracts_by_type
220/100: contracts_above_estimate_without_frameworks.shape
220/101:

contracts_above_estimate_without_frameworks.describe()
220/102: contracts_above_estimate_without_frameworks.head(2)
220/103: contracts_above_estimate_without_frameworks.head()
220/104: works_contracts_above = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['type' == 'Works']]
220/105: works_contracts_above = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['type'] == 'Works']
220/106:
works_contracts_above = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['type'] == 'Works']

works_contracts_above.shape
220/107:
works_contracts_above = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['type'] == 'Works']

works_contracts_above.shape
220/108:
### Group by method

works_contracts_above_by_method = works_contracts_above.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)
220/109:
### Group by method

works_contracts_above_by_method = works_contracts_above.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)

works_contracts_above_by_method
220/110: works_contracts_above.head(1)
220/111:
### Order the works procurements from highest to lowest

works_contracts_above_sort_by_avg_readout = pd.sort_values(by="avg_read_out_price", ascending=False)

works_contracts_above_sort_by_avg_readout.head()
220/112:
### Order the works procurements from highest to lowest

works_contracts_above_sort_by_avg_readout = pd.sort_value(by="avg_read_out_price", ascending=False)

works_contracts_above_sort_by_avg_readout.head()
220/113:
### Order the works procurements from highest to lowest

works_contracts_above_sort_by_avg_readout = works_contracts_above.sort_value(by="avg_read_out_price", ascending=False)

works_contracts_above_sort_by_avg_readout.head()
220/114:
### Order the works procurements from highest to lowest

works_contracts_above_sort_by_avg_readout = works_contracts_above.sort_values(by="avg_read_out_price", ascending=False)

works_contracts_above_sort_by_avg_readout.head()
220/115:
### Order the works procurements from highest to lowest

works_contracts_above_sort_by_avg_readout = works_contracts_above.sort_values(by="avg_read_out_price", ascending=False)

works_contracts_above_sort_by_avg_readout.head(3)
220/116:
### Group By method

works_contracts_above_sort_by_avg_readout = works_contracts_above_sort_by_avg_readout.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)
220/117:
### Group By method

works_contracts_above_sort_by_avg_readout = works_contracts_above_sort_by_avg_readout.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)

works_contracts_above_sort_by_avg_readout
220/118:
### Group By method

works_contracts_above_sort_by_avg_readout_group = works_contracts_above_sort_by_avg_readout.groupby(by="method").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)

works_contracts_above_sort_by_avg_readout_group
220/119:
### Order the works procurements from highest to lowest

works_contracts_above_sort_by_avg_readout = works_contracts_above.sort_values(by="avg_read_out_price", ascending=False)

works_contracts_above_sort_by_avg_readout.to_csv('works_contracts_above_sort_by_avg_readout.csv')

works_contracts_above_sort_by_avg_readout.head(3)
220/120:
### Order the works procurements from highest to lowest

works_contracts_above_sort_by_avg_readout = works_contracts_above.sort_values(by="avg_read_out_price", ascending=False)

works_contracts_above_sort_by_avg_readout.to_csv('works_contracts_above_sort_by_avg_readout.csv')

works_contracts_above_sort_by_avg_readout.head()
220/121:
### Filter out non consultancy services

non_consultancy_contracts_above = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['type'] == 'Non-Consultancy Services']

non_consultancy_contracts_above.shape
220/122:
### Order the non consultancy procurements from highest to lowest

non_consultancy_contracts_above_sort_by_avg_readout = non_consultancy_contracts_above.sort_values(by="avg_read_out_price", ascending=False)

non_consultancy_contracts_above_sort_by_avg_readout.to_csv('non_consultancy_contracts_above_sort_by_avg_readout.csv')

non_consultancy_contracts_above_sort_by_avg_readout.head()
220/123:
### Order the non consultancy procurements from highest to lowest

non_consultancy_contracts_above_sort_by_avg_readout = non_consultancy_contracts_above.sort_values(by="avg_read_out_price", ascending=False)

non_consultancy_contracts_above_sort_by_avg_readout.to_csv('non_consultancy_contracts_above_sort_by_avg_readout.csv')

non_consultancy_contracts_above_sort_by_avg_readout.head(50)
220/124:
### Order the non consultancy procurements from highest to lowest

non_consultancy_contracts_above_sort_by_avg_readout = non_consultancy_contracts_above.sort_values(by="avg_read_out_price", ascending=False)

non_consultancy_contracts_above_sort_by_avg_readout.to_csv('non_consultancy_contracts_above_sort_by_avg_readout.csv')

non_consultancy_contracts_above_sort_by_avg_readout.head()
220/125:
### Filter out non consultancy services


supplies_contracts_above = contracts_above_estimate_without_frameworks[contracts_above_estimate_without_frameworks['type'] == 'Supplies']

supplies_contracts_above.shape
220/126:
### Order the non consultancy procurements from highest to lowest

supplies_contracts_above_sort_by_avg_readout = supplies_contracts_above.sort_values(by="avg_read_out_price", ascending=False)

supplies_contracts_above_sort_by_avg_readout.to_csv('supplies_contracts_above_sort_by_avg_readout.csv')

supplies_contracts_above_sort_by_avg_readout.head()
220/127: supplies_contracts_above_sort_by_avg_readout['entity'].value_counts
220/128:
supplies_contracts_above_sort_by_avg_readoutgroupby('entity')agg(
    {
        'contract_price': ['sum', 'count']
    }
)
220/129:
supplies_contracts_above_sort_by_avg_readoutgroupby(by='entity')agg(
    {
        'contract_price': ['sum', 'count']
    }
)
220/130:
supplies_contracts_above_sort_by_avg_readoutgroup(by='entity')agg(
    {
        'contract_price': ['sum', 'count']
    }
)
220/131:
supplies_contracts_above_sort_by_avg_readout.groupby(by='entity')agg(
    {
        'contract_price': ['sum', 'count']
    }
)
220/132:
supplies_contracts_above_sort_by_avg_readout.groupby(by="type").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)
220/133:
supplies_contracts_above_sort_by_avg_readout.groupby(by="entity").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)
220/134:
supplies_contracts_above_sort_by_avg_readout.groupby(by="entity").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)
supplies_contracts_above_sort_by_avg_readout.to_csv('supplies_contracts_above_sort_by_avg_readout_group_by_entity.csv')
220/135: supplies_contracts_above_sort_by_avg_readout['entity'].value_counts
220/136: supplies_contracts_above_sort_by_avg_readout['entity']['contract_price'].value_counts
220/137: supplies_contracts_above_sort_by_avg_readout['entity'].value_counts
220/138: supplies_contracts_above_sort_by_avg_readout['entity'].value_counts()
220/139:
supplies_contracts_above_sort_by_avg_readout.groupby(by="entity").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)
220/140:
supplies_contracts_above_sort_by_avg_readout_group_by_entity = supplies_contracts_above_sort_by_avg_readout.groupby(by="entity").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)

supplies_contracts_above_sort_by_avg_readout_group_by_entity.to_csv('supplies_contracts_above_sort_by_avg_readout_group_by_entity.csv')
220/141:
supplies_contracts_above_sort_by_avg_readout_group_by_entity = supplies_contracts_above_sort_by_avg_readout.groupby(by="entity").agg(
    {
        'estimated_amount': ['sum'],
        'avg_read_out_price': ['sum'],
        'contract_price': ['sum', 'count']
    }
)

supplies_contracts_above_sort_by_avg_readout_group_by_entity.to_csv('supplies_contracts_above_sort_by_avg_readout_group_by_entity.csv')
221/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
221/2:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
221/3:
# Check the types of the dataset
timelines.dtypes
221/4:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
221/5: timelines.head()
221/6: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
221/7: timelines2020.info()
221/8: timelines2020
221/9:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
221/10: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
221/11: timelines2020.head()
221/12: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
221/13: timelines2020
221/14: timelines2020.sample(frac=.7)
221/15: timelines2020.info()
221/16:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
221/17: timelines2020
221/18: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
221/19: timelines2020.sample(frac=.5)
221/20: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
221/21: timelines2020.sample(frac=.5)
221/22: timelines2020.sample(frac=.5)
221/23: timelines2020.dtypes
221/24:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
221/25: timelines2020.info()
221/26:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
221/27: timelines2020.sample(frac=.5)
221/28: timelines2020.to_csv('timeliness-in-days.csv', index=False)
221/29:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
221/30: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
221/31: timelines2020.info()
221/32: timelines2020.groupby(['method']).mean()
221/33: timelines2020.groupby(['type']).mean()
221/34: # Import the file
221/35:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
221/36: leadtime.info()
221/37:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
221/38:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
221/39:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
221/40:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
221/41: leadtime.head()
221/42: leadtime.sample(frac=.5)
221/43: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
221/44: leadtimeOpen.info()
221/45: leadtimeOpen
221/46:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
221/47: sortedleadtime = leadtime.sort_values(by=['contract_price'])
221/48: sortedleadtime.to_csv('sortedleadtime.csv')
221/49: timelines2020.head()
221/50: timelines2020.describe()
221/51:

timelines2020Exceeded = timelines2020[timelines2020['implementation_period'] >= timelines2020['maximum_indicative_time']]

timelines2020Exceeded.head()
221/52: timelines2020Exceeded.to_csv('timelines2020Exceeded.csv')
221/53:
no_of_contracts = timelines2020Exceeded.groupby(['method']).count()
no_of_contracts.to_csv('no_of_contracts.csv')
221/54:
value_Of_Contracts = timelines2020Exceeded.groupby(['method']).sum()
value_Of_Contracts.to_csv('value_Of_Contracts.csv')
221/55:
# Load file for those that exceeded timelines

exceededTimelines = pd.read_csv('timelines2020Exceeded.csv')

exceededTimelines.head()
221/56: exceededTimelines.describe()
221/57: exceededTimelines[exceededTimelines['method'] == 'Direct Procurement']
221/58: timelinessInDays = pd.read_csv('timeliness-in-days.csv')
221/59: timelinessInDays.shape()
221/60: timelinessInDays.shape
221/61: timelinessInDays.head()
221/62: timelinessInDays.sample(frac=.5)
221/63: timelinessInDays.sample(frac=.7)
221/64: timelinessInDays.sort_values(by='contract_price', ascending=False)
221/65: timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
221/66: timelinessInDays.sample(frac=.8)
221/67: timelinessInDays.head(50)
221/68: timelinessInDays.tail(50)
221/69:
eoi_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Expression of Interest']
eoi_timelinessInDays.head()
221/70:
eoi_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Expression Of Interest']
eoi_timelinessInDays.head()
221/71:
eoi_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Expression Of Interest']
eoi_timelinessInDays.head(1)
221/72:
eoi_timelinessInDays = eoi_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'])
eoi_timelinessInDays.head(10)
221/73:
eoi_timelinessInDays = eoi_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
eoi_timelinessInDays.head(10)
221/74:
eoi_timelinessInDays = eoi_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
eoi_timelinessInDays.head(10)
221/75:
eoi_timelinessInDays = eoi_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
eoi_timelinessInDays.head(10)
221/76:
oib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open International Bidding(OIB)']
oib_timelinessInDays.head(1)
221/77: oib_timelinessInDays.shape
221/78: #### Sort Open International Bidding by contract price
221/79:
oib_timelinessInDays = oib_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
oib_timelinessInDays.head(5)
221/80:
oib_timelinessInDays = oib_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
oib_timelinessInDays.head(5)
221/81:
odb_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open Domestic Bidding']
odb_timelinessInDays.head(1)
221/82: odb_timelinessInDays.shape
221/83:
oib_timelinessInDays = oib_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
oib_timelinessInDays.head(5)
221/84: oib_timelinessInDays.shape
221/85:
odb_timelinessInDays = odb_timelinessInDays.sort_values(by=['contract_price'], ascending=False)
odb_timelinessInDays.head(5)
221/86:
odb_timelinessInDays = odb_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
odb_timelinessInDays.head(5)
221/87:
rib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open Domestic Bidding']
rib_timelinessInDays.head(1)
221/88:
rib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Restricted International Bidding']
rib_timelinessInDays.head(1)
221/89:
rib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Restricted International Bidding (RIB)']
rib_timelinessInDays.head(1)
221/90: rib_timelinessInDays.shape
221/91:
rib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Restricted International Bidding (RIB)']
rib_timelinessInDays.head(10)
221/92:
rib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Restricted International Bidding (RIB)']
rib_timelinessInDays.head(1)
221/93:
rib_timelinessInDays = rib_timelinessInDays.sort_values(by=['contract_price'], ascending=False)
rib_timelinessInDays.head(5)
221/94:
rib_timelinessInDays = rib_timelinessInDays.sort_values(by=['implementation_method'], ascending=False)
rib_timelinessInDays.head(5)
221/95:
rib_timelinessInDays = rib_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
rib_timelinessInDays.head(5)
221/96:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
221/97:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
221/98:
# Check the types of the dataset
timelines.dtypes
221/99:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
221/100: timelines.head()
221/101: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
221/102: timelines2020.info()
221/103: timelines2020
221/104:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
221/105: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
221/106: timelines2020.head()
221/107: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
221/108: timelines2020
221/109: timelines2020.sample(frac=.7)
221/110: timelines2020.info()
221/111:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
221/112: timelines2020
221/113: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
221/114: timelines2020.sample(frac=.5)
221/115: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
221/116: timelines2020.sample(frac=.5)
221/117: timelines2020.sample(frac=.5)
221/118: timelines2020.dtypes
221/119:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
221/120: timelines2020.info()
221/121:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
221/122: timelines2020.sample(frac=.5)
221/123: timelines2020.to_csv('timeliness-in-days.csv', index=False)
221/124:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
221/125: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
221/126: timelines2020.info()
221/127: timelines2020.groupby(['method']).mean()
221/128: timelines2020.groupby(['type']).mean()
221/129: # Import the file
221/130:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
221/131: leadtime.info()
221/132:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
221/133:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
221/134:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
221/135:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
221/136: leadtime.head()
221/137: leadtime.sample(frac=.5)
221/138: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
221/139: leadtimeOpen.info()
221/140: leadtimeOpen
221/141:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
221/142: sortedleadtime = leadtime.sort_values(by=['contract_price'])
221/143: sortedleadtime.to_csv('sortedleadtime.csv')
221/144: timelines2020.head()
221/145: timelines2020.describe()
221/146:

timelines2020Exceeded = timelines2020[timelines2020['implementation_period'] >= timelines2020['maximum_indicative_time']]

timelines2020Exceeded.head()
221/147: timelines2020Exceeded.to_csv('timelines2020Exceeded.csv')
221/148:
no_of_contracts = timelines2020Exceeded.groupby(['method']).count()
no_of_contracts.to_csv('no_of_contracts.csv')
221/149:
value_Of_Contracts = timelines2020Exceeded.groupby(['method']).sum()
value_Of_Contracts.to_csv('value_Of_Contracts.csv')
221/150:
# Load file for those that exceeded timelines

exceededTimelines = pd.read_csv('timelines2020Exceeded.csv')

exceededTimelines.head()
221/151: exceededTimelines.describe()
221/152: exceededTimelines[exceededTimelines['method'] == 'Direct Procurement']
221/153: timelinessInDays = pd.read_csv('timeliness-in-days.csv')
221/154: timelinessInDays.shape
221/155: timelinessInDays.tail(50)
221/156:
timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
timelinessInDays.to_csv('timelinessInDays.csv')
221/157:
eoi_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Expression Of Interest']
eoi_timelinessInDays.head(1)
221/158:
eoi_timelinessInDays = eoi_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
eoi_timelinessInDays.head(10)
eoi_timelinessInDays.to_csv('eoi_timelinessInDays.csv')
221/159:
eoi_timelinessInDays = eoi_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
eoi_timelinessInDays.head(10)
221/160:
oib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open International Bidding(OIB)']
oib_timelinessInDays.head(1)
221/161: oib_timelinessInDays.shape
221/162:
oib_timelinessInDays = oib_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
oib_timelinessInDays.head(5)
oib_timelinessInDays.to_csv('oib_timelinessInDays.csv')
221/163:
oib_timelinessInDays = oib_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
oib_timelinessInDays.head(5)
oib_timelinessInDays.to_csv('oib_timelinessInDays.csv')
221/164: oib_timelinessInDays.shape
221/165:
odb_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open Domestic Bidding']
odb_timelinessInDays.head(1)
221/166: odb_timelinessInDays.shape
221/167:
odb_timelinessInDays = odb_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
odb_timelinessInDays.head(5)
odb_timelinessInDays.to_csv('odb_timelinessInDays.csv')
221/168:
odb_timelinessInDays = odb_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
odb_timelinessInDays.head(5)
221/169:
rib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Restricted International Bidding (RIB)']
rib_timelinessInDays.head(1)
221/170: rib_timelinessInDays.shape
221/171:
rib_timelinessInDays = rib_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
rib_timelinessInDays.head(5)
rib_timelinessInDays.to_csv('rib_timelinessInDays.csv')
221/172:
rib_timelinessInDays = rib_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
rib_timelinessInDays.head(5)
221/173:
rdb_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Restricted Domestic Bidding (RDB)']
rdb_timelinessInDays.head(1)
221/174: rdb_timelinessInDays.shape
221/175:
snb_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Selective National Bidding']
snb_timelinessInDays.head(1)
221/176: snb_timelinessInDays.shape
221/177:
odb_timelinessInDays = odb_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
odb_timelinessInDays.head(5)
221/178:
oib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open International Bidding (OIB)']
oib_timelinessInDays.head(1)
221/179:
oib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open International Bidding']
oib_timelinessInDays.head(1)
221/180:
oib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open International Bidding(OIB)']
oib_timelinessInDays.head(1)
221/181: oib_timelinessInDays.shape
221/182:

timelinesInDays = pd.concat([eoi_timelinessInDays, odb_timelinessInDays, oib_timelinessInDays, rdb_timelinessInDays, snb_timelinessInDays])

timelinesInDays.shape
221/183:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
221/184:
# import the file
timelines = pd.read_csv('timeliness.csv')
timelines.head()
221/185:
# Check the types of the dataset
timelines.dtypes
221/186:
# Display a sample of .5 of the dataset
timelines.sample(frac=.5)
221/187: timelines.head()
221/188: timelines2020 = timelines[timelines['financial_year'] == '2019-2020']
221/189: timelines2020.info()
221/190: timelines2020
221/191:
timelines2020.to_csv('timelines-FY-2019-2020.csv')

# timelines2020.to_excel('timelines-FY-2019-2020.xlsx')
221/192: timelines2020['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
221/193: timelines2020.head()
221/194: timelines2020['initiation_date'].fillna(value="0000-00-00", inplace=True)
221/195: timelines2020
221/196: timelines2020.sample(frac=.7)
221/197: timelines2020.info()
221/198:
timelines2020['initiation_date'] = pd.to_datetime(timelines2020['initiation_date'], errors='coerce')

timelines2020['contract_signature_date'] = pd.to_datetime(timelines2020['contract_signature_date'], errors='coerce')

timelines2020['actual_contract_signature_date'] = pd.to_datetime(timelines2020['actual_contract_signature_date'], errors='coerce')
221/199: timelines2020
221/200: timelines2020['planning_period'] = timelines2020['contract_signature_date'] - timelines2020['initiation_date']
221/201: timelines2020.sample(frac=.5)
221/202: timelines2020['implementation_period'] = timelines2020['actual_contract_signature_date'] - timelines2020['initiation_date']
221/203: timelines2020.sample(frac=.5)
221/204: timelines2020.sample(frac=.5)
221/205: timelines2020.dtypes
221/206:
timelines2020['planning_period'] = pd.to_numeric(timelines2020['planning_period'].dt.days, downcast='integer')

timelines2020['implementation_period'] = pd.to_numeric(timelines2020['implementation_period'].dt.days, downcast='integer')
221/207: timelines2020.info()
221/208:
timelines2020.to_csv('timeliness-in-days.csv', date_format='%Y-%m-%d %H:%M:%S')

#timelines2020.to_excel('timeliness-in-days.xlsx')
221/209: timelines2020.sample(frac=.5)
221/210: timelines2020.to_csv('timeliness-in-days.csv', index=False)
221/211:
timelines2020ByMethod = pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).sum()

timelines2020ByMethod
221/212: pd.to_numeric(timelines2020['planning_period']).groupby(timelines2020['method']).mean()
221/213: timelines2020.info()
221/214: timelines2020.groupby(['method']).mean()
221/215: timelines2020.groupby(['type']).mean()
221/216: # Import the file
221/217:
leadtime = pd.read_csv('timeliness.csv')
leadtime.head()
221/218: leadtime.info()
221/219:
# Replace the NaN Values
leadtime['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

leadtime['initiation_date'].fillna(value="0000-00-00", inplace=True)
221/220:
# Convert the dates to datetime
leadtime['initiation_date'] = pd.to_datetime(leadtime['initiation_date'], errors='coerce')

leadtime['contract_signature_date'] = pd.to_datetime(leadtime['contract_signature_date'], errors='coerce')

leadtime['actual_contract_signature_date'] = pd.to_datetime(leadtime['actual_contract_signature_date'], errors='coerce')
221/221:
# Add planning and implementation periods

leadtime['planning_period'] = leadtime['contract_signature_date'] - leadtime['initiation_date']

leadtime['implementation_period'] = leadtime['actual_contract_signature_date'] - leadtime['initiation_date']
221/222:
# Convert the planning and implementation periods

leadtime['planning_period'] = pd.to_numeric(leadtime['planning_period'].dt.days, downcast='integer')

leadtime['implementation_period'] = pd.to_numeric(leadtime['implementation_period'].dt.days, downcast='integer')
221/223: leadtime.head()
221/224: leadtime.sample(frac=.5)
221/225: leadtimeOpen = leadtime[leadtime['method'] == 'Open International Bidding(OIB)']
221/226: leadtimeOpen.info()
221/227: leadtimeOpen
221/228:
# Save to csv
leadtimeOpen.to_csv('leadtimeOpen.csv', index=None)
221/229: sortedleadtime = leadtime.sort_values(by=['contract_price'])
221/230: sortedleadtime.to_csv('sortedleadtime.csv')
221/231: timelines2020.head()
221/232: timelines2020.describe()
221/233:

timelines2020Exceeded = timelines2020[timelines2020['implementation_period'] >= timelines2020['maximum_indicative_time']]

timelines2020Exceeded.head()
221/234: timelines2020Exceeded.to_csv('timelines2020Exceeded.csv')
221/235:
no_of_contracts = timelines2020Exceeded.groupby(['method']).count()
no_of_contracts.to_csv('no_of_contracts.csv')
221/236:
value_Of_Contracts = timelines2020Exceeded.groupby(['method']).sum()
value_Of_Contracts.to_csv('value_Of_Contracts.csv')
221/237:
# Load file for those that exceeded timelines

exceededTimelines = pd.read_csv('timelines2020Exceeded.csv')

exceededTimelines.head()
221/238: exceededTimelines.describe()
221/239: exceededTimelines[exceededTimelines['method'] == 'Direct Procurement']
221/240: timelinessInDays = pd.read_csv('timeliness-in-days.csv')
221/241: timelinessInDays.shape
221/242: timelinessInDays.tail(50)
221/243:
timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
timelinessInDays.to_csv('timelinessInDays.csv')
221/244:
eoi_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Expression Of Interest']
eoi_timelinessInDays.head(1)
221/245:
eoi_timelinessInDays = eoi_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
eoi_timelinessInDays.head(10)
eoi_timelinessInDays.to_csv('eoi_timelinessInDays.csv')
221/246:
eoi_timelinessInDays = eoi_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
eoi_timelinessInDays.head(10)
221/247:
oib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open International Bidding(OIB)']
oib_timelinessInDays.head(1)
221/248: oib_timelinessInDays.shape
221/249:
oib_timelinessInDays = oib_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
oib_timelinessInDays.head(5)
oib_timelinessInDays.to_csv('oib_timelinessInDays.csv')
221/250:
oib_timelinessInDays = oib_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
oib_timelinessInDays.head(5)
oib_timelinessInDays.to_csv('oib_timelinessInDays.csv')
221/251: oib_timelinessInDays.shape
221/252:
odb_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open Domestic Bidding']
odb_timelinessInDays.head(1)
221/253: odb_timelinessInDays.shape
221/254:
odb_timelinessInDays = odb_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
odb_timelinessInDays.head(5)
odb_timelinessInDays.to_csv('odb_timelinessInDays.csv')
221/255:
odb_timelinessInDays = odb_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
odb_timelinessInDays.head(5)
221/256:
oib_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Open International Bidding(OIB)']
oib_timelinessInDays.head(1)
221/257: oib_timelinessInDays.shape
221/258:
rdb_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Restricted Domestic Bidding (RDB)']
rdb_timelinessInDays.head(1)
221/259: rdb_timelinessInDays.shape
221/260:
rdb_timelinessInDays = rdb_timelinessInDays.sort_values(by=['contract_price', 'implementation_period'], ascending=False)
rdb_timelinessInDays.head(5)
# rdb_timelinessInDays.to_csv('rdb_timelinessInDays.csv')
221/261:
rdb_timelinessInDays = rdb_timelinessInDays.sort_values(by=['implementation_period'], ascending=False)
rdb_timelinessInDays.head(5)
221/262:
snb_timelinessInDays = timelinessInDays[timelinessInDays['method'] == 'Selective National Bidding']
snb_timelinessInDays.head(1)
221/263: snb_timelinessInDays.shape
221/264:

timelinesInDays = pd.concat([eoi_timelinessInDays, odb_timelinessInDays, oib_timelinessInDays, rdb_timelinessInDays, snb_timelinessInDays])

timelinesInDays.to_csv('timelinesInDays.csv')

timelinesInDays.shape
222/1:
completedDelayedContracts = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']

completedDelayedContracts.shape
223/1:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
223/2:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
223/3:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
223/4: completed_contracts.head()
223/5: completed_contracts.describe()
223/6:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
223/7: completed_contracts.sample(frac=.7)
223/8:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
223/9:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
223/10:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
223/11:


completed_contracts['completion_price_difference'] = completed_contracts['completion_contract_price'] - completed_contracts['contract_price']
223/12: completed_contracts.head()
223/13:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
223/14: completed_contracts['completion_period'].fillna(0.0, inplace=True)
223/15: completed_contracts['completion_period'].isnull()
223/16: completed_contracts.head()
223/17:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] >= completed_contracts['completion_contract_price'], 'within', 'above')
223/18:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
223/19:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
223/20:


completed_contracts['completeionPriceStatus'] = np.where(completed_contracts['completion_price_difference'] <= 0, 'within', 'above')
223/21: completed_contracts.sample(frac=.8)
223/22:
# Write the results to a csv and excel file

# completed_contracts.to_csv('completed_contracts_tm.csv')

# completed_contracts.to_excel('completed_contracts_tm.xlsx')
223/23: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
223/24: above['completion_contract_price'].sum()
223/25: above[above['completion_period'] > 0]
223/26: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
223/27: within[within['completion_period'] == 0]
223/28:
completedDelayedContracts = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']

completedDelayedContracts.shape
222/2:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
222/3:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
222/4:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
222/5: completed_contracts.head()
222/6: completed_contracts.describe()
222/7:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
222/8: completed_contracts.sample(frac=.7)
222/9:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
222/10:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
222/11:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
222/12:


completed_contracts['completion_price_difference'] = completed_contracts['completion_contract_price'] - completed_contracts['contract_price']
222/13: completed_contracts.head()
222/14:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
222/15: completed_contracts['completion_period'].fillna(0.0, inplace=True)
222/16: completed_contracts['completion_period'].isnull()
222/17: completed_contracts.head()
222/18:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] >= completed_contracts['completion_contract_price'], 'within', 'above')
222/19:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
222/20:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
222/21:


completed_contracts['completeionPriceStatus'] = np.where(completed_contracts['completion_price_difference'] <= 0, 'within', 'above')
222/22: completed_contracts.sample(frac=.8)
222/23:
# Write the results to a csv and excel file

# completed_contracts.to_csv('completed_contracts_tm.csv')

# completed_contracts.to_excel('completed_contracts_tm.xlsx')
222/24: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
222/25: above['completion_contract_price'].sum()
222/26: above[above['completion_period'] > 0]
222/27: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
222/28: within[within['completion_period'] == 0]
222/29:
completedDelayedContracts = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']

completedDelayedContracts.shape
222/30:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
222/31:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
222/32:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
222/33: completed_contracts.head()
222/34: completed_contracts.describe()
222/35:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
222/36: completed_contracts.sample(frac=.7)
222/37:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
222/38:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
222/39:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
222/40:


completed_contracts['completion_price_difference'] = completed_contracts['completion_contract_price'] - completed_contracts['contract_price']
222/41: completed_contracts.head()
222/42:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
222/43: completed_contracts['completion_period'].fillna(0.0, inplace=True)
222/44: completed_contracts['completion_period'].isnull()
222/45: completed_contracts.head()
222/46:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] >= completed_contracts['completion_contract_price'], 'within', 'above')
222/47:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
222/48:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
222/49:


completed_contracts['completeionPriceStatus'] = np.where(completed_contracts['completion_price_difference'] <= 0, 'within', 'above')
222/50: completed_contracts.sample(frac=.8)
222/51:
# Write the results to a csv and excel file

# completed_contracts.to_csv('completed_contracts_tm.csv')

# completed_contracts.to_excel('completed_contracts_tm.xlsx')
222/52: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
222/53: above['completion_contract_price'].sum()
222/54: above[above['completion_period'] > 0]
222/55: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
222/56: within[within['completion_period'] == 0]
222/57:
completedDelayedContracts = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']

completedDelayedContracts.shape
222/58: completedDelayedContracts.to_csv('completedDelayedContracts.csv')
224/1:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
224/2:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
224/3:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
224/4: all_awarded_contracts.sample(frac=.7)
224/5: all_awarded_contracts.groupby(by='completion_status').count()
224/6: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
224/7: awarded_contracts_not_completed.head()
224/8: awarded_contracts_not_completed.describe()
224/9:

# awarded_contracts_not_completed_type_num = awarded_contracts_not_completed.groupby(by='type').count()

# awarded_contracts_not_completed_type_num.to_csv('awarded_contracts_not_completed_type_num.csv')

# awarded_contracts_not_completed_type_val = awarded_contracts_not_completed.groupby(by='type').sum()

# awarded_contracts_not_completed_type_val.to_csv('awarded_contracts_not_completed_type_val.csv')
224/10:
# awarded_contracts_not_completed_method_num = awarded_contracts_not_completed.groupby(by='method').count()

# awarded_contracts_not_completed_method_num.to_csv('awarded_contracts_not_completed_method_num.csv')

# awarded_contracts_not_completed_method_val = awarded_contracts_not_completed.groupby(by='method').sum()

# awarded_contracts_not_completed_method_val.to_csv('awarded_contracts_not_completed_method_val.csv')
224/11:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
224/12: awarded_contracts_not_completed.dtypes
224/13:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

# awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
#                                                                     errors='coerce')
224/14: awarded_contracts_not_completed.dtypes
224/15: awarded_contracts_not_completed.head()
224/16:

awarded_contracts_not_completed['provisional_completion_period'] = awarded_contracts_not_completed['provisional_actual_completion_date'] - awarded_contracts_not_completed['planned_completion_date']
224/17:

awarded_contracts_not_completed['provisional_completion_period'] = pd.to_numeric(awarded_contracts_not_completed['provisional_completion_period'].dt.days, downcast="integer")
224/18: awarded_contracts_not_completed.sample(frac=.7)
224/19: awarded_contracts_not_completed.sample(frac=.7)
224/20:

awarded_contracts_not_completed['provisional_completion_status'] = np.where(awarded_contracts_not_completed['provisional_completion_period'] <= 0, 'In Time', 'Late')
224/21: awarded_contracts_not_completed.tail()
224/22:
awarded_contracts_not_completed.to_csv('all_awarded_contracts_not_completed.csv')
awarded_contracts_not_completed.shape
224/23:

awarded_contracts_not_completed_late = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'Late']

# awarded_contracts_not_completed_late.to_csv('awarded_contracts_not_completed_late.csv')
224/24:

awarded_contracts_not_completed_in_time = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'In Time']

# awarded_contracts_not_completed_in_time.to_csv('awarded_contracts_not_completed_in_time.csv')
224/25:

awarded_contracts_not_completed_late_type_num = awarded_contracts_not_completed_late.groupby(by='type').count()

# awarded_contracts_not_completed_late_type_num.to_csv('awarded_contracts_not_completed_late_type_num.csv')

awarded_contracts_not_completed_late_type_val = awarded_contracts_not_completed_late.groupby(by='type').sum()

# awarded_contracts_not_completed_late_type_val.to_csv('awarded_contracts_not_completed_late_type_val.csv')
224/26:

awarded_contracts_not_completed_late_method_num = awarded_contracts_not_completed_late.groupby(by='method').count()

# awarded_contracts_not_completed_late_method_num.to_csv('awarded_contracts_not_completed_late_method_num.csv')

awarded_contracts_not_completed_late_method_val = awarded_contracts_not_completed_late.groupby(by='method').sum()

# awarded_contracts_not_completed_late_method_val.to_csv('awarded_contracts_not_completed_late_method_val.csv')
224/27:


awarded_contracts_not_completed_in_time_type_num = awarded_contracts_not_completed_in_time.groupby(by='type').count()

# awarded_contracts_not_completed_in_time_type_num.to_csv('awarded_contracts_not_completed_in_time_type_num.csv')

awarded_contracts_not_completed_in_time_type_val = awarded_contracts_not_completed_in_time.groupby(by='type').sum()

# awarded_contracts_not_completed_in_time_type_val.to_csv('awarded_contracts_not_completed_in_time_type_val.csv')
224/28:


awarded_contracts_not_completed_in_time_method_num = awarded_contracts_not_completed_in_time.groupby(by='method').count()

# awarded_contracts_not_completed_in_time_method_num.to_csv('awarded_contracts_not_completed_in_time_method_num.csv')

awarded_contracts_not_completed_in_time_method_val = awarded_contracts_not_completed_in_time.groupby(by='method').sum()

# awarded_contracts_not_completed_in_time_method_val.to_csv('awarded_contracts_not_completed_in_time_method_val.csv')
224/29:


awarded_contracts_not_completed_in_time_method_num = awarded_contracts_not_completed_in_time.groupby(by='method').count()

# awarded_contracts_not_completed_in_time_method_num.to_csv('awarded_contracts_not_completed_in_time_method_num.csv')

awarded_contracts_not_completed_in_time_method_val = awarded_contracts_not_completed_in_time.groupby(by='method').sum()

# awarded_contracts_not_completed_in_time_method_val.to_csv('awarded_contracts_not_completed_in_time_method_val.csv')
224/30: awarded_contracts_not_completed.head()
224/31: awarded_contracts_not_completed.sort_values(by="contract_price", ascending=False)
224/32:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
224/33:
all_awarded_contracts = pd.read_csv('all_awarded_contracts.csv')

all_awarded_contracts.head()
224/34:

all_awarded_contracts['completion_status'] = np.where((pd.isnull(all_awarded_contracts['actual_contract_completion_date']) & pd.isnull(all_awarded_contracts['completion_contract_price'])), 'Awarded', 'Awarded and Completed')
224/35: all_awarded_contracts.sample(frac=.7)
224/36: all_awarded_contracts.groupby(by='completion_status').count()
224/37: awarded_contracts_not_completed = all_awarded_contracts[all_awarded_contracts['completion_status'] == 'Awarded']
224/38: awarded_contracts_not_completed.head()
224/39: awarded_contracts_not_completed.describe()
224/40:

# awarded_contracts_not_completed_type_num = awarded_contracts_not_completed.groupby(by='type').count()

# awarded_contracts_not_completed_type_num.to_csv('awarded_contracts_not_completed_type_num.csv')

# awarded_contracts_not_completed_type_val = awarded_contracts_not_completed.groupby(by='type').sum()

# awarded_contracts_not_completed_type_val.to_csv('awarded_contracts_not_completed_type_val.csv')
224/41:
# awarded_contracts_not_completed_method_num = awarded_contracts_not_completed.groupby(by='method').count()

# awarded_contracts_not_completed_method_num.to_csv('awarded_contracts_not_completed_method_num.csv')

# awarded_contracts_not_completed_method_val = awarded_contracts_not_completed.groupby(by='method').sum()

# awarded_contracts_not_completed_method_val.to_csv('awarded_contracts_not_completed_method_val.csv')
224/42:
awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(
    '2020-08-01',
    errors='coerce')
224/43: awarded_contracts_not_completed.dtypes
224/44:
awarded_contracts_not_completed['initiation_date'] = pd.to_datetime(awarded_contracts_not_completed['initiation_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_signature_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_signature_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['planned_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['planned_completion_date'],
                                                                    errors='coerce')

awarded_contracts_not_completed['actual_contract_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
                                                                    errors='coerce')

# awarded_contracts_not_completed['provisional_actual_completion_date'] = pd.to_datetime(awarded_contracts_not_completed['actual_contract_completion_date'],
#                                                                     errors='coerce')
224/45: awarded_contracts_not_completed.dtypes
224/46: awarded_contracts_not_completed.head()
224/47:

awarded_contracts_not_completed['provisional_completion_period'] = awarded_contracts_not_completed['provisional_actual_completion_date'] - awarded_contracts_not_completed['planned_completion_date']
224/48:

awarded_contracts_not_completed['provisional_completion_period'] = pd.to_numeric(awarded_contracts_not_completed['provisional_completion_period'].dt.days, downcast="integer")
224/49: awarded_contracts_not_completed.sample(frac=.7)
224/50: awarded_contracts_not_completed.sample(frac=.7)
224/51:

awarded_contracts_not_completed['provisional_completion_status'] = np.where(awarded_contracts_not_completed['provisional_completion_period'] <= 0, 'In Time', 'Late')
224/52: awarded_contracts_not_completed.tail()
224/53:
awarded_contracts_not_completed = awarded_contracts_not_completed.sort_values(by="contract_price", ascending=False)
awarded_contracts_not_completed.to_csv('all_awarded_contracts_not_completed.csv')
awarded_contracts_not_completed.shape
224/54:

awarded_contracts_not_completed_late = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'Late']

# awarded_contracts_not_completed_late.to_csv('awarded_contracts_not_completed_late.csv')
224/55:

awarded_contracts_not_completed_in_time = awarded_contracts_not_completed[awarded_contracts_not_completed['provisional_completion_status'] == 'In Time']

# awarded_contracts_not_completed_in_time.to_csv('awarded_contracts_not_completed_in_time.csv')
224/56:

awarded_contracts_not_completed_late_type_num = awarded_contracts_not_completed_late.groupby(by='type').count()

# awarded_contracts_not_completed_late_type_num.to_csv('awarded_contracts_not_completed_late_type_num.csv')

awarded_contracts_not_completed_late_type_val = awarded_contracts_not_completed_late.groupby(by='type').sum()

# awarded_contracts_not_completed_late_type_val.to_csv('awarded_contracts_not_completed_late_type_val.csv')
224/57:

awarded_contracts_not_completed_late_method_num = awarded_contracts_not_completed_late.groupby(by='method').count()

# awarded_contracts_not_completed_late_method_num.to_csv('awarded_contracts_not_completed_late_method_num.csv')

awarded_contracts_not_completed_late_method_val = awarded_contracts_not_completed_late.groupby(by='method').sum()

# awarded_contracts_not_completed_late_method_val.to_csv('awarded_contracts_not_completed_late_method_val.csv')
224/58:


awarded_contracts_not_completed_in_time_type_num = awarded_contracts_not_completed_in_time.groupby(by='type').count()

# awarded_contracts_not_completed_in_time_type_num.to_csv('awarded_contracts_not_completed_in_time_type_num.csv')

awarded_contracts_not_completed_in_time_type_val = awarded_contracts_not_completed_in_time.groupby(by='type').sum()

# awarded_contracts_not_completed_in_time_type_val.to_csv('awarded_contracts_not_completed_in_time_type_val.csv')
224/59:


awarded_contracts_not_completed_in_time_method_num = awarded_contracts_not_completed_in_time.groupby(by='method').count()

# awarded_contracts_not_completed_in_time_method_num.to_csv('awarded_contracts_not_completed_in_time_method_num.csv')

awarded_contracts_not_completed_in_time_method_val = awarded_contracts_not_completed_in_time.groupby(by='method').sum()

# awarded_contracts_not_completed_in_time_method_val.to_csv('awarded_contracts_not_completed_in_time_method_val.csv')
224/60: awarded_contracts_not_completed.sort_values(by="contract_price", ascending=False)
224/61:
awarded_contracts_not_completed = awarded_contracts_not_completed.sort_values(by="contract_price", ascending=False)
awarded_contracts_not_completed.to_csv('all_awarded_contracts_not_completed.csv')
awarded_contracts_not_completed.shape
224/62: awarded_contracts_not_completed
225/1:
# imports

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
225/2:
completed_contracts = pd.read_csv('completed_contracts.csv')
completed_contracts.head()
225/3:
completed_contracts['contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
completed_contracts['initiation_date'].fillna(value="0000-00-00", inplace=True)
225/4: completed_contracts.head()
225/5: completed_contracts.describe()
225/6:
completed_contracts['initiation_date'] = pd.to_datetime(completed_contracts['initiation_date'], errors='coerce')

completed_contracts['contract_signature_date'] = pd.to_datetime(completed_contracts['contract_signature_date'], errors='coerce')

completed_contracts['actual_contract_signature_date'] = pd.to_datetime(completed_contracts['actual_contract_signature_date'], errors='coerce')

completed_contracts['planned_completion_date'] = pd.to_datetime(completed_contracts['planned_completion_date'], errors='coerce')

completed_contracts['actual_contract_completion_date'] = pd.to_datetime(completed_contracts['actual_contract_completion_date'], errors='coerce')
225/7: completed_contracts.sample(frac=.7)
225/8:


completed_contracts['planning_period'] = completed_contracts['contract_signature_date'] - completed_contracts['initiation_date']
225/9:


completed_contracts['implementation_period'] = completed_contracts['actual_contract_signature_date'] - completed_contracts['initiation_date']
225/10:

completed_contracts['completion_period'] = completed_contracts['actual_contract_completion_date'] - completed_contracts['planned_completion_date']
225/11:


completed_contracts['completion_price_difference'] = completed_contracts['completion_contract_price'] - completed_contracts['contract_price']
225/12: completed_contracts.head()
225/13:
completed_contracts['planning_period'] = pd.to_numeric(completed_contracts['planning_period'].dt.days, downcast='integer')

completed_contracts['implementation_period'] = pd.to_numeric(completed_contracts['implementation_period'].dt.days, downcast='integer')

completed_contracts['completion_period'] = pd.to_numeric(completed_contracts['completion_period'].dt.days, downcast='integer')
225/14: completed_contracts['completion_period'].fillna(0.0, inplace=True)
225/15: completed_contracts['completion_period'].isnull()
225/16: completed_contracts.head()
225/17:

completed_contracts['competion_price_status'] = np.where(completed_contracts['contract_price'] >= completed_contracts['completion_contract_price'], 'within', 'above')
225/18:

completed_contracts['competion_time_status'] = np.where(completed_contracts['planned_completion_date'] <= completed_contracts['actual_contract_completion_date'], 'within', 'above')
225/19:

completed_contracts['completeionStatusPeriod'] = np.where(completed_contracts['completion_period'] <= 0, 'within', 'above')
225/20:


completed_contracts['completeionPriceStatus'] = np.where(completed_contracts['completion_price_difference'] <= 0, 'within', 'above')
225/21: completed_contracts.sample(frac=.8)
225/22:
# Write the results to a csv and excel file

# completed_contracts.to_csv('completed_contracts_tm.csv')

# completed_contracts.to_excel('completed_contracts_tm.xlsx')
225/23: above = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']
225/24: above['completion_contract_price'].sum()
225/25: above[above['completion_period'] > 0]
225/26: within = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'within']
225/27: within[within['completion_period'] == 0]
225/28:
completedDelayedContracts = completed_contracts[completed_contracts['completeionStatusPeriod'] == 'above']

completedDelayedContracts.shape
225/29: completedDelayedContracts.to_csv('completedDelayedContracts.csv')
227/1:
import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interactive
%matplotlib inline
228/1:
import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interactive
%matplotlib inline
228/2:
def plot_func(a, f):
    plt.figure(2)
    x = np.linspace(0, 2*np.pi, num=1000)
    y = a*np.sin(1/f*x)
    plt.plot(x,y)
    plt.ylim(-1.1, 1.1)
    plt.title('a sin(f)')
    plt.show()

interactive_plot = interactive(plot_func, a=(-1,0,0.1), f=(0.1, 1))
output = interactive_plot.children[-1]
output.layout.height = '300px'
interactive_plot
229/1:
import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interactive
%matplotlib inline
229/2:
def plot_func(a, f):
    plt.figure(2)
    x = np.linspace(0, 2*np.pi, num=1000)
    y = a*np.sin(1/f*x)
    plt.plot(x,y)
    plt.ylim(-1.1, 1.1)
    plt.title('a sin(f)')
    plt.show()

interactive_plot = interactive(plot_func, a=(-1,0,0.1), f=(0.1, 1))
output = interactive_plot.children[-1]
output.layout.height = '300px'
interactive_plot
231/1:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
231/2:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
231/3:
# profile_data

# background_data

# experience_data

# hobbies_data

#  membership_data

committe_table_data
231/4:
df = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df.dropna(axis=0, inplace=True)
df
231/5:
df = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df.dropna(axis=0, inplace=True)
df
231/6:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df
231/7:
df = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df.dropna(axis=0, inplace=True)
df
231/8:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
231/9:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
231/10:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
231/11:
df = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df.dropna(axis=0, inplace=True)
df
231/12:
df = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df.dropna(axis=0, inplace=True)
df
231/13:
df = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df.dropna(axis=0, inplace=True)
df
231/14:
df = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df.dropna(axis=0, inplace=True)
df
231/15: bg_data_rows
231/16: background_table
231/17: profile_data
231/18: profile_data
231/19: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
231/20: rows
231/21:

df = pd.DataFrame(profile_data, index=None, columns=None)
231/22: df.to_csv('profile_data.csv', index=False, index_label=False)
231/23: df.sample(frac=.3)
231/24: df.T
231/25: rows
231/26: profile_table
231/27: table = soup.find_all('table')
231/28: table[7]
231/29: len(table)
231/30:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
231/31:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
231/32:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
231/33:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
231/34: data
231/35: table
231/36: new_table = pd.DataFrame(columns=range(0,2), index=[0])
231/37:
# row_marker = 0
# for row in table.find_all('tr'):
#     column_marker = 0
#     columns = row.find_all('td')
#     for column in columns:
#         new_table.iat[row_marker,column_marker] = column.get_text()
#         column_marker += 1

# new_table
231/38: print(soup.td)
231/39: print(res.text)
231/40: link = 'https://www.parliament.go.ug/mp_database/profile.php?mid='
231/41:
member_list = []
for i in range(1, 5):
    link = f'https://www.parliament.go.ug/mp_database/profile.php?mid={i}'
    print('Hit Link ', link)
    res = req.get(link)
    member_list.append()
231/42:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
231/43:
df_profile = pd.DataFrame(profile_data,
                         index=None,
                         columns=['Name', 'Professional Body', 'Membership Type'])
df_profile.dropna(axis=0, inplace=True)
df_profile
231/44:
df_profile = pd.DataFrame(profile_data,
                         index=None,
                         columns=['Name', 'Proffessional Body', 'Membership Type'])
df_profile.dropna(axis=0, inplace=True)
df_profile
231/45:
df_profile = pd.DataFrame(profile_data,
                         index=None,
                         columns=['Name', 'Proffessional Body', 'Membership Type'])
df_profile.dropna(axis=0, inplace=True)
df_profile
231/46:
# df_profile = pd.DataFrame(profile_data,
#                          index=None,
#                          columns=['Name', 'Proffessional Body', 'Membership Type'])
# df_profile.dropna(axis=0, inplace=True)
# df_profile
231/47:
all_data = df_bg.merge(df_exp, how="left", on="Name", left_on="Name")

all_data.head()
231/48:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
231/49:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
231/50:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
231/51:
# df_profile = pd.DataFrame(profile_data,
#                          index=None,
#                          columns=['Name', 'Proffessional Body', 'Membership Type'])
# df_profile.dropna(axis=0, inplace=True)
# df_profile
231/52:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
231/53:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
231/54:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
231/55:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
231/56:
all_data = df_bg.merge(df_exp, how="left", on="Name", left_on="Name")

all_data.head()
231/57:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data.head()
231/58: df_bg.shape
231/59: df_bg.head(1)
231/60: df_exp.shape
231/61: df_exp.head(1)
231/62:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data.head()
231/63:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.head()
231/64:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.tail()
231/65:
df_profile = pd.read_csv('member_list.csv')

df_profile.head(1)
231/66:
df_profile = pd.read_csv('member_list.csv', skiprows=1)

df_profile.head(1)
231/67:
df_profile = pd.read_csv('member_list.csv', skiprows=1, index_col=None)

df_profile.head(1)
231/68:
df_profile = pd.read_csv('member_list.csv', skiprows=1, index_col=None, usecols=[1, 2, 3, 4, 5])

df_profile.head(1)
232/1:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
232/2:
res = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(res.text, 'html.parser')
232/3: member_list = soup.find('table', {'class': 'table-striped'})
232/4:
rows = member_list.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
232/5: data
232/6: member_list = pd.DataFrame(data, index=None, columns=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])
232/7:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
232/8:
res = req.get('https://www.parliament.go.ug/mp_database/rpt_mps.php')
soup = BeautifulSoup(res.text, 'html.parser')
232/9: member_list = soup.find('table', {'class': 'table-striped'})
232/10:
rows = member_list.find_all('tr')

data = []

for row in rows:
    cols = row.find_all('td')
    cols = [item.text.strip() for item in cols]
    data.append([item for item in cols if item])
232/11: data
232/12: member_list = pd.DataFrame(data, index=None, columns=['Name', 'District', 'Constituency', 'Political Party', 'Religion', ''])
232/13: member_list
232/14: member_list.to_csv('member_list.csv')
232/15: member_list.head
232/16: member_list.head()
231/69:
df_profile = pd.read_csv('member_list.csv', skiprows=1, index_col=None, usecols=['Name', 'District', , 4, 5])

df_profile.head(1)
231/70:
df_profile = pd.read_csv('member_list.csv', skiprows=1)

df_profile.head(1)
231/71:
df_profile = pd.read_csv('member_list.csv', skiprows=1, index_col=None)

df_profile.head(1)
231/72:
df_profile = pd.read_csv('member_list.csv', skiprows=0)

df_profile.head(1)
231/73:
df_profile = pd.read_csv('member_list.csv', skiprows=2)

df_profile.head(1)
231/74:
df_profile = pd.read_csv('member_list.csv')

df_profile.head(1)
231/75:
df_profile = pd.read_csv('member_list.csv', skiprows=0)

df_profile.head(1)
231/76:
df_profile = pd.read_csv('member_list.csv', skiprows=1)

df_profile.head(1)
231/77:
df_profile = pd.read_csv('member_list.csv', skiprows=0)

df_profile.head(1)
231/78:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.head(1)
231/79:
df_profile = pd.read_csv('member_list.csv', skiprows=1, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.head(1)
231/80:
df_profile = pd.read_csv('member_list.csv', skiprows=1, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion', ''])

df_profile.head(1)
231/81:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion', ''])

df_profile.head(1)
231/82:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.head(1)
231/83:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.head()
231/84:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/85:
all_data = df_bg.merge(df_profile, how="left", on="Name")

all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.tail()
231/86:
all_data = df_bg.merge(df_profile, how="left", on="Name")

all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.shape
231/87:
all_data = df_bg.merge(df_profile, how="left", on="Name")

all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.head()
231/88:
all_data = df_bg.merge(df_profile, how="left", on="Name")

all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.head()
231/89: all_data.set_index(['Name'])
231/90: all_data.set_index(['Name', 'Qualification'])
231/91:
all_data = all_data.set_index(['Name', 'Qualification'])

all_data
231/92:
all_data = all_data.set_index(['Name', 'Qualification'])

all_data.tail()
231/93:
all_data = all_data.set_index(['Name', 'Qualification'])

all_data
231/94: all_data = all_data.set_index(['Name', 'Qualification'])
231/95: all_data.set_index(['Name', 'Qualification'])
231/96:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
231/97:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
231/98:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
231/99:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/100:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
231/101:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
231/102:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
231/103:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
231/104:
all_data = df_bg.merge(df_profile, how="left", on="Name")

all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.head()
231/105: all_data.set_index(['Name', 'Qualification'])
231/106: df_bg.shape
231/107: df_bg.head(1)
231/108: df_exp.shape
231/109: df_exp.head(1)
231/110: bg_data_rows
231/111: background_table
231/112: profile_data
231/113: profile_data
231/114: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
231/115: rows
231/116:

df = pd.DataFrame(profile_data, index=None, columns=None)
231/117: df.to_csv('profile_data.csv', index=False, index_label=False)
231/118: df.sample(frac=.3)
231/119: df.T
231/120: rows
231/121: profile_table
231/122: table = soup.find_all('table')
231/123: table[7]
231/124: len(table)
231/125:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
231/126:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
231/127:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
231/128:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
231/129: data
231/130: table
231/131: new_table = pd.DataFrame(columns=range(0,2), index=[0])
231/132:
# row_marker = 0
# for row in table.find_all('tr'):
#     column_marker = 0
#     columns = row.find_all('td')
#     for column in columns:
#         new_table.iat[row_marker,column_marker] = column.get_text()
#         column_marker += 1

# new_table
231/133: print(soup.td)
231/134: print(res.text)
231/135: link = 'https://www.parliament.go.ug/mp_database/profile.php?mid='
231/136:
member_list = []
for i in range(1, 5):
    link = f'https://www.parliament.go.ug/mp_database/profile.php?mid={i}'
    print('Hit Link ', link)
    res = req.get(link)
    member_list.append()
231/137: all_data.set_index(['Name', 'Year Attained', 'Qualification'])
231/138: all_data.set_index(['Name'])
231/139:
# all_data.set_index(['Name'])

all_data.groupby(by='Name')
231/140:
# all_data.set_index(['Name'])

all_data.groupby(by='Name').count()
231/141:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
231/142:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
231/143:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
231/144:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/145:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
231/146:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
231/147:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
231/148:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
231/149:
all_data = df_profile.merge(df_bg, how="left", on="Name")

all_data = all_data.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.head()
231/150:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
231/151: df_bg.shape
231/152: df_bg.head(1)
231/153: df_exp.shape
231/154: df_exp.head(1)
231/155: bg_data_rows
231/156: background_table
231/157: profile_data
231/158: profile_data
231/159: # profile_table.find_all('td', attrs={'class': 'img-polaroid'})
231/160: rows
231/161:

df = pd.DataFrame(profile_data, index=None, columns=None)
231/162: df.to_csv('profile_data.csv', index=False, index_label=False)
231/163: df.sample(frac=.3)
231/164: df.T
231/165: rows
231/166: profile_table
231/167: table = soup.find_all('table')
231/168: table[7]
231/169: len(table)
231/170:
print(table[0].find_all('td')[8])

# print(table[2].find_all('td')[2])

# len(table[2].find_all('td'))

# len(table[0].find_all('td'))
231/171:
for i in range(0, len(table[0].find_all('td'))):
#     print(len(table[i]))
    print(table[0].find_all('td')[i].get_text())
231/172:
# Print the list of tables

for i in range(0, len(table)):
    print(len(table[i]))
231/173:
data = []
table = soup.find('table', attrs={'class':'lineItemsTable'})
table_body = soup.find('tbody')

rows = table_body.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
231/174: data
231/175: table
231/176: new_table = pd.DataFrame(columns=range(0,2), index=[0])
231/177:
# row_marker = 0
# for row in table.find_all('tr'):
#     column_marker = 0
#     columns = row.find_all('td')
#     for column in columns:
#         new_table.iat[row_marker,column_marker] = column.get_text()
#         column_marker += 1

# new_table
231/178: print(soup.td)
231/179: print(res.text)
231/180: link = 'https://www.parliament.go.ug/mp_database/profile.php?mid='
231/181:
member_list = []
for i in range(1, 5):
    link = f'https://www.parliament.go.ug/mp_database/profile.php?mid={i}'
    print('Hit Link ', link)
    res = req.get(link)
    member_list.append()
231/182:
all_data = df_profile.merge(df_bg, how="left", on="Name")

all_data = all_data.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.head()
231/183: all_data.shape()
231/184: all_data.shape
231/185:
all_data = df_profile.merge(df_bg, how="left", on="Name")

all_data = all_data.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.head()
231/186: df_bg
231/187:
all_data = df_profile.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_bg, how="left", on="Name")


all_data.head()
231/188:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
231/189:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
231/190:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
231/191:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/192:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
231/193:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
231/194:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
231/195:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
231/196:
all_data = df_profile.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_bg, how="left", on="Name")


all_data.head()
231/197: df_bg
231/198: all_data.shape
231/199:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
231/200:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data.tail()
231/201:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", on="Name")

all_data.tail()
231/202:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="inner", on="Name")

all_data.tail()
231/203:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", on="Name")

all_data.tail()
231/204:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
231/205:
pagination_max = 10

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
231/206:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
231/207:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/208:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
231/209:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
231/210:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
231/211:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
231/212:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", on="Name")

all_data.tail()
231/213: df_bg
231/214: all_data.shape
231/215:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
231/216:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", on="Name")

all_data.head()
231/217:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", on="Name")

all_data.head()
231/218: all_data.shape
231/219:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", on="Name")

all_data.tail()
231/220: df_profile
231/221:
df_profile = pd.read_csv('member_list.csv', skiprows=0, index=None, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/222:
df_profile = pd.read_csv('member_list.csv', skiprows=0, index=False, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/223:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/224:
df_profile = pd.read_csv('member_list.csv', skiprows=0, index_col=None, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/225:
df_profile = pd.read_csv('member_list.csv', skiprows=0, index_col=None, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.reset_index()

df_profile.head()
231/226:
df_profile = pd.read_csv('member_list.csv', skiprows=0, index_col=None, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.reset_index(inplace=True)

df_profile.head()
231/227:
df_profile = pd.read_csv('member_list.csv', skiprows=0, index_col=None, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.reset_index(inplace=True)

df_profile.head()
231/228:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.reset_index(inplace=True)

df_profile.head()
231/229:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.reset_index(drop=True)

df_profile.head()
231/230:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.reset_index(level=1, drop=True)

df_profile.head()
231/231:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.reset_index(drop=True, inplace=True)

df_profile.head()
231/232:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
231/233:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.tail()
233/1: import numpy as np
233/2: x = np.random.random(100000000000)
235/1: x = np.random.random(10)
235/2: import numpy as np
235/3: x = np.random.random(10)
235/4: x
235/5: sample_name = 'e Data Patterns' * 10000
235/6: sample_name
235/7: sample_name = 'e Data Patterns' * 10000
235/8: sample_name
235/9: sample_name = 'e Data Patterns ' * 10000
235/10: sample_name
235/11: sample_name = ['e Data Patterns ', 'e Data Patterns, ' * 10000, 'e Data Patterns']
235/12: sample_name
235/13:
import numpy as np
import pandas as pd
235/14:
import numpy as np
import pandas as pd
235/15: languages_list = pd.read_csv('https://raw.githubusercontent.com/jamhall/programming-languages-csv/master/languages.csv')
235/16: languages_list.head()
235/17: languages_list.head()
235/18: languages_list.tail()
235/19: languages = languages_list['name']
235/20: languages
235/21: sample_name = pd.DataFrame(sample_name)
235/22: sample_name.head
235/23: sample_name.head()
235/24: sample_name = pd.DataFrame(sample_name, columns=['name'])
235/25: sample_name.head()
235/26: sample_name = pd.DataFrame({'name': sample_name})
235/27: sample_name.head()
235/28: sample_name = ['e Data Patterns ', 'e Data Patterns, ' * 10000, 'e Data Patterns']
235/29: sample_name = pd.DataFrame({'name': sample_name})
235/30: sample_name.head()
235/31: sample_name.tail()
236/1: import pandas as pd
236/2:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx')

df.head(1)
236/3:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=0)

df.head(1)
236/4:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', skiprows=0)

df.head(1)
236/5:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=0)

df.head(1)
236/6:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1)

df.head(1)
236/7:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=0)

df.head()
236/8:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1)

df.head()
236/9:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1)

df.head()
236/10: df.shape
236/11:
contracts_2019_2020.to_csv('awarded_contracts_2019_2020.csv')

contracts_2018_2019.to_csv('awarded_contracts_2018_2019.csv')

contracts_2018_2019.to_csv('awarded_contracts_2017_2018.csv')
236/12:
contracts_2019_2020.to_csv('awarded_contracts_2019_2020.csv')

contracts_2018_2019.to_csv('awarded_contracts_2018_2019.csv')

contracts_2017_2018.to_csv('awarded_contracts_2017_2018.csv')
236/13: import pandas as pd
236/14:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1)

df.head()
236/15: df.shape
236/16:
# Filter out awarded contracts for FY-2019-2020

contracts_2019_2020 = df[df['Financial Year'] == '2019-2020']

# Filter out awarded contracts for FY-2018-2019

contracts_2018_2019 = df[df['Financial Year'] == '2018-2019']

# Filter out awarded contracts for FY-2017-2018

contracts_2018_2019 = df[df['Financial Year'] == '2017-2018']
236/17:
contracts_2019_2020.to_csv('awarded_contracts_2019_2020.csv')

contracts_2018_2019.to_csv('awarded_contracts_2018_2019.csv')

contracts_2017_2018.to_csv('awarded_contracts_2017_2018.csv')
236/18: import pandas as pd
236/19:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1)

df.head()
236/20: df.shape
236/21:
# Filter out awarded contracts for FY-2019-2020

contracts_2019_2020 = df[df['Financial Year'] == '2019-2020']

# Filter out awarded contracts for FY-2018-2019

contracts_2018_2019 = df[df['Financial Year'] == '2018-2019']

# Filter out awarded contracts for FY-2017-2018

contracts_2017_2018 = df[df['Financial Year'] == '2017-2018']
236/22:
contracts_2019_2020.to_csv('awarded_contracts_2019_2020.csv')

contracts_2018_2019.to_csv('awarded_contracts_2018_2019.csv')

contracts_2017_2018.to_csv('awarded_contracts_2017_2018.csv')
236/23:
contracts_2019_2020.to_csv('awarded_contracts_2019_2020.csv')

contracts_2018_2019.to_csv('awarded_contracts_2018_2019.csv')

contracts_2017_2018.to_csv('awarded_contracts_2017_2018.csv')
236/24: contracts_2019_2020.shape
236/25: contracts_2018_2019.shape
236/26: contracts_2017_2018.shape
236/27:
# Load the file

# Load the dataset
df_cod = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', sheet_name='Call Off Orders')

df_cod.head()
236/28:
# Load the file

# Load the dataset
df_cod = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1, sheet_name='Call Off Orders')

df_cod.head()
236/29:
# Load the file

# Load the dataset
df_cod = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1, sheet_name='Call Off Orders')

df_cod.head()
236/30: df_cod.shape
236/31:
df_cod_2017_2018 = df_cod[df_cod['Financial Year'] == '2017-2018']

df_cod_2018_2019 = df_cod[df_cod['Financial Year'] == '2018-2019']

df_cod_2019_2020 = df_cod[df_cod['Financial Year'] == '2019-2020']
236/32: df_cod_2017_2018.shape
236/33: df_cod_2018_2019.shape
236/34: df_cod_2019_2020.shape
236/35: 114+820+1946
236/36: df_cod_2017_2018.shape[0]
236/37: df_cod_2017_2018.shape
236/38: df_cod_2017_2018.shape[0] + df_cod_2018_2019.shape[0] + df_cod_2019_2020.shape[0]
236/39:
df_cod_2017_2018.to_csv('cod_2017_2018.csv')

df_cod_2018_2019.to_csv('cod_2018_2019.csv')

df_cod_2019_2020.to_csv('cod_2019_2020.csv')
237/1: import nltk
237/2:
sentence = """At eight o'clock on Thursday morning
... Arthur didn't feel very good."""
237/3: tokens = nltk.word_tokenize(sentence)
237/4:
import nltk
nltk.download()
237/5:
import nltk
# nltk.download()
237/6:
sentence = """At eight o'clock on Thursday morning
... Arthur didn't feel very good."""
237/7: tokens = nltk.word_tokenize(sentence)
237/8: tokens
237/9: tagged = nltk.pos_tag(tokens)
237/10: tagged
237/11: tagged[0:6]
237/12: entities = nltk.chunk.ne_chunk(tagged)
237/13: entities
237/14: entities = nltk.chunk.ne_chunk(tagged)
237/15: entities
237/16: from nltk.corpus import treebank
237/17: t = treebank.parsed_sents('wsj_0001.mrg')[0]
237/18: t.draw()
237/19: nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')
237/20: from nltk import load_parser
237/21:
cp = load_parser('grammars/book_grammars/sql0.fcfg')
query = 'What cities are located in China'
trees = cp.nbest_parse(query.split())
answer = trees[0].node['SEM']
q = ' '.join(answer)

print q
237/22:
cp = load_parser('grammars/book_grammars/sql0.fcfg')
query = 'What cities are located in China'
trees = cp.nbest_parse(query.split())
answer = trees[0].node['SEM']
q = ' '.join(answer)

print(q)
237/23: cp = load_parser('grammars/book_grammars/sql0.fcfg')
237/24: query = 'What cities are located in China'
237/25: trees = cp.nbest_parse(query.split())
237/26: trees = next(cp.nbest_parse(query.split()))
237/27: trees = next(cp.parse(query.split()))
237/28:
answer = trees[0].node['SEM']
q = ' '.join(answer)

print(q)
237/29:
answer = trees[0].label
q = ' '.join(answer)

print(q)
237/30:
answer = trees[0].label

print(answer)
237/31:
from sklearn.datasets import fetch_20newsgroups
twenty_train = fetch_20newsgroups(subset='train', shuffle=True)
239/1:
import pandas as pd
import numpy as np
239/2:
# Load the data

df = pd.read_excel('GPP-Awarded-Contracts-FY-2019-2020-11-09-2020.xlsx')

df.shape()
239/3:
# Load the data

df = pd.read_excel('GPP-Awarded-Contracts-FY-2019-2020-11-09-2020.xlsx')

df.head
239/4:
# Load the data

df = pd.read_excel('GPP-Awarded-Contracts-FY-2019-2020-11-09-2020.xlsx', header=0)

df.head
239/5:
# Load the data

df = pd.read_excel('GPP-Awarded-Contracts-FY-2019-2020-11-09-2020.xlsx', header=2)

df.head
239/6:
# Load the data

df = pd.read_excel('GPP-Awarded-Contracts-FY-2019-2020-11-09-2020.xlsx', header=2)

df.head()
239/7:
# Load the data

df = pd.read_excel('GPP-Awarded-Contracts-FY-2019-2020-11-09-2020.xlsx', header=1)

df.head()
239/8: df_kcc = df[df['entity'] == 'Kampala Capital City Authority']
239/9: df_kcc.shape()
239/10: df_kcc.shape
239/11:
# Load the data

df = pd.read_excel('GPP-Awarded-Contracts-FY-2019-2020-11-09-2020.xlsx', header=1)

df.head()
239/12: df.shape
240/1: import pandas as pd
240/2:
# Load the dataset
df = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1)

df.head()
240/3: df.shape
240/4:
# Filter out awarded contracts for FY-2019-2020

contracts_2019_2020 = df[df['Financial Year'] == '2019-2020']

# Filter out awarded contracts for FY-2018-2019

contracts_2018_2019 = df[df['Financial Year'] == '2018-2019']

# Filter out awarded contracts for FY-2017-2018

contracts_2017_2018 = df[df['Financial Year'] == '2017-2018']
240/5:
contracts_2019_2020.to_csv('awarded_contracts_2019_2020.csv')

contracts_2018_2019.to_csv('awarded_contracts_2018_2019.csv')

contracts_2017_2018.to_csv('awarded_contracts_2017_2018.csv')
240/6: contracts_2019_2020.shape
240/7: contracts_2018_2019.shape
240/8: contracts_2017_2018.shape
240/9:
# Load the file

# Load the dataset
df_cod = pd.read_excel('Awarded Contracts For FYs (2017-2018, 2018-2019, 2019-2020).xlsx', header=1, sheet_name='Call Off Orders')

df_cod.head()
240/10: df_cod.shape
240/11:
df_cod_2017_2018 = df_cod[df_cod['Financial Year'] == '2017-2018']

df_cod_2018_2019 = df_cod[df_cod['Financial Year'] == '2018-2019']

df_cod_2019_2020 = df_cod[df_cod['Financial Year'] == '2019-2020']
240/12: df_cod_2017_2018.shape
240/13: df_cod_2018_2019.shape
240/14: df_cod_2019_2020.shape
240/15: df_cod_2017_2018.shape[0] + df_cod_2018_2019.shape[0] + df_cod_2019_2020.shape[0]
240/16:
df_cod_2017_2018.to_csv('cod_2017_2018.csv')

df_cod_2018_2019.to_csv('cod_2018_2019.csv')

df_cod_2019_2020.to_csv('cod_2019_2020.csv')
240/17:
df = pd.read_csv('GPP-Awarded Contracts FY-2019-2020.csv')

df.shape()
240/18:
df = pd.read_csv('GPP-Awarded Contracts FY-2019-2020.csv')

df.shape
240/19: df_kcc = df[df['entity'] == 'Kampala Capital City Authority']
240/20: df_kcc.shape
241/1:
import pandas as pd
import numpy as np
241/2:
df = pd.read_csv('gpp_all.csv')

df.shape
241/3:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
241/4: df.is_null()
241/5: df.head()
241/6: df['dateOfFinalEvaluationReportApproval'].isnull()
241/7: df['dateOfFinalEvaluationReportApproval'].fillna('0000-00-00')
241/8:
import pandas as pd
import numpy as np
241/9:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
241/10: df.head()
241/11: df.isnull()
241/12: df['entity'].isnull()
241/13: pd.isnull(df)
241/14: df[df.isnull()]
241/15: df.dropna()
241/16: df.dropna().shape
241/17: df_nulls = df.dropna()
241/18: df_nulls.shale
241/19: df_nulls.shape
241/20: df_nulls = df.isnull()
241/21: df_nulls.shape
241/22: df_nulls = df[df.isnull()]
241/23: df_nulls.shape
241/24: df_nulls.shape
241/25: df['entity'].isnull()
243/1:
import pandas as pd
import numpy as np
243/2:
# Load the report

df = pd.read_exel('timeliness-in-days.xlsx')

df.head()
243/3:
# Load the report

df = pd.read_exel('timeliness-in-days.xlsx')

df.head(3)
243/4:
import pandas as pd
import numpy as np
243/5:
# Load the report

df = pd.read_excel('timeliness-in-days.xlsx')

df.head(3)
243/6:
# Load the report

df = pd.read_excel('timeliness-in-days.xlsx', header=1)

df.head(3)
243/7:
# Filter out UGANDA NATIONAL COUNCIL OF SCIENCE & TECHNOLOGY 

df_uncst = df[df['entity'] == 'UGANDA NATIONAL COUNCIL OF SCIENCE & TECHNOLOGY ']

df_uncst.shape
243/8: df_uncst.to_excel('uncst.xlsx')
244/1:
df = df['entity'].fillna()

df.shape
244/2:
import pandas as pd
import numpy as np
244/3:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
244/4: df_nulls = df[df.isnull()]
244/5: df_nulls.shape
244/6:
# df['entity'].fillna(inplace)

df.shape
244/7: df.head()
244/8:
df = df['entity'].fillna()

df.shape
244/9:
df = df['entity'].dropna()

df.shape
244/10: df.head()
244/11: df.to_csv('gpp_all.csv')
244/12:
import pandas as pd
import numpy as np
244/13:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
244/14: df_nulls = df[df.isnull()]
244/15: df_nulls.shape
244/16:
df = df['entity'].dropna()

df.shape
244/17: df.head()
244/18: df.to_csv('gpp_all.csv')
244/19: df.head(3)
244/20:
df = df.dropna()

df.shape
244/21:
import pandas as pd
import numpy as np
244/22:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
244/23: df_nulls = df[df.isnull()]
244/24: df_nulls.shape
244/25:
df = df.dropna()

df.shape
244/26: df.head()
244/27: df.to_csv('gpp_all.csv')
244/28: df.head(3)
244/29:
df = pd.read_excel('gpp_all.xlsx', low_memory=False)

df.shape
244/30:
import pandas as pd
import numpy as np
244/31:
df = pd.read_csv('gpp_all.csv', low_memory=False, header=2)

df.shape
244/32:
import pandas as pd
import numpy as np
244/33:
df = pd.read_csv('gpp_all.csv', low_memory=False, header=2)

df.shape
244/34:
import pandas as pd
import numpy as np
244/35:
df = pd.read_csv('gpp_all.csv', low_memory=False, header=2)

df.shape
244/36: df_nulls = df[df.isnull()]
244/37: df_nulls.shape
244/38:
df = df.dropna()

df.shape
244/39: df.head()
244/40: df.to_csv('gpp_all.csv')
244/41: df.head(3)
244/42:
import pandas as pd
import numpy as np
244/43:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
244/44: df.head()
244/45:
import pandas as pd
import numpy as np
244/46:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
244/47: df_nulls = df[df.isnull()]
244/48: df_nulls.shape
244/49:
df = df.dropna()

df.shape
244/50: df.head()
244/51: df.to_csv('gpp_all.csv')
244/52: df.head(3)
244/53: df.head()
244/54: df.shape
244/55: df.shape
244/56: df[df['financial_year'] == '2015-2016']
244/57: df[df['financialYear'] == '2015-2016']
244/58: df[df['financialYear'] == '2015-2016' & df['entity'] == NULL]
244/59: df[df['financialYear'] == '2015-2016' & df['entity'].isnull()]
244/60: df[df['financialYear'] == '2015-2016' and df['entity'].isnull()]
244/61: df[df['entity'].isnull()]
244/62: df[df['entity'].notnull()]
244/63:
import pandas as pd
import numpy as np
244/64:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
244/65: df_nulls = df[df.isnull()]
244/66: df_nulls.shape
244/67:
df = df[df['entity'].notna()]

df.shape
244/68: df.head()
244/69: df.shape
244/70: df[df['entity'].notnull()]
244/71: df.to_csv('gpp_all.csv')
244/72: df.head(3)
244/73:
import pandas as pd
import numpy as np
244/74:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
244/75: df_nulls = df[df.isnull()]
244/76: df_nulls.shape
244/77:
df = df[df['entity'].notna()]

df.shape
244/78: df.head()
244/79: df.shape
244/80: # df[df['entity'].notnull()]
244/81: df.to_csv('gpp_all.csv')
244/82: df.head(3)
244/83:
import pandas as pd
import numpy as np
244/84:
df = pd.read_csv('gpp_all.csv', low_memory=False)

df.shape
244/85: df_nulls = df[df.isnull()]
244/86: df_nulls.shape
244/87:
df = df[df['entity'].notna()]

df.shape
244/88: df.head()
244/89: df.shape
244/90: # df[df['entity'].notnull()]
244/91: df.to_csv('gpp_all.csv')
244/92: df.head(3)
244/93: df.shape
245/1:
import pandas as pd
import numpy as np
245/2:
df = pd.read_csv('procurement_plans.csv')

df.shape
245/3:
df_not_null = df[df['entity'].notna()]

df_not_null
245/4:
df_not_null = df[df['entity'].notna()]

df_not_null.shape
245/5:
import pandas as pd
import numpy as np
245/6:
df = pd.read_csv('procurement_plans.csv')

df.shape
245/7:
df = df[df['entity'].notna()]

df.shape
245/8:
df = df[df['procurement_type'].notna()]

df.shape
245/9:
df = df[df['procurement_type'].notna()]

df.shape
245/10:
df = df[df['procurement_method'].notna()]

df.shape
245/11:
df = df[df['procurement_method'].notna()]

df.shape
245/12: df.head(3)
245/13:
df = df[df['estimated_amount'].notna()]

df.shape
245/14:
df = df[df['financial_year'].notna()]

df.shape
245/15:
df = pd.read_csv('procurement_plans.csv')

df.shape
245/16:
df = pd.read_csv('procurement_plans.csv')

df.shape
245/17:
df = df.notna()

df.shape
245/18:
df = pd.read_csv('procurement_plans.csv')

df.shape
245/19:
df = pd.read_csv('procurement_plans.csv')

df.shape
245/20:

df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

df.shape
245/21:
df = pd.read_csv('procurement_plans.csv')

df.shape
245/22:

df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

df.shape
245/23: df.to_csv('procurementplans.csv')
245/24:
import pandas as pd
import numpy as np
245/25:
df = pd.read_csv('procurement_plans.csv')

df.shape
245/26:

df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

df.shape
245/27: df.to_csv('procurementplans.csv')
247/1:
import pandas as pd
import numpy as np
247/2:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/3: df.head(3)
247/4: df = df.set_index(['procurementReferenceNo'])
247/5: df
247/6:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/7: df.head(3)
247/8: df = df.set_index(['procurementReferenceNo', 'subject_of_procurement'])
247/9: df
247/10:
import pandas as pd
import numpy as np
247/11:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/12:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/13: # df.to_csv('procurementplans.csv')
247/14:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/15: df.head(3)
247/16: df = df.set_index(['procurementReferenceNo', 'subject_of_procurement'])
247/17: df
247/18: df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/19:
import pandas as pd
import numpy as np
247/20:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/21:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/22: # df.to_csv('procurementplans.csv')
247/23:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/24: df.head(3)
247/25: df = df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True)
247/26: df
247/27: df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/28: df
247/29: df = df.set_index(['procurementReferenceNo', 'subject_of_procurement'])
247/30:
import pandas as pd
import numpy as np
247/31:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/32:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/33: # df.to_csv('procurementplans.csv')
247/34:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/35: df.head(3)
247/36: df = df.set_index(['procurementReferenceNo', 'subject_of_procurement'])
247/37: df
247/38: df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/39:
import pandas as pd
import numpy as np
247/40:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/41:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/42: # df.to_csv('procurementplans.csv')
247/43:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/44: df.head(3)
247/45: df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True)
247/46: df
247/47: df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/48:
import pandas as pd
import numpy as np
247/49:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/50:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/51: # df.to_csv('procurementplans.csv')
247/52:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/53: df.head(3)
247/54: df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=False)
247/55: df
247/56: df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/57:
import pandas as pd
import numpy as np
247/58:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/59:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/60: # df.to_csv('procurementplans.csv')
247/61:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/62: df.head(3)
247/63: df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=False)
247/64: df
247/65: df.to_csv('Uganda_lands_procurements_2019-2020.csv', index_col=[0, 1])
247/66:
import pandas as pd
import numpy as np
247/67:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/68:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/69: # df.to_csv('procurementplans.csv')
247/70:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/71: df.head(3)
247/72: df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=False)
247/73: df
247/74: df.to_csv('Uganda_lands_procurements_2019-2020.csv', index_col=0).ffill().set_index(['procurementReferenceNo','subject_of_procurement'])
247/75:
import pandas as pd
import numpy as np
247/76:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/77:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/78: # df.to_csv('procurementplans.csv')
247/79:
df = pd.read_csv('bid_submissions_lands_commission.csv')

df.shape
247/80: df.head(3)
247/81: df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=True)
247/82: df
247/83: df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/84:
# Best evaluated bidders

df = pd.read_csv('best_evaluated_bidders.csv')

df.shape
247/85: df.head(3)
247/86: df = df[df['subject_of_procurement'].notna()]
247/87: df.shape
247/88: df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna()]
247/89: df.shape
247/90:

df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna() & 
        df['procurement_method'].notna()]
247/91: df.shape
247/92:

df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna() & 
        df['procurement_method'].notna() & df['provider'].notna()]
247/93:

df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna() & 
        df['procurement_method'].notna() & df['Provider'].notna()]
247/94: df.shape
247/95: wrong_providers = np.where(len(df['Provider']) < 3)
247/96: wrong_providers.shape
247/97: wrong_providers
247/98: wrong_providers = np.where(len(df['Provider']) < 3, 1, 0)
247/99: wrong_providers
247/100: wrong_providers = np.where(len(df['Provider']) < 3, df['Provider'], 0)
247/101: wrong_providers
247/102: df.shape
247/103: len(df['Provider']
247/104: len(df['Provider'])
247/105: df[len(df['Provider'])]
247/106: wrong_providers = np.where(df['Provider'].str.len < 3, df['Provider'])
247/107: wrong_providers = np.where(df['Provider'].str.len < 3, df['Provider'], ' ')
247/108: df.shape
247/109:
import pandas as pd
import numpy as np
247/110:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/111:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/112: # df.to_csv('procurementplans.csv')
247/113: # Bid Submissions lands commission
247/114:
# df = pd.read_csv('bid_submissions_lands_commission.csv')

# df.shape
247/115: # df.head(3)
247/116: # df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=True)
247/117: # df
247/118: # df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/119:
# Best evaluated bidders

df = pd.read_csv('best_evaluated_bidders.csv')

df.shape
247/120: df.head(3)
247/121:

df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna() & 
        df['procurement_method'].notna() & df['Provider'].notna()]
247/122: df.shape
247/123: wrong_providers = np.where(df['Provider'].str.len < 3, df['Provider'], ' ')
247/124: wrong_providers = np.where(df['Provider'].str.len() < 3, df['Provider'], ' ')
247/125: wrong_providers.shape
247/126: wrong_providers
247/127: wrong_providers.head
247/128: wrong_providers[0:5]
247/129: wrong_providers = np.where(df['Provider'].str.len() > 3, df['Provider'], ' ')
247/130: wrong_providers.shape
247/131: wrong_providers[0:5]
247/132: wrong_providers[75242:75252]
247/133: wrong_providers = np.where(df['Provider'].str.len() < 5, df['Provider'], ' ')
247/134: wrong_providers.shape
247/135: wrong_providers[75242:75252]
247/136: wrong_providers
247/137: wrong_providers.shape
247/138: wrong_providers = np.where(df['Provider'].str.len() < 25, df['Provider'], ' ')
247/139: wrong_providers.shape
247/140: wrong_providers
247/141: wrong_providers = np.where(df['Provider'].str.len() < 5, df['Provider'], ' ')
247/142: wrong_providers.shape
247/143: wrong_providers
247/144: df.shape
247/145: wrong_providers
247/146: df
247/147: df.head(3)
247/148: df.to_csv('bestEvaluatedBidders.csv')
247/149:
import pandas as pd
import numpy as np
247/150:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/151:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/152: # df.to_csv('procurementplans.csv')
247/153: # Bid Submissions lands commission
247/154:
# df = pd.read_csv('bid_submissions_lands_commission.csv')

# df.shape
247/155: # df.head(3)
247/156: # df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=True)
247/157: # df
247/158: # df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/159:
# Best evaluated bidders

df = pd.read_csv('best_evaluated_bidders.csv')

df.shape
247/160: df.head(3)
247/161:

df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna() & 
        df['procurement_method'].notna() & df['Provider'].notna()]
247/162: df.shape
247/163: df = np.where(df['Provider'].str.len() < 5, df['Provider'], ' ')
247/164: df.shape
247/165: df.head(3)
247/166:
import pandas as pd
import numpy as np
247/167:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/168:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/169: # df.to_csv('procurementplans.csv')
247/170: # Bid Submissions lands commission
247/171:
# df = pd.read_csv('bid_submissions_lands_commission.csv')

# df.shape
247/172: # df.head(3)
247/173: # df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=True)
247/174: # df
247/175: # df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/176:
# Best evaluated bidders

df = pd.read_csv('best_evaluated_bidders.csv')

df.shape
247/177: df.head(3)
247/178:

df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna() & 
        df['procurement_method'].notna() & df['Provider'].notna()]
247/179: df.shape
247/180: # df = np.where(df['Provider'].str.len() < 5, df['Provider'], ' ')
247/181: # df.shape
247/182: df.head(3)
247/183: df.to_csv('bestEvaluatedBidders.csv')
247/184:
import pandas as pd
import numpy as np
247/185:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/186:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/187: # df.to_csv('procurementplans.csv')
247/188: # Bid Submissions lands commission
247/189:
# df = pd.read_csv('bid_submissions_lands_commission.csv')
df = pd.read_csv('bid_submissions.csv')

df.shape
247/190: df.head(3)
247/191: df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=True)
247/192:
import pandas as pd
import numpy as np
247/193:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/194:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/195: # df.to_csv('procurementplans.csv')
247/196: # Bid Submissions lands commission
247/197:
# df = pd.read_csv('bid_submissions_lands_commission.csv')
df = pd.read_csv('bid_submissions.csv')

df.shape
247/198: df.head(3)
247/199: df.set_index(['procurementReferenceNo', 'subject_of_procurement'], inplace=True, drop=True)
247/200:
import pandas as pd
import numpy as np
247/201:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/202:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/203: # df.to_csv('procurementplans.csv')
247/204: # Bid Submissions lands commission
247/205:
# df = pd.read_csv('bid_submissions_lands_commission.csv')
df = pd.read_csv('bid_submissions.csv')

df.shape
247/206: df.head(3)
247/207: df.set_index(['procurementReferenceNo', 'subnject_of_procurement'], inplace=True, drop=True)
247/208: df
247/209: df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/210:
# Best evaluated bidders

df = pd.read_csv('best_evaluated_bidders.csv')

df.shape
247/211: df.head(3)
247/212:

df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna() & 
        df['procurement_method'].notna() & df['Provider'].notna()]
247/213: df.shape
247/214: # df = np.where(df['Provider'].str.len() < 5, df['Provider'], ' ')
247/215: # df.shape
247/216: df.head(3)
247/217: df.to_csv('bestEvaluatedBidders.csv')
247/218:
# df = pd.read_csv('bid_submissions_lands_commission.csv')
df = pd.read_csv('bid_submissions.csv')

df.shape
247/219:
import pandas as pd
import numpy as np
247/220:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/221:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/222: # df.to_csv('procurementplans.csv')
247/223: # Bid Submissions lands commission
247/224:
# df = pd.read_csv('bid_submissions_lands_commission.csv')
df = pd.read_csv('bid_submissions.csv')

df.shape
247/225: df = df[df['entity'].notna() & df['entity'].notna()]
247/226:
import pandas as pd
import numpy as np
247/227:
# df = pd.read_csv('procurement_plans.csv')

# df.shape
247/228:

# df = df[df['entity'].notna() & df['procurement_type'].notna() & df['procurement_method'].notna() & df['estimated_amount'].notna() & df['financial_year'].notna()]

# df.shape
247/229: # df.to_csv('procurementplans.csv')
247/230: # Bid Submissions lands commission
247/231:
# df = pd.read_csv('bid_submissions_lands_commission.csv')
df = pd.read_csv('bid_submissions.csv')

df.shape
247/232: df.head(3)
247/233: df.set_index(['procurementReferenceNo', 'subnject_of_procurement'], inplace=True, drop=True)
247/234: df
247/235: df.to_csv('Uganda_lands_procurements_2019-2020.csv')
247/236:
# Best evaluated bidders

df = pd.read_csv('best_evaluated_bidders.csv')

df.shape
247/237: df.head(3)
247/238:

df = df[df['subject_of_procurement'].notna() & df['procurement_type'].notna() & 
        df['procurement_method'].notna() & df['Provider'].notna() & df['financial_year']]
247/239: df.shape
247/240: # df = np.where(df['Provider'].str.len() < 5, df['Provider'], ' ')
247/241: # df.shape
247/242: df.head(3)
247/243: df.to_csv('bestEvaluatedBidders.csv')
247/244: df.shape
251/1:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/2:
pagination_max = 2

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/3:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/4:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/5:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
251/6:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
251/7:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
251/8:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
251/9:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
251/10:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/11:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/12:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/13:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/14:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
251/15:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
251/16:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
251/17:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
251/18:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.tail()
251/19: all_data.shape
251/20: df_profile
251/21: all_data.shape
251/22:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
251/23:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/24: df_profile.T
251/25:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/26:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/27:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/28:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/29: df_profile = df_profile.T
251/30:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
251/31:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
251/32:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
251/33:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
251/34:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.tail()
251/35:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/36:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/37:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/38:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/39: #
251/40:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
251/41:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
251/42:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
251/43:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
251/44:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.tail()
251/45: all_data.shape
251/46: df_profile
251/47: all_data.shape
251/48:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
251/49:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.tail()
251/50:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/51:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/52:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/53:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/54: #
251/55:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg
251/56:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
251/57:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
251/58:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
251/59:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.tail()
251/60: all_data.shape
251/61: df_profile
251/62: all_data.shape
251/63:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
251/64:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.head()
251/65:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.head(10)
251/66:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.head(20)
251/67:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.head(50)
251/68:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.head(30)
251/69:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.head(25)
251/70:
all_data = df_bg.merge(df_exp, how="left", on="Name")

all_data = all_data.merge(df_hobby, how="left", on="Name")

all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

all_data.head(20)
251/71:
# all_data.set_index(['Name'])

all_data.groupby(by='Name').count()
251/72:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/73:
pagination_max = 5

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/74:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/75:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/76: #
251/77:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg

df_bg.to_csv('background_data.csv')
251/78:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
df_exp.to_csv('experience_data.csv')
251/79:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
df_hobby.to_csv('hobby_data.csv')
251/80:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
df_member.to_csv('membership_data.csv')
251/81:
# all_data = df_bg.merge(df_exp, how="left", on="Name")

#all_data = all_data.merge(df_hobby, how="left", on="Name")

#all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

# all_data.head(20)
251/82: all_data.shape
251/83: df_profile
251/84: all_data.shape
251/85:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
251/86:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/87:
pagination_max = 554

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/88:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/89:
pagination_max = 500

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/90:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/91:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/92: #
251/93:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg

df_bg.to_csv('background_data.csv')
251/94:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
df_exp.to_csv('experience_data.csv')
251/95:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
df_hobby.to_csv('hobby_data.csv')
251/96:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
df_member.to_csv('membership_data.csv')
251/97:
# all_data = df_bg.merge(df_exp, how="left", on="Name")

#all_data = all_data.merge(df_hobby, how="left", on="Name")

#all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

# all_data.head(20)
251/98: all_data.shape
251/99: df_profile
251/100: all_data.shape
251/101:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
251/102:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/103:
pagination_max = 559

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    bg_data_rows = background_table.find_all('tr')
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    experience_data_rows = experience_table.find_all('tr')
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    hobbies_data_rows = hobbies_table.find_all('tr')
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    membership_table_rows = membership_table.find_all('tr')
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/104:
import requests as req
from bs4 import BeautifulSoup
import pandas as pd
251/105:
pagination_max = 559

data = []

profile_data = []
global profile_name

background_data = []
backgroung_heading = []
experience_data = []
hobbies_data = []
membership_data = []
commitee_data = []


for pagination_num in range(1, pagination_max):
    url = f'https://www.parliament.go.ug/mp_database/profile.php?mid={pagination_num}'
    res = req.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    profile_table = soup.find('table', {'class': 'table-striped'})

    background_table = soup.find_all(id='example')[0]

    experience_table = soup.find_all(id='example')[1]

    hobbies_table = soup.find_all(id='example')[2]

    membership_table = soup.find_all(id='example5')[0]
    
    committe_table = soup.find_all('table', class_='table table-striped table-bordered')[4]
    
    committe_table_data = soup.find_all('tr', class_='odd gradeX')[-2:]
    
#   'tr', class_='odd gradeX'
    
    # Profile Table

    rows = profile_table.find_all('tr')

    for index, row in enumerate(rows):
        if 3 < index < 11:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_data.append([item for item in cols if item])

        if index == 0:
            cols = row.find_all('td')
            cols = [item.text.replace('\n', ' ').strip() for item in cols]
            profile_name = cols[0]
            profile_data.append(['NAME', cols[0]])

    # Background table
    
    try:
        bg_data_rows = background_table.find_all('tr')
    except:
        print("Index Error")
    
    for index, row in enumerate(bg_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        background_data.append([headings])
        background_data.append([item for item in cols if item])
    
    # Experience table
    try:
        experience_data_rows = experience_table.find_all('tr')
    except:
        print("Index Error")
    
    for index, row in enumerate(experience_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        experience_data.append([headings])
        experience_data.append([item for item in cols if item])
        
    # Hobbies table
    try:
        hobbies_data_rows = hobbies_table.find_all('tr')
    except:
        print("Index Error")
    
    for index, row in enumerate(hobbies_data_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        hobbies_data.append([headings])
        hobbies_data.append([item for item in cols if item])
        
    # Membership table
    try:
        membership_table_rows = membership_table.find_all('tr')
    except:
        print("Index Error")
    
    for index, row in enumerate(membership_table_rows):
        headings = row.find_all('th')
        headings = [title.text.replace('\n', ' ').strip() for title in headings]
        headings.insert(0, 'Name')
        cols = row.find_all('td')
        cols = [item.text.replace('\n', ' ').strip() for item in cols]
        cols.insert(0, profile_name)
        membership_data.append([headings])
        membership_data.append([item for item in cols if item])
    
    # Committee table
    try:
        committe_table_rows = committe_table.find_all('tr', class_='odd gradeX')
    except:
        print("Index Error")
    
#     for index, row in enumerate(committe_table_rows):
#         headings = row.find_all('th')
#         headings = [title.text.replace('\n', ' ').strip() for title in headings]
#         headings.insert(0, 'Name')
#         cols = row.find_all('td')
#         cols = [item.text.replace('\n', ' ').strip() for item in cols]
#         cols.insert(0, profile_name)
#         commitee_data.append([headings])
#         commitee_data.append([item for item in cols if item])
251/106:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/107:
df_profile = pd.read_csv('member_list.csv', skiprows=0, usecols=['Name', 'District', 'Constituency', 'Political Party', 'Religion'])

df_profile.dropna(inplace=True)

df_profile.head()
251/108: #
251/109:
df_bg = pd.DataFrame(background_data,
                  index=None,
                  columns=['Name', 'Year Attained', 'Qualification', 'Type', 'Institution'])
df_bg.dropna(axis=0, inplace=True)
df_bg

df_bg.to_csv('background_data.csv')
251/110:
df_exp = pd.DataFrame(experience_data,
                  index=None,
                  columns=['Name', 'Job Title', 'Organisation', 'Period Of Work'])
df_exp.dropna(axis=0, inplace=True)
df_exp
df_exp.to_csv('experience_data.csv')
251/111:
df_hobby = pd.DataFrame(hobbies_data,
                  index=None,
                  columns=['Name', 'Hobbies', 'Special Intrests'])
df_hobby.dropna(axis=0, inplace=True)
df_hobby
df_hobby.to_csv('hobby_data.csv')
251/112:
df_member = pd.DataFrame(membership_data,
                  index=None,
                  columns=['Name', 'Proffessional Body', 'Membership Type'])
df_member.dropna(axis=0, inplace=True)
df_member
df_member.to_csv('membership_data.csv')
251/113:
# all_data = df_bg.merge(df_exp, how="left", on="Name")

#all_data = all_data.merge(df_hobby, how="left", on="Name")

#all_data = all_data.merge(df_member, how="left", on="Name")

# all_data = all_data.merge(df_profile, how="left", left_on="Name", right_on="Name")

# all_data.head(20)
251/114: all_data.shape
251/115: df_profile
251/116: all_data.shape
251/117:
# all_data.set_index(['Name'])

# all_data.groupby(by='Name').count()
251/118:
profile_data

background_data

experience_data

hobbies_data

membership_data

# committe_table_data
251/119: profile_data
251/120: profile_data
251/121: profile_data.T
251/122: profile_data.T
251/123: profile_data
251/124:
df_profile_data = pd.DataFrame(profile_data,
                               index=None,
                               columns=['NAME', 'CONSTITUENCY', 'POLITICAL PARTY', 'PROFESSION', 'MARITAL STATUS', 'PHONE NUMBER', 'DATE OF BIRTH', 'EMAIL'])


df_profile_data.head()
251/125: profile_data.T
251/126:
df_profile_data = pd.DataFrame(profile_data,
                               index=None
                              )

df_profile_data.head()
251/127: profile_data
251/128:
df_profile_data = pd.DataFrame(profile_data,
                               index=None
                              )

df_profile_data.head(10)
251/129:
df_profile_data = pd.DataFrame(profile_data,
                               index=None
                              )

df_profile_data.tail(10)
251/130:
df_profile_data = pd.DataFrame(profile_data,
                               index=None
                              )

df_profile_data.head(20)
251/131:
df_profile_data = pd.DataFrame(profile_data,
                               index=None
                              )

df_profile_data.T
251/132:
df_profile_data = pd.DataFrame(profile_data,
                               index=None
                              )

df_profile_data.head(20)
253/1: %matplotlib notebook
253/2: import numpy as np
253/3: %matplotlib notebook
253/4: data = np.arange(8)
253/5: data
253/6: plt.data
253/7: plt.plot(data)
253/8:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
253/9: data = np.arange(8)
253/10: data
253/11: plt.plot(data)
253/12:
# Creating a new figure

fig = plt.figure()
253/13: # Creating mre plots using subplots
253/14: ax1 = fig.add_subplot(2, 2, 1)
253/15: ax2 = fig.add_subplot(2, 2, 2)
253/16: ax3 = fig.add_subplot(2, 3, 3)
253/17:
# Creating a new figure

fig = plt.figure()
253/18: # Creating mre plots using subplots
253/19: ax1 = fig.add_subplot(2, 2, 1)
253/20: ax2 = fig.add_subplot(2, 2, 2)
253/21: ax3 = fig.add_subplot(2, 3, 3)
254/1: import matplotlib as plt;
254/2: %maplotlib inline
254/3: %matplotlib inline
254/4: fig = plt.figure()
253/22:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
253/23: plt.plot(np.random.randn(50).cumsum(), 'K--')
253/24: plt.plot(np.random.randn(50).cumsum(), 'k--')
253/25:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
253/26:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
253/27: _ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
253/28:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
253/29: _ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
253/30:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=50, color='k', alpha=0.3)
253/31:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
253/32: ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/33:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/34:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/35: fig, axes = plt.subplots(2, 3)
253/36: axes
253/37:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/38:
`subplots_adjust(left=None, bottom=None, right=None, top=None,
                    wspace=None, hspace=None)`
253/39:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
253/40:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=10, hspace=10)
253/41:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=10)
253/42:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=50)
253/43:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=5)
253/44:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
253/45:
fig, axes = plt.subplots(2, 4, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
253/46:
fig, axes = plt.subplots(4, 4, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
253/47:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
253/48:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
253/49:
# Ploting x and y with green dashes

ax.plot(x, y, 'g--')
253/50:
# Ploting x and y with green dashes

x = np.arange(10)
y = np.random.randn(10)

ax.plot(x, y, 'g--')
253/51:
# Ploting x and y with green dashes

x = np.arange(10)
y = np.random.randn(10)
x
# ax.plot(x, y, 'g--')
253/52: y
253/53:
# Ploting x and y with green dashes

x = np.arange(10)
y = np.arange(50)
x
# ax.plot(x, y, 'g--')
253/54: y
253/55:
# Ploting x and y with green dashes

x = np.arange(10)
y = np.arange(50)


ax.plot(x, y, 'g--')
253/56:
# Ploting x and y with green dashes

x = np.arange(10)
y = np.arange(50)


ax1.plot(x, y, 'g--')
253/57:
# Ploting x and y with green dashes

x = np.arange(10)
y = np.arange(50)


ax.plot(x, y, 'g--')
253/58:
# Ploting x and y with green dashes

ax.plot(x, y, 'g--')

ax.plot(x, y, linestyle='--', color='g')
253/59: plot ?
253/60: plot?
253/61: plt ?
253/62: use plot ?
253/63: plt.plot(randn(30).cumsum(), 'ko--')
253/64:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
253/65: data = np.arange(8)
253/66: data
253/67: plt.plot(data)
253/68:
# Creating a new figure

fig = plt.figure()
253/69: # Creating mre plots using subplots
253/70: ax1 = fig.add_subplot(2, 2, 1)
253/71: ax2 = fig.add_subplot(2, 2, 2)
253/72: ax3 = fig.add_subplot(2, 2, 3)
253/73:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/74: fig, axes = plt.subplots(2, 3)
253/75: axes
253/76:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
253/77:
# Ploting x and y with green dashes

ax.plot(x, y, 'g--')

ax.plot(x, y, linestyle='--', color='g')
253/78: plt.plot(randn(30).cumsum(), 'ko--')
253/79:
from numpy.random import randn

plt.plot(randn(30).cumsum(), 'ko--')
253/80:
# from numpy.random import randn

plt.plot(randn(30).cumsum(), 'ko--')
253/81: plt.plot(randn(30).cumsum(), 'ko--')
253/82: plt.plot(randn(30).cumsum(), 'ko--')
253/83: plt.plot(randn(30).cumsum(), 'ko--')
253/84: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')
253/85: plt.plot(randn(30).cumsum(), color='g', linestyle='dashed', marker='o')
253/86: plt.plot(randn(30).cumsum(), color='b', linestyle='dashed', marker='o')
253/87: plt.plot(randn(30).cumsum(), color='red', linestyle='dashed', marker='o')
253/88: plt.plot(randn(30).cumsum(), color='y', linestyle='dashed', marker='o')
253/89: plt.plot(randn(30).cumsum(), color='b', linestyle='dashed', marker='o')
253/90: plt.plot(randn(30).cumsum(), color='black', linestyle='dashed', marker='o')
253/91: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')
253/92: plt.plot(randn(30).cumsum(), color='#aabcaa', linestyle='dashed', marker='o')
253/93: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')
253/94: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')
253/95: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o');
253/96:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
253/97: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
253/98: plt.plot(data, 'k--', drawstyle='steps-post', label='steps-post')
253/99: plt.plot(data, 'k', drawstyle='steps-post', label='steps-post')
253/100: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
253/101: plt.plot(data, 'k', drawstyle='steps-post', label='steps-post')
253/102: plt.legend(loc='best')
253/103:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
plt.legend(loc='best')
253/104:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
plt.legend(loc='best')
253/105:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
plt.legend(loc='best');
253/106:
fig - plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000))
253/107:
fig - plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum())
253/108:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
253/109: data = np.arange(8)
253/110: data
253/111: plt.plot(data)
253/112:
# Creating a new figure

fig = plt.figure()
253/113: # Creating mre plots using subplots
253/114: ax1 = fig.add_subplot(2, 2, 1)
253/115: ax2 = fig.add_subplot(2, 2, 2)
253/116: ax3 = fig.add_subplot(2, 2, 3)
253/117:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/118: fig, axes = plt.subplots(2, 3)
253/119: axes
253/120:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
253/121:
# Ploting x and y with green dashes

ax.plot(x, y, 'g--')

ax.plot(x, y, linestyle='--', color='g')
253/122:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
253/123: data = np.arange(8)
253/124: data
253/125: plt.plot(data)
253/126:
# Creating a new figure

fig = plt.figure()
253/127: # Creating mre plots using subplots
253/128: ax1 = fig.add_subplot(2, 2, 1)
253/129: ax2 = fig.add_subplot(2, 2, 2)
253/130: ax3 = fig.add_subplot(2, 2, 3)
253/131:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/132: fig, axes = plt.subplots(2, 3)
253/133: axes
253/134:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
253/135:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
253/136: data = np.arange(8)
253/137: data
253/138: plt.plot(data)
253/139:
# Creating a new figure

fig = plt.figure()
253/140: # Creating mre plots using subplots
253/141: ax1 = fig.add_subplot(2, 2, 1)
253/142: ax2 = fig.add_subplot(2, 2, 2)
253/143: ax3 = fig.add_subplot(2, 2, 3)
253/144:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/145: fig, axes = plt.subplots(2, 3)
253/146: axes
253/147:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
253/148:
# Ploting x and y with green dashes

# ax.plot(x, y, 'g--')

# ax.plot(x, y, linestyle='--', color='g')
253/149:
# Line Plots with markers

plt.plot(randn(30).cumsum(), 'ko--')
253/150: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o');
253/151:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
plt.legend(loc='best');
253/152:
fig - plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum())
253/153:
fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum())
253/154:
fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum());
253/155:
fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum());

#Simple plot for illustrating xtricks with labels
253/156:
fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum());

#Simple plot for illustrating xtricks with labels
253/157: ticks = ax.set_xticks([0, 250, 500, 750, 1000])
253/158: labels = ax.set_xticklabel(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')
253/159: labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')
253/160:
ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
253/161:
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
253/162:
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
ax.plot(np.random.randn(1000).cumsum());
253/163:
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
ax.plot(np.random.randn(1000).cumsum())
253/164:
ax.plot(np.random.randn(1000).cumsum())
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
253/165:
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
253/166:
props = {
    'title': 'My first matplotlib plot',
    'xlabel': 'Stages'
}

ax.set(**props)
253/167: from numpy.random import randn
253/168: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
253/169: ax.plot(randn(1000).cumsum(), 'k', label='one')
253/170: ax.plot(randn(1000).cumsum(), 'k--', label='two')
253/171: ax.plot(randn(1000).cumsum(), 'k.', label='three')
253/172: ax.legend(loc='best')
253/173:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/174:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/175:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/176:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k**', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/177:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--||', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/178:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/179:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best', label='_nolegend_')
253/180:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/181:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
ax.label='_nolegend_'
253/182:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/183:
fig = plt.figure()
ax = figure.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)
253/184:
fig = plt.figure()
ax = figure.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)
253/185:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
253/186: data = np.arange(8)
253/187: data
253/188: plt.plot(data)
253/189:
# Creating a new figure

fig = plt.figure()
253/190: # Creating mre plots using subplots
253/191: ax1 = fig.add_subplot(2, 2, 1)
253/192: ax2 = fig.add_subplot(2, 2, 2)
253/193: ax3 = fig.add_subplot(2, 2, 3)
253/194:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/195: fig, axes = plt.subplots(2, 3)
253/196: axes
253/197:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
253/198:
# Ploting x and y with green dashes

# ax.plot(x, y, 'g--')

# ax.plot(x, y, linestyle='--', color='g')
253/199:
# Line Plots with markers

plt.plot(randn(30).cumsum(), 'ko--')
253/200: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o');
253/201:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
plt.legend(loc='best');
253/202:
fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum());

#Simple plot for illustrating xtricks with labels
253/203:
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
253/204:
props = {
    'title': 'My first matplotlib plot',
    'xlabel': 'Stages'
}

ax.set(**props)
253/205: from numpy.random import randn
253/206:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/207:
fig = plt.figure()
ax = figure.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)
253/208:
fig = plt.figure(); ax = figure.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)
253/209:
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)
253/210:
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)
253/211:
# To save an svg version of the plot

plt.savefig('figpath.svg')
253/212: s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))
253/213:
s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))

s.plot()
253/214:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
253/215: data = np.arange(8)
253/216: data
253/217: plt.plot(data)
253/218:
# Creating a new figure

fig = plt.figure()
253/219: # Creating mre plots using subplots
253/220: ax1 = fig.add_subplot(2, 2, 1)
253/221: ax2 = fig.add_subplot(2, 2, 2)
253/222: ax3 = fig.add_subplot(2, 2, 3)
253/223:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/224: fig, axes = plt.subplots(2, 3)
253/225: axes
253/226:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
253/227:
# Ploting x and y with green dashes

# ax.plot(x, y, 'g--')

# ax.plot(x, y, linestyle='--', color='g')
253/228:
# Line Plots with markers

plt.plot(randn(30).cumsum(), 'ko--')
253/229: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o');
253/230:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
plt.legend(loc='best');
253/231:
fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum());

#Simple plot for illustrating xtricks with labels
253/232:
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
253/233:
props = {
    'title': 'My first matplotlib plot',
    'xlabel': 'Stages'
}

ax.set(**props)
253/234: from numpy.random import randn
253/235:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/236:
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)
253/237:
# To save an svg version of the plot

plt.savefig('figpath.svg')
253/238:
s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))

s.plot()
253/239:
s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))

s.plot()
253/240:

df = pd.DataFrame(np.random(.randn(10, 4).cumsum(0)),
                  columns=['A', 'B', 'C', 'D'],
                  index=np.arange(0, 100, 10))

df.plot()
253/241:

df = pd.DataFrame(np.random.randn(10, 4).cumsum(0)),
                  columns=['A', 'B', 'C', 'D'],
                  index=np.arange(0, 100, 10))

df.plot()
253/242:

df = pd.DataFrame(np.random.randn(10, 4).cumsum(0),
                  columns=['A', 'B', 'C', 'D'],
                  index=np.arange(0, 100, 10))

df.plot()
253/243:
fig, axes = subplots(2, 1)

data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

data.plot.bar(ax=axes[0], color='k', alpha=0.7)

data,plot.barh(ax=axes[1], color='k', alpha=0.7)
253/244:
fig, axes = subplot(2, 1)

data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

data.plot.bar(ax=axes[0], color='k', alpha=0.7)

data,plot.barh(ax=axes[1], color='k', alpha=0.7)
253/245:
fig, axes = plt.subplot(2, 1)

data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

data.plot.bar(ax=axes[0], color='k', alpha=0.7)

data,plot.barh(ax=axes[1], color='k', alpha=0.7)
253/246:
fig, axes = plt.subplots(2, 1)

data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

data.plot.bar(ax=axes[0], color='k', alpha=0.7)

data,plot.barh(ax=axes[1], color='k', alpha=0.7)
253/247:
fig, axes = plt.subplots(2, 1)

data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

data.plot.bar(ax=axes[0], color='k', alpha=0.7)

data.plot.barh(ax=axes[1], color='k', alpha=0.7)
253/248:
df = pd.DataFrame(np.random.rand(6, 4),
                 index=['one', 'two', 'three', 'four', 'five', 'six'],
                 columns=pd.Index(['A', 'B', 'C', 'D'], name='Genius'))
253/249: df
253/250: df.plot.bar()
253/251: df.plot.barh(starked=True, alpha=0.5)
253/252: df.plot.barh(stacked=True, alpha=0.5)
253/253: df.plot.barh(stacked=True, alpha=0.9)
253/254: df.plot.barh(stacked=True, alpha=0.5)
253/255: df.plot.barh(stacked=True, alpha=0.5);
253/256: df.plot.barh(stacked=True, alpha=0.9);
253/257: df.plot.barh(stacked=True, alpha=0.5);
255/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
255/2: data = np.arange(8)
255/3: data
255/4: plt.plot(data)
255/5:
# Creating a new figure

fig = plt.figure()
255/6: # Creating mre plots using subplots
255/7: ax1 = fig.add_subplot(2, 2, 1)
255/8: ax2 = fig.add_subplot(2, 2, 2)
255/9: ax3 = fig.add_subplot(2, 2, 3)
255/10:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
255/11: fig, axes = plt.subplots(2, 3)
255/12: axes
255/13:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
255/14:
# Ploting x and y with green dashes

# ax.plot(x, y, 'g--')

# ax.plot(x, y, linestyle='--', color='g')
255/15:
# Line Plots with markers

plt.plot(randn(30).cumsum(), 'ko--')
255/16: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o');
255/17:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
plt.legend(loc='best');
255/18:
fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum());

#Simple plot for illustrating xtricks with labels
255/19:
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
255/20:
props = {
    'title': 'My first matplotlib plot',
    'xlabel': 'Stages'
}

ax.set(**props)
255/21: from numpy.random import randn
255/22:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
255/23:
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)
255/24:
# To save an svg version of the plot

plt.savefig('figpath.svg')
255/25:
s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))

s.plot()
255/26:

df = pd.DataFrame(np.random.randn(10, 4).cumsum(0),
                  columns=['A', 'B', 'C', 'D'],
                  index=np.arange(0, 100, 10))

df.plot()
255/27:
fig, axes = plt.subplots(2, 1)

data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

data.plot.bar(ax=axes[0], color='k', alpha=0.7)

data.plot.barh(ax=axes[1], color='k', alpha=0.7)
255/28:
df = pd.DataFrame(np.random.rand(6, 4),
                 index=['one', 'two', 'three', 'four', 'five', 'six'],
                 columns=pd.Index(['A', 'B', 'C', 'D'], name='Genius'))
255/29: df
255/30: df.plot.bar()
255/31: df.plot.barh(stacked=True, alpha=0.5);
253/258: df.plot.barh(stacked=True, alpha=0.78);
253/259: df.plot.barh(stacked=True, alpha=0.68);
253/260: df.plot.barh(stacked=True, alpha=0.7);
253/261: df.plot.barh(stacked=True, alpha=0.5);
253/262:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
253/263: data = np.arange(8)
253/264: data
253/265: plt.plot(data)
253/266:
# Creating a new figure

fig = plt.figure()
253/267: # Creating mre plots using subplots
253/268: ax1 = fig.add_subplot(2, 2, 1)
253/269: ax2 = fig.add_subplot(2, 2, 2)
253/270: ax3 = fig.add_subplot(2, 2, 3)
253/271:
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
plt.plot(np.random.randn(50).cumsum(), 'k--')
_ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)
ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
253/272: fig, axes = plt.subplots(2, 3)
253/273: axes
253/274:
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
    plt.subplots_adjust(wspace=0, hspace=0)
    
# Visualization with no inter-suplot spacing
253/275:
# Ploting x and y with green dashes

# ax.plot(x, y, 'g--')

# ax.plot(x, y, linestyle='--', color='g')
253/276:
# Line Plots with markers

plt.plot(randn(30).cumsum(), 'ko--')
253/277: plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o');
253/278:
data = np.random.randn(30).cumsum()

plt.plot(data, 'k--', label='Default')
plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
plt.legend(loc='best');
253/279:
fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)

ax.plot(np.random.randn(1000).cumsum());

#Simple plot for illustrating xtricks with labels
253/280:
ticks = ax.set_xticks([0, 250, 500, 750, 1000])

labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')

ax.set_title('My first matplotlib plot')
ax.set_xlabel('Stages')
253/281:
props = {
    'title': 'My first matplotlib plot',
    'xlabel': 'Stages'
}

ax.set(**props)
253/282: from numpy.random import randn
253/283:
fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
ax.plot(randn(1000).cumsum(), 'k', label='one')
ax.plot(randn(1000).cumsum(), 'k--', label='two')
ax.plot(randn(1000).cumsum(), 'k.', label='three')
ax.legend(loc='best')
253/284:
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)
253/285:
# To save an svg version of the plot

plt.savefig('figpath.svg')
253/286:
s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))

s.plot()
253/287:

df = pd.DataFrame(np.random.randn(10, 4).cumsum(0),
                  columns=['A', 'B', 'C', 'D'],
                  index=np.arange(0, 100, 10))

df.plot()
253/288:
fig, axes = plt.subplots(2, 1)

data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

data.plot.bar(ax=axes[0], color='k', alpha=0.7)

data.plot.barh(ax=axes[1], color='k', alpha=0.7)
253/289:
df = pd.DataFrame(np.random.rand(6, 4),
                 index=['one', 'two', 'three', 'four', 'five', 'six'],
                 columns=pd.Index(['A', 'B', 'C', 'D'], name='Genius'))
253/290: df
253/291: df.plot.bar()
253/292: df.plot.barh(stacked=True, alpha=0.5);
253/293:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concatenate([comp1, comp2]))

sns.displot(value, bins=100, color='k')
253/294:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concatenate([comp1, comp2]))

sns.distplot(value, bins=100, color='k')
253/295:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concatenate([comp1, comp2]))

sns.distplot(values, bins=100, color='k')
253/296:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concatenate([comp1, comp2]))

sns.distplot(values, bins=100, color='blue')
253/297:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concatenate([comp1, comp2]))

sns.distplot(values, bins=100, color='green')
253/298:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concatenate([comp1, comp2]))

sns.distplot(values, bins=100, color='k')
253/299:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concatenate([comp1, comp2]))

sns.distplot(values, bins=100, color='k');
253/300:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concat([comp1, comp2]))

sns.distplot(values, bins=100, color='k');
253/301:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concate([comp1, comp2]))

sns.distplot(values, bins=100, color='k');
253/302:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concat([comp1, comp2]))

sns.distplot(values, bins=100, color='k');
253/303:
comp1 = np.random.normal(0, 1, size=200)

comp2 = np.random.normal(10, 2, size=200)

values = pd.Series(np.concatenate([comp1, comp2]))

sns.distplot(values, bins=100, color='k');
256/1:
import pandas as pd
import numpy as np
import matplotlib as plt
%matplotlib inline
import seaborn as sns
256/2:
df = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a'],
                   'key2': ['one', 'two', 'one', 'two', 'one'],
                   'data1': np.random.randn(5),
                   'data2': np.random.randn(5)})
256/3: df
256/4: grouped = df['data1'].groupby(df['key1'])
256/5: grouped
256/6: grouped.mean()
256/7: means = df['data1'].groupby([df['key1'], df['key2']]).mean()
256/8: means
256/9: means.unstack()
256/10:
states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])

years = np.array([2005, 2005, 2006, 2005])
256/11: df['data1'].groupby([states, years]).mean()
256/12:
states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])

years = np.array([2005, 2005, 2006, 2005, 2006])
256/13: df['data1'].groupby([states, years]).mean()
256/14:
states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])

years = np.array([2005, 2005, 2006, 2005, 2006])
256/15: df['data1'].groupby([states, years]).mean()
256/16: df.groupby('key1').mean()
256/17: df.groupby('key1').mean()
256/18: df.groupby(['key1', 'key2']).mean()
256/19: df.groupby(['key1', 'key2']).size()
256/20:
for name, group in df.groupby('key1'):
    print(name)
    print(group)
256/21:
for (k1, k2), group in df.groupby(['key1', 'key2']):
    print((k1, k2))
    print(group)
256/22: pieces = dict(list(df.groupby('key1')))
256/23: pieces['b']
256/24: df.dtypes
256/25: grouped = df.groupby(df.dtypes, axis=1)
256/26:
for dtype, group in grouped:
    print(dtype)
    print(group)
256/27: df.groupby('key1')['data1']
256/28: df.groupby('key2')['data2']
256/29: df['data1'].groupby(df['key1'])
256/30: df['data2'].groupby(df['key2'])
256/31: df.groupby(['key1', 'key2'])[['data2']].mean()
256/32: s_grouped = df.groupby(['key1', 'key2'])['data2']
256/33: s_grouped
256/34: s_grouped.mean()
256/35:
people = pd.DataFrame(np.random.randn(5, 5),
                      columns=['a', 'b', 'c', 'd', 'e'],
                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])
256/36: people.iloc[2:3, [1, 2]] = np.nan
256/37: people
256/38:
maping = {
    'a': 'red',
    'b': 'red',
    'c': 'blue',
    'd': 'blue',
    'e': 'red',
    'f': 'orange'
}
256/39: by_column = people.groupby(mapping, axis=1)
256/40:
mapping = {
    'a': 'red',
    'b': 'red',
    'c': 'blue',
    'd': 'blue',
    'e': 'red',
    'f': 'orange'
}
256/41: by_column = people.groupby(mapping, axis=1)
256/42: by_column.sum()
256/43: map_series = pd.Series(mapping)
256/44: map_series
256/45: people.groupby(map_series, axis=1).count()
256/46: people.groupby(map_series, axis=1).sum()
256/47: people.groupby(map_series, axis=1).count()
256/48: people.groupby(len).sum()
256/49:
key_list = ['one', 'one', 'one', 'two', 'two']

people.groupby([len, key_list]).min()
256/50: ### Grouping by Index Level
256/51:
columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],
                                    [1, 3, 5, 1, 3]],
                                    names=['cty', 'tenor'])
256/52: hier_df = pd.DataFrame(np.random.randn(4, 5), columns=columns)
256/53: hier_df
256/54: hier.groupby(level='cty', axis=1).count()
256/55: hier_df.groupby(level='cty', axis=1).count()
256/56: df
256/57: grouped = df.groupby('key1')
256/58: grouped
256/59: grouped['data1'].quantile(0.9)
256/60:
def peak_to_peak():
    return arr.max() - arr.min()
256/61: grouped.agg(peak_to_peak)
256/62:
def peak_to_peak():
    return arr.max() - arr.min()
256/63: grouped.agg(peak_to_peak())
256/64:
def peak_to_peak(arr):
    return arr.max() - arr.min()
256/65: grouped.agg(peak_to_peak())
256/66: grouped.agg(peak_to_peak)
256/67: grouped.describe()
256/68: grouped.describe().T
256/69: grouped.describe()
256/70: `grouped = grouped['columns3']`
256/71: grouped.describe().T
256/72: grouped.describe()
256/73:
frame = pd.DataFrame({'data1': np.random.randn(1000),
                      'data2': np.random.randn(1000)})

quartiles = pd.cut(frame, 4)
256/74:
frame = pd.DataFrame({'data1': np.random.randn(1000),
                      'data2': np.random.randn(1000)})

quartiles = pd.cut(frame.data1, 4)
256/75: quartiles[:10]
256/76:
def get_stats(group):
    return {
        'min': group.min(),
        'max': group.max(),
        'count': group.count()
        'mean': group.mean()
    }
256/77:
def get_stats(group):
    return {
        'min': group.min(),
        'max': group.max(),
        'count': group.count(),
        'mean': group.mean()
    }
256/78:
def get_stats(group):
    return {
        'min': group.min(),
        'max': group.max(),
        'count': group.count(),
        'mean': group.mean()
    }
256/79: grouped = frame.data2.groupby(quartiles)
256/80: grouped.apply(get_stats).unstack()
256/81: grouping = pd.qcut(frame.data1, 10, labels = False)
256/82: grouped = frame.data2.groupby(grouping)
256/83: grouped.apply(get_stats).unstack()
256/84: s = pd.Series(np.random.randn(6))
256/85: s[::2] = np.nan
256/86: s
256/87: s.fillna(s.mean())
256/88: s
256/89: s.fillna(s.mean())
256/90:
states = ['Ohio', 'New York', 'Vermont', 'Florida',
          'Oregon', 'Nevada', 'California', 'Idaho']
256/91: group_key = ['East'] * 4 + ['West'] * 4
256/92: data = pd.Series(np.random.randn(8), index=states)
256/93: data
256/94: data[['Vermont', 'Nevada', 'Idaho']] = np.nan
256/95: data
256/96: data.groupby(group_key).mean()
256/97:
# We can fill the NA values using the group means like so

fill_mean = lambda g: g.fillna(g.mean())
256/98: data.groupby(group_key).apply(fill_mean)
256/99: fill_values = {'East': 0.5, 'West': -1}
256/100: fill_func = lambda g: g.fillna(fill_value[g.name])
256/101: data.groupby(group_key).apply(fill_func)
256/102: fill_func = lambda g: g.fillna(fill_values[g.name])
256/103: data.groupby(group_key).apply(fill_func)
256/104:
# Hearts, Spades, Clubs, Diamonds

suits = ['H', 'S', 'C', 'D']
card_val = (list(range(1, 11)) + [10] * 3) * 4
best_names = ['A'] + list(range(2, 11)) + ['J', 'K', 'Q']
cards = []

for suit in ['H', 'S', 'C', 'D']:
    cards.extend(str(num) + suit for num in base_names)
    
deck = pd.Series(card_val, index=cards)
256/105:
# Hearts, Spades, Clubs, Diamonds

suits = ['H', 'S', 'C', 'D']
card_val = (list(range(1, 11)) + [10] * 3) * 4
base_names = ['A'] + list(range(2, 11)) + ['J', 'K', 'Q']
cards = []

for suit in ['H', 'S', 'C', 'D']:
    cards.extend(str(num) + suit for num in base_names)
    
deck = pd.Series(card_val, index=cards)
256/106: deck[:13]
256/107:
def draw(deck, n=5):
    return deck.sample(n)
256/108: draw(deck)
256/109: draw(deck)
256/110: draw(deck)
256/111: get_suit = lambda card: card[-1]
256/112: deck.groupby(get_suit).apply(draw, n=2)
256/113: deck.groupby(get_suit).apply(draw, n=2)
256/114: deck.groupby(get_suit).apply(draw, n=2)
256/115: deck.groupby(get_suit).apply(draw, n=2)
256/116: deck.groupby(get_suit).apply(draw, n=2)
256/117: deck.groupby(get_suit).apply(draw, n=2)
256/118: deck.groupby(get_suit).apply(draw, n=2)
256/119: deck.groupby(get_suit).apply(draw, n=2)
256/120: deck.groupby(get_suit, group_keys=False).apply(draw, n=2)
256/121:
df = pd.DatFrame({'category': ['a', 'a', 'a', 'a',
                               'b', 'b', 'b', 'b'],
                  'data': np.random.randn(8),
                  'weights': np.random.randn(8)})
256/122:
df = pd.DataFrame({'category': ['a', 'a', 'a', 'a',
                               'b', 'b', 'b', 'b'],
                  'data': np.random.randn(8),
                  'weights': np.random.randn(8)})
256/123: df
256/124: grouped = df.groupby('category')
256/125: get_wavg = lambda g: p.average(g['data'],  weights=g['weights'])
256/126: grouped.apply(get_wavg)
256/127: get_wavg = lambda g: g.average(g['data'],  weights=g['weights'])
256/128: grouped.apply(get_wavg)
256/129: get_wavg = lambda g: np.average(g['data'],  weights=g['weights'])
256/130: grouped.apply(get_wavg)
256/131: import statsmodel.api as sm
256/132: import statsmodels.api as sm
257/1:
import pandas as pd
import numpy as np
import matplotlib as plt
%matplotlin inline
import seaborn as sns
257/2:
import pandas as pd
import numpy as np
import matplotlib as plt
%matplotlib inline
import seaborn as sns
257/3: from datetime import datetime
257/4: now = datetime.now()
257/5: now
257/6: now.year, now.month, now.day
257/7: delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)
257/8: delta
257/9: from datetime import datetime, timedelta
257/10: start = datetime(2011, 1, 7)
259/1: import pandas as pd
259/2: import pandas as pd
259/3: data = pd.read_csv('SampleSubmission.csv')
259/4: data = pd.read_csv('SampleSubmission')
259/5: data = pd.read_csv('SampleSubmission.csv')
260/1: import pandas as pd
260/2: data = pd.read_csv('SampleSubmission.csv')
260/3: data.head()
260/4: df = pd.read_sql_query('SELECT Title, Body Tags, COUNT(*) AS count_dup GROUP BY Title Body Tags')
260/5: df = pd.read_sql_query('SELECT Title, Body Tags, COUNT(*) AS count_dup GROUP BY Title Body Tags', con='')
260/6: df = pd.read_sql_query('SELECT Title, Body Tags, COUNT(*) AS count_dup GROUP BY Title Body Tags')
260/7: df.head()
260/8: import pandas as pd
260/9: df = pd.read_csv('SampleSubmission.csv')
260/10: df.head()
260/11: df = pd.read_sql_query('SELECT Title, Body Tags, COUNT(*) AS count_dup GROUP BY Title Body Tags')
260/12: df
260/13: df.duplicated()
260/14: df.duplicated().value_counts
260/15: df.duplicated().sum()
260/16: df.duplicated().count()
260/17: df.shape
260/18: df.duplicated()
260/19:
df = pd.read_csv('SampleSubmission.csv')
train = pd.read_csv('Train.csv')
test = pd.read_csv('Test.csv')
261/1: import pandas as pd
261/2:
df = pd.read_csv('SampleSubmission.csv')
train = pd.read_csv('Train.csv')
test = pd.read_csv('Test.csv')
261/3: df.head()
261/4: train.head(1)
261/5: test.head(3)
261/6: import pandas as pd
261/7:
df = pd.read_csv('SampleSubmission.csv')
train = pd.read_csv('Train.csv')
test = pd.read_csv('Test.csv')
263/1: df.head()
263/2: import pandas as pd
263/3:
df = pd.read_csv('SampleSubmission.csv')
train = pd.read_csv('Train.csv')
test = pd.read_csv('Test.csv')
264/1: import pandas as pd
264/2:
# df = pd.read_csv('SampleSubmission.csv')
train = pd.read_csv('Train.csv')
# test = pd.read_csv('Test.csv')
264/3: train.head(1)
269/1:
import numpy as np
import pandas as pd
269/2: nms = pd.read_csv('nms_awarded_contracts.csv')
269/3: nms.head
269/4: nms.head()
269/5: nms[nms['Local/Foreign'] == 'Foreign']
269/6: nms[nms['Local/Foreign'] == 'Foreign'].value_counts
269/7: nms[nms['Local/Foreign'] == 'Foreign'].value_counts()
269/8: nms[nms['Local/Foreign'] == 'Foreign']
269/9:
import numpy as np
import pandas as pd
269/10: nms = pd.read_csv('nms_awarded_contracts.csv')
269/11: nms.head()
269/12:
import numpy as np
import pandas as pd
269/13: nms = pd.read_csv('nms_awarded_contracts.csv')
269/14: nms.head()
269/15: nms[nms['Local/Foreign'] == 'Foreign']
269/16:
import numpy as np
import pandas as pd
269/17: nms = pd.read_csv('nms_awarded_contracts.csv')
269/18: nms.head()
269/19: nms[nms['Local/Foreign'] == 'Foreign']
273/1:
import numpy as np
import pandas as pd
273/2: nms = pd.read_csv('nms_awarded_contracts.csv')
273/3: nms.head()
273/4: nms[nms['Local/Foreign'] == 'Foreign']
273/5:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local.head(3)
273/6: nms[nms['is_framework'] == 'Framework']
273/7:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local.head(3)
273/8: nms_local = np.where(nms['is_framework'] == 'Framework' | nms['is_framework'] == 'Lotted Framework')
273/9: nms_local = np.where(nms['is_framework'] == 'Framework' or nms['is_framework'] == 'Lotted Framework')
273/10: nms_local = np.where(nms['is_framework'] == 'Framework' || nms['is_framework'] == 'Lotted Framework')
273/11: nms_local = np.where(nms['is_framework'] == 'Framework' | nms['is_framework'] == 'Lotted Framework')
273/12: nms_local = np.where(nms['is_framework'] == 'Framework')
273/13: nms_local
273/14:
import numpy as np
import pandas as pd
273/15: nms = pd.read_csv('nms_awarded_contracts.csv')
273/16: nms.head()
273/17: nms[nms['Local/Foreign'] == 'Foreign']
273/18:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local.head(3)
273/19:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']

nms_local.head(3)
273/20:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']

nms_local = nms[nms['financial_year'] != '2016-2017']

nms_local.head(3)
273/21: nms_local.shape
273/22: nms_local.to_csv('nms_awarded_contracts_3.csv')
273/23: nms_local.describe()
273/24: nms_local.mean()
273/25: nms_local.corr()
273/26: nms_local.head(1)
273/27: nms_local.groupby('type')
273/28: grouped = nms_local.groupby('type')
273/29:
for i, j in grouped:
    print(i)
    print('  ')
    print(j)
273/30:
for i, j in grouped:
    print(i)
    print(j)
    print('\n')
273/31: nms_local.info()
273/32: nms_local.groupby('Provider').sum()
273/33: # Uneed global group
273/34: nms_local = nms_local[nms_local['Provider'] == 'Uneed global group']
273/35: nms_local
273/36: nms_local.shape
273/37: nms_local
273/38: nms_local.to_csv('uneed_global.csv')
273/39:
import numpy as np
import pandas as pd
273/40: nms = pd.read_csv('nms_awarded_contracts.csv')
273/41: nms.head()
273/42: nms[nms['Local/Foreign'] == 'Foreign']
273/43:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']

nms_local = nms[nms_local['financial_year'] != '2016-2017']

nms_local.head(3)
273/44:
import numpy as np
import pandas as pd
273/45: nms = pd.read_csv('nms_awarded_contracts.csv')
273/46: nms.head()
273/47: nms[nms['Local/Foreign'] == 'Foreign']
273/48:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']

nms_local = nms[nms_local['financial_year'] != '2016-2017']

nms_local.head(3)
273/49:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']
273/50:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']
273/51: nms_local.head(1)
273/52:
nms_local = nms_local[nms_local['financial_year'] != '2016-2017']

nms_local.head(3)
273/53: nms_local.shape
273/54: nms_local.to_csv('nms_awarded_contracts_3.csv')
273/55: nms_local.describe()
273/56: nms_local.mean()
273/57: nms_local.corr()
273/58: nms_local.head(1)
273/59: grouped = nms_local.groupby('type')
273/60: nms_local.info()
273/61: nms_local.groupby('Provider').sum()
273/62: # Uneed global group
273/63: nms_local = nms_local[nms_local['Provider'] == 'Uneed global group']
273/64: nms_local.shape
273/65: nms_local
273/66: nms_local.to_csv('uneed_global.csv')
274/1:
import numpy as np
import pandas as pd
274/2: nms = pd.read_csv('nms_awarded_contracts.csv')
274/3: nms.head()
274/4: nms[nms['Local/Foreign'] == 'Foreign']
274/5:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']
274/6: nms_local.head(1)
274/7:
nms_local = nms_local[nms_local['financial_year'] != '2016-2017']

nms_local.head(3)
274/8: nms_local.shape
274/9: nms_local.to_csv('nms_awarded_contracts_3.csv')
274/10: nms_local.describe()
274/11: nms_local.mean()
274/12: nms_local.corr()
274/13: nms_local.head(1)
274/14: grouped = nms_local.groupby('type')
274/15: nms_local.info()
274/16: nms_local.groupby('Provider').sum()
274/17: # Uneed global group
274/18: nms_local = nms_local[nms_local['Provider'] == 'Uneed global group']
274/19: nms_local.shape
274/20: nms_local
274/21: nms_local.to_csv('uneed_global.csv')
274/22: grouped = nms_local.groupby('financial_year').sum()
274/23: grouped = nms_local.groupby('financial_year').sum()
274/24: grouped
274/25: grouped = nms_local.groupby('no_of_call_off_orders').sum()
274/26: grouped
274/27: nms_local['no_of_call_off_orders']
274/28: nms_local['no_of_call_off_orders'].sum()
274/29: call_offs = nms_local['no_of_call_off_orders'].sum()
274/30: call_offs
274/31: call_offs = nms_local['no_of_call_off_orders']
274/32: call_offs
274/33: call_offs.length
274/34: call_offs.info
274/35: call_offs.info()
274/36: call_offs
274/37: call_offs = nms_local['no_of_call_off_orders'].sum(axis=0)
274/38: call_offs
274/39: call_offs = nms_local['no_of_call_off_orders'].sum(axis=0, skipna=True)
274/40: call_offs
274/41: call_offs = nms_local['no_of_call_off_orders'].sum(axis=1, skipna=True)
274/42: call_offs
274/43:
import numpy as np
import pandas as pd
274/44: nms = pd.read_csv('nms_awarded_contracts.csv')
274/45: nms.head()
274/46: nms[nms['Local/Foreign'] == 'Foreign']
274/47:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']
274/48: nms_local.head(1)
274/49:
nms_local = nms_local[nms_local['financial_year'] != '2016-2017']

nms_local.head(3)
274/50: nms_local.shape
274/51: nms_local.to_csv('nms_awarded_contracts_3.csv')
274/52: nms_local.describe()
274/53: nms_local.mean()
274/54: nms_local.corr()
274/55: nms_local.head(1)
274/56: call_offs = nms_local['no_of_call_off_orders'].sum(axis=0, skipna=True)
274/57: call_offs
274/58: nms_local.info()
274/59: nms_local.groupby('Provider').sum()
274/60: # Uneed global group
274/61: nms_local = nms_local[nms_local['Provider'] == 'Uneed global group']
274/62: nms_local.shape
274/63: nms_local
274/64: nms_local.to_csv('uneed_global.csv')
274/65: nms_local.shape
274/66:
import numpy as np
import pandas as pd
274/67: nms = pd.read_csv('nms_awarded_contracts.csv')
274/68: nms.head()
274/69: nms[nms['Local/Foreign'] == 'Foreign']
274/70:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']
274/71: nms_local.head(1)
274/72:
nms_local = nms_local[nms_local['financial_year'] != '2016-2017']

nms_local.head(3)
274/73: nms_local.shape
274/74: nms_local.to_csv('nms_awarded_contracts_3.csv')
274/75: nms_local.describe()
274/76: nms_local.mean()
274/77: nms_local.corr()
274/78: nms_local.head(1)
274/79: call_offs = nms_local['no_of_call_off_orders'].sum(axis=0, skipna=True)
274/80: call_offs
274/81: nms_local.info()
274/82: nms_local.groupby('Provider').sum()
274/83: # Uneed global group
274/84: nms_local = nms_local[nms_local['Provider'] == 'Uneed global group']
274/85: nms_local.shape
274/86: nms_local
274/87: nms_local.to_csv('uneed_global.csv')
274/88: nms_local.shape
275/1:
import numpy as np
import pandas as pd
275/2: nms = pd.read_csv('nms_awarded_contracts.csv')
275/3: nms.head()
275/4: nms[nms['Local/Foreign'] == 'Foreign']
275/5:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']
275/6: nms_local.head(1)
275/7:
nms_local = nms_local[nms_local['financial_year'] != '2016-2017']

nms_local.head(3)
275/8: nms_local.shape
275/9: nms_local.to_csv('nms_awarded_contracts_3.csv')
275/10: nms_local.describe()
275/11: nms_local.mean()
275/12: nms_local.corr()
275/13: nms_local.head(1)
275/14: call_offs = nms_local['no_of_call_off_orders'].sum(axis=0, skipna=True)
275/15: call_offs
275/16: nms_local.info()
275/17: nms_local.groupby('Provider').sum()
275/18: # Uneed global group
275/19: nms_local = nms_local[nms_local['Provider'] == 'Uneed global group']
275/20: nms_local.shape
275/21: nms_local
275/22: nms_local.to_csv('uneed_global.csv')
275/23: nms_local.shape
275/24:
import numpy as np
import pandas as pd
275/25: nms = pd.read_csv('nms_awarded_contracts.csv')
275/26: nms.head()
275/27: nms[nms['Local/Foreign'] == 'Foreign']
275/28:
nms_local = nms[nms['Local/Foreign'] == 'Local']

nms_local = nms[nms['financial_year'] != '2015-2016']
275/29: nms_local.head(1)
275/30:
nms_local = nms_local[nms_local['financial_year'] != '2016-2017']

nms_local.head(3)
275/31: nms_local.shape
275/32: nms_local.to_csv('nms_awarded_contracts_3.csv')
275/33: nms_local.describe()
275/34: nms_local.mean()
275/35: nms_local.corr()
275/36: nms_local.head(1)
275/37: call_offs = nms_local['no_of_call_off_orders'].sum(axis=0, skipna=True)
275/38: call_offs
275/39: nms_local.info()
275/40: nms_local.groupby('Provider').sum()
275/41: # Uneed global group
275/42:
nms_local = nms_local[nms_local['Provider'] == 'Uneed global group']

nms_local_git = nms_local[nms_local['Provider'] == 'Gittoes Pharmaceuticals Limited']

nms_local_git.to_csv('gittoes.csv')
275/43: nms_local.shape
275/44: nms_local
275/45: nms_local.to_csv('uneed_global.csv')
275/46: nms_local.shape
276/1:
import pandas as pd
import numpy as np
276/2: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx')
276/3: nms_contracts.head(3)
276/4: nms_contracts.head(3)
276/5: nms = nms_contracts[['Supplier Name', 'Description', 'Line Total', 'Procurement Type']]
276/6:
import pandas as pd
import numpy as np
276/7: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name=1)
276/8: nms_contracts.head(3)
276/9: nms_contracts.head(3)
276/10: nms_contracts.head(3)
276/11: nms = nms_contracts[['Supplier Name', 'Description', 'Line Total', 'Procurement Type']]
276/12:
import pandas as pd
import numpy as np
276/13: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/14: nms_contracts.head(3)
276/15: nms = nms_contracts[['Supplier Name', 'Description', 'Line Total', 'Procurement Type']]
276/16: nms = nms_contracts[['Supplier Name', 'Description', 'Line Total', 'Procurement Type']]
276/17: nms = nms_contracts['Supplier Name', 'Description', 'Line Total', 'Procurement Type']
276/18: nms = nms_contracts[[5, 7, 16, 17]]
276/19: nms = nms_contracts[['Receipt Date']]
276/20: nms = nms_contracts[['Receipt Date']]
276/21: nms = nms_contracts['Receipt Date']
276/22: nms = nms_contracts['Receipt Date' == '2019-07-09']
276/23: nms = nms_contracts[['Receipt Date' == '2019-07-09']]
276/24: nms_contracts.head(3)
276/25: nms_contracts.head(3)
276/26: nms.shape()
276/27: nms_contracts.shape
276/28: nms_contracts.shape
276/29: nms = nms_contracts[[GRN]]
276/30: nms = nms_contracts[['GRN']]
276/31: nms = nms_contracts[['GRN']]
276/32: nms = nms_contracts[['Receipt Date' == '2019-07-09']]
276/33: nms = nms_contracts[['GRN']]
276/34: nms
276/35: nms_contracts.shape
276/36: list(nms_contracts)
276/37: nms = nms_contracts[['GRN']]
276/38: nms = nms_contracts[['Suppier Name']]
276/39: nms
276/40: nms = nms_contracts[['Suppier Name', 'Description', 'Line Total ', 'Procurement Type']]
276/41: nms
276/42: nms_micros = nms['Procurement Type' == 'Micro']
276/43: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/44: nms_micros.shape
276/45: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/46: nms_frameworks.shape
276/47: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/48: nms_third_party.shape
276/49:
import pandas as pd
import numpy as np
276/50: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/51: nms_contracts.head(3)
276/52: nms_contracts.shape
276/53: list(nms_contracts)
276/54: nms = nms_contracts[['Receipt Date ', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
276/55: nms
276/56: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/57: nms_micros.shape
276/58: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/59: nms_frameworks.shape
276/60: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/61: nms_third_party.shape
276/62: nms
276/63:
import pandas as pd
import numpy as np
276/64: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/65: nms_contracts.head(3)
276/66: nms_contracts.shape
276/67: list(nms_contracts)
276/68: nms = nms_contracts[['Receipt Date ', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
276/69: nms
276/70: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/71: nms_micros.shape
276/72: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/73: nms_frameworks.shape
276/74: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/75: nms_third_party.shape
276/76: nms
276/77: nms
276/78: ### Top procured item
276/79: nms.sort_values(['Line Total'], ascending=False)
276/80: nms.sort_values(['Line Total'], ascending=[False])
276/81: nms
276/82: nms.to_csv('nms_details.csv')
276/83:
# nms.sort_values(['Line Total'], ascending=[False])

nms.sum()
276/84:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/85:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/86: nms['Line Total'].sum()
276/87: nms[nms['Line Total']].sum()
276/88: nms['Line Total'].sum()
276/89: nms.sum()
276/90:
import pandas as pd
import numpy as np
276/91: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/92: nms_contracts.head(3)
276/93: nms_contracts.shape
276/94: list(nms_contracts)
276/95:

nms = nms_contracts[['Receipt Date ', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
276/96: nms
276/97: nms.to_csv('nms_details.csv')
276/98:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/99: nms.sum()
276/100: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/101: nms_micros.shape
276/102: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/103: nms_frameworks.shape
276/104: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/105: nms_third_party.shape
276/106:
import pandas as pd
import numpy as np
276/107: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/108:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
276/109: nms_contracts.shape
276/110: list(nms_contracts)
276/111:

nms = nms_contracts[['Receipt Date ', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
276/112: nms
276/113: nms.to_csv('nms_details.csv')
276/114:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/115: nms.sum()
276/116: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/117: nms_micros.shape
276/118: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/119: nms_frameworks.shape
276/120: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/121: nms_third_party.shape
276/122:
import pandas as pd
import numpy as np
276/123: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/124:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
276/125: nms_contracts.shape
276/126: list(nms_contracts)
276/127:

nms = nms_contracts[['Receipt Date ', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
276/128: nms
276/129: nms.to_csv('nms_details.csv')
276/130:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/131: nms.sum()
276/132: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/133: nms_micros.shape
276/134: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/135: nms_frameworks.shape
276/136: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/137: nms_third_party.shape
276/138: nms.sum()
276/139:
import pandas as pd
import numpy as np
276/140: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/141:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
276/142: nms_contracts.shape
276/143: list(nms_contracts)
276/144:

nms = nms_contracts[['Receipt Date ', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
276/145: nms
276/146: nms.to_csv('nms_details.csv')
276/147:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/148: nms.sum()
276/149: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/150: nms_micros.shape
276/151: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/152: nms_frameworks.shape
276/153: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/154: nms_third_party.shape
276/155: nms_contracts.shape
276/156: nms_contracts.duplicated
276/157: nms_contracts.duplicated()
276/158: nms_contracts[nms_contracts.duplicated()]
276/159:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
276/160: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/161:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
276/162: nms_contracts.shape
276/163: nms_contracts[nms_contracts.duplicated()]
276/164: list(nms_contracts)
276/165:

nms = nms_contracts[['Receipt Date ', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
276/166: nms
276/167: nms.to_csv('nms_details.csv')
276/168:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/169: nms.sum()
276/170: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/171: nms_micros.shape
276/172: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/173: nms_frameworks.shape
276/174: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/175: nms_third_party.shape
276/176: nms_contracts.shape
276/177:
plt.hist(nms_contracts['Description'])

plt.show()
278/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
278/2: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
278/3:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
278/4: nms_contracts.shape
278/5:
plt.hist(nms_contracts['Description'])

plt.show()
278/6: nms_contracts[nms_contracts.duplicated()]
278/7: list(nms_contracts)
278/8:

nms = nms_contracts[['Receipt Date ', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
278/9: nms
278/10: nms.to_csv('nms_details.csv')
278/11:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
278/12: nms.sum()
278/13: nms_micros = nms[nms['Procurement Type'] == 'Micro']
278/14: nms_micros.shape
278/15: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
278/16: nms_frameworks.shape
278/17: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
278/18: nms_third_party.shape
276/178:
plt.hist(nms_contracts['Description'], bins=2000)

plt.show()
279/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
279/2: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
279/3:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
279/4: nms_contracts.shape
279/5:
plt.hist(nms_contracts['Description'])

plt.show()
276/179:
plt.scatter(nms_contracts['Description'])

plt.show()
276/180:
plt.scatter(nms_contracts['Description'], nms_contracts['Line Total '])

plt.show()
276/181:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '])

plt.show()
276/182:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.hist(nms_contracts['Description'], bins=2000)

plt.show()
276/183:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '], alpha=0.8)

plt.show()
276/184:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '], c = col, alpha=0.8)

plt.show()
276/185:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '], c = col, alpha=0.75)

plt.show()
276/186:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)
plt.grid = True

col = 'green'

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '], c = col, alpha=0.75)

plt.show()
276/187:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)
plt.grid(True)

col = 'green'

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '], c = col, alpha=0.75)

plt.show()
276/188:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)
plt.grid(True)

col = 'green'

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '], c = col, alpha=0.75)

plt.show()
276/189:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)
plt.grid(False)

col = 'green'

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '], c = col, alpha=0.75)

plt.show()
276/190:
xlab = 'Category'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Category'], nms_contracts['Line Total '], c = col, alpha=0.75)

plt.grid(True)

plt.show()
276/191:
xlab = 'Procurement Type'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.hist(nms_contracts['Description'], bins=2000)

plt.show()
276/192:
xlab = 'Procurement Type'
ylab = 'Line Total'
title = 'Total Contract Value by Procurement Type'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Procurement Type'], nms_contracts['Line Total '], c = col, alpha=0.75)

plt.grid(True)

plt.show()
276/193:
xlab = 'Procurement Type'
ylab = 'Line Total'
title = 'Total Contract Value by Procurement Type'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Procurement Type'], nms_contracts['Line Total '], c = col, alpha=0.75)

plt.show()
276/194:
xlab = 'Receipt Date '
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.hist(nms_contracts['Receipt Date '], bins=2000)

plt.show()
276/195:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
276/196:
nms_contracts[nms_contracts['Category'] == 'ARVS']

nms_contracts.head()
276/197:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS']

acts.head()
276/198:
acts = nms_contracts[nms_contracts['Category '] == 'ARVS']

acts.head()
276/199:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS']

acts.head()
276/200:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS ']

acts.head()
276/201:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS']

acts.head()
276/202:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/203:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()

acts.shape
276/204:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()

# acts.shape
276/205:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()

# acts.shape
276/206: acts.to_csv('acts.csv')
276/207:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/208: acts.shape
276/209: acts.to_csv('acts.csv')
276/210:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.xlabel(xlab)
plt.ylabel(ylab)


plt.show()
276/211:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.plot(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/212:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/213:
xlab = 'Suppliers'
ylab = 'Total Contract Value'


fig = matplotlib.pyplot.gcf()
fig.set_size_inches(18.5, 10.5)

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/214:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(3,4))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/215:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(10,10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/216:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(10,20))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/217:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/218:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/219:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.plot(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/220:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/221:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/222:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/223: suppliers_value = acts[acts['Supplier Name', 'Line Total']]
276/224: acts.groupby('Supplier Name')
276/225: acts_grouped = acts.groupby('Supplier Name')
276/226: acts_grouped = acts.groupby(['Supplier Name'])
276/227:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/228: acts_grouped = acts.groupby(['Suppier Name'])
276/229:
acts_grouped = acts.groupby(['Suppier Name'])

acts_grouped
276/230:
acts_grouped = acts.groupby(['Suppier Name'])

acts_grouped.shape
276/231:
acts_grouped = acts.groupby(['Suppier Name'])

acts_grouped
276/232:
acts_grouped = acts.groupby(['Suppier Name']).sum('Line Total')

acts_grouped
276/233:
acts_grouped = acts.groupby(['Suppier Name']).sum('Line Total ')

acts_grouped
276/234:
acts_grouped = acts.groupby(['Suppier Name']).sum()

acts_grouped
276/235:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/236:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.plot(acts['Suppier Name'])

plt.show()
276/237:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hbar(acts['Suppier Name'])

plt.show()
276/238:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hbar(acts_grouped['Suppier Name'], acts_grouped['Line Total'])

plt.show()
276/239:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.bar(acts_grouped['Suppier Name'], acts_grouped['Line Total'])

plt.show()
276/240:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts_grouped['Suppier Name'], acts_grouped['Line Total'])

plt.show()
276/241:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/242:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
276/243: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/244:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
276/245:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/246: acts.shape
276/247: acts.to_csv('acts.csv')
276/248:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/249:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/250:
labels = acts['Suppier Name']
men_means = [20, 34, 30, 35, 27]
women_means = [25, 32, 34, 20, 25]

x = np.arange(len(labels))  # the label locations
width = 0.35  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, men_means, width, label='Men')
rects2 = ax.bar(x + width/2, women_means, width, label='Women')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('Scores by group and gender')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()

plt.show()
276/251:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/252:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/253: acts_grouped.describe()
276/254: acts_grouped.describe()
276/255:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/256:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True)['Line Total '].count()

acts_grouped_number
276/257:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/258:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/259: acts.tail()
276/260: suppliers = acts[acts['Suppier Name', 'Line Total ']]
276/261: suppliers = acts[acts[['Suppier Name', 'Line Total ']]]
276/262: acts.tail()
276/263: acts.corr(method=1, min_periods=1)
276/264: acts.corr(method='pearson', min_periods=1)
276/265: acts.corr(method='pearson', min_periods=5)
276/266: acts.corr(method='pearson', min_periods=0)
276/267:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR']

cipla.shape
276/268:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR']

cipla.shape
276/269: cipla.tail()
276/270:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.shape
276/271: cipla.tail()
276/272:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/273:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Suppier Name'], acts_grouped['Line Total'])

plt.show()
276/274:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], acts_grouped['Line Total'])

plt.show()
276/275:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date '], acts_grouped['Line Total'])

plt.show()
276/276:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date '], acts_grouped['Line Total'])

plt.show()
276/277:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.plot(cipla['Receipt Date '], acts_grouped['Line Total'])

plt.show()
276/278:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.plot(cipla['Receipt Date '], acts_grouped['Line Total '])

plt.show()
276/279:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '])

plt.show()
276/280:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=12)

plt.show()
276/281:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=20)

plt.show()
276/282:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=50)

plt.show()
276/283:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=50)

plt.show()
276/284:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date '], bins=50)

plt.show()
276/285:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date '], bins=80)

plt.show()
276/286:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date '], bins=80)

plt.show()
276/287:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=80)

plt.show()
276/288:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=100)

plt.show()
276/289:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=200)

plt.show()
276/290:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=500)

plt.show()
276/291:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=1000)

plt.show()
276/292:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=2000)

plt.show()
276/293:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=100)

plt.show()
276/294:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=100)

plt.show()
276/295:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date '], bins=100)

plt.show()
276/296:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date '], cipla['Line Total '], bins=100)

plt.show()
276/297:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date '], cipla['Line Total '])

plt.show()
276/298: cipla.descibe()
276/299:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date '], cipla['Line Total '])

plt.show()
276/300:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date '], acts['Line Total '])

plt.show()
276/301:
xlab = 'Receipt Dates'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date '], acts['Line Total '])

plt.show()
276/302:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(nms_contracts['Suppier Name'])

plt.show()
276/303:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(nms_contracts['Suppier Name', bins=3000])

plt.show()
276/304:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(nms_contracts['Suppier Name'], bins=3000)

plt.show()
276/305:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True).count()

nms_contracts_grouped_by_provider
276/306:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True)['Line Total '].count()

nms_contracts_grouped_by_provider
276/307:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True)['Line Total '].count().reset_index(['count']).sort_values(['count'], ascending=False).head(5)

nms_contracts_grouped_by_provider
276/308:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True)['Line Total '].count().reset_index(['Line Total ']).sort_values(['Line Total '], ascending=False).head(5)

nms_contracts_grouped_by_provider
276/309:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True)['Line Total '].count().sort_values(['Line Total '], ascending=False).head(5)

nms_contracts_grouped_by_provider
276/310:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True)['Line Total '].count()

nms_contracts_grouped_by_provider
276/311:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True).count()

nms_contracts_grouped_by_provider
276/312:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=False).count()

nms_contracts_grouped_by_provider
276/313:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True).count()

nms_contracts_grouped_by_provider
276/314:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True).count()

nms_contracts_grouped_by_provider.to_csv('nms_contracts_grouped_by_provider.csv')

nms_contracts_grouped_by_provider
276/315:
jms = nms_contracts[nms_contracts['Suppier Name']]

jms.shape
276/316:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE ']

jms.shape
276/317:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
276/318:
# Function to strip space

def remove_space(col):
    return col.str.strip()

for(df in nms_contracts):
    print(df)
276/319:
# Function to strip space

def remove_space(col):
    return col.str.strip()

for(df in len(nms_contracts.columns):
    print(df)
276/320:
# Function to strip space

def remove_space(col):
    return col.str.strip()

for(df in len(nms_contracts.columns)):
    print(df)
276/321:
nms_contracts.to_csv('FY_2019-2020_nms.csv')

nms_contracts.head(3)
276/322: nms_contracts.columns
276/323: len(nms_contracts.columns)
276/324:
for item of len(nms_contracts.columns):
    print(item)
276/325:
for item in len(nms_contracts.columns):
    print(item)
276/326:
for item in range(0, len(nms_contracts.columns)):
    print(item)
276/327:
for item in range(len(nms_contracts.columns)):
    print(item)
276/328:
for item enumerate range(len(nms_contracts.columns)):
    print(item)
276/329:
for item, enumerate in range(len(nms_contracts.columns)):
    print(item)
276/330:
for item in range(len(nms_contracts.columns)):
    print(item)
276/331:
for item, value in enumerate(len(nms_contracts.columns)):
    print(item)
276/332:
for item, value in enumerate(nms_contracts.columns):
    print(item)
276/333:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
276/334:
# Function to strip space

def remove_space(col):
    return col.str.strip()

for(item, value in enumerate(nms_contracts.columns)):
    print(value)
276/335:
# Function to strip space

def remove_space(col):
    return col.str.strip()

for(item, value in enumerate(nms_contracts.columns):
    print(value)
276/336:
# Function to strip space

def remove_space(col):
    return col.str.strip()

for item, value in enumerate(nms_contracts.columns):
    print(value)
276/337:
# Function to strip space

def remove_space(val):
    return val.str.strip()

def list_columns(data_frame_columns):
    for item, value in enumerate(data_frame_columns):
    # df[value] = df[value].str.strip
        print(value)
    
list_columns(nms_contracts.columns)
276/338:
# Function to strip space

def remove_space(val):
    return val.str.strip()

def list_columns(data_frame_columns):
    for item, value in enumerate(data_frame_columns):
        df[value] = df[value].str.strip
        print(value)
    
list_columns(nms_contracts.columns)
276/339:
# Function to strip space

def remove_space(val):
    return val.str.strip()

def list_columns(data_frame_columns):
    for item, value in enumerate(data_frame_columns):
        value = value.str.strip
        print(value)
    
list_columns(nms_contracts.columns)
276/340:
# Function to strip space

def remove_space(val):
    return val.str.strip()

def list_columns(data_frame_columns):
    for item, value in enumerate(data_frame_columns):
        value = value.strip
        print(value)
    
list_columns(nms_contracts.columns)
276/341:
# Function to strip space

def remove_space(val):
    return val.strip()

def list_columns(data_frame_columns):
    for item, value in enumerate(data_frame_columns):
        remove_space(value)
        print(value)
    
list_columns(nms_contracts.columns)
276/342:

nms_contracts.columns = nms_contracts.columns.str.strip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/343:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
276/344: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/345:

nms_contracts.head(3)
276/346:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
276/347:

nms_contracts.columns = nms_contracts.columns.str.strip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/348:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/349: acts.shape
276/350: acts.to_csv('acts.csv')
276/351:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/352:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
276/353: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/354:

nms_contracts.head(3)
276/355:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
276/356:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/357:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/358: acts.shape
276/359: acts.to_csv('acts.csv')
276/360:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total '])

plt.show()
276/361:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
276/362:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

for item, value in enumerate(nms_contracts):
    nms_contracts[value] = nms_contracts[value].str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/363:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

for item, value in enumerate(nms_contracts):
    nms_contracts[value] = nms_contracts[value].rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/364:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

for item, value in enumerate(nms_contracts):
    nms_contracts['{}'.format(value)] = nms_contracts['{}'.format(value)].rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/365:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

for item, value in enumerate(nms_contracts):
    nms_contracts['{}'.format(value)] = nms_contracts['{}'.format(value)].str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/366:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
276/367: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/368:

nms_contracts.head(3)
276/369:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
276/370:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

# for item, value in enumerate(nms_contracts):
#     nms_contracts['{}'.format(value)] = nms_contracts['{}'.format(value)].str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/371:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/372: acts.shape
276/373: acts.to_csv('acts.csv')
276/374:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
276/375:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/376:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/377: acts_grouped.describe()
276/378:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/379:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/380: cipla.tail()
276/381:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date '], bins=100)

plt.show()
276/382:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
276/383:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE']

jms.shape
276/384:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/385:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
276/386: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/387:

nms_contracts.head(3)
276/388:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
276/389:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

# for item, value in enumerate(nms_contracts):
#     nms_contracts['{}'.format(value)] = nms_contracts['{}'.format(value)].str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/390:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/391: acts.shape
276/392: acts.to_csv('acts.csv')
276/393:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
276/394:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/395:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/396: acts_grouped.describe()
276/397:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/398:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/399: cipla.tail()
276/400:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
276/401:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date '], bins=100)

plt.show()
276/402:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date'], bins=100)

plt.show()
276/403:
# Configuration options
num_samples_total = 1000
cluster_centers = [(20,20), (4,4)]
num_classes = len(cluster_centers)
276/404:
# Generate data
X, targets = make_blobs(n_samples = num_samples_total, centers = cluster_centers, n_features = num_classes, center_box=(0, 1), cluster_std = 2)
276/405:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE']

jms.shape
276/406:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE']

jms.shape

jms.head(1)
276/407:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

jms.head(1)
276/408:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
276/409: acts.shape
276/410: acts.to_csv('acts.csv')
276/411:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
276/412:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/413:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/414: acts_grouped.describe()
276/415:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/416:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/417: cipla.tail()
276/418:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
276/419:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date'], bins=100)

plt.show()
276/420:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date '], cipla['Line Total'])

plt.show()
276/421:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
276/422: acts.to_csv('acts.csv')
276/423:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
276/424:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/425:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/426: acts_grouped.describe()
276/427:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/428:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/429: cipla.tail()
276/430:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
276/431:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date'], bins=100)

plt.show()
276/432:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
276/433:
xlab = 'Receipt Dates'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date '], acts['Line Total'])

plt.show()
276/434:
xlab = 'Receipt Dates'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date'], acts['Line Total'])

plt.show()
276/435:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
276/436:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/437: acts.shape
276/438: acts.to_csv('acts.csv')
276/439:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
276/440:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/441:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/442: acts_grouped.describe()
276/443:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/444:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/445: cipla.tail()
276/446:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
276/447:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date'], bins=100)

plt.show()
276/448:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
276/449:
xlab = 'Receipt Dates'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date'], acts['Line Total'])

plt.show()
276/450: nms_contracts.shape
276/451:
xlab = 'Receipt Date'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.hist(nms_contracts['Receipt Date'], bins=2000)

plt.show()
276/452:
xlab = 'Procurement Type'
ylab = 'Line Total'
title = 'Total Contract Value by Procurement Type'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Procurement Type'], nms_contracts['Line Total'], c = col, alpha=0.75)

plt.show()
276/453: nms_contracts[nms_contracts.duplicated()]
276/454: list(nms_contracts)
276/455:

nms = nms_contracts[['Receipt Date', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total ', 'Procurement Type', 'Prod Code', 'Category']]
276/456:

nms = nms_contracts[['Receipt Date', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total', 'Procurement Type', 'Prod Code', 'Category']]
276/457:
vrs = nms_contracts[nms_contracts['Category'] == 'VRS']

vrs.shape
276/458:
vrs = nms_contracts[nms_contracts['Category'] == 'VRS                 ']

vrs.shape
276/459: ## ACTS Category
276/460:
acts = nms_contracts[nms_contracts['Category'] == 'ACTS                ']

acts.shape
276/461:
vrs = nms_contracts[nms_contracts['Category'] == 'VRS                 ']

vrs.shape
276/462:
atb = nms_contracts[nms_contracts['Category'] == 'ATB                 ']

atb.shape
276/463:
dsf = nms_contracts[nms_contracts['Category'] == 'DSF                 ']

dsf.shape
276/464: nms_contracts.groupby(['Category']).sum()
276/465: nms_contracts.groupby(['Category']).sum()
276/466: nms_contracts.groupby(['Category']).count()
276/467: nms_contracts.groupby(['Category']).count().sort_values()
276/468: nms_contracts.groupby(['Category']).sum(by='Line Total')
276/469: nms_contracts.groupby(['Category']).sum()
276/470: nms_contracts.groupby(['Category']).count().sort_values(by='Line Total')
276/471: nms_contracts.groupby(['Category']).count().sort_values(by='Line Total', ascending=False)
276/472: nms_contracts.groupby(['Category']).sum().sort_values(by='Line Total', ascending=False)
276/473: nms_contracts.groupby(['Category', 'Suppier']).sum().sort_values(by='Line Total', ascending=False)
276/474: nms_contracts.groupby(['Category', 'Suppier Name']).sum().sort_values(by='Line Total', ascending=False)
276/475:

nms_contracts.groupby(['Category', 'Suppier Name'])['Line Total'].sum().sort_values(by='Line Total', ascending=False)
276/476:

nms_contracts.groupby(['Category', 'Suppier Name'])[['Line Total']].sum().sort_values(by='Line Total', ascending=False)
276/477:

nms_contracts.groupby(['Category'])[['Line Total']].sum().sort_values(by='Line Total', ascending=False)
276/478: nms_contracts.groupby(['Category'])[['Line Total']].count().sort_values(by='Line Total', ascending=False)
276/479:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pd.set_option('display.max_rows', 500)
276/480:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pd.set_option('display.max_rows', 500)
276/481: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/482:

nms_contracts.head(3)
276/483:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
276/484:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

# for item, value in enumerate(nms_contracts):
#     nms_contracts['{}'.format(value)] = nms_contracts['{}'.format(value)].str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/485:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/486: acts.shape
276/487: acts.to_csv('acts.csv')
276/488:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
276/489:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/490:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/491: acts_grouped.describe()
276/492:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/493:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/494: cipla.tail()
276/495:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
276/496:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date'], bins=100)

plt.show()
276/497:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
276/498:
xlab = 'Receipt Dates'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date'], acts['Line Total'])

plt.show()
276/499: nms_contracts.shape
276/500:
xlab = 'Receipt Date'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.hist(nms_contracts['Receipt Date'], bins=2000)

plt.show()
276/501:
xlab = 'Procurement Type'
ylab = 'Line Total'
title = 'Total Contract Value by Procurement Type'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Procurement Type'], nms_contracts['Line Total'], c = col, alpha=0.75)

plt.show()
276/502: nms_contracts[nms_contracts.duplicated()]
276/503: list(nms_contracts)
276/504:

nms = nms_contracts[['Receipt Date', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total', 'Procurement Type', 'Prod Code', 'Category']]
276/505: nms
276/506: nms.to_csv('nms_details.csv')
276/507:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True).count()

nms_contracts_grouped_by_provider.to_csv('nms_contracts_grouped_by_provider.csv')

nms_contracts_grouped_by_provider
276/508:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

jms.head(1)
276/509:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/510: nms.sum()
276/511:

nms_contracts.groupby(['Category'])[['Line Total']].sum().sort_values(by='Line Total', ascending=False)
276/512: nms_contracts.groupby(['Category'])[['Line Total']].count().sort_values(by='Line Total', ascending=False)
276/513:
vrs = nms_contracts[nms_contracts['Category'] == 'VRS                 ']

vrs.shape
276/514:
acts = nms_contracts[nms_contracts['Category'] == 'ACTS                ']

acts.shape
276/515:
atb = nms_contracts[nms_contracts['Category'] == 'ATB                 ']

atb.shape
276/516:
dsf = nms_contracts[nms_contracts['Category'] == 'DSF                 ']

dsf.shape
276/517: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/518: nms_micros.shape
276/519: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/520: nms_frameworks.shape
276/521: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/522: nms_third_party.shape
276/523:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pd.set_option('display.max_rows', 5000)
276/524: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
276/525:

nms_contracts.head(3)
276/526:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
276/527:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

# for item, value in enumerate(nms_contracts):
#     nms_contracts['{}'.format(value)] = nms_contracts['{}'.format(value)].str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
276/528:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
276/529: acts.shape
276/530: acts.to_csv('acts.csv')
276/531:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
276/532:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
276/533:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
276/534: acts_grouped.describe()
276/535:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
276/536:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
276/537: cipla.tail()
276/538:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
276/539:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date'], bins=100)

plt.show()
276/540:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
276/541:
xlab = 'Receipt Dates'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date'], acts['Line Total'])

plt.show()
276/542: nms_contracts.shape
276/543:
xlab = 'Receipt Date'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.hist(nms_contracts['Receipt Date'], bins=2000)

plt.show()
276/544:
xlab = 'Procurement Type'
ylab = 'Line Total'
title = 'Total Contract Value by Procurement Type'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Procurement Type'], nms_contracts['Line Total'], c = col, alpha=0.75)

plt.show()
276/545: nms_contracts[nms_contracts.duplicated()]
276/546: list(nms_contracts)
276/547:

nms = nms_contracts[['Receipt Date', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total', 'Procurement Type', 'Prod Code', 'Category']]
276/548: nms
276/549: nms.to_csv('nms_details.csv')
276/550:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True).count()

nms_contracts_grouped_by_provider.to_csv('nms_contracts_grouped_by_provider.csv')

nms_contracts_grouped_by_provider
276/551:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

jms.head(1)
276/552:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
276/553: nms.sum()
276/554:

nms_contracts.groupby(['Category'])[['Line Total']].sum().sort_values(by='Line Total', ascending=False)
276/555: nms_contracts.groupby(['Category'])[['Line Total']].count().sort_values(by='Line Total', ascending=False)
276/556:
vrs = nms_contracts[nms_contracts['Category'] == 'VRS                 ']

vrs.shape
276/557:
acts = nms_contracts[nms_contracts['Category'] == 'ACTS                ']

acts.shape
276/558:
atb = nms_contracts[nms_contracts['Category'] == 'ATB                 ']

atb.shape
276/559:
dsf = nms_contracts[nms_contracts['Category'] == 'DSF                 ']

dsf.shape
276/560: nms_micros = nms[nms['Procurement Type'] == 'Micro']
276/561: nms_micros.shape
276/562: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
276/563: nms_frameworks.shape
276/564: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
276/565: nms_third_party.shape
276/566:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True).count().sort_values(by='Line Total', ascending=False)

nms_contracts_grouped_by_provider.to_csv('nms_contracts_grouped_by_provider.csv')

nms_contracts_grouped_by_provider
276/567:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True).count().sort_values(by='Line Total', ascending=False)

nms_contracts_grouped_by_provider.to_csv('nms_contracts_grouped_by_provider.csv')

nms_contracts_grouped_by_provider
276/568:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name'], sort=True).sum().sort_values(by='Line Total', ascending=False)
276/569:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name'], sort=True).sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value
276/570:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value
276/571:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True)[['Line Total']].count().sort_values(by='Line Total', ascending=False)

nms_contracts_grouped_by_provider.to_csv('nms_contracts_grouped_by_provider.csv')

nms_contracts_grouped_by_provider
276/572:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value
276/573:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value[top_supplier_by_value['Suppier Name'] == 'JOINT MEDICAL STORES']
276/574:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value[top_supplier_by_value['Suppier Name'] == 'JOINT MEDICAL STORES']
276/575:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value['Suppier Name' == 'JOINT MEDICAL STORES']
276/576:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value
276/577:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value['Suppier Name' == 'MOH GLOBALFUND']
276/578:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value[top_supplier_by_value['Suppier Name' == 'MOH GLOBALFUND']]
276/579:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value
276/580:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

jms.head(1)
276/581:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

# jms.head(1)
276/582:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

jms.head()
276/583:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

jms
276/584: jms.groupby('Category')sum().sort_values('Line Total', ascending=False)
276/585: jms.groupby('Category').sum().sort_values('Line Total', ascending=False)
276/586: jms.groupby('Category')[['Qty', 'Line Total']].sum().sort_values('Line Total', ascending=False)
276/587: jms.describe()
276/588: jms.describe(['Qty', 'Line Total'])
276/589: jms.describe('Qty', 'Line Total')
276/590: jms.describe()['Qty', 'Line Total']
276/591: jms.describe()
276/592: jms['Line Total'].sum()
280/1:
%matplotlib notebook
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

fruits = pd.read_table('fruit_data_with_colors.txt')
281/1:
%matplotlib notebook
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

fruits = pd.read_table('fruit_data_with_colors.txt')
281/2: fruits.head()
281/3: fruits.shape
281/4:

lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))

lookup_fruit_name
282/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pd.set_option('display.max_rows', 5000)
282/2: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
282/3:

nms_contracts.head(3)
282/4:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
282/5:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

# for item, value in enumerate(nms_contracts):
#     nms_contracts['{}'.format(value)] = nms_contracts['{}'.format(value)].str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
282/6:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
282/7: acts.shape
282/8: acts.to_csv('acts.csv')
282/9:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
282/10:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
282/11:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
282/12: acts_grouped.describe()
282/13:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
282/14:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
282/15: cipla.tail()
282/16:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
282/17:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date'], bins=100)

plt.show()
282/18:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
282/19:
xlab = 'Receipt Dates'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date'], acts['Line Total'])

plt.show()
282/20: nms_contracts.shape
282/21:
xlab = 'Receipt Date'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.hist(nms_contracts['Receipt Date'], bins=2000)

plt.show()
282/22:
xlab = 'Procurement Type'
ylab = 'Line Total'
title = 'Total Contract Value by Procurement Type'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Procurement Type'], nms_contracts['Line Total'], c = col, alpha=0.75)

plt.show()
282/23: nms_contracts[nms_contracts.duplicated()]
282/24: list(nms_contracts)
282/25:

nms = nms_contracts[['Receipt Date', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total', 'Procurement Type', 'Prod Code', 'Category']]
282/26: nms
282/27: nms.to_csv('nms_details.csv')
282/28:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True)[['Line Total']].count().sort_values(by='Line Total', ascending=False)

nms_contracts_grouped_by_provider.to_csv('nms_contracts_grouped_by_provider.csv')

nms_contracts_grouped_by_provider
282/29:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value
282/30:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

jms
282/31: jms.groupby('Category')[['Qty', 'Line Total']].sum().sort_values('Line Total', ascending=False)
282/32: jms['Line Total'].sum()
282/33:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
282/34: nms.sum()
282/35:

nms_contracts.groupby(['Category'])[['Line Total']].sum().sort_values(by='Line Total', ascending=False)
282/36: nms_contracts.groupby(['Category'])[['Line Total']].count().sort_values(by='Line Total', ascending=False)
282/37:
vrs = nms_contracts[nms_contracts['Category'] == 'VRS                 ']

vrs.shape
282/38:
acts = nms_contracts[nms_contracts['Category'] == 'ACTS                ']

acts.shape
282/39:
atb = nms_contracts[nms_contracts['Category'] == 'ATB                 ']

atb.shape
282/40:
dsf = nms_contracts[nms_contracts['Category'] == 'DSF                 ']

dsf.shape
282/41: nms_micros = nms[nms['Procurement Type'] == 'Micro']
282/42: nms_micros.shape
282/43: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
282/44: nms_frameworks.shape
282/45: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
282/46: nms_third_party.shape
282/47:
lookup_drug_name = dict(zip(nms_contracts['Prod Code'].unique(), nms_contracts['Description'].unique()))

lookup_drug_name
282/48:
lookup_drug_name = dict(zip(nms_contracts['Category'].unique(), nms_contracts['Description'].unique()))

lookup_drug_name
281/5:

lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))

lookup_fruit_name
281/6:
X = fruits[['mass', 'width', 'height']]
y = fruits['fruit_label']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
281/7:
from matplotlib import cm
cmap = cm.get_cmap('gnuplot')
scatter = pd.scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(12,12), cmap=cmap)
281/8:
%matplotlib notebook
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

fruits = pd.read_table('fruit_data_with_colors.txt')
281/9: fruits.head()
281/10: fruits.shape
281/11:

lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))

lookup_fruit_name
281/12:
X = fruits[['mass', 'width', 'height']]
y = fruits['fruit_label']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
281/13:
from matplotlib import cm
cmap = cm.get_cmap('gnuplot')
scatter = pd.scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(12,12), cmap=cmap)
281/14:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(12,12), cmap=cmap)
281/15:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(20,10), cmap=cmap)
281/16:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(5,10), cmap=cmap)
281/17:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(5,10), cmap=cmap);
281/18:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(5, 12), cmap=cmap);
281/19:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(5, 15), cmap=cmap);
281/20:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(5, 5), cmap=cmap);
281/21:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='x', s=40, hist_kwds={'bins':15}, figsize=(5, 5), cmap=cmap);
281/22:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(5, 5), cmap=cmap);
282/49:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Description'])

plt.show()
282/50:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(50, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Description'])

plt.show()
282/51:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(50, 50))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Description'])

plt.show()
282/52:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Description'])

plt.show()
282/53:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
282/54:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
282/55:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Description'], cipla['Receipt Date'])

plt.show()
282/56:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
281/23:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_suplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c, y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
281/24:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c, y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
281/25:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c, y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
281/26:
%matplotlib notebook
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

fruits = pd.read_table('fruit_data_with_colors.txt')
281/27: fruits.head()
281/28: fruits.shape
281/29:

lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))

lookup_fruit_name
281/30:
X = fruits[['mass', 'width', 'height']]
y = fruits['fruit_label']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
281/31:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(5, 5), cmap=cmap);
281/32:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c, y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
281/33:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c, y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
281/34:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c, y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
281/35:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
281/36:
from matplotlib import cm
from pandas.plotting import scatter_matrix
cmap = cm.get_cmap('gnuplot')
scatter_matrix(X_train, c=y_train, marker='o', s=40, hist_kwds={'bins':15}, figsize=(5, 5), cmap=cmap);
281/37:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
281/38:
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker='o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')
ax.set_zlabel('color_score')
plt.show()
283/1:
import numpy as np;
import pandas as pd;
283/2: values = pd.Series(['apple', 'orange', 'apple', 'apple'] * 2)
283/3: values
283/4: pd.unique
283/5: pd.unique(values)
283/6: pd.value_counts()
283/7: pd.value_counts(values)
284/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pd.set_option('display.max_rows', 5000)
284/2: nms_contracts = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name='Details')
284/3:

nms_contracts.head(3)
284/4:
lookup_drug_name = dict(zip(nms_contracts['Category'].unique(), nms_contracts['Description'].unique()))

lookup_drug_name
284/5:
for item, value in enumerate(nms_contracts.columns):
    print(item, value)
284/6:

nms_contracts.columns = nms_contracts.columns.str.rstrip()

# for item, value in enumerate(nms_contracts):
#     nms_contracts['{}'.format(value)] = nms_contracts['{}'.format(value)].str.rstrip()

nms_contracts.to_csv('FY_2019-2020_nms.csv')
284/7:
acts = nms_contracts[nms_contracts['Category'] == 'ARVS                ']

acts.head()
284/8: acts.shape
284/9: acts.to_csv('acts.csv')
284/10:
xlab = 'Suppliers'
ylab = 'Total Contract Value'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Suppier Name'], acts['Line Total'])

plt.show()
284/11:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Suppier Name'])

plt.show()
284/12:
acts_grouped = acts.groupby(['Suppier Name'], sort=True).sum()

acts_grouped
284/13: acts_grouped.describe()
284/14:
acts_grouped_number = acts.groupby(['Suppier Name'], sort=True).count()

acts_grouped_number
284/15:
cipla = acts[acts['Suppier Name'] == 'CIPLA QUALITY CHEMICAL INDUSTR                    ']

cipla.to_csv('cipla.csv')

cipla.shape
284/16: cipla.tail()
284/17:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(acts['Receipt Date'], bins=100)

plt.show()
284/18:
xlab = 'Suppliers'
ylab = 'Total Number of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.hist(cipla['Receipt Date'], bins=100)

plt.show()
284/19:
xlab = 'Suppliers'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(cipla['Receipt Date'], cipla['Line Total'])

plt.show()
284/20:
xlab = 'Receipt Dates'
ylab = 'Total Value of Contracts'

plt.figure(figsize=(20, 10))

plt.xlabel(xlab)
plt.ylabel(ylab)

plt.scatter(acts['Receipt Date'], acts['Line Total'])

plt.show()
284/21: nms_contracts.shape
284/22:
xlab = 'Receipt Date'
ylab = 'Line Total'
title = 'Total Contract Value by category'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

plt.hist(nms_contracts['Receipt Date'], bins=2000)

plt.show()
284/23:
xlab = 'Procurement Type'
ylab = 'Line Total'
title = 'Total Contract Value by Procurement Type'

plt.xlabel(xlab)
plt.ylabel(ylab)
plt.title(title)

col = 'green'

plt.scatter(nms_contracts['Procurement Type'], nms_contracts['Line Total'], c = col, alpha=0.75)

plt.show()
284/24: nms_contracts[nms_contracts.duplicated()]
284/25: list(nms_contracts)
284/26:

nms = nms_contracts[['Receipt Date', 'Budget Holder', 'Suppier Name', 'Description', 'Line Total', 'Procurement Type', 'Prod Code', 'Category']]
284/27: nms
284/28: nms.to_csv('nms_details.csv')
284/29:
nms_contracts_grouped_by_provider = nms_contracts.groupby(['Suppier Name'], sort=True)[['Line Total']].count().sort_values(by='Line Total', ascending=False)

nms_contracts_grouped_by_provider.to_csv('nms_contracts_grouped_by_provider.csv')

nms_contracts_grouped_by_provider
284/30:

top_supplier_by_value = nms_contracts.groupby(['Suppier Name', 'Category'], sort=True)[['Qty', 'Line Total']].sum().sort_values(by='Line Total', ascending=False)

top_supplier_by_value
284/31:
jms = nms_contracts[nms_contracts['Suppier Name'] == 'JOINT MEDICAL STORE                               ']

jms.shape

jms
284/32: jms.groupby('Category')[['Qty', 'Line Total']].sum().sort_values('Line Total', ascending=False)
284/33: jms['Line Total'].sum()
284/34:
# nms.sort_values(['Line Total'], ascending=[False])

nms.describe()
284/35: nms.sum()
284/36:

nms_contracts.groupby(['Category'])[['Line Total']].sum().sort_values(by='Line Total', ascending=False)
284/37: nms_contracts.groupby(['Category'])[['Line Total']].count().sort_values(by='Line Total', ascending=False)
284/38:
vrs = nms_contracts[nms_contracts['Category'] == 'VRS                 ']

vrs.shape
284/39:
acts = nms_contracts[nms_contracts['Category'] == 'ACTS                ']

acts.shape
284/40:
atb = nms_contracts[nms_contracts['Category'] == 'ATB                 ']

atb.shape
284/41:
dsf = nms_contracts[nms_contracts['Category'] == 'DSF                 ']

dsf.shape
284/42: nms_micros = nms[nms['Procurement Type'] == 'Micro']
284/43: nms_micros.shape
284/44: nms_frameworks = nms[nms['Procurement Type'] == 'Frame Work Contracts']
284/45: nms_frameworks.shape
284/46: nms_third_party = nms[nms['Procurement Type'] == 'Third Party']
284/47: nms_third_party.shape
284/48: nms_contracts.groupby(['Category'])[['Line Total']].count().sort_values(by='Line Total', ascending=False)
284/49: pd.value_counts(nms_contracts[nms_contracts['category']])
284/50: pd.value_counts(nms_contracts[nms_contracts['Category']])
283/8: values = pd.Series([0, 1, 0, 0] * 2)
283/9: dim = pd.Series(['apple' 'orange'])
283/10: values
283/11: dim.take(values)
283/12: dim = pd.Series(['apple', 'orange'])
283/13: values
283/14: dim.take(values)
283/15: fruits = ['apple', 'orange', 'apple', 'apple']
283/16: N = len(fruits)
283/17:
df = pd.DataFrame({'fruit': fruits,
                   'basket_id': np.arange(N),
                   'count': np.random.randint(3, 15, size=N),
                   'weight': np.random.uniform(0, 4, size=N)},
                 columns=['basket_id', 'fruit', 'count', 'weight'])
283/18: df
283/19: fruit_cat = df['fruit'].astype('category')
283/20: fruit_cat
284/51: nms_cat = nms_contracts['Category'].astype('category')
284/52: nms_cat
284/53: nms_cat
284/54: pd.value_counts(nms_contracts['Category'])
283/21: c = fruit_cat.values
283/22: type(c)
283/23: c.categories
283/24: c.codes
284/55: nms_cat
284/56: type(nms_cat)
284/57: type(nms_cat)
284/58: nms.categories
284/59: nms.values
284/60: c = nms.values
284/61: c = nms.values
284/62: c.categories
284/63: c = nms_cat.values
284/64: c.categories
284/65: c.categories
284/66: c.codes
283/25: df['fruit'] = df['fruit'].astype('category')
283/26: df.fruit
283/27:
my_categories = pd.Categorical(['foo', 'bar', 'baz', 'foo', 'bar'])

my_categories
285/1: path = 'datasets/bitly_usagov/example.txt'
285/2: open(path).readline()
285/3:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
285/4: records[0]
285/5: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
285/6: time_zones[:10]
285/7:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
285/8: counts = getCount(time_zones)
285/9: counts['America/New_York']
285/10: len(time_zones)
285/11:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
285/12: top_counts(counts)
285/13: from collections import Counter
285/14: counts = Counter(time_zones)
285/15: counts.most_common(10)
285/16: ### Counting Time Zones with pandas
285/17: import pandas as pd
285/18: frame = pd.DataFrame(records)
285/19: frame.info()
285/20: frame['tz'][:10]
285/21: tz_counts = frame['tz'][:10].value_counts()
285/22: tz_counts
285/23: tz_counts = frame['tz'].value_counts()
285/24: tz_counts[:10]
285/25: clean_tz = frame['tz'].fillna('Missing')
285/26: clean_tz[clean_tz == ''] = 'Unknown'
285/27: tz_counts = clean_tz.value_counts()
285/28: tz_counts[:10]
285/29: import seaborn as sns
285/30: subset = tz_counts[:10]
285/31: sns.barplot(y=subset.index, x=subset.values)
285/32: sns.barplot(y=subset.index, x=subset.values);
285/33: path = 'datasets/bitly_usagov/example.txt'
285/34: open(path).readline()
285/35:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
285/36: records[0]
285/37: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
285/38: time_zones[:10]
285/39:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
285/40: counts = getCount(time_zones)
285/41: counts['America/New_York']
285/42: len(time_zones)
285/43:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
285/44: top_counts(counts)
285/45: from collections import Counter
285/46: counts = Counter(time_zones)
285/47: counts.most_common(10)
285/48: import pandas as pd
285/49: frame = pd.DataFrame(records)
285/50: frame.info()
285/51: frame['tz'][:10]
285/52: tz_counts = frame['tz'].value_counts()
285/53: tz_counts[:10]
285/54: clean_tz = frame['tz'].fillna('Missing')
285/55: clean_tz[clean_tz == ''] = 'Unknown'
285/56: tz_counts = clean_tz.value_counts()
285/57: tz_counts[:10]
285/58: import seaborn as sns
285/59: subset = tz_counts[:10]
285/60: sns.barplot(y=subset.index, x=subset.values);
285/61: path = 'datasets/bitly_usagov/example.txt'
285/62: open(path).readline()
285/63:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
285/64: records[0]
285/65: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
285/66: time_zones[:10]
285/67:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
285/68: counts = getCount(time_zones)
285/69: counts['America/New_York']
285/70: len(time_zones)
285/71:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
285/72: top_counts(counts)
285/73: from collections import Counter
285/74: counts = Counter(time_zones)
285/75: counts.most_common(10)
285/76: import pandas as pd
285/77: frame = pd.DataFrame(records)
285/78: frame.info()
285/79: frame['tz'][:10]
285/80: tz_counts = frame['tz'].value_counts()
285/81: tz_counts[:10]
285/82: clean_tz = frame['tz'].fillna('Missing')
285/83: clean_tz[clean_tz == ''] = 'Unknown'
285/84: tz_counts = clean_tz.value_counts()
285/85: tz_counts[:10]
285/86: import seaborn as sns
285/87: subset = tz_counts[:10]
285/88: sns.barplot(y=subset.index, x=subset.values);
286/1: path = 'datasets/bitly_usagov/example.txt'
286/2: open(path).readline()
286/3:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
286/4: records[0]
286/5: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
286/6: time_zones[:10]
286/7:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
286/8: counts = getCount(time_zones)
286/9: counts['America/New_York']
286/10: len(time_zones)
286/11:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
286/12: top_counts(counts)
286/13: from collections import Counter
286/14: counts = Counter(time_zones)
286/15: counts.most_common(10)
286/16: import pandas as pd
286/17: frame = pd.DataFrame(records)
286/18: frame.info()
286/19: frame['tz'][:10]
286/20: tz_counts = frame['tz'].value_counts()
286/21: tz_counts[:10]
286/22: clean_tz = frame['tz'].fillna('Missing')
286/23: clean_tz[clean_tz == ''] = 'Unknown'
286/24: tz_counts = clean_tz.value_counts()
286/25: tz_counts[:10]
286/26:
import seaborn as sns
%config InlineBackend.figure_format = 'retina'
286/27: subset = tz_counts[:10]
286/28: sns.barplot(y=subset.index, x=subset.values);
286/29: path = 'datasets/bitly_usagov/example.txt'
286/30: open(path).readline()
286/31:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
286/32: records[0]
286/33: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
286/34: time_zones[:10]
286/35:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
286/36: counts = getCount(time_zones)
286/37: counts['America/New_York']
286/38: len(time_zones)
286/39:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
286/40: top_counts(counts)
286/41: from collections import Counter
286/42: counts = Counter(time_zones)
286/43: counts.most_common(10)
286/44: import pandas as pd
286/45: frame = pd.DataFrame(records)
286/46: frame.info()
286/47: frame['tz'][:10]
286/48: tz_counts = frame['tz'].value_counts()
286/49: tz_counts[:10]
286/50: clean_tz = frame['tz'].fillna('Missing')
286/51: clean_tz[clean_tz == ''] = 'Unknown'
286/52: tz_counts = clean_tz.value_counts()
286/53: tz_counts[:10]
286/54:
import seaborn as sns
# %config InlineBackend.figure_format = 'retina'
286/55: subset = tz_counts[:10]
286/56: sns.barplot(y=subset.index, x=subset.values);
286/57: path = 'datasets/bitly_usagov/example.txt'
286/58: open(path).readline()
286/59:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
286/60: records[0]
286/61: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
286/62: time_zones[:10]
286/63:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
286/64: counts = getCount(time_zones)
286/65: counts['America/New_York']
286/66: len(time_zones)
286/67:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
286/68: top_counts(counts)
286/69: from collections import Counter
286/70: counts = Counter(time_zones)
286/71: counts.most_common(10)
286/72: import pandas as pd
286/73: frame = pd.DataFrame(records)
286/74: frame.info()
286/75: frame['tz'][:10]
286/76: tz_counts = frame['tz'].value_counts()
286/77: tz_counts[:10]
286/78: clean_tz = frame['tz'].fillna('Missing')
286/79: clean_tz[clean_tz == ''] = 'Unknown'
286/80: tz_counts = clean_tz.value_counts()
286/81: tz_counts[:10]
286/82:
import seaborn as sns
# %config InlineBackend.figure_format = 'retina'
286/83: subset = tz_counts[:10]
286/84: sns.barplot(y=subset.index, x=subset.values);
286/85: path = 'datasets/bitly_usagov/example.txt'
286/86: open(path).readline()
286/87:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
286/88: records[0]
286/89: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
286/90: time_zones[:10]
286/91:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
286/92: counts = getCount(time_zones)
286/93: counts['America/New_York']
286/94: len(time_zones)
286/95:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
286/96: top_counts(counts)
286/97: from collections import Counter
286/98: counts = Counter(time_zones)
286/99: counts.most_common(10)
286/100: import pandas as pd
286/101: frame = pd.DataFrame(records)
286/102: frame.info()
286/103: frame['tz'][:10]
286/104: tz_counts = frame['tz'].value_counts()
286/105: tz_counts[:10]
286/106: clean_tz = frame['tz'].fillna('Missing')
286/107: clean_tz[clean_tz == ''] = 'Unknown'
286/108: tz_counts = clean_tz.value_counts()
286/109: tz_counts[:10]
286/110: import seaborn as sns
286/111: subset = tz_counts[:10]
286/112: sns.barplot(y=subset.index, x=subset.values);
286/113: path = 'datasets/bitly_usagov/example.txt'
286/114: open(path).readline()
286/115:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
286/116: records[0]
286/117: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
286/118: time_zones[:10]
286/119:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
286/120: counts = getCount(time_zones)
286/121: counts['America/New_York']
286/122: len(time_zones)
286/123:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
286/124: top_counts(counts)
286/125: from collections import Counter
286/126: counts = Counter(time_zones)
286/127: counts.most_common(10)
286/128: import pandas as pd
286/129: frame = pd.DataFrame(records)
286/130: frame.info()
286/131: frame['tz'][:10]
286/132: tz_counts = frame['tz'].value_counts()
286/133: tz_counts[:10]
286/134: clean_tz = frame['tz'].fillna('Missing')
286/135: clean_tz[clean_tz == ''] = 'Unknown'
286/136: tz_counts = clean_tz.value_counts()
286/137: tz_counts[:10]
286/138:
import seaborn as sns
%config InlineBackend.figure_format = 'retina'
286/139: subset = tz_counts[:10]
286/140: sns.barplot(y=subset.index, x=subset.values);
287/1:
import numpy as np;
import pandas as pd;
287/2: values = pd.Series(['apple', 'orange', 'apple', 'apple'] * 2)
287/3: values
287/4: pd.unique(values)
287/5: pd.value_counts(values)
287/6: values = pd.Series([0, 1, 0, 0] * 2)
287/7: dim = pd.Series(['apple', 'orange'])
287/8: values
287/9: dim.take(values)
287/10: fruits = ['apple', 'orange', 'apple', 'apple']
287/11: N = len(fruits)
287/12:
df = pd.DataFrame({'fruit': fruits,
                   'basket_id': np.arange(N),
                   'count': np.random.randint(3, 15, size=N),
                   'weight': np.random.uniform(0, 4, size=N)},
                 columns=['basket_id', 'fruit', 'count', 'weight'])
287/13: df
287/14: fruit_cat = df['fruit'].astype('category')
287/15: fruit_cat
287/16: c = fruit_cat.values
287/17: type(c)
287/18: c.categories
287/19: c.codes
287/20: df['fruit'] = df['fruit'].astype('category')
287/21: df.fruit
287/22:
my_categories = pd.Categorical(['foo', 'bar', 'baz', 'foo', 'bar'])

my_categories
286/141: path = 'datasets/bitly_usagov/example.txt'
286/142: open(path).readline()
286/143:
import json

path = 'datasets/bitly_usagov/example.txt'

records = [json.loads(line) for line in open(path)]
286/144: records[0]
286/145: time_zones = [rec['tz'] for rec in records if 'tz' in rec]
286/146: time_zones[:10]
286/147:
from collections import defaultdict

def getCount(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts
286/148: counts = getCount(time_zones)
286/149: counts['America/New_York']
286/150: len(time_zones)
286/151:
def top_counts(count_dict, n=10):
    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
    value_key_pairs.sort()
    return value_key_pairs[-n:]
286/152: top_counts(counts)
286/153: from collections import Counter
286/154: counts = Counter(time_zones)
286/155: counts.most_common(10)
286/156: import pandas as pd
286/157: frame = pd.DataFrame(records)
286/158: frame.info()
286/159: frame['tz'][:10]
286/160: tz_counts = frame['tz'].value_counts()
286/161: tz_counts[:10]
286/162: clean_tz = frame['tz'].fillna('Missing')
286/163: clean_tz[clean_tz == ''] = 'Unknown'
286/164: tz_counts = clean_tz.value_counts()
286/165: tz_counts[:10]
286/166:
import seaborn as sns
%config InlineBackend.figure_format = 'retina'
286/167: subset = tz_counts[:10]
286/168: sns.barplot(y=subset.index, x=subset.values);
288/1:
data = [1, 2, 1, 3, 3, 1, 4, 2]

%matplotlib inline
import matplotlib as pyplot
plt.hist(data)
288/2:
data = [1, 2, 1, 3, 3, 1, 4, 2]

%matplotlib inline
import matplotlib as plt
plt.hist(data)
288/3:
data = [1, 2, 1, 3, 3, 1, 4, 2]

%matplotlib inline
import matplotlib.pyplot as plt
plt.hist(data)
288/4:
data = [1, 2, 1, 3, 3, 1, 4, 2]

%matplotlib inline
import matplotlib.pyplot as plt
plt.hist(data);
288/5:
%matplotlib inline
import matplotlib.pyplot as plt
%config InlineBackend.figure_format = 'retina'
288/6:
data = [1, 2, 1, 3, 3, 1, 4, 2]


plt.hist(data);
288/7:
# %matplotlib inline
import matplotlib.pyplot as plt
%config InlineBackend.figure_format = 'retina'
288/8:
data = [1, 2, 1, 3, 3, 1, 4, 2]


plt.hist(data);
288/9:
%matplotlib inline
import matplotlib.pyplot as plt
%config InlineBackend.figure_format = 'retina'
288/10:
data = [1, 2, 1, 3, 3, 1, 4, 2]


plt.hist(data);
288/11:
%matplotlib inline
%pylab inline
import matplotlib.pyplot as plt
%config InlineBackend.figure_format = 'retina'
288/12:
%matplotlib inline
import matplotlib.pyplot as plt
%config InlineBackend.figure_format = 'retina'
288/13:
data = [1, 2, 1, 3, 3, 1, 4, 2]

%pylab inline
plt.hist(data);
288/14:
data = [1, 2, 1, 3, 3, 1, 4, 2]

plt.hist(data);
288/15:
data = [1, 2, 1, 3, 3, 1, 4, 2]

plt.hist(data, bins=5);
288/16:
data = [1, 2, 1, 3, 3, 1, 4, 2]

plt.hist(data, bins=55);
288/17:
data = [1, 2, 1, 3, 3, 1, 4, 2]

plt.hist(data);
288/18:
%matplotlib inline
import matplotlib.pyplot as plt
%config InlineBackend.figure_format = 'retina'
import seaborn as sns
288/19:
data = [1, 2, 1, 3, 3, 1, 4, 2]

plt.hist(data);
288/20:
%matplotlib inline
import matplotlib.pyplot as plt
%config InlineBackend.figure_format = 'retina'
288/21:
import seaborn as sns

data = [1, 2, 1, 3, 3, 1, 4, 2]

plt.hist(data);
288/22:
import seaborn as sns

data = [1, 2, 1, 3, 3, 1, 4, 2]

plt.hist(data);

plt.xlabel('index')
288/23:
import seaborn as sns

data = [1, 2, 1, 3, 3, 1, 4, 2]

plt.hist(data);

plt.xlabel('index');
288/24:
import unicodecsv

def read_csv(filename):
    with open(filename, 'rb') as f:
        reader = unicodecsv.DictReader(f)
        return list(reader)
    
enrolements = read_csv('enrollments.csv')
daily_engagements = read_csv('daily_engagement_full.csv')
project_submissions = read_csv('project_submissions.csv')
288/25:
len(enrollments)

unique_enrolled_students = set()
for enrollment in enrollments:
    unique_enrolled_students.add(enrollment['account_key'])
len(unique_enrolled_students)

len(daily_engagement)

unique_engagement_students = set()
for engagement_record in daily_engagement:
    unique_engagement_students.add(engagement_record['acct'])
len(unique_engagement_students)

len(project_submissions)

unique_project_submitters = set()
for submission in project_submissions:
    unique_project_submitters.add(submission['account_key'])
len(unique_project_submitters)
288/26:
for engagement_record in daily_engagement:
    engagement_record['account_key'] = engagement_record['acct']
    del[engagement_record['acct']]
288/27:
for enrollment in enrollments:
    student = enrollment['account_key']
    if student not in unique_engagement_students:
        print enrollment
        break
288/28:
num_problem_students = 0
for enrollment in enrollments:
    student = enrollment['account_key']
    if (student not in unique_engagement_students and 
            enrollment['join_date'] != enrollment['cancel_date']):
        print enrollment
        num_problem_students += 1

num_problem_students
288/29:
import unicodecsv

def read_csv(filename):
    with open(filename, 'rb') as f:
        reader = unicodecsv.DictReader(f)
        return list(reader)
    
enrollements = read_csv('enrollments.csv')
daily_engagements = read_csv('daily_engagement_full.csv')
project_submissions = read_csv('project_submissions.csv')
288/30:
len(enrollments)

unique_enrolled_students = set()
for enrollment in enrollments:
    unique_enrolled_students.add(enrollment['account_key'])
len(unique_enrolled_students)

len(daily_engagement)

unique_engagement_students = set()
for engagement_record in daily_engagement:
    unique_engagement_students.add(engagement_record['acct'])
len(unique_engagement_students)

len(project_submissions)

unique_project_submitters = set()
for submission in project_submissions:
    unique_project_submitters.add(submission['account_key'])
len(unique_project_submitters)
288/31:
paid_students = {}
for enrollment in non_udacity_enrollments:
    if (not enrollment['is_canceled'] or
            enrollment['days_to_cancel'] > 7):
        account_key = enrollment['account_key']
        enrollment_date = enrollment['join_date']
        if (account_key not in paid_students or
                enrollment_date > paid_students[account_key]):
            paid_students[account_key] = enrollment_date
len(paid_students)
288/32:
def within_one_week(join_date, engagement_date):
    time_delta = engagement_date - join_date
    return time_delta.days < 7

def remove_free_trial_cancels(data):
    new_data = []
    for data_point in data:
        if data_point['account_key'] in paid_students:
            new_data.append(data_point)
    return new_data

paid_enrollments = remove_free_trial_cancels(non_udacity_enrollments)
paid_engagement = remove_free_trial_cancels(non_udacity_engagement)
paid_submissions = remove_free_trial_cancels(non_udacity_submissions)

print len(paid_enrollments)
print len(paid_engagement)
print len(paid_submissions)

paid_engagement_in_first_week = []
for engagement_record in paid_engagement:
    account_key = engagement_record['account_key']
    join_date = paid_students[account_key]
    engagement_record_date = engagement_record['utc_date']

    if within_one_week(join_date, engagement_record_date):
         paid_engagement_in_first_week.append(engagement_record)

len(paid_engagement_in_first_week)
288/33:
student_with_max_minutes = None
max_minutes = 0

for student, total_minutes in total_minutes_by_account.items():
    if total_minutes > max_minutes:
        max_minutes = total_minutes
        student_with_max_minutes = student

max_minutes

for engagement_record in paid_engagement_in_first_week:
    if engagement_record['account_key'] == student_with_max_minutes:
        print engagement_record
289/1: import pandas as pd
289/2: daily_engargement = pd.read_csv('daily_engagement_full.csv')
289/3: len(daily_engargement['acct'].unique())
289/4:
# First 20 countries with employment data
countries = np.array([
    'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina',
    'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas',
    'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium',
    'Belize', 'Benin', 'Bhutan', 'Bolivia',
    'Bosnia and Herzegovina'
])

# Employment data in 2007 for those 20 countries
employment = np.array([
    55.70000076,  51.40000153,  50.5       ,  75.69999695,
    58.40000153,  40.09999847,  61.5       ,  57.09999847,
    60.90000153,  66.59999847,  60.40000153,  68.09999847,
    66.90000153,  53.40000153,  48.59999847,  56.79999924,
    71.59999847,  58.40000153,  70.40000153,  41.20000076
])
289/5:
import pandas as pd
import numpy as np
289/6: daily_engargement = pd.read_csv('daily_engagement_full.csv')
289/7: len(daily_engargement['acct'].unique())
289/8:
# First 20 countries with employment data
countries = np.array([
    'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina',
    'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas',
    'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium',
    'Belize', 'Benin', 'Bhutan', 'Bolivia',
    'Bosnia and Herzegovina'
])

# Employment data in 2007 for those 20 countries
employment = np.array([
    55.70000076,  51.40000153,  50.5       ,  75.69999695,
    58.40000153,  40.09999847,  61.5       ,  57.09999847,
    60.90000153,  66.59999847,  60.40000153,  68.09999847,
    66.90000153,  53.40000153,  48.59999847,  56.79999924,
    71.59999847,  58.40000153,  70.40000153,  41.20000076
])
289/9:
# Accessing elements
if False:
    print countries[0]
    print countries[3]
289/10:
# Accessing elements
if False:
    print countries[0]
    print countries[3]
289/11:
# Accessing elements
if False:
    print(countries[0])
    print(countries[3])
289/12:
# Accessing elements
if True:
    print(countries[0])
    print(countries[3])
289/13:
# Slicing
if False:
    print(countries[0:3])
    print(countries[:3])
    print(countries[17:])
    print(countries[:])
289/14:
# Slicing
if True:
    print(countries[0:3])
    print(countries[:3])
    print(countries[17:])
    print(countries[:])
289/15:
# Element types
if False:
    print(countries.dtype)
    print(employment.dtype)
    print(np.array([0, 1, 2, 3]))
    print(np.array([1.0, 1.5, 2.0, 2.5]).dtype)
    print(np.array([True, False, True]).dtype)
    print(np.array(['AL', 'AK', 'AZ', 'AR', 'CA']).dtype)
289/16:
# Element types
if True:
    print(countries.dtype)
    print(employment.dtype)
    print(np.array([0, 1, 2, 3]))
    print(np.array([1.0, 1.5, 2.0, 2.5]).dtype)
    print(np.array([True, False, True]).dtype)
    print(np.array(['AL', 'AK', 'AZ', 'AR', 'CA']).dtype)
289/17:
# Element types
if True:
    print(countries.dtype)
    print(employment.dtype)
    print(np.array([0, 1, 2, 3]))
    print(np.array([1.0, 1.5, 2.0, 2.5]).dtype)
    print(np.array([True, False, True]).dtype)
    print(np.array(['AL', 'AK', 'AZ', 'AR', 'CA']).dtype)
289/18:
# Looping
if False:
    for country in countries:
        print('Examining country {}'.format(country))

    for i in range(len(countries)):
        country = countries[i]
        country_employment = employment[i]
        print('Country {} has employment {}'.format(country, country_employment))
289/19:
# Looping
if True:
    for country in countries:
        print('Examining country {}'.format(country))

    for i in range(len(countries)):
        country = countries[i]
        country_employment = employment[i]
        print('Country {} has employment {}'.format(country, country_employment))
289/20:
# Numpy functions
if True:
    print(employment.mean())
    print(employment.std())
    print(employment.max())
    print(employment.sum())
289/21:
# Numpy functions
if True:
    print(employment.mean())
    print(employment.std())
    print(employment.max())
    print(employment.sum())
289/22: employment.max()
289/23:
def max_employment(countries, employment):
    '''
    Fill in this function to return the name of the country
    with the highest employment in the given employment data,
    and the employment in that country
    '''
    max_country = employment[3]
    max_value = employment.max()
    
    return (max_country, max_value)
289/24: employment.max().index()
289/25: employment.max().index
289/26: employment.max()
289/27: np.where(employment == employment.max())
289/28: print(np.where(employment == employment.max()))
289/29: print(np.where(country == employment.max()))
289/30: print(np.where(countries == employment.max()))
289/31:
def max_employment(countries, employment):
    max_country = None
    max_value = 0
    
    for i in range(len(countries)):
        country = countries[i]
        country_employment = employment[i]
        
        if country_employment > max_employment:
            max_country = country
            max_employment = country_employment
    
    return (max_country, max_value)
289/32: employment.max()
289/33: employment.argmax()
289/34:
def max_employment(countries, employment):
    max_country = countries[argmax()]
    max_value = employment.max()
    
    return (max_country, max_value)
289/35:
def max_employment(countries, employment):
    max_country = countries[argmax()]
    max_value = employment.max()
    
    return (max_country, max_value)
289/36: max_employment(countries, employment)
289/37:
def max_employment(countries, employment):
    max_country = countries.argmax()
    max_value = employment.max()
    
    return (max_country, max_value)
289/38: max_employment(countries, employment)
289/39:
def max_employment(countries, employment):
    max_country = countries[employment.argmax()]
    max_value = employment.max()
    
    return (max_country, max_value)
289/40: max_employment(countries, employment)
289/41:
# Arithmetic operations between 2 NumPy arrays
if True:
    a = np.array([1, 2, 3, 4])
    b = np.array([1, 2, 1, 2])
    
    print a + b
    print a - b
    print a * b
    print a / b
    print a ** b
289/42:
# Arithmetic operations between 2 NumPy arrays
if True:
    a = np.array([1, 2, 3, 4])
    b = np.array([1, 2, 1, 2])
    
    print(a + b)
    print(a - b)
    print(a * b)
    print(a / b)
    print(a ** b)
289/43:
# Arithmetic operations between a NumPy array and a single number
if True:
    a = np.array([1, 2, 3, 4])
    b = 2
    
    print(a + b)
    print(a - b)
    print(a * b)
    print(a / b)
    print(a ** b)
289/44:
# Logical operations with NumPy arrays
if True:
    a = np.array([True, True, False, False])
    b = np.array([True, False, True, False])
    
    print a & b
    print a | b
    print ~a
    
    print a & True
    print a & False
    
    print a | True
    print a | False
289/45:
# Logical operations with NumPy arrays
if True:
    a = np.array([True, True, False, False])
    b = np.array([True, False, True, False])
    
    print(a & b)
    print(a | b)
    print(~a)
    
    print(a & True)
    print(a & False)
    
    print(a | True)
    print(a | False)
289/46:
# Comparison operations between 2 NumPy Arrays
if True:
    a = np.array([1, 2, 3, 4, 5])
    b = np.array([5, 4, 3, 2, 1])
    
    print(a > b)
    print(a >= b)
    print(a < b)
    print(a <= b)
    print(a == b)
    print(a != b)
289/47:
# Comparison operations between a NumPy array and a single number
if True:
    a = np.array([1, 2, 3, 4])
    b = 2
    
    print(a > b)
    print(a >= b)
    print(a < b)
    print(a <= b)
    print(a == b)
    print(a != b)
289/48:
# First 20 countries with school completion data
countries = np.array([
       'Algeria', 'Argentina', 'Armenia', 'Aruba', 'Austria','Azerbaijan',
       'Bahamas', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Bolivia',
       'Botswana', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi',
       'Cambodia', 'Cameroon', 'Cape Verde'
])
289/49:
# Female school completion rate in 2007 for those 20 countries
female_completion = np.array([
    97.35583,  104.62379,  103.02998,   95.14321,  103.69019,
    98.49185,  100.88828,   95.43974,   92.11484,   91.54804,
    95.98029,   98.22902,   96.12179,  119.28105,   97.84627,
    29.07386,   38.41644,   90.70509,   51.7478 ,   95.45072
])
289/50:
# Male school completion rate in 2007 for those 20 countries
male_completion = np.array([
     95.47622,  100.66476,   99.7926 ,   91.48936,  103.22096,
     97.80458,  103.81398,   88.11736,   93.55611,   87.76347,
    102.45714,   98.73953,   92.22388,  115.3892 ,   98.70502,
     37.00692,   45.39401,   91.22084,   62.42028,   90.66958
])
289/51:
def overall_completion_rate(female_completion, male_completion):
    '''
    Fill in this function to return a NumPy array containing the overall
    school completion rate for each country. The arguments are NumPy
    arrays giving the female and male completion of each country in
    the same order.
    '''
    return (female_completion + male_completion)
289/52: overall_completion_rate(female_completion, male_completion)
289/53:
def overall_completion_rate(female_completion, male_completion):
    '''
    Fill in this function to return a NumPy array containing the overall
    school completion rate for each country. The arguments are NumPy
    arrays giving the female and male completion of each country in
    the same order.
    '''
    return (female_completion + male_completion) / 2
289/54: overall_completion_rate(female_completion, male_completion)
289/55:
# First 20 countries with employment data
countries = np.array([
    'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina',
    'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas',
    'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium',
    'Belize', 'Benin', 'Bhutan', 'Bolivia',
    'Bosnia and Herzegovina'
])
289/56:
# Employment data in 2007 for those 20 countries
employment = np.array([
    55.70000076,  51.40000153,  50.5       ,  75.69999695,
    58.40000153,  40.09999847,  61.5       ,  57.09999847,
    60.90000153,  66.59999847,  60.40000153,  68.09999847,
    66.90000153,  53.40000153,  48.59999847,  56.79999924,
    71.59999847,  58.40000153,  70.40000153,  41.20000076
])
289/57:
# Change this country name to change what country will be printed when you
# click "Test Run". Your function will be called to determine the standardized
# score for this country for each of the given 5 Gapminder variables in 2007.
# The possible country names are available in the Downloadables section.

country_name = 'United States'

def standardize_data(values):
    '''
    Fill in this function to return a standardized version of the given values,
    which will be in a NumPy array. Each value should be translated into the
    number of standard deviations that value is away from the mean of the data.
    (A positive number indicates a value higher than the mean, and a negative
    number indicates a value lower than the mean.)
    '''
    return None
289/58:
# Change this country name to change what country will be printed when you
# click "Test Run". Your function will be called to determine the standardized
# score for this country for each of the given 5 Gapminder variables in 2007.
# The possible country names are available in the Downloadables section.

country_name = 'United States'

def standardize_data(values):
    '''
    Fill in this function to return a standardized version of the given values,
    which will be in a NumPy array. Each value should be translated into the
    number of standard deviations that value is away from the mean of the data.
    (A positive number indicates a value higher than the mean, and a negative
    number indicates a value lower than the mean.)
    '''
    standardized_values = (values - values.mean()) / value.std()
    return standardized_values
289/59:
# Using index arrays
if True:
    a = np.array([1, 2, 3, 4])
    b = np.array([True, True, False, False])
    
    print a[b]
    print a[np.array([True, False, True, False])]
289/60:
# Using index arrays
if True:
    a = np.array([1, 2, 3, 4])
    b = np.array([True, True, False, False])
    
    print(a[b])
    print(a[np.array([True, False, True, False])])
289/61:
# Creating the index array using vectorized operations
if True:
    a = np.array([1, 2, 3, 2, 1])
    b = (a >= 2)
    
    print(a[b])
    print(a[a >= 2])
289/62:
# Creating the index array using vectorized operations on another array
if True:
    a = np.array([1, 2, 3, 4, 5])
    b = np.array([1, 2, 3, 2, 1])
    
    print(b == 2)
    print(a[b == 2])
289/63:
# Time spent in the classroom in the first week for 20 students
time_spent = np.array([
       12.89697233,    0.        ,   64.55043217,    0.        ,
       24.2315615 ,   39.991625  ,    0.        ,    0.        ,
      147.20683783,    0.        ,    0.        ,    0.        ,
       45.18261617,  157.60454283,  133.2434615 ,   52.85000767,
        0.        ,   54.9204785 ,   26.78142417,    0.
])

# Days to cancel for 20 students
days_to_cancel = np.array([
      4,   5,  37,   3,  12,   4,  35,  38,   5,  37,   3,   3,  68,
     38,  98,   2, 249,   2, 127,  35
])
289/64:
# Time spent in the classroom in the first week for 20 students
time_spent = np.array([
       12.89697233,    0.        ,   64.55043217,    0.        ,
       24.2315615 ,   39.991625  ,    0.        ,    0.        ,
      147.20683783,    0.        ,    0.        ,    0.        ,
       45.18261617,  157.60454283,  133.2434615 ,   52.85000767,
        0.        ,   54.9204785 ,   26.78142417,    0.
])

# Days to cancel for 20 students
days_to_cancel = np.array([
      4,   5,  37,   3,  12,   4,  35,  38,   5,  37,   3,   3,  68,
     38,  98,   2, 249,   2, 127,  35
])
289/65: [days_to_cancel >= 7]
289/66: days_to_cancel[days_to_cancel >= 7]
289/67: days_to_cancel[days_to_cancel >= 7].mean()
289/68: days_to_cancel[days_to_cancel >= 7].mean()
289/69: time_spent[days_to_cancel >= 7]
289/70: len(time_spent[days_to_cancel >= 7])
289/71: time_spent[days_to_cancel >= 7]
289/72: time_spent[days_to_cancel >= 7].mean()
289/73:
def mean_time_for_paid_students(time_spent, days_to_cancel):
    '''
    Fill in this function to calculate the mean time spent in the classroom
    for students who stayed enrolled at least (greater than or equal to) 7 days.
    Unlike in Lesson 1, you can assume that days_to_cancel will contain only
    integers (there are no students who have not canceled yet).
    
    The arguments are NumPy arrays. time_spent contains the amount of time spent
    in the classroom for each student, and days_to_cancel contains the number
    of days until each student cancel. The data is given in the same order
    in both arrays.
    '''

    return time_spent[days_to_cancel >= 7].mean()
289/74: mean_time_for_paid_students(time_spent, days_to_cancel)
289/75:
countries = [
    'Afghanistan', 'Albania', 'Algeria', 'Angola',
    'Argentina', 'Armenia', 'Australia', 'Austria',
    'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh',
    'Barbados', 'Belarus', 'Belgium', 'Belize',
    'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina',
]


employment_values = [
    55.70000076,  51.40000153,  50.5       ,  75.69999695,
    58.40000153,  40.09999847,  61.5       ,  57.09999847,
    60.90000153,  66.59999847,  60.40000153,  68.09999847,
    66.90000153,  53.40000153,  48.59999847,  56.79999924,
    71.59999847,  58.40000153,  70.40000153,  41.20000076,
]
289/76:
countries = [
    'Afghanistan', 'Albania', 'Algeria', 'Angola',
    'Argentina', 'Armenia', 'Australia', 'Austria',
    'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh',
    'Barbados', 'Belarus', 'Belgium', 'Belize',
    'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina',
]


employment_values = [
    55.70000076,  51.40000153,  50.5       ,  75.69999695,
    58.40000153,  40.09999847,  61.5       ,  57.09999847,
    60.90000153,  66.59999847,  60.40000153,  68.09999847,
    66.90000153,  53.40000153,  48.59999847,  56.79999924,
    71.59999847,  58.40000153,  70.40000153,  41.20000076,
]
289/77:
def max_employment(employment):
    '''
    Fill in this function to return the name of the country
    with the highest employment in the given employment
    data, and the employment in that country.
    
    The input will be a Pandas series where the values
    are employment and the index is country names.
    
    Try using the Pandas idxmax() function. Documention can
    be found here:
    http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.idxmax.html
    '''
    max_country = employment.argmax()
    max_value = employment.loc[max_country]

    return (max_country, max_value)
289/78:
# Addition when indexes are the same
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])
    print s1 + s2
289/79:
# Addition when indexes are the same
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])
    print(s1 + s2)
289/80:
# Indexes have same elements in a different order
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['b', 'd', 'a', 'c'])
    print(s1 + s2)
289/81:
# Indexes overlap, but do not have exactly the same elements
if False:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])
    print(s1 + s2)
289/82:
# Indexes overlap, but do not have exactly the same elements
if False:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])
    print(s1 + s2)
289/83:
# Indexes overlap, but do not have exactly the same elements
if False:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])
    print(s1 + s2)
289/84:
# Indexes overlap, but do not have exactly the same elements
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])
    print(s1 + s2)
289/85:
# Indexes overlap, but do not have exactly the same elements
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])
    print s1 + s2
289/86:
# Indexes overlap, but do not have exactly the same elements
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])
    print(s1 + s2)
289/87:
# Indexes do not overlap
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['e', 'f', 'g', 'h'])
    print(s1 + s2)
289/88:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])

print(s1 + s2)
289/89:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])

print(s1.dropna(axis=0) + s2.dropna(axis=0))
289/90:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])

print(s1.dropna() + s2.dropna())
289/91: s1.dropna()
289/92: s2.dropna()
289/93:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
print(s1.dropna(inplace=True) + s2.dropna(inplace=True))
289/94:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
print(s1.dropna() + s2.dropna())
289/95:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.dropna()
s2 = s2.dropna()
print(s1 + s2)
289/96:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.notna()
s2 = s2.notna()
print(s1 + s2)
289/97:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
print(s1 + s2)
289/98:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
print(s1, s2)
289/99:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
addition = s1 + s2
print(addition)
289/100:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
addition = s1 + s2
print(s1)
289/101:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
addition = s1 + s2
print(s2)
289/102:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
addition = s1 + s2
print(s1)
289/103:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
addition = s1 + s2
print(s1.dtype)
289/104:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
addition = s1 + s2
print(s1.describe)
289/105:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
addition = s1 + s2
print(s1.describe())
289/106:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
s1 = s1.fillna(0)
s2 = s2.fillna(0)
addition = s1 + s2
print(s1)
289/107:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
non_zero_s1 = s1.fillna(0)
non_zero_s2 = s2.fillna(0)
addition = non_zero_s1 + non_zero_s2
print(s1)
289/108:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
non_zero_s1 = s1.fillna(0)
non_zero_s2 = s2.fillna(0)
addition = non_zero_s1 + non_zero_s2
print(addition)
289/109:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
non_zero_s1 = s1.fillna(0)
non_zero_s2 = s2.fillna(0)
addition = non_zero_s1 + non_zero_s2
print(addition.dropna())
289/110:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
print(s1.add(s2, fill_value=0))
289/111:
import pandas as pd
import numpy as np
289/112: daily_engargement = pd.read_csv('daily_engagement_full.csv')
289/113: len(daily_engargement['acct'].unique())
289/114:
# First 20 countries with employment data
countries = np.array([
    'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina',
    'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas',
    'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium',
    'Belize', 'Benin', 'Bhutan', 'Bolivia',
    'Bosnia and Herzegovina'
])

# Employment data in 2007 for those 20 countries
employment = np.array([
    55.70000076,  51.40000153,  50.5       ,  75.69999695,
    58.40000153,  40.09999847,  61.5       ,  57.09999847,
    60.90000153,  66.59999847,  60.40000153,  68.09999847,
    66.90000153,  53.40000153,  48.59999847,  56.79999924,
    71.59999847,  58.40000153,  70.40000153,  41.20000076
])
289/115:
# Accessing elements
if True:
    print(countries[0])
    print(countries[3])
289/116:
# Slicing
if True:
    print(countries[0:3])
    print(countries[:3])
    print(countries[17:])
    print(countries[:])
289/117:
# Element types
if True:
    print(countries.dtype)
    print(employment.dtype)
    print(np.array([0, 1, 2, 3]))
    print(np.array([1.0, 1.5, 2.0, 2.5]).dtype)
    print(np.array([True, False, True]).dtype)
    print(np.array(['AL', 'AK', 'AZ', 'AR', 'CA']).dtype)
289/118:
# Looping
if True:
    for country in countries:
        print('Examining country {}'.format(country))

    for i in range(len(countries)):
        country = countries[i]
        country_employment = employment[i]
        print('Country {} has employment {}'.format(country, country_employment))
289/119:
# Numpy functions
if True:
    print(employment.mean())
    print(employment.std())
    print(employment.max())
    print(employment.sum())
289/120:
def max_employment(countries, employment):
    max_country = None
    max_value = 0
    
    for i in range(len(countries)):
        country = countries[i]
        country_employment = employment[i]
        
        if country_employment > max_employment:
            max_country = country
            max_employment = country_employment
    
    return (max_country, max_value)
289/121:
def max_employment(countries, employment):
    max_country = countries[employment.argmax()]
    max_value = employment.max()
    
    return (max_country, max_value)
289/122: max_employment(countries, employment)
289/123:
# Arithmetic operations between 2 NumPy arrays
if True:
    a = np.array([1, 2, 3, 4])
    b = np.array([1, 2, 1, 2])
    
    print(a + b)
    print(a - b)
    print(a * b)
    print(a / b)
    print(a ** b)
289/124:
# Arithmetic operations between a NumPy array and a single number
if True:
    a = np.array([1, 2, 3, 4])
    b = 2
    
    print(a + b)
    print(a - b)
    print(a * b)
    print(a / b)
    print(a ** b)
289/125:
# Logical operations with NumPy arrays
if True:
    a = np.array([True, True, False, False])
    b = np.array([True, False, True, False])
    
    print(a & b)
    print(a | b)
    print(~a)
    
    print(a & True)
    print(a & False)
    
    print(a | True)
    print(a | False)
289/126:
# Comparison operations between 2 NumPy Arrays
if True:
    a = np.array([1, 2, 3, 4, 5])
    b = np.array([5, 4, 3, 2, 1])
    
    print(a > b)
    print(a >= b)
    print(a < b)
    print(a <= b)
    print(a == b)
    print(a != b)
289/127:
# Comparison operations between a NumPy array and a single number
if True:
    a = np.array([1, 2, 3, 4])
    b = 2
    
    print(a > b)
    print(a >= b)
    print(a < b)
    print(a <= b)
    print(a == b)
    print(a != b)
289/128:
# First 20 countries with school completion data
countries = np.array([
       'Algeria', 'Argentina', 'Armenia', 'Aruba', 'Austria','Azerbaijan',
       'Bahamas', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Bolivia',
       'Botswana', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi',
       'Cambodia', 'Cameroon', 'Cape Verde'
])
289/129:
# Female school completion rate in 2007 for those 20 countries
female_completion = np.array([
    97.35583,  104.62379,  103.02998,   95.14321,  103.69019,
    98.49185,  100.88828,   95.43974,   92.11484,   91.54804,
    95.98029,   98.22902,   96.12179,  119.28105,   97.84627,
    29.07386,   38.41644,   90.70509,   51.7478 ,   95.45072
])
289/130:
# Male school completion rate in 2007 for those 20 countries
male_completion = np.array([
     95.47622,  100.66476,   99.7926 ,   91.48936,  103.22096,
     97.80458,  103.81398,   88.11736,   93.55611,   87.76347,
    102.45714,   98.73953,   92.22388,  115.3892 ,   98.70502,
     37.00692,   45.39401,   91.22084,   62.42028,   90.66958
])
289/131:
def overall_completion_rate(female_completion, male_completion):
    '''
    Fill in this function to return a NumPy array containing the overall
    school completion rate for each country. The arguments are NumPy
    arrays giving the female and male completion of each country in
    the same order.
    '''
    return (female_completion + male_completion) / 2
289/132: overall_completion_rate(female_completion, male_completion)
289/133:
# First 20 countries with employment data
countries = np.array([
    'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina',
    'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas',
    'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium',
    'Belize', 'Benin', 'Bhutan', 'Bolivia',
    'Bosnia and Herzegovina'
])
289/134:
# Employment data in 2007 for those 20 countries
employment = np.array([
    55.70000076,  51.40000153,  50.5       ,  75.69999695,
    58.40000153,  40.09999847,  61.5       ,  57.09999847,
    60.90000153,  66.59999847,  60.40000153,  68.09999847,
    66.90000153,  53.40000153,  48.59999847,  56.79999924,
    71.59999847,  58.40000153,  70.40000153,  41.20000076
])
289/135:
# Change this country name to change what country will be printed when you
# click "Test Run". Your function will be called to determine the standardized
# score for this country for each of the given 5 Gapminder variables in 2007.
# The possible country names are available in the Downloadables section.

country_name = 'United States'

def standardize_data(values):
    '''
    Fill in this function to return a standardized version of the given values,
    which will be in a NumPy array. Each value should be translated into the
    number of standard deviations that value is away from the mean of the data.
    (A positive number indicates a value higher than the mean, and a negative
    number indicates a value lower than the mean.)
    '''
    standardized_values = (values - values.mean()) / value.std()
    return standardized_values
289/136:
# Using index arrays
if True:
    a = np.array([1, 2, 3, 4])
    b = np.array([True, True, False, False])
    
    print(a[b])
    print(a[np.array([True, False, True, False])])
289/137:
# Creating the index array using vectorized operations
if True:
    a = np.array([1, 2, 3, 2, 1])
    b = (a >= 2)
    
    print(a[b])
    print(a[a >= 2])
289/138:
# Creating the index array using vectorized operations on another array
if True:
    a = np.array([1, 2, 3, 4, 5])
    b = np.array([1, 2, 3, 2, 1])
    
    print(b == 2)
    print(a[b == 2])
289/139:
# Time spent in the classroom in the first week for 20 students
time_spent = np.array([
       12.89697233,    0.        ,   64.55043217,    0.        ,
       24.2315615 ,   39.991625  ,    0.        ,    0.        ,
      147.20683783,    0.        ,    0.        ,    0.        ,
       45.18261617,  157.60454283,  133.2434615 ,   52.85000767,
        0.        ,   54.9204785 ,   26.78142417,    0.
])

# Days to cancel for 20 students
days_to_cancel = np.array([
      4,   5,  37,   3,  12,   4,  35,  38,   5,  37,   3,   3,  68,
     38,  98,   2, 249,   2, 127,  35
])
289/140: days_to_cancel[days_to_cancel >= 7].mean()
289/141: time_spent[days_to_cancel >= 7].mean()
289/142:
def mean_time_for_paid_students(time_spent, days_to_cancel):
    '''
    Fill in this function to calculate the mean time spent in the classroom
    for students who stayed enrolled at least (greater than or equal to) 7 days.
    Unlike in Lesson 1, you can assume that days_to_cancel will contain only
    integers (there are no students who have not canceled yet).
    
    The arguments are NumPy arrays. time_spent contains the amount of time spent
    in the classroom for each student, and days_to_cancel contains the number
    of days until each student cancel. The data is given in the same order
    in both arrays.
    '''

    return time_spent[days_to_cancel >= 7].mean()
289/143: mean_time_for_paid_students(time_spent, days_to_cancel)
289/144:
countries = [
    'Afghanistan', 'Albania', 'Algeria', 'Angola',
    'Argentina', 'Armenia', 'Australia', 'Austria',
    'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh',
    'Barbados', 'Belarus', 'Belgium', 'Belize',
    'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina',
]


employment_values = [
    55.70000076,  51.40000153,  50.5       ,  75.69999695,
    58.40000153,  40.09999847,  61.5       ,  57.09999847,
    60.90000153,  66.59999847,  60.40000153,  68.09999847,
    66.90000153,  53.40000153,  48.59999847,  56.79999924,
    71.59999847,  58.40000153,  70.40000153,  41.20000076,
]
289/145:
def max_employment(employment):
    '''
    Fill in this function to return the name of the country
    with the highest employment in the given employment
    data, and the employment in that country.
    
    The input will be a Pandas series where the values
    are employment and the index is country names.
    
    Try using the Pandas idxmax() function. Documention can
    be found here:
    http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.idxmax.html
    '''
    max_country = employment.argmax()
    max_value = employment.loc[max_country]

    return (max_country, max_value)
289/146:
# Addition when indexes are the same
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])
    print(s1 + s2)
289/147:
# Indexes have same elements in a different order
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['b', 'd', 'a', 'c'])
    print(s1 + s2)
289/148:
# Indexes overlap, but do not have exactly the same elements
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])
    print(s1 + s2)
289/149:
# Indexes overlap, but do not have exactly the same elements
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])
    print(s1 + s2)
289/150:
# Indexes do not overlap
if True:
    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
    s2 = pd.Series([10, 20, 30, 40], index=['e', 'f', 'g', 'h'])
    print(s1 + s2)
289/151:
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])

# Try to write code that will add the 2 previous series together,
# but treating missing values from either series as 0. The result
# when printed out should be similar to the following line:
# print pd.Series([1, 2, 13, 24, 30, 40], index=['a', 'b', 'c', 'd', 'e', 'f'])
print(s1.add(s2, fill_value=0))
291/1:
import numpy as np
import pandas as np
import matplotlib as plt

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
291/2: The indicators looked at are as follows
291/3:
import numpy as np
import pandas as np
import matplotlib as plt

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
291/4:
import numpy as np
import pandas as pd
import matplotlib as plt

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
291/5: works_consult = pd.read_csv('works_contracts.csv')
291/6: works_consult.head()
291/7: works_consult.describe()
291/8: works_consult.groupby('Entity').count()
291/9: works_consult.groupby('Entity')['subject_of_procurement'].count()
291/10: works_consult.groupby('Entity')['contract_value'].count()
291/11: works_consult.groupby('Entity')['contract_value'].sum()
291/12:
sum_works = works_consult.groupby('Entity')['contract_value'].sum()

sum_works
291/13: works_consult.groupby('Entity')['subject_of_procurement'].count()
291/14: works_consult.groupby('Entity')['contract_value'].sum()
291/15: works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
291/16: works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()
291/17:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works.head()
291/18:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
291/19:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works
291/20:
import numpy as np
import pandas as pd
import matplotlib as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
291/21: works_consult = pd.read_csv('works_contracts.csv')
291/22: works_consult.head()
291/23: works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
291/24:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works
291/25:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
291/26:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
291/27: sns.pairplot(works_consult, hue='rating');
291/28: sns.pairplot(works_consult, hue='Entity');
291/29: sns.pairplot(works_consult, hue='financial_year');
291/30: sns.pairplot(works_consult, hue='financial_year', kind='scatter');
291/31: sns.pairplot(works_consult, hue='financial_year');
291/32:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
291/33: framework_contracts.head()
291/34:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(sns.histplot)
g.map_offdiag(sns.scatterplot)
g.add_legend()
291/35:
import numpy as np
import pandas as pd
import matplotlib as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
291/36:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
291/37: works_consult.head()
291/38: works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
291/39:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works
291/40:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
291/41: sns.pairplot(works_consult, hue='financial_year');
291/42: framework_contracts.head()
291/43:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(sns.histplot)
g.map_offdiag(sns.scatterplot)
g.add_legend()
291/44:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(sns.histplot())
g.map_offdiag(sns.scatterplot())
g.add_legend()
291/45:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/46:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist())
g.map_offdiag(plt.scatter())
g.add_legend()
291/47:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
291/48:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
291/49: works_consult.head()
291/50: works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
291/51:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works
291/52:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
291/53: sns.pairplot(works_consult, hue='financial_year');
291/54: framework_contracts.head()
291/55:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist())
g.map_offdiag(plt.scatter())
g.add_legend()
291/56:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/57:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/58:
g = sns.PairGrid(framework_contracts, hue="subject_of_procurement")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/59:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/60:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/61:
g = sns.PairGrid(framework_contracts, hue="type")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/62:
g = sns.PairGrid(framework_contracts, hue="type")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/63:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend()
291/64:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
291/65:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
291/66:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
291/67: framework_contracts.groupby(['type'])['type'].count()
291/68:
sns.catplot(y="deck", hue="class", kind="count",
            palette="pastel", edgecolor=".6",
            data=group_by_type)
291/69: group_by_type = framework_contracts.groupby(['type'])['type'].count()
291/70:
sns.catplot(y="deck", hue="class", kind="count",
            palette="pastel", edgecolor=".6",
            data=group_by_type)
291/71:
sns.catplot(y="deck", hue="class", kind="count",
            palette="pastel", edgecolor=".6",
            data={group_by_type_count, group_by_type_sum})
291/72:
group_by_type_count = framework_contracts.groupby(['type'])['type'].count()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum()
291/73:
sns.catplot(y="deck", hue="class", kind="count",
            palette="pastel", edgecolor=".6",
            data={group_by_type_count, group_by_type_sum})
291/74: group_by_type_count
291/75: group_by_type_sum
291/76: group_by_type_sum
291/77:
group_by_type_count = framework_contracts.groupby(['type'])['type'].count()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()
291/78: group_by_type_count
291/79: group_by_type_sum
291/80:
group_by_type_count = framework_contracts.groupby(['type'])['type'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()
291/81:
group_by_type_count = framework_contracts.groupby(['type'])['type'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()
291/82:
group_by_type_count = framework_contracts.groupby(['type'])['type'].count()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()
291/83: group_by_type_count
291/84: group_by_type_sum
291/85:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()
291/86: group_by_type_count
291/87:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()
291/88: group_by_type_count
291/89: sns.pairplot(works_consult, hue='subject_of_procurement');
291/90: sns.pairplot(works_consult, hue='subject_of_procurement');
292/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/2:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/3: works_consult.head()
292/4: works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
292/5:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works
292/6:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/7:
sns.set_theme(style="ticks")

sns.pairplot(works_consult, hue='financial_year');
292/8:

sns.pairplot(works_consult, hue='financial_year');
292/9:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/10:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/11: works_consult.head()
292/12: works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
292/13:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works
292/14:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/15:

sns.pairplot(works_consult, hue='financial_year');
292/16: framework_contracts.head()
292/17:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/18:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/19:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/20:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/21:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()
292/22: group_by_type_count
292/23: group_by_type_sum
292/24: group_by_type_count
292/25:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/26: group_by_type_count
292/27: group_by_type_sum
292/28: group_by_method_count
292/29: group_by_method_sum
292/30: framework_contracts.head()
292/31: sns.pairplot(framework_contracts, hue='financial_year');
292/32:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/33:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/34:
group_by_entity_count = works_only.groupby(['type'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/35:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/36:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/37:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/38:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/39:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/40:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/41:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
293/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
293/2: local_gov = pd.read_csv('awarded_contracts_local_government.csv')
293/3:

sns.pairplot(local_gov, hue='financial_year');
293/4:

sns.pairplot(local_gov, hue='Entity');
294/1:

sns.pairplot(local_gov, hue='financial_year');
294/2:

sns.pairplot(local_gov, hue='financial_year');
294/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
294/4: local_gov = pd.read_csv('awarded_contracts_local_government.csv')
294/5:

sns.pairplot(local_gov, hue='financial_year');
294/6: local_gov = pd.read_csv('awarded_contracts_local_government.csv')
294/7: local_gov.head(3)
294/8: local_gov['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
292/42: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/9:

local_gov['market_price_status'] = np.where(local_gov['contract_value'] <= local_gov['estimated_amount'], 'within', 'above')

local_gov.head(3)
294/10: local_gov['market_price_status'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/11: local_gov.groupby(['market_price_status'])['market_price_status'].count().reset_index()
294/12: local_gov.groupby(['market_price_status'])['contract_value'].count().reset_index()
294/13:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
294/14:
local_gov = pd.read_csv('awarded_contracts_local_government.csv')

local_gov = local_gov[local_gov['financial_year'] == '2018-2019']
294/15: local_gov.head(3)
294/16:

sns.pairplot(local_gov, hue='financial_year');
294/17: local_gov['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/18:

local_gov['market_price_status'] = np.where(local_gov['contract_value'] <= local_gov['estimated_amount'], 'within', 'above')

local_gov.head(3)
294/19: local_gov['market_price_status'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/20: local_gov.groupby(['market_price_status'])['contract_value'].count().reset_index()
294/21:
local_gov = pd.read_csv('awarded_contracts_local_government.csv')

local_gov = local_gov[local_gov['financial_year'] == '2018-2019']
294/22: local_gov.shape()
294/23: local_gov.shape
294/24:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
294/25:
local_gov = pd.read_csv('awarded_contracts_local_government.csv')

local_gov = local_gov[local_gov['financial_year'] == '2018-2019']
294/26: local_gov.shape
294/27: local_gov.head(3)
294/28:

sns.pairplot(local_gov, hue='financial_year');
294/29: local_gov['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/30:

local_gov['market_price_status'] = np.where(local_gov['contract_value'] <= local_gov['estimated_amount'], 'within', 'above')

local_gov.head(3)
294/31: local_gov['market_price_status'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/32: local_gov.groupby(['market_price_status'])['contract_value'].count().reset_index()
294/33: local_gov.shape
294/34: local_gov.dtypes
294/35: local_gov.dtypes
294/36: ### Fill the NaN values with default dates of 0000-00-00 00:00:00 and 0000-00-00
294/37:
local_gov['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
294/38: local_gov.info()
294/39: local_gov.head(3)
294/40:
local_gov['initiation_date'] = pd.to_datetime(local_gov['initiation_date'], errors='coerce')

local_gov['planned_contract_award_date'] = pd.to_datetime(local_gov['planned_contract_award_date'], errors='coerce')

local_gov['planned_contract_signature_date'] = pd.to_datetime(local_gov['planned_contract_signature_date'], errors='coerce')

local_gov['actual_signed_date'] = pd.to_datetime(local_gov['actual_signed_date'], errors='coerce')

local_gov['planned_completion_date'] = pd.to_datetime(local_gov['planned_completion_date'], errors='coerce')

local_gov['actual_completion_date'] = pd.to_datetime(local_gov['actual_completion_date'], errors='coerce')
294/41: local_gov['planning_period'] = local_gov['planned_contract_signature_date'] - local_gov['initiation_date']
294/42: local_gov['implementation_period'] = local_gov['actual_signed_date'] - local_gov['initiation_date']
294/43: ### Convert the planning and implementation period
294/44:
local_gov['planning_period'] = pd.to_numeric(local_gov['planning_period'].dt.days, downcast='integer')


local_gov['implementation_period'] = pd.to_numeric(local_gov['implementation_period'].dt.days, downcast='integer')
294/45: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum()
294/46:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
294/47:
local_gov = pd.read_csv('awarded_contracts_local_government.csv')

local_gov = local_gov[local_gov['financial_year'] == '2018-2019']
294/48: local_gov.shape
294/49: local_gov.dtypes
294/50:
local_gov['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
294/51: local_gov.info()
294/52: local_gov.head(3)
294/53:
local_gov['initiation_date'] = pd.to_datetime(local_gov['initiation_date'], errors='coerce')

local_gov['planned_contract_award_date'] = pd.to_datetime(local_gov['planned_contract_award_date'], errors='coerce')

local_gov['planned_contract_signature_date'] = pd.to_datetime(local_gov['planned_contract_signature_date'], errors='coerce')

local_gov['actual_signed_date'] = pd.to_datetime(local_gov['actual_signed_date'], errors='coerce')

local_gov['planned_completion_date'] = pd.to_datetime(local_gov['planned_completion_date'], errors='coerce')

local_gov['actual_completion_date'] = pd.to_datetime(local_gov['actual_completion_date'], errors='coerce')
294/54: local_gov['planning_period'] = local_gov['planned_contract_signature_date'] - local_gov['initiation_date']
294/55: local_gov['implementation_period'] = local_gov['actual_signed_date'] - local_gov['initiation_date']
294/56:
local_gov['planning_period'] = pd.to_numeric(local_gov['planning_period'].dt.days, downcast='integer')


local_gov['implementation_period'] = pd.to_numeric(local_gov['implementation_period'].dt.days, downcast='integer')
294/57: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum()
294/58:

sns.pairplot(local_gov, hue='financial_year');
294/59: local_gov['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/60:

local_gov['market_price_status'] = np.where(local_gov['contract_value'] <= local_gov['estimated_amount'], 'within', 'above')

local_gov.head(3)
294/61: local_gov['market_price_status'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/62: local_gov.groupby(['market_price_status'])['contract_value'].count().reset_index()
294/63:
local_gov['planning_period'] = pd.to_numeric(local_gov['planning_period'].dt.days, downcast='integer')


local_gov['implementation_period'] = pd.to_numeric(local_gov['implementation_period'].dt.days, downcast='integer')
294/64:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
294/65:
local_gov = pd.read_csv('awarded_contracts_local_government.csv')

local_gov = local_gov[local_gov['financial_year'] == '2018-2019']
294/66: local_gov.shape
294/67: local_gov.dtypes
294/68:
local_gov['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
294/69: local_gov.info()
294/70: local_gov.head(3)
294/71:
local_gov['initiation_date'] = pd.to_datetime(local_gov['initiation_date'], errors='coerce')

local_gov['planned_contract_award_date'] = pd.to_datetime(local_gov['planned_contract_award_date'], errors='coerce')

local_gov['planned_contract_signature_date'] = pd.to_datetime(local_gov['planned_contract_signature_date'], errors='coerce')

local_gov['actual_signed_date'] = pd.to_datetime(local_gov['actual_signed_date'], errors='coerce')

local_gov['planned_completion_date'] = pd.to_datetime(local_gov['planned_completion_date'], errors='coerce')

local_gov['actual_completion_date'] = pd.to_datetime(local_gov['actual_completion_date'], errors='coerce')
294/72: local_gov['planning_period'] = local_gov['planned_contract_signature_date'] - local_gov['initiation_date']
294/73: local_gov['implementation_period'] = local_gov['actual_signed_date'] - local_gov['initiation_date']
294/74:
local_gov['planning_period'] = pd.to_numeric(local_gov['planning_period'].dt.days, downcast='integer')


local_gov['implementation_period'] = pd.to_numeric(local_gov['implementation_period'].dt.days, downcast='integer')
294/75: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum()
294/76:

sns.pairplot(local_gov, hue='financial_year');
294/77: local_gov['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/78:

local_gov['market_price_status'] = np.where(local_gov['contract_value'] <= local_gov['estimated_amount'], 'within', 'above')

local_gov.head(3)
294/79: local_gov['market_price_status'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/80: local_gov.groupby(['market_price_status'])['contract_value'].count().reset_index()
294/81: local_gov
294/82: local_gov.shape
294/83: local_gov.dtypes
294/84: local_gov.shape
294/85:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
294/86:
local_gov = pd.read_csv('awarded_contracts_local_government.csv')

local_gov = local_gov[local_gov['financial_year'] == '2018-2019']
294/87: local_gov.shape
294/88: local_gov.to_csv('local_gov.csv')
294/89: local_gov.dtypes
294/90:
local_gov['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
294/91: local_gov.info()
294/92: local_gov.head(3)
294/93:
local_gov['initiation_date'] = pd.to_datetime(local_gov['initiation_date'], errors='coerce')

local_gov['planned_contract_award_date'] = pd.to_datetime(local_gov['planned_contract_award_date'], errors='coerce')

local_gov['planned_contract_signature_date'] = pd.to_datetime(local_gov['planned_contract_signature_date'], errors='coerce')

local_gov['actual_signed_date'] = pd.to_datetime(local_gov['actual_signed_date'], errors='coerce')

local_gov['planned_completion_date'] = pd.to_datetime(local_gov['planned_completion_date'], errors='coerce')

local_gov['actual_completion_date'] = pd.to_datetime(local_gov['actual_completion_date'], errors='coerce')
294/94: local_gov['planning_period'] = local_gov['planned_contract_signature_date'] - local_gov['initiation_date']
294/95: local_gov['implementation_period'] = local_gov['actual_signed_date'] - local_gov['initiation_date']
294/96:
local_gov['planning_period'] = pd.to_numeric(local_gov['planning_period'].dt.days, downcast='integer')


local_gov['implementation_period'] = pd.to_numeric(local_gov['implementation_period'].dt.days, downcast='integer')
294/97: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum()
294/98:

sns.pairplot(local_gov, hue='financial_year');
294/99: local_gov['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/100:

local_gov['market_price_status'] = np.where(local_gov['contract_value'] <= local_gov['estimated_amount'], 'within', 'above')

local_gov.head(3)
294/101: local_gov['market_price_status'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/102: local_gov.groupby(['market_price_status'])['contract_value'].count().reset_index()
294/103: local_gov.to_csv('')
294/104:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
294/105:
local_gov = pd.read_csv('awarded_contracts_local_government.csv')

local_gov = local_gov[local_gov['financial_year'] == '2018-2019']
294/106: local_gov.shape
294/107: local_gov.dtypes
294/108:
local_gov['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)

local_gov['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
294/109: local_gov.info()
294/110: local_gov.head(3)
294/111:
local_gov['initiation_date'] = pd.to_datetime(local_gov['initiation_date'], errors='coerce')

local_gov['planned_contract_award_date'] = pd.to_datetime(local_gov['planned_contract_award_date'], errors='coerce')

local_gov['planned_contract_signature_date'] = pd.to_datetime(local_gov['planned_contract_signature_date'], errors='coerce')

local_gov['actual_signed_date'] = pd.to_datetime(local_gov['actual_signed_date'], errors='coerce')

local_gov['planned_completion_date'] = pd.to_datetime(local_gov['planned_completion_date'], errors='coerce')

local_gov['actual_completion_date'] = pd.to_datetime(local_gov['actual_completion_date'], errors='coerce')
294/112: local_gov['planning_period'] = local_gov['planned_contract_signature_date'] - local_gov['initiation_date']
294/113: local_gov['implementation_period'] = local_gov['actual_signed_date'] - local_gov['initiation_date']
294/114:
local_gov['planning_period'] = pd.to_numeric(local_gov['planning_period'].dt.days, downcast='integer')


local_gov['implementation_period'] = pd.to_numeric(local_gov['implementation_period'].dt.days, downcast='integer')
294/115: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum()
294/116:

sns.pairplot(local_gov, hue='financial_year');
294/117: local_gov['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/118:

local_gov['market_price_status'] = np.where(local_gov['contract_value'] <= local_gov['estimated_amount'], 'within', 'above')

local_gov.head(3)
294/119: local_gov['market_price_status'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
294/120: local_gov.groupby(['market_price_status'])['contract_value'].count().reset_index()
294/121: local_gov.to_csv('local_gov.csv')
294/122: local_gov.groupby(['method'])['planning_period', 'implementation_period'].avg()
294/123: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum()
294/124: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum() / local_gov.groupby(['method'])['planning_period', 'implementation_period'].count()
294/125: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum()
294/126: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum() / local_gov.groupby(['method'])['planning_period', 'implementation_period'].count()
294/127: local_gov.groupby(['method'])['planning_period', 'implementation_period'].sum() / local_gov.groupby(['method'])['planning_period', 'implementation_period'].count()
294/128: local_gov.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].sum() / local_gov.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].count()
294/129:
group_by_method = local_gov.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].sum() / local_gov.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].count()

group_by_method
294/130: group_by_method.to_excel('group_by_method.xlsx')
294/131:
lg = local_gov.groupby(['market_price_status'])['contract_value'].count().reset_index()
lg
294/132: sns.countplot(x=lg)
298/1:
import seaborn as sns
import matplotlib.pyplot as plt
298/2:
height = [
    62, 64, 69, 75, 66
    68, 65, 71, 76, 73
]

weight = [
    120, 136, 148, 175, 137,
    165, 154, 172, 200, 187
]

sns.scatterplot(x=height, y=weight)

plt.show()
298/3:
height = [
    62, 64, 69, 75, 66
    68, 65, 71, 76, 73
]

weight = [
    120, 136, 148, 175, 137,
    165, 154, 172, 200, 187
]

sns.scatterplot(x=height, y=weight)

plt.show()
298/4:
height = [
    62, 64, 69, 75, 66, 68, 65, 71, 76, 73
]

weight = [
    120, 136, 148, 175, 137, 165, 154, 172, 200, 187
]

sns.scatterplot(x=height, y=weight)

plt.show()
298/5:
import seaborn as sns
import matplotlib.pyplot as plt
298/6:
import seaborn as sns
import matplotlib.pyplot as plt

%config InlineBackend.figure_format = 'retina'
298/7:
height = [
    62, 64, 69, 75, 66, 68, 65, 71, 76, 73
]

weight = [
    120, 136, 148, 175, 137, 165, 154, 172, 200, 187
]

sns.scatterplot(x=height, y=weight)

plt.show()
298/8:
height = [
    62, 64, 69, 75, 66, 68, 65, 71, 76, 73
]

weight = [
    120, 136, 148, 175, 137, 165, 154, 172, 200, 187
]

sns.scatterplot(x=height, y=weight)

plt.show()
298/9:
gender = [
    "Female", "Female", "Female", "Female", "Male", "Male", "Male", "Male", "Male", "Male"
]

sns.countplot(gender)
298/10:
gender = [
    "Female", "Female", "Female", "Female", "Male", "Male", "Male", "Male", "Male", "Male"
]

sns.countplot(gender);
298/11:
gender = [
    "Female", "Female", "Female", "Female", "Male", "Male", "Male", "Male", "Male", "Male"
]

sns.countplot(x=gender);
298/12:
gender = [
    "Female", "Female", "Female", "Female", "Male", "Male", "Male", "Male", "Male", "Male"
]

sns.countplot(x=gender)

plt.show()
298/13:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

%config InlineBackend.figure_format = 'retina'
298/14:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

%config InlineBackend.figure_format = 'retina'
298/15:
height = [
    62, 64, 69, 75, 66, 68, 65, 71, 76, 73
]

weight = [
    120, 136, 148, 175, 137, 165, 154, 172, 200, 187
]

sns.scatterplot(x=height, y=weight)

plt.show()
298/16:
gender = [
    "Female", "Female", "Female", "Female", "Male", "Male", "Male", "Male", "Male", "Male"
]

sns.countplot(x=gender)

plt.show()
298/17: df = pd
298/18: df = pd.read_excel('FY_2019-2020_Receipts.xlsx')
298/19: df.head()
298/20: sns.countplot(x="PCATEGORY", data=df)
298/21:
sns.countplot(x="PCATEGORY", data=df)
plt.show()
298/22:
sns.countplot(x="Category", data=df)
plt.show()
298/23: df = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name="Details")
298/24: df.head()
298/25: df = pd.read_excel('FY_2019-2020_Receipts.xlsx', sheet_name="Details")
298/26: df.head()
298/27: df.head(3)
298/28:
sns.countplot(x="Category", data=df)
plt.show()
298/29:
sns.countplot(x="Receipt Date", data=df)
plt.show()
298/30:
sns.countplot(x="Date", data=df)
plt.show()
298/31:
sns.countplot(x="Procurement Type", data=df)
plt.show()
298/32:
tips = sns.load_dataset('tips')

tips.head()
298/33:
sns.scatterplot(x="total_bill",
                y="tips",
                data=tips)
298/34:
sns.scatterplot(x="total_bill",
                y="tips",
                data=tips)

plt.show()
298/35:
tips = sns.load_dataset('tips.csv')

tips.head()
298/36:
tips = sns.load_dataset('tips')

tips.head()
298/37:
sns.scatterplot(x="total_bill",
                y="tip",
                data=tips)

plt.show()
298/38:
sns.scatterplot(x="total_bill",
                y="tip",
                data=tips
                hue="smoker"
               )

plt.show()
298/39:
sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker"
               )

plt.show()
298/40:
sns.countplot(x="Procurement Type", data=df)
plt.show()
298/41: recods = sns.load_dataset('FY_2019-2020_Receipts')
298/42:
sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                hue_order=[
                    "Yes",
                    "No"
                ])

plt.show()
298/43:
sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                hue_order=[
                    "No",
                    "Yes"
                ])

plt.show()
298/44:
sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                hue_order=[
                    "Yes",
                    "No"
                ])

plt.show()
298/45:
hue_colors = {
    "Yes": "black",
    "No": "red"
}

sns.scatterplot(x="total_bill",
                y="tip",
                hue="smoker",
                hue_order = [
                    "Yes",
                    "No"
                ],
               palette=hue_colors)

plt.show()
298/46:
hue_colors = {
    "Yes": "black",
    "No": "red"
}

sns.scatterplot(x="total_bill",
                y="tip",
                hue="smoker",
                hue_order = [
                    "Yes",
                    "No"
                ],
               palette=hue_colors)

plt.show()
298/47:
hue_colors = {
    "Yes": "black",
    "No": "red"
}

sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                hue_order = [
                    "Yes",
                    "No"
                ],
               palette=hue_colors)

plt.show()
298/48:
hue_colors = {
    "Yes": "#8080800",
    "No": "00FF00"
}

sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                pallete=hue_colors)
298/49:
hue_colors = {
    "Yes": "#8080800",
    "No": "00FF00"
}

sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                pallete=hue_colors)

plt.show()
298/50:
hue_colors = {
    "Yes": "#8080800",
    "No": "00FF00"
}

sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                palete=hue_colors)

plt.show()
298/51:
hue_colors = {
    "Yes": "#8080800",
    "No": "00FF00"
}

sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                palette=hue_colors)

plt.show()
298/52:
hue_colors = {
    "Yes": "#808080",
    "No": "#00FF00"
}

sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                palette=hue_colors)

plt.show()
298/53:
hue_colors = {
    "Yes": "#808080",
    "No": "#00FF00"
}

sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                hue_order = [
                    "Yes",
                    "No"
                ]
                palette=hue_colors)

plt.show()
298/54:
hue_colors = {
    "Yes": "#808080",
    "No": "#00FF00"
}

sns.scatterplot(x="total_bill",
                y="tip",
                data=tips,
                hue="smoker",
                hue_order = [
                    "Yes",
                    "No"
                ],
                palette=hue_colors)

plt.show()
298/55:
sns.countplot(x="smoker",
              data=tips,
              sue="sex")

plt.show()
298/56:
sns.scatterplot(x="total_bill",
                y=tip,
                data=tips)
298/57:
sns.scatterplot(x="total_bill",
                y=tip,
                data=tips)


plt.show()
298/58:
sns.scatterplot(x="total_bill",
                y="tip",
                data=tips)


plt.show()
292/43:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/44:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/45: works_consult.head()
292/46: works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
292/47:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works
292/48:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/49:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/50:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/51:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/52:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/53:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/54:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/55:

sns.pairplot(works_consult, hue='financial_year');
292/56: framework_contracts.head()
292/57: sns.pairplot(framework_contracts, hue='financial_year');
292/58:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/59:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/60:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/61:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/62:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/63: group_by_type_count
292/64: group_by_type_sum
292/65: group_by_method_count
292/66: group_by_method_sum
292/67: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue'])
292/68: works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
292/69:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works.reset_index()
292/70:
works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works.reset_index()
292/71:
count_works = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works.reset_index()
292/72: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/73: ### Delays in the procurement process (Lead Time)
292/74: works_consult.head()
292/75: ### Standadize the dates
292/76:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/77:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/78: #### Add the planning_period and implementation_period
292/79:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/80:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = timelines2020['actual_signed_date'] - works_consult['initiation_date']
292/81:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/82:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/83: works_consult.info()
292/84: works_consult.info().reset_index()
292/85: works_consult.info()
292/86:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/87:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/88: works_consult.head()
292/89:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/90:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/91:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/92:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/93: works_consult.info()
292/94:
count_works = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works.reset_index()
292/95:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works.reset_index()
292/96:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/97:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/98:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/99:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/100:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/101:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/102:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/103:

sns.pairplot(works_consult, hue='financial_year');
292/104: framework_contracts.head()
292/105: sns.pairplot(framework_contracts, hue='financial_year');
292/106:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/107:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/108:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/109:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/110:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/111: group_by_type_count
292/112: group_by_type_sum
292/113: group_by_method_count
292/114: group_by_method_sum
292/115: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/116: works_only.head(3)
292/117: works_only.shape
292/118: works_only.tail(3)
292/119: works_only.head(3)
292/120: works_only.groupby('method')['maximum_indicative_time'].mean()
292/121: works_only.groupby('method')['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()
292/122: works_only.groupby('method')['maximum_indicative_time'].sum()
292/123: works_only.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].sum()
292/124: works_only.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].sum() / works_only.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].sum()
292/125: works_only.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].sum() / works_only.groupby(['method', 'maximum_indicative_time'])['planning_period', 'implementation_period'].count()
292/126: works_only.groupby('method', 'financial_year')['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()
292/127: works_only.groupby('method', 'financial_year')['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()
292/128: works_only.groupby('method')['maximum_indicative_time', 'planning_period', 'implementation_period', 'financial_year'].mean()
292/129: works_only.groupby('method')['maximum_indicative_time', 'planning_period', 'implementation_period', 'financial_year'].mean()
292/130: works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()
292/131:
lead_time_by_method = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_method
292/132: sns.pairplot(lead_time_by_method, hue="financial_year")
292/133: sns.pairplot(lead_time_by_method, hue="method")
292/134:
lead_time_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_method
292/135: sns.pairplot(lead_time_by_method, hue="method")
292/136:
lead_time_by_method = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_method
292/137:
lead_time_by_method = works_only.groupby(['financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_method
292/138:
lead_time_by_method = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_method
292/139:
lead_time_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_method
292/140: sns.pairplot(lead_time_by_method, hue="method")
292/141:
lead_time_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_method
292/142:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/143:
restricted_international = consult_only[consult_only['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/144: works_consult.info()
292/145:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/146:
count_works = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works.reset_index()
292/147:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/148:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/149:
sum_works = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works.reset_index()
292/150:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/151:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/152:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/153: sns.pairplot(sum_works_fy, hue="entity")
292/154: sns.pairplot(sum_works_fy, hue="Entity")
292/155: sns.pairplot(sum_works_fy, hue="Entity");
292/156:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/157:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/158:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/159:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/160:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/161:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/162:

sns.pairplot(works_consult, hue='financial_year');
292/163:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/164:
lead_time_by_consult_method = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/165: works_consult.head()
292/166: works_consult.head()
292/167:

works_consult = works_consult[(works_consult['financial_year'] == '2018-2019') & (works_consult['financial_year'] == '2019-2020') & (works_consult['financial_year'] == '2020-2021')]

works_consult.shape
292/168:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/169:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/170:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/171: works_consult.head()
292/172:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/173:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/174:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/175:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/176:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/177: works_consult.info()
292/178:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/179:

sns.pairplot(works_consult, hue='financial_year');
292/180:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/181:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/182:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/183:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/184:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/185:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/186:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/187:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/188:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/189:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/190:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/191:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/192:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/193: framework_contracts.head()
292/194: sns.pairplot(framework_contracts, hue='financial_year');
292/195:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/196:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/197:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/198:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/199:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/200: group_by_type_count
292/201: group_by_type_sum
292/202: group_by_method_count
292/203: group_by_method_sum
292/204: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/205: works_only.head(3)
292/206: works_only.shape
292/207:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/208:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/209:
lead_time_by_consult_method = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/210:
restricted_international = consult_only[consult_only['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/211: sns.pairplot(lead_time_by_method, hue="method")
292/212: works_consult.sample(frac=0.5)
292/213: consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].sum() / consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].count()
292/214:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/215:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/216:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/217:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/218:
lead_time_works_by_method_entity = works_only.groupby(['method', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/219:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/220:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/221: lead_time_works_by_method_entity['method']['financial_year']['Entity']
292/222:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.pairplot(lead_time_works_by_method_entity, hue="Entity")
292/223:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.pairplot(lead_time_works_by_method_entity, hue="Entity")

plt.show()
292/224:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv', index_col=None)

sns.pairplot(lead_time_works_by_method_entity, hue="financial")

plt.show()
292/225:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv', index=None)

sns.pairplot(lead_time_works_by_method_entity, hue="financial")

plt.show()
292/226:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv', index=False)

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.pairplot(lead_time_works_by_method_entity, hue="financial")

plt.show()
292/227:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv', index=False)

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/228:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.replot(x="financial_year",
           y="",
           data=lead_time_works_by_method_entity,
           kid="bar",
           col='Entity'
           row="method")

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/229:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.replot(x="financial_year",
           y="",
           data=lead_time_works_by_method_entity,
           kid="bar",
           col='Entity',
           row="method")

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/230:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.relplot(x="financial_year",
           y="",
           data=lead_time_works_by_method_entity,
           kid="bar",
           col='Entity',
           row="method")

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/231:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.relplot(x="financial_year",
           y="",
           data=lead_time_works_by_method_entity,
           kid="scatter",
           col='Entity',
           row="method")

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/232:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/233:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/234:
sns.countplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

plt.show()
292/235:
sns.countplot(x=lead_time_works_by_method_entity['financial_year'])

plt.show()
292/236:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/237:
sns.countplot(x='planning_period', data=lead_time_works_by_method_entity)

plt.show()
292/238:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/239:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/240:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/241:
lt = sns.load_dataset('lead_time_works_by_method_entity')

lt
292/242:
lt = sns.load_dataset('lead_time_works_by_method_entity')

lt.head()
292/243:
lt = sns.load_dataset('lead_time_works_by_method_entity.csv')

lt.head()
292/244:
sns.scatterplot(x="financial_year",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="smoker"
               )

plt.show()
292/245:
sns.scatterplot(x="financial_year",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="method"
               )

plt.show()
292/246:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/247:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/248:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            col="financial_year",
            col_wrap=3
           )

plt.show()
292/249:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            col="financial_year",
            col_wrap=2
           )

plt.show()
292/250:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            col="financial_year",
            col_wrap=3
           )

plt.show()
292/251:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="bar",
            col="financial_year",
            col_wrap=3
           )

plt.show()
292/252:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="line",
            col="financial_year",
            col_wrap=3
           )

plt.show()
292/253:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="line",
            hue="method"
            col="financial_year",
            col_wrap=3
           )

plt.show()
292/254:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="line",
            hue="method",
            col="financial_year",
            col_wrap=3
           )

plt.show()
292/255:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3
           )

plt.show()
292/256:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3,
            markers=True
           )

plt.show()
292/257:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3,
            size="panning_period"
            markers=True
           )

plt.show()
292/258:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3,
            size="panning_period",
            markers=True
           )

plt.show()
292/259:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3,
            size="planning_period",
            markers=True
           )

plt.show()
292/260:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3,
            markers=True
           )

plt.show()
292/261:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="hist",
            hue="method",
            col="financial_year",
            col_wrap=3,
            markers=True
           )

plt.show()
292/262:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3,
            markers=True
           )

plt.show()
292/263:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=1,
            markers=True
           )

plt.show()
292/264:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=5,
            markers=True
           )

plt.show()
292/265:
sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/266:
g = sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/267:
g = sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3,
            markers=True
           )

plt.show()
292/268:
g = sns.catplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="box",
            hue="method",
            col="financial_year",
            col_wrap=3,
            markers=True
           )

plt.show()
292/269:
g = sns.catplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="box",
            hue="method",
            col="financial_year",
            col_wrap=3
#             markers=True
           )

plt.show()
292/270:
g = sns.catplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="count",
            hue="method",
            col="financial_year",
            col_wrap=3
#             markers=True
           )

plt.show()
292/271:
g = sns.catplot(x="planning_period",
#             y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="count",
            hue="method",
            col="financial_year",
            col_wrap=3
#             markers=True
           )

plt.show()
292/272:
g = sns.catplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=3,
            markers=True
           )

plt.show()
292/273:
g = sns.relplot(x="planning_period",
            y="maximum_indicative_time",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/274:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/275:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/276:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/277:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/278:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works
292/279:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works_fy
292/280:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works_entity
292/281:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works_entity
292/282:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/283:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_consult
292/284:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_consult_fy
292/285:
indicative_planned_consult_entity = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_consult_entity
292/286:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'entity'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_consult_entity
292/287:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_consult_entity
292/288:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/289:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/290:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/291: indicative_planned_consult_entity.describe()
292/292: indicative_planned_consult_entity.describe()['implementation_period']
292/293: indicative_planned_consult_entity.describe()['implementation_period'].reset_index()
292/294:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/295:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/296:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/297: works_consult.head()
292/298:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/299: works_consult.sample(frac=0.5)
292/300:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/301:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/302:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/303:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/304: works_consult.info()
292/305:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/306:

sns.pairplot(works_consult, hue='financial_year');
292/307:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/308:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/309:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/310:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/311:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/312:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/313:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/314:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/315:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/316:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/317:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/318:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/319:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/320: framework_contracts.head()
292/321: sns.pairplot(framework_contracts, hue='financial_year');
292/322:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/323:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/324:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/325:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/326:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/327: group_by_type_count
292/328: group_by_type_sum
292/329: group_by_method_count
292/330: group_by_method_sum
292/331: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/332: works_only.head(3)
292/333: works_only.shape
292/334:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/335:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/336:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/337:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/338:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/339:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/340:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/341:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/342:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/343:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/344:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/345:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/346:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/347:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/348:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/349:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/350: sns.FacetGrid(indicative_planned_consult_entity, col="financial_year", row="Entity")
292/351: sns.FacetGrid(indicative_planned_consult_entity, col="financial_year", row="Entity")
292/352: sns.FacetGrid(indicative_planned_consult_entity, col="financial_year", row="method")
292/353:

sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])
292/354:

sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])
292/355:

sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])
292/356:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/357:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])

tips = sns.load_dataset("tips")
sns.FacetGrid(tips)
292/358:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])

tips = sns.load_dataset("tips")
sns.FacetGrid(tips);
292/359: sns.FacetGrid(tips, col="time", row="sex")
292/360: sns.FacetGrid(tips, col="time", row="sex");
292/361:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])

# tips = sns.load_dataset("tips")
sns.FacetGrid(indicative_planned_consult_entity);
292/362:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])


sns.FacetGrid(indicative_planned_consult_entity);

sns.FacetGrid(tips, col="financial_year", row="method");
292/363:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])


sns.FacetGrid(indicative_planned_consult_entity);

sns.FacetGrid(tips, col="planning_period", row="Direct Procurement");
292/364:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])


sns.FacetGrid(indicative_planned_consult_entity);

sns.FacetGrid(tips, col="planning_period");
292/365:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])


sds = sns.FacetGrid(indicative_planned_consult_entity);

sns.FacetGrid(sds, col="planning_period", row="Direct Procurement");
292/366:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])


sds = sns.FacetGrid(indicative_planned_consult_entity);

sns.FacetGrid(indicative_planned_consult_entity, col="planning_period", row="Direct Procurement");
292/367:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])


sds = sns.FacetGrid(indicative_planned_consult_entity);

sns.FacetGrid(indicative_planned_consult_entity, col="planning_period");
292/368:

# sns.FacetGrid(indicative_planned_consult_entity, col=indicative_planned_consult_entity["financial_year"], row=indicative_planned_consult_entity["method"])


sds = sns.FacetGrid(indicative_planned_consult_entity);

# sns.FacetGrid(indicative_planned_consult_entity, col="planning_period");
292/369:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/370:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/371:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/372: works_consult.info()
292/373:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/374:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/375: works_consult.head()
292/376:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/377: works_consult.sample(frac=0.5)
292/378:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/379:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/380:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/381:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/382:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/383: works_consult.info()
292/384:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/385:

sns.pairplot(works_consult, hue='financial_year');
292/386:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/387:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/388:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/389:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/390:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/391:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/392:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/393:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/394:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/395:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/396:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/397:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/398:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/399: framework_contracts.head()
292/400: sns.pairplot(framework_contracts, hue='financial_year');
292/401:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/402:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/403:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/404:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/405:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/406: group_by_type_count
292/407: group_by_type_sum
292/408: group_by_method_count
292/409: group_by_method_sum
292/410: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/411: works_only.head(3)
292/412: works_only.shape
292/413:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/414:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/415:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/416:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/417:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/418:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/419:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/420:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/421:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/422:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/423:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/424:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/425:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/426:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/427:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/428:
titanic = sns.load_dataset("titanic")
sns.catplot(x="sex", y="survived", hue="class", kind="bar", data=titanic)
292/429: titanic.head(3)
292/430:
works_within = works_only[works_only['market_price_status'] == 'within']

works_within.shape
292/431:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape
292/432: sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/433: works_within.head(3)
292/434: works_within_method = works_within.groupby(['method'])['contract_value'].sum()
292/435:
works_within_method = works_within.groupby(['method'])['contract_value'].sum()

works_within_method
292/436:
works_within_method = works_within.groupby(['method'])['contract_value'].sum()

works_within_method.reset_index()
292/437:
works_within_method = works_within.groupby(['method'])['contract_value'].sum().reset_index()

works_within_method
292/438: sns.catplot(x="method", kind="count", palette="ch:.25", data=works_within_method);
292/439:
works_within_method = works_within.groupby(['method'])['contract_value'].sum().reset_index()

works_within_method
292/440:
works_within_method.to_csv('works_within_method.csv')

works_within_method = pd.read_csv('works_within_method.csv')
292/441: sns.catplot(x="method", kind="count", palette="ch:.25", data=works_within_method);
292/442:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/443:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/444:
works_within_entity = works_within.groupby(['method', 'financial_year', 'entity'])['contract_value'].sum().reset_index()

works_within_entity
292/445:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/446:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/447:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/448: works_consult.head()
292/449:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/450: works_consult.sample(frac=0.5)
292/451:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/452:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/453:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/454:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/455:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/456: works_consult.info()
292/457:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/458:

sns.pairplot(works_consult, hue='financial_year');
292/459:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/460:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/461:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/462:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/463:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/464:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/465:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/466:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/467:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/468:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/469:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/470:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/471:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/472: framework_contracts.head()
292/473: sns.pairplot(framework_contracts, hue='financial_year');
292/474:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/475:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/476:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/477:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/478:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/479: group_by_type_count
292/480: group_by_type_sum
292/481: group_by_method_count
292/482: group_by_method_sum
292/483: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/484: works_only.head(3)
292/485: works_only.shape
292/486:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/487:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/488:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/489:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/490:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/491:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/492:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/493:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/494:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/495:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/496:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/497:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/498:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/499:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/500:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/501:
titanic = sns.load_dataset("titanic")
sns.catplot(x="sex", y="survived", hue="class", kind="bar", data=titanic)
292/502: titanic.head(3)
292/503:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape
292/504: sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/505:
works_within_method = works_within.groupby(['method'])['contract_value'].sum().reset_index()

works_within_method
292/506:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/507:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/508:
works_within_method = works_within.groupby(['method'])['contract_value'].sum()

works_within_method
292/509:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/510:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/511:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/512:
works_within_entity = works_within.groupby(['Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/513:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/514:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/515:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape
292/516:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/517: sns.catplot(x="Entity", y="financial_year", hue="class", kind="bar", data=works_within)
292/518: sns.catplot(x="Entity", y="method", hue="financial_year", kind="bar", data=works_within)
292/519: sns.catplot(x="Entity", y="contract_value", hue="financial_year", kind="bar", data=works_within)
292/520:
sns.catplot(x="Entity", y="contract_value", hue="financial_year", kind="bar", data=works_within)

plt.show()
292/521:
sns.catplot(x="financial_year", y="contract_value", hue="Entity", kind="bar", data=works_within)

plt.show()
292/522:
sns.countplot(x="financial_year", y="contract_value", hue="Entity", kind="bar", data=works_within)

plt.show()
292/523:
sns.catplot(x="financial_year", y="contract_value", hue="Entity", kind="bar", data=works_within)

plt.show()
292/524:
sns.catplot(x="financial_year", y="contract_value", hue="Entity", kind="bar", ci=None, data=works_within)

plt.show()
292/525:
sns.catplot(x="financial_year", y="contract_value", hue="Entity", kind="bar", ci=None, data=works_within, height=8.27, aspect=11.7/8.27)

plt.show()
292/526:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/527:
sns.catplot(x="financial_year", y="contract_value", hue="Entity", kind="bar", ci=None, data=works_within, height=8.27, aspect=11.7/8.27)

plt.show()
292/528:
sns.catplot(x="financial_year", y="contract_value", hue="Entity", kind="bar", ci=None, data=works_within, height=5.27, aspect=11.7/8.27)

plt.show()
292/529:
sns.catplot(x="financial_year", y="contract_value", hue="Entity", kind="bar", ci=None, data=works_within, height=7.27, aspect=11.7/8.27)

plt.show()
292/530:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27
           )

plt.show()
292/531:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27
           )

plt.show()
292/532:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27
           )

plt.show()
292/533:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/534:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/535:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/536:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/537:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/538:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/539:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/540:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/541:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/542:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/543: consult_above
292/544: consult_above.shape
292/545: consult_above.head()
292/546: consult_above.head(10)
292/547:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/548:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/549: works_consult.head()
292/550:

# works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

# works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/551: works_consult.sample(frac=0.5)
292/552:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/553:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/554:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/555:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/556:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/557: works_consult.info()
292/558:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/559:

sns.pairplot(works_consult, hue='financial_year');
292/560:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/561:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/562:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/563:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/564:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/565:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/566:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/567:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/568:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/569:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/570:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/571:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/572:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/573: framework_contracts.head()
292/574: sns.pairplot(framework_contracts, hue='financial_year');
292/575:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/576:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/577:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/578:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/579:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/580: group_by_type_count
292/581: group_by_type_sum
292/582: group_by_method_count
292/583: group_by_method_sum
292/584: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/585: works_only.head(3)
292/586: works_only.shape
292/587:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/588:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/589:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/590:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/591:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/592:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/593:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/594:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/595:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/596:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/597:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/598:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/599:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/600:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/601:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/602:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/603:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/604:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/605:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/606:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/607:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/608:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/609:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/610:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/611:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/612:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/613:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/614: consult_above.head(10)
292/615:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/616:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/617: works_consult.head()
292/618:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/619: works_consult.sample(frac=0.5)
292/620:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/621:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/622:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/623:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/624:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/625: works_consult.info()
292/626:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/627:

sns.pairplot(works_consult, hue='financial_year');
292/628:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/629:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/630:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/631:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/632:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/633:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/634:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/635:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/636:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/637:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/638:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/639:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/640:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/641: framework_contracts.head()
292/642: sns.pairplot(framework_contracts, hue='financial_year');
292/643:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/644:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/645:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/646:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/647:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/648: group_by_type_count
292/649: group_by_type_sum
292/650: group_by_method_count
292/651: group_by_method_sum
292/652: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/653: works_only.head(3)
292/654: works_only.shape
292/655:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/656:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/657:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/658:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/659:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/660:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/661:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/662:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/663:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/664:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/665:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/666:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/667:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/668:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/669:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/670:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/671:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/672:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/673:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/674:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/675:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/676:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/677:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/678:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/679:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/680:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/681:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/682: consult_above.head(10)
292/683:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/684: consult_above.head(10)
292/685:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/686:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/687:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/688: works_consult.head()
292/689:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/690: works_consult.sample(frac=0.5)
292/691:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/692:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/693:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/694:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/695:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/696:

# works_consult['execution_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/697: works_consult.info()
292/698:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/699:

sns.pairplot(works_consult, hue='financial_year');
292/700:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/701:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/702:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/703:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/704:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/705:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/706:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/707:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/708:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/709:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/710:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/711:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/712:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/713: framework_contracts.head()
292/714: sns.pairplot(framework_contracts, hue='financial_year');
292/715:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/716:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/717:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/718:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/719:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/720: group_by_type_count
292/721: group_by_type_sum
292/722: group_by_method_count
292/723: group_by_method_sum
292/724: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/725: works_only.head(3)
292/726: works_only.shape
292/727:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/728:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/729:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/730:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/731:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/732:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/733:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/734:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/735:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/736:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/737:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/738:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/739:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/740:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/741:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/742:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/743:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/744:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/745:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/746:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/747:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/748:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/749:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/750:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/751:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/752:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/753:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/754: consult_above.head(10)
292/755: - Completed within contractual time
292/756: works_only.head(3)
292/757:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/758: works_consult.describe()
292/759: works_consult['contract_value'].sum()
292/760: works_consult['contract_value'].sum().reset_index()
292/761: works_consult['contract_value'].sum()
292/762:  Completed within contractual time
292/763:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
292/764:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/765: works_consult.head()
292/766:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/767: works_consult['contract_value'].sum()
292/768: works_consult.sample(frac=0.5)
292/769:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/770:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/771:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/772:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/773:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/774:

works_consult['time_status'] = np.where(works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date'], 'In time', 'Delayed')
292/775: works_consult.info()
292/776:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/777:

sns.pairplot(works_consult, hue='financial_year');
292/778:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/779:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/780:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/781:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/782:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/783:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/784:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/785:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/786:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/787:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/788:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/789:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/790:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/791: framework_contracts.head()
292/792: sns.pairplot(framework_contracts, hue='financial_year');
292/793:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/794:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/795:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/796:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/797:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/798: group_by_type_count
292/799: group_by_type_sum
292/800: group_by_method_count
292/801: group_by_method_sum
292/802: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/803:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/804: works_only.head(3)
292/805: works_only.shape
292/806:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/807:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/808:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/809:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/810:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/811:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/812:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/813:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/814:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/815:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/816:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/817:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/818:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/819:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/820:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/821:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/822:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/823:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/824:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/825:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/826:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/827:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/828:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/829:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/830:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/831:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/832:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/833: consult_above.head(10)
292/834:  Completed within contractual time
292/835:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/836:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

works_delayed.head(3)
292/837:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

works_delayed.shape
292/838:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

works_in_time.shape
292/839:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_delayed.shape)
print(works_in_time.shape)
292/840:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_delayed.shape)
print(consult_in_time.shape)
292/841:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
292/842:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
292/843:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
292/844:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_delayed['contract_value']);
292/845:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_delayed);
292/846:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/847:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only.contract_value);
292/848:

sns.catplot(x="time_status", y="contract_value", kind="count", palette="ch:.25", data=works_delayed);
292/849:

sns.catplot( y="contract_value", kind="count", palette="ch:.25", data=works_delayed);
292/850:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_delayed);
292/851:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/852:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
292/853: works_delayed['contract_value'].sum()
292/854:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print(works_delayed['contract_value'].sum())
292/855:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print(works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
292/856:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print(works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())
292/857:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/858:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/859:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
292/860:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
292/861:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
292/862:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/863: works_consult.head()
292/864:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/865: works_consult['contract_value'].sum()
292/866: works_consult.sample(frac=0.5)
292/867:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/868:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/869:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/870:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/871:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/872:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/873:

works_consult['time_status'] = np.where(works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date'], 'In time', 'Delayed')
292/874: works_consult.info()
292/875:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/876:

sns.pairplot(works_consult, hue='financial_year');
292/877:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/878:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/879:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()

sum_works.reset_index()
292/880:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/881:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/882:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/883:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/884:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_sum
292/885:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/886:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/887:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/888:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/889:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/890: framework_contracts.head()
292/891: sns.pairplot(framework_contracts, hue='financial_year');
292/892:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/893:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/894:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/895:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/896:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/897: group_by_type_count
292/898: group_by_type_sum
292/899: group_by_method_count
292/900: group_by_method_sum
292/901: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/902:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print(works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())
292/903:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/904:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/905:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
292/906: works_only.head(3)
292/907: works_only.shape
292/908:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/909:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/910:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/911:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/912:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/913:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/914:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/915:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/916:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/917:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/918:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/919:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/920:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/921:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/922:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/923:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/924:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/925:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/926:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/927:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/928:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/929:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/930:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/931:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/932:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/933:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/934:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/935: consult_above.head(10)
292/936: works_consult['contract_value'].sum()
292/937: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
292/938: works_consult['contract_value'].sum()
292/939: works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index()
292/940:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
292/941:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/942: works_only.shape
292/943:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
292/944:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
292/945:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/946: works_consult.head()
292/947:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/948: works_consult['contract_value'].sum()
292/949:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
292/950: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
292/951: works_consult.sample(frac=0.5)
292/952:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/953:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/954:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/955:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/956:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/957:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/958:

works_consult['time_status'] = np.where(works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date'], 'In time', 'Delayed')
292/959: works_consult.info()
292/960:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/961:

sns.pairplot(works_consult, hue='financial_year');
292/962:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/963:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/964:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
292/965:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/966:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/967:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/968: works_only.shape
292/969:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/970:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
292/971:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/972:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/973:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/974:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum
292/975:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/976: framework_contracts.head()
292/977: sns.pairplot(framework_contracts, hue='financial_year');
292/978:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/979:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/980:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/981:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/982:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/983: group_by_type_count
292/984: group_by_type_sum
292/985: group_by_method_count
292/986: group_by_method_sum
292/987: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/988:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print(works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())
292/989:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/990:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/991:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
292/992: works_only.head(3)
292/993: works_only.shape
292/994:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/995:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/996:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/997:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/998:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/999:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/1000:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/1001:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/1002:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/1003:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/1004:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/1005:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/1006:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/1007:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/1008:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/1009:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/1010:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/1011:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1012:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1013:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/1014:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/1015:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/1016:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/1017:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/1018:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/1019:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1020:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1021: consult_above.head(10)
292/1022: works_only.shape
292/1023:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].sum())
292/1024:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
292/1025:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/1026:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
292/1027:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
292/1028: framework_contracts.head()
292/1029:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
292/1030:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
292/1031: framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
292/1032:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
292/1033: framework_contracts.groupby(['financial_year'])['call_off_value'].count()
292/1034: framework_contracts.groupby(['financial_year'])['call_off_value'].sum()
292/1035: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
292/1036: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
292/1037: framework_contracts['call_off_value'].sum()
292/1038:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print(works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
292/1039:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print(works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1040:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print('Total Contract Value: Delayed' {}).format(works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1041:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print('Total Contract Value: Delayed' works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1042:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print(works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1043:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print(works_only.shape)
print(works_delayed.shape)
print(works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print(works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1044:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: 'works_only.shape)
print('Works Delayed Number: 'works_delayed.shape)
print('Works In Time Number: 'works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1045:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1046:

works_consult['time_status'] = np.where((works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) & (works_consult['planned_contract_signature_date'] != None), 'In time', 'Delayed')
292/1047:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
292/1048:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/1049: works_consult.head()
292/1050:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/1051: works_consult['contract_value'].sum()
292/1052:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
292/1053: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
292/1054: works_consult.sample(frac=0.5)
292/1055:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1056:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/1057:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/1058:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/1059:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/1060:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/1061:

works_consult['time_status'] = np.where((works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) & (works_consult['planned_contract_signature_date'] != None), 'In time', 'Delayed')
292/1062: works_consult.info()
292/1063:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/1064:

sns.pairplot(works_consult, hue='financial_year');
292/1065:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/1066:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/1067:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
292/1068:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/1069:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/1070:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/1071: works_only.shape
292/1072:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
292/1073:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/1074:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
292/1075:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/1076:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
292/1077:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/1078:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/1079:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
292/1080:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/1081: framework_contracts.head()
292/1082:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
292/1083:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
292/1084: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
292/1085: framework_contracts['call_off_value'].sum()
292/1086: sns.pairplot(framework_contracts, hue='financial_year');
292/1087:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1088:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1089:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1090:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1091:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/1092: group_by_type_count
292/1093: group_by_type_sum
292/1094: group_by_method_count
292/1095: group_by_method_sum
292/1096: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/1097:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1098:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/1099:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/1100:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
292/1101: works_only.head(3)
292/1102: works_only.shape
292/1103:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/1104:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/1105:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/1106:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/1107:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/1108:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/1109:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/1110:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/1111:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/1112:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/1113:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/1114:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/1115:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/1116:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/1117:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/1118:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/1119:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/1120:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1121:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1122:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/1123:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/1124:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/1125:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/1126:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/1127:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/1128:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1129:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1130: consult_above.head(10)
292/1131:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
292/1132:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
292/1133:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/1134: works_consult.head()
292/1135:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']


works_consult.shape
292/1136: works_consult['contract_value'].sum()
292/1137:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
292/1138: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
292/1139: works_consult.sample(frac=0.5)
292/1140:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1141:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/1142:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/1143:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/1144:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/1145:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/1146:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
292/1147: works_consult.info()
292/1148:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/1149:

sns.pairplot(works_consult, hue='financial_year');
292/1150:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/1151:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/1152:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
292/1153:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/1154:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/1155:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/1156: works_only.shape
292/1157:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
292/1158:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/1159:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
292/1160:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/1161:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
292/1162:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/1163:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/1164:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
292/1165:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/1166: framework_contracts.head()
292/1167:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
292/1168:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
292/1169: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
292/1170: framework_contracts['call_off_value'].sum()
292/1171: sns.pairplot(framework_contracts, hue='financial_year');
292/1172:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1173:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1174:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1175:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1176:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/1177: group_by_type_count
292/1178: group_by_type_sum
292/1179: group_by_method_count
292/1180: group_by_method_sum
292/1181: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/1182:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1183:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/1184:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/1185:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
292/1186: works_only.head(3)
292/1187: works_only.shape
292/1188:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/1189:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy
292/1190:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/1191:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/1192:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/1193:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/1194:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/1195:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/1196:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/1197:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/1198:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/1199:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/1200:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult
292/1201:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/1202:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/1203:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

works_within.shape

works_above.shape

works_within.head(3)
292/1204:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/1205:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1206:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1207:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/1208:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/1209:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/1210:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/1211:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/1212:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/1213:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1214:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1215: consult_above.head(10)
292/1216:
lead_time_works_by_method = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/1217:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method
292/1218:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
292/1219:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
292/1220:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
292/1221:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print(works_within.shape)

print(works_above.shape)

works_within.head(3)
292/1222:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

works_within.head(3)
292/1223:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'])

print('Works above total: ', works_above['contract_value'])

works_within.head(3)
292/1224:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
292/1225:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
292/1226:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/1227: works_consult.head()
292/1228:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.to_csv('works_consult.csv')

works_consult.shape
292/1229: works_consult['contract_value'].sum()
292/1230:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
292/1231: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
292/1232: works_consult.sample(frac=0.5)
292/1233:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1234:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/1235:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/1236:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/1237:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/1238:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/1239:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
292/1240: works_consult.info()
292/1241:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/1242:

sns.pairplot(works_consult, hue='financial_year');
292/1243:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/1244:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/1245:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
292/1246:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/1247:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/1248:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/1249: works_only.shape
292/1250:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
292/1251:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/1252:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
292/1253:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/1254:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
292/1255:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/1256:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/1257:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
292/1258:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/1259: framework_contracts.head()
292/1260:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
292/1261:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
292/1262: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
292/1263: framework_contracts['call_off_value'].sum()
292/1264: sns.pairplot(framework_contracts, hue='financial_year');
292/1265:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1266:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1267:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1268:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1269:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/1270: group_by_type_count
292/1271: group_by_type_sum
292/1272: group_by_method_count
292/1273: group_by_method_sum
292/1274: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/1275:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1276:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/1277:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/1278:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
292/1279: works_only.head(3)
292/1280: works_only.shape
292/1281:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
292/1282:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
292/1283:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/1284:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/1285:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/1286:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/1287:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/1288:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/1289:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/1290:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/1291:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/1292:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/1293:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
292/1294:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/1295:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/1296:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
292/1297:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/1298:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1299:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1300:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/1301:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/1302:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/1303:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/1304:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/1305:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/1306:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1307:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1308: consult_above.head(10)
292/1309:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
292/1310:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
292/1311:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/1312: works_consult.head()
292/1313:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
292/1314: works_consult['contract_value'].sum()
292/1315:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
292/1316: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
292/1317: works_consult.sample(frac=0.5)
292/1318:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1319:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/1320:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/1321:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/1322:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/1323:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/1324:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
292/1325: works_consult.to_csv('works_consult.csv')
292/1326: works_consult.info()
292/1327:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/1328:

sns.pairplot(works_consult, hue='financial_year');
292/1329:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/1330:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/1331:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
292/1332:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/1333:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/1334:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/1335: works_only.shape
292/1336:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
292/1337:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/1338:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
292/1339:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/1340:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
292/1341:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/1342:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/1343:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
292/1344:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/1345: framework_contracts.head()
292/1346:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
292/1347:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
292/1348: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
292/1349: framework_contracts['call_off_value'].sum()
292/1350: sns.pairplot(framework_contracts, hue='financial_year');
292/1351:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1352:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1353:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1354:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1355:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/1356: group_by_type_count
292/1357: group_by_type_sum
292/1358: group_by_method_count
292/1359: group_by_method_sum
292/1360: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/1361:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1362:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/1363:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/1364:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
292/1365: works_only.head(3)
292/1366: works_only.shape
292/1367:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
292/1368:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
292/1369:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/1370:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/1371:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/1372:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/1373:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/1374:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/1375:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/1376:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/1377:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/1378:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/1379:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
292/1380:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/1381:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/1382:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
292/1383:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/1384:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1385:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1386:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/1387:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/1388:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/1389:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/1390:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/1391:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/1392:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1393:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1394: consult_above.head(10)
292/1395:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/1396:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
292/1397:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
292/1398:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
292/1399: works_consult.head()
292/1400:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
292/1401: works_consult['contract_value'].sum()
292/1402:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
292/1403: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
292/1404: works_consult.sample(frac=0.5)
292/1405:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1406:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
292/1407:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
292/1408:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
292/1409:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
292/1410:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
292/1411:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
292/1412:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
292/1413: works_consult.to_csv('works_consult.csv')
292/1414: works_consult.info()
292/1415:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
292/1416:

sns.pairplot(works_consult, hue='financial_year');
292/1417:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
292/1418:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
292/1419:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
292/1420:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
292/1421:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
292/1422:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
292/1423: works_only.shape
292/1424:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
292/1425:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
292/1426:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
292/1427:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
292/1428:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
292/1429:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
292/1430:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
292/1431:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
292/1432:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
292/1433: framework_contracts.head()
292/1434:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
292/1435:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
292/1436: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
292/1437: framework_contracts['call_off_value'].sum()
292/1438: sns.pairplot(framework_contracts, hue='financial_year');
292/1439:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1440:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1441:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1442:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
292/1443:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
292/1444: group_by_type_count
292/1445: group_by_type_sum
292/1446: group_by_method_count
292/1447: group_by_method_sum
292/1448: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
292/1449:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
292/1450:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
292/1451:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
292/1452:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
292/1453: works_only.head(3)
292/1454: works_only.shape
292/1455:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
292/1456:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
292/1457:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
292/1458:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
292/1459:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
292/1460:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
292/1461:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
292/1462:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
292/1463:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
292/1464:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
292/1465:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
292/1466:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
292/1467:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
292/1468:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
292/1469:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
292/1470:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
292/1471:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
292/1472:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1473:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1474:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
292/1475:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
292/1476:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
292/1477:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
292/1478:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
292/1479:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
292/1480:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1481:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
292/1482: consult_above.head(10)
300/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
300/2:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

bids_works_consult.head(3)

print(bids_works_consult.shape)
300/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
300/4:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

bids_works_consult.head(3)

print(bids_works_consult.shape)
300/5: bids_works_consult
300/6:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
300/7:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

bids_works_consult.head(3)

print(bids_works_consult.shape)
300/8: bids_works_consult
300/9: bids_works_consult['no_of_bids'].max()
300/10: bids_works_consult[bids_works_consult['no_of_bids'].max()]
300/11: bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
300/12: bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]
300/13: bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()].value_counts
300/14: bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()].value_counts()
300/15: bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
300/16:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

print(bids_works_consult.shape)
302/1: works_consult.sample(frac=0.4)
302/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/3:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/4: works_consult.head()
302/5:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
302/6: works_consult['contract_value'].sum()
302/7:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
302/8: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
302/9: works_consult.sample(frac=0.4)
302/10:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/11:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
302/12:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
302/13:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
302/14:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
302/15:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
302/16:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
302/17:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
302/18: works_consult.to_csv('works_consult.csv')
302/19: works_consult.info()
302/20:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
302/21:

sns.pairplot(works_consult, hue='financial_year');
302/22:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
302/23:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
302/24:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
302/25:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
302/26:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
302/27:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
302/28: works_only.shape
302/29:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
302/30:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
302/31:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
302/32:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
302/33:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
302/34:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
302/35:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
302/36:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
302/37:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
302/38: framework_contracts.head()
302/39:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
302/40:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
302/41: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
302/42: framework_contracts['call_off_value'].sum()
302/43: sns.pairplot(framework_contracts, hue='financial_year');
302/44:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/45:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/46:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/47:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/48:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
302/49: group_by_type_count
302/50: group_by_type_sum
302/51: group_by_method_count
302/52: group_by_method_sum
302/53: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
302/54:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
302/55:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
302/56:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
302/57:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
302/58: works_only.head(3)
302/59: works_only.shape
302/60:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
302/61:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
302/62:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
302/63:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
302/64:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
302/65:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
302/66:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
302/67:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
302/68:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
302/69:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
302/70:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
302/71:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
302/72:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
302/73:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
302/74:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
302/75:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
302/76:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
302/77:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/78:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/79:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
302/80:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
302/81:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
302/82:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
302/83:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
302/84:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
302/85:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/86:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/87: consult_above.head(10)
302/88: works_consult.sample(frac=0.5)
302/89: works_consult[['initation_date'] == NaT]
302/90: works_consult[['initation_date'] is NaT]
302/91: works_consult[['initation_date'] == 'NaT']
302/92: works_consult[['initation_date'] == '']
302/93: works_consult[pd.isnull(['initation_date'])]
302/94: pd.isnull(works_consult['initation_date'])
302/95: works_consult[works_consult['initiation_date'].isnull()]
302/96:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]


print(works_consult_without_initiation_date.shape)
302/97: works_consult.info()
302/98: works_consult.shape
302/99:
works_consult_without_initiation_date = works_consult[~works_consult['initiation_date'].isnull()]


print(works_consult_without_initiation_date.shape)

# works_consult =
302/100:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/101:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/102: works_consult.head()
302/103:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
302/104: works_consult['contract_value'].sum()
302/105:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
302/106: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
302/107: works_consult.sample(frac=0.4)
302/108:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/109:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
302/110:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
302/111:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
302/112:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
302/113:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
302/114:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
302/115:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
302/116: works_consult.to_csv('works_consult.csv')
302/117: works_consult.info()
302/118: works_consult.shape
302/119:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]


print(works_consult_without_initiation_date.shape)

works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning.to_csv('works_consult_planning.csv')
302/120: works_consult.sample(frac=0.5)
302/121:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
302/122:

sns.pairplot(works_consult, hue='financial_year');
302/123:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
302/124:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
302/125:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
302/126:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
302/127:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
302/128:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
302/129: works_only.shape
302/130:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
302/131:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
302/132:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
302/133:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
302/134:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
302/135:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
302/136:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
302/137:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
302/138:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
302/139: framework_contracts.head()
302/140:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
302/141:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
302/142: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
302/143: framework_contracts['call_off_value'].sum()
302/144: sns.pairplot(framework_contracts, hue='financial_year');
302/145:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/146:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/147:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/148:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/149:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
302/150: group_by_type_count
302/151: group_by_type_sum
302/152: group_by_method_count
302/153: group_by_method_sum
302/154: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
302/155:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
302/156:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
302/157:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
302/158:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
302/159: works_only.head(3)
302/160: works_only.shape
302/161:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
302/162:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
302/163:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
302/164:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
302/165:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
302/166:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
302/167:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
302/168:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
302/169:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
302/170:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
302/171:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
302/172:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
302/173:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
302/174:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
302/175:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
302/176:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
302/177:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
302/178:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/179:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/180:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
302/181:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
302/182:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
302/183:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
302/184:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
302/185:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
302/186:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/187:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/188: consult_above.head(10)
302/189:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/190:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/191: works_consult.head()
302/192:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
302/193: works_consult['contract_value'].sum()
302/194:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
302/195: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
302/196: works_consult.sample(frac=0.4)
302/197:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/198:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
302/199:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
302/200:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
302/201:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
302/202:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
302/203:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
302/204:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
302/205: works_consult.to_csv('works_consult.csv')
302/206: works_consult.info()
302/207: works_consult.shape
302/208:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]


print(works_consult_without_initiation_date.shape)

works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning.to_csv('works_consult_planning.csv')
302/209: works_consult.sample(frac=0.5)
302/210:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
302/211:

sns.pairplot(works_consult, hue='financial_year');
302/212:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
302/213:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
302/214:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
302/215:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
302/216:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
302/217:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
302/218: works_only.shape
302/219:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
302/220:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
302/221:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
302/222:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
302/223:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
302/224:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
302/225:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
302/226:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
302/227:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
302/228: framework_contracts.head()
302/229:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
302/230:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
302/231: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
302/232: framework_contracts['call_off_value'].sum()
302/233: sns.pairplot(framework_contracts, hue='financial_year');
302/234:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/235:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/236:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/237:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/238:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
302/239: group_by_type_count
302/240: group_by_type_sum
302/241: group_by_method_count
302/242: group_by_method_sum
302/243: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
302/244:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
302/245:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
302/246:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
302/247:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
302/248: works_only.head(3)
302/249: works_only.shape
302/250:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
302/251:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
302/252:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
302/253:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
302/254:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
302/255:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
302/256:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
302/257:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
302/258:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
302/259:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
302/260:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
302/261:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
302/262:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
302/263:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
302/264:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
302/265:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
302/266:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
302/267:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/268:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/269:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
302/270:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
302/271:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
302/272:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
302/273:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
302/274:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
302/275:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/276:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/277: consult_above.head(10)
302/278:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/279:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/280:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
302/281:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
302/282:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
302/283:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
302/284: works_only.shape
302/285:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
302/286:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
302/287:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
302/288:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
302/289:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
302/290:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
302/291:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
302/292:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
302/293:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
302/294: framework_contracts.head()
302/295:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
302/296:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
302/297: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
302/298: framework_contracts['call_off_value'].sum()
302/299: sns.pairplot(framework_contracts, hue='financial_year');
302/300:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/301:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/302:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/303:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/304:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
302/305: group_by_type_count
302/306: group_by_type_sum
302/307: group_by_method_count
302/308: group_by_method_sum
302/309: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
302/310:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
302/311:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
302/312:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
302/313:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
302/314: works_only.head(3)
302/315: works_only.shape
302/316:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
302/317:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
302/318:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
302/319:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
302/320:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
302/321:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
302/322:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
302/323:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
302/324:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
302/325:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
302/326:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
302/327:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
302/328:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
302/329:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
302/330:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
302/331:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
302/332:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
302/333:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/334:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/335:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
302/336:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
302/337:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
302/338:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
302/339:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
302/340:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
302/341:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/342:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/343: consult_above.head(10)
302/344:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/345:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/346:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/347: works_consult.head()
302/348:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/349:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/350: works_consult.head()
302/351:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
302/352: works_consult['contract_value'].sum()
302/353:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
302/354: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
302/355: works_consult.sample(frac=0.4)
302/356:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/357:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
302/358:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
302/359:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
302/360:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
302/361:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
302/362:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
302/363:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
302/364: works_consult.to_csv('works_consult.csv')
302/365: works_consult.info()
302/366: works_consult.shape
302/367:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]


print(works_consult_without_initiation_date.shape)

works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning.to_csv('works_consult_planning.csv')
302/368: works_consult.sample(frac=0.5)
302/369:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
302/370:

sns.pairplot(works_consult, hue='financial_year');
302/371:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
302/372:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
302/373:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
302/374:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
302/375:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
302/376:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
302/377: works_only.shape
302/378:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
302/379:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
302/380:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
302/381:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
302/382:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
302/383:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
302/384:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
302/385:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
302/386:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
302/387: framework_contracts.head()
302/388:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
302/389:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
302/390: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
302/391: framework_contracts['call_off_value'].sum()
302/392: sns.pairplot(framework_contracts, hue='financial_year');
302/393:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/394:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/395:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/396:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/397:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
302/398: group_by_type_count
302/399: group_by_type_sum
302/400: group_by_method_count
302/401: group_by_method_sum
302/402: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
302/403:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
302/404:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
302/405:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
302/406:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
302/407: works_only.head(3)
302/408: works_only.shape
302/409:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
302/410:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
302/411:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
302/412:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
302/413:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
302/414:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
302/415:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
302/416:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
302/417:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
302/418:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
302/419:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
302/420:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
302/421:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
302/422:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
302/423:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
302/424:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
302/425:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
302/426:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/427:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/428:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
302/429:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
302/430:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
302/431:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
302/432:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
302/433:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
302/434:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/435:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/436: consult_above.head(10)
302/437:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/438:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/439: works_consult.head()
302/440:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
302/441: works_consult['contract_value'].sum()
302/442:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
302/443: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
302/444: works_consult.sample(frac=0.4)
302/445:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/446:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
302/447:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
302/448:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
302/449:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
302/450:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
302/451:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
302/452:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
302/453: works_consult.to_csv('works_consult.csv')
302/454: works_consult.info()
302/455: works_consult.shape
302/456:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]

works_consult_without_initiation_date = works_consult[works_consult['planned_contract_signature_date'].isnull()]

works_consult_without_initiation_date = works_consult[works_consult['actual_signed_date'].isnull()]

print(works_consult_without_initiation_date.shape)

works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning.to_csv('works_consult_planning.csv')
302/457: works_consult.sample(frac=0.5)
302/458:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
302/459:

sns.pairplot(works_consult, hue='financial_year');
302/460:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
302/461:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
302/462:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
302/463:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
302/464:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
302/465:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
302/466: works_only.shape
302/467:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
302/468:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
302/469:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
302/470:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
302/471:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
302/472:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
302/473:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
302/474:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
302/475:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
302/476: framework_contracts.head()
302/477:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
302/478:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
302/479: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
302/480: framework_contracts['call_off_value'].sum()
302/481: sns.pairplot(framework_contracts, hue='financial_year');
302/482:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/483:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/484:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/485:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/486:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
302/487: group_by_type_count
302/488: group_by_type_sum
302/489: group_by_method_count
302/490: group_by_method_sum
302/491: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
302/492:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
302/493:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
302/494:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
302/495:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
302/496: works_only.head(3)
302/497: works_only.shape
302/498:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
302/499:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
302/500:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
302/501:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
302/502:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
302/503:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
302/504:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
302/505:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
302/506:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
302/507:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
302/508:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
302/509:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
302/510:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
302/511:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
302/512:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
302/513:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
302/514:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
302/515:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/516:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/517:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
302/518:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
302/519:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
302/520:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
302/521:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
302/522:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
302/523:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/524:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/525: consult_above.head(10)
302/526:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/527:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/528: works_consult.head()
302/529:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
302/530: works_consult['contract_value'].sum()
302/531:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
302/532: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
302/533: works_consult.sample(frac=0.4)
302/534:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/535:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
302/536:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
302/537:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
302/538:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
302/539:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
302/540:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
302/541:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
302/542: works_consult.to_csv('works_consult.csv')
302/543: works_consult.info()
302/544: works_consult.shape
302/545:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['planned_contract_signature_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['actual_signed_date'].isnull()]

print(works_consult_without_initiation_date.shape)

works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning.to_csv('works_consult_planning.csv')
302/546: works_consult.sample(frac=0.5)
302/547:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
302/548:

sns.pairplot(works_consult, hue='financial_year');
302/549:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
302/550:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
302/551:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
302/552:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
302/553:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
302/554:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
302/555: works_only.shape
302/556:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
302/557:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
302/558:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
302/559:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
302/560:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
302/561:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
302/562:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
302/563:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
302/564:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
302/565: framework_contracts.head()
302/566:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
302/567:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
302/568: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
302/569: framework_contracts['call_off_value'].sum()
302/570: sns.pairplot(framework_contracts, hue='financial_year');
302/571:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/572:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/573:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/574:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/575:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
302/576: group_by_type_count
302/577: group_by_type_sum
302/578: group_by_method_count
302/579: group_by_method_sum
302/580: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
302/581:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
302/582:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
302/583:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
302/584:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
302/585: works_only.head(3)
302/586: works_only.shape
302/587:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
302/588:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
302/589:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
302/590:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
302/591:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
302/592:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
302/593:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
302/594:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
302/595:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
302/596:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
302/597:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
302/598:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
302/599:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
302/600:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
302/601:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
302/602:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
302/603:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
302/604:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/605:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/606:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
302/607:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
302/608:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
302/609:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
302/610:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
302/611:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
302/612:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/613:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/614: consult_above.head(10)
302/615:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['planned_contract_signature_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['actual_signed_date'].isnull()]

print(works_consult_without_initiation_date.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning = works_consult_planning[~works_consult_planning['planned_contract_signature_date'].isnull()]

works_consult_planning = works_consult_planning[~works_consult_planninf['actual_signed_date']]

works_consult_planning.to_csv('works_consult_planning.csv')

print(works_consult_planning.shape)
302/616:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['planned_contract_signature_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['actual_signed_date'].isnull()]

print(works_consult_without_initiation_date.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning = works_consult_planning[~works_consult_planning['planned_contract_signature_date'].isnull()]

works_consult_planning = works_consult_planning[~works_consult_planning['actual_signed_date']]

works_consult_planning.to_csv('works_consult_planning.csv')

print(works_consult_planning.shape)
302/617:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['planned_contract_signature_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['actual_signed_date'].isnull()]

print(works_consult_without_initiation_date.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning = works_consult_planning[~works_consult_planning['planned_contract_signature_date'].isnull()]

works_consult_planning = works_consult_planning[~works_consult_planning['actual_signed_date'].isnull()]

works_consult_planning.to_csv('works_consult_planning.csv')

print(works_consult_planning.shape)
302/618: 298 - 230
302/619: works_consult.sample(frac=0.7)
302/620:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
302/621:
works_consult = pd.read_csv('works_contracts.csv')
framework_contracts = pd.read_csv('framework_contracts_works.csv')
302/622: works_consult.head()
302/623:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
302/624: works_consult['contract_value'].sum()
302/625:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
302/626: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
302/627: works_consult.sample(frac=0.4)
302/628:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/629:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
302/630:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
302/631:
works_consult['planning_period'] = works_consult['planned_contract_signature_date'] - works_consult['initiation_date']

works_consult['implementation_period'] = works_consult['actual_signed_date'] - works_consult['initiation_date']
302/632:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')
302/633:

works_consult['difference_in_days'] = works_consult['implementation_period'] - works_consult['planning_period']
302/634:

works_consult['market_price_status'] = np.where(works_consult['contract_value'] <= works_consult['estimated_amount'], 'within', 'above')
302/635:

works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
302/636: works_consult.to_csv('works_consult.csv')
302/637: works_consult.info()
302/638: works_consult.shape
302/639:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['planned_contract_signature_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['actual_signed_date'].isnull()]

print(works_consult_without_initiation_date.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult_planning = works_consult[~works_consult['initiation_date'].isnull()]

works_consult_planning = works_consult_planning[~works_consult_planning['planned_contract_signature_date'].isnull()]

works_consult_planning = works_consult_planning[~works_consult_planning['actual_signed_date'].isnull()]

works_consult_planning.to_csv('works_consult_planning.csv')

print(works_consult_planning.shape)
302/640: 298 - 230
302/641: works_consult.sample(frac=0.7)
302/642: works_consult.sample(frac=0.5)
302/643:
restricted_international = works_consult[works_consult['method'] == 'Restricted International Bidding (RIB)']

restricted_international
302/644:

sns.pairplot(works_consult, hue='financial_year');
302/645:
count_works = works_consult.groupby(['Entity'])['subject_of_procurement'].count()
count_works.reset_index()
302/646:
count_works_fy = works_consult.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count()
count_works_fy.reset_index()
302/647:
sum_works = works_consult.groupby(['Entity'])['contract_value'].sum()



sum_works.reset_index()
302/648:
sum_works_fy = works_consult.groupby(['Entity', 'financial_year'])['contract_value'].sum()

sum_works_fy.reset_index()
302/649:
sum_works.to_csv('sum_works.csv')

sum_works = pd.read_csv('sum_works.csv')

sum_works
302/650:
works_only = works_consult[works_consult['type'] == 'Works']

works_only.head()
302/651: works_only.shape
302/652:
print(works_only['contract_value'].sum())

print(works_only['subject_of_procurement'].count())
302/653:
group_by_entity_count = works_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_count
302/654:
group_by_entity_sum = works_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_sum.to_csv('group_by_entity_sum_works.csv')

group_by_entity_sum
302/655:
consult_only = works_consult[works_consult['type'] == 'Consultancy Services']

consult_only.head()
302/656:
print(consult_only.shape)

print(consult_only['contract_value'].sum())

print(consult_only['subject_of_procurement'].count())
302/657:
group_by_entity_consult_count = consult_only.groupby(['Entity'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count
302/658:
group_by_entity_consult_count_fy = consult_only.groupby(['Entity', 'financial_year'])['subject_of_procurement'].count().reset_index()
group_by_entity_consult_count_fy
302/659:
group_by_entity_consult_sum = consult_only.groupby(['Entity'])['contract_value'].sum().reset_index()

group_by_entity_consult_sum.to_csv('group_by_entity_consult_sum.csv')

group_by_entity_consult_sum
302/660:
group_by_entity_consult_sum_fy = consult_only.groupby(['Entity', 'financial_year'])['contract_value'].sum().reset_index()
group_by_entity_consult_sum_fy
302/661: framework_contracts.head()
302/662:
print(framework_contracts['call_off_value'].sum())

print(framework_contracts['subject_of_procurement'].count())
302/663:

framework_contracts.groupby(['financial_year'])['subject_of_procurement'].count()
302/664: framework_contracts.groupby(['financial_year'])['call_off_value'].sum().reset_index()
302/665: framework_contracts['call_off_value'].sum()
302/666: sns.pairplot(framework_contracts, hue='financial_year');
302/667:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/668:
g = sns.PairGrid(framework_contracts, hue="method")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/669:
g = sns.PairGrid(framework_contracts, hue="Entity")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/670:
g = sns.PairGrid(framework_contracts, hue="financial_year")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g.add_legend();
302/671:
group_by_type_count = framework_contracts.groupby(['type'])['subject_of_procurement'].count().reset_index()

group_by_type_sum = framework_contracts.groupby(['type'])['call_off_value'].sum().reset_index()

group_by_method_count = framework_contracts.groupby(['method'])['subject_of_procurement'].count().reset_index()

group_by_method_sum = framework_contracts.groupby(['method'])['call_off_value'].sum().reset_index()
302/672: group_by_type_count
302/673: group_by_type_sum
302/674: group_by_method_count
302/675: group_by_method_sum
302/676: works_consult['local_vs_forreign'].value_counts().plot(kind="barh", color=['darkblue', 'lightblue']);
302/677:
works_delayed = works_only[works_only['time_status'] == 'Delayed']

works_in_time = works_only[works_only['time_status'] == 'In time']

print('Works Only: ', works_only.shape)
print('Works Delayed Number: ', works_delayed.shape)
print('Works In Time Number: ', works_in_time.shape)
print('Total Contract Value: Delayed', works_delayed['contract_value'].sum())
print('Total Contract Value: In time', works_in_time['contract_value'].sum())
print('Total Contract Value: Works Only', works_only['contract_value'].sum())

works_delayed.to_csv('works_delayed.csv')
works_in_time.to_csv('works_in_time.csv')
302/678:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=works_only);
302/679:
consult_delayed = consult_only[consult_only['time_status'] == 'Delayed']

consult_in_time = consult_only[consult_only['time_status'] == 'In time']

print(consult_only.shape)
print(consult_delayed.shape)
print(consult_in_time.shape)
print(consult_delayed['contract_value'].sum())
print(consult_in_time['contract_value'].sum())
print(consult_only['contract_value'].sum())
302/680:

sns.catplot(x="time_status", kind="count", palette="ch:.25", data=consult_only);
302/681: works_only.head(3)
302/682: works_only.shape
302/683:
lead_time_works_by_method = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method.to_csv('lead_time_works_by_method.csv')

lead_time_works_by_method
302/684:
lead_time_works_by_method_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_fy.to_csv('lead_time_works_by_method_fy.csv')

lead_time_works_by_method_fy
302/685:
lead_time_works_by_method_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_works_by_method_entity
302/686:
lead_time_works_by_method_entity.to_csv('lead_time_works_by_method_entity.csv')

lead_time_works_by_method_entity = pd.read_csv('lead_time_works_by_method_entity.csv')

sns.scatterplot(x=lead_time_works_by_method_entity['financial_year'],
                y=lead_time_works_by_method_entity['planning_period'])

# sns.pairplot(lead_time_works_by_method_entity, hue="financial_year")

plt.show()
302/687:
sns.countplot(x='financial_year', data=lead_time_works_by_method_entity)

plt.show()
302/688:
sns.scatterplot(x="method",
                y="implementation_period",
                data=lead_time_works_by_method_entity,
                hue="financial_year"
               )

plt.show()
302/689:
g = sns.relplot(x="implementation_period",
            y="planning_period",
            data=lead_time_works_by_method_entity,
            kind="scatter",
            hue="method",
            col="financial_year",
            col_wrap=2,
            markers=True
           )

plt.show()
302/690:
lead_time_by_consult_method = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method
302/691:
lead_time_by_consult_method_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

lead_time_by_consult_method_fy
302/692:
indicative_planned_works = works_only.groupby(['method'])['maximum_indicative_time', 'planning_period'].mean()

indicative_planned_works
302/693:
indicative_planned_works_fy = works_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_fy
302/694:
indicative_planned_works_entity = works_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_works_entity
302/695:
indicative_planned_consult = consult_only.groupby(['method'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult.to_csv('indicative_planned_consult.csv')

indicative_planned_consult
302/696:
indicative_planned_consult_fy = consult_only.groupby(['method', 'financial_year'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_fy
302/697:
indicative_planned_consult_entity = consult_only.groupby(['method', 'financial_year', 'Entity'])['maximum_indicative_time', 'planning_period', 'implementation_period'].mean()

indicative_planned_consult_entity
302/698:
works_within = works_only[works_only['market_price_status'] == 'within']

works_above = works_only[works_only['market_price_status'] == 'above']

print('Works within: ', works_within.shape)

print('Works above: ', works_above.shape)

print('Works within total: ', works_within['contract_value'].sum())

print('Works above total: ', works_above['contract_value'].sum())

works_within.head(3)
302/699:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=works_only);
302/700:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/701:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=works_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/702:
works_within_num = works_within.groupby('method')['contract_value'].count().reset_index()

works_within_num
302/703:
works_within_method = works_within.groupby('method')['contract_value'].sum().reset_index()

works_within_method
302/704:
works_within_fy = works_within.groupby(['method', 'financial_year'])['contract_value'].sum().reset_index()

works_within_fy
302/705:
works_within_entity = works_within.groupby(['method', 'financial_year', 'Entity'])['contract_value'].sum().reset_index()

works_within_entity
302/706:
consult_within = consult_only[consult_only['market_price_status'] == 'within']

consult_above = consult_only[consult_only['market_price_status'] == 'above']

consult_within.shape

consult_above.shape

consult_within.head(3)
302/707:

sns.catplot(x="market_price_status", kind="count", palette="ch:.25", data=consult_only);
302/708:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=consult_within,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/709:
sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity",
            kind="bar",
            ci=None,
            data=consult_above,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
302/710: consult_above.head(10)
302/711:
oct_dls = pd.date_range(start=0, end=0, freq='15min', closed='left')

print(oct_dls)
302/712: works_consult['initiation_date'].isna()
302/713: works_consult[works_consult['initiation_date'].isna()]
302/714:
works_consult_without_initiation_date = works_consult[works_consult['initiation_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['planned_contract_signature_date'].isnull()]

works_consult_without_initiation_date = works_consult_without_initiation_date[works_consult_without_initiation_date['actual_signed_date'].isnull()]

print(works_consult_without_initiation_date.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult_planning = works_consult[~works_consult['initiation_date'].isna()]

works_consult_planning = works_consult_planning[~works_consult_planning['planned_contract_signature_date'].isna()]

works_consult_planning = works_consult_planning[~works_consult_planning['actual_signed_date'].isna()]

works_consult_planning.to_csv('works_consult_planning.csv')

print(works_consult_planning.shape)
302/715: works_consult_planning.sample(frac=0.7)
302/716: works_consult_planning.sample(frac=0.7)
302/717: ### Effective Planning on the contracts that have all the dates
302/718: works_consult_planning.shape
302/719: works_consult_planning.sample(frac=0.7)
302/720: works_consult_planning.groupby(['method'])['planning_period']
302/721: works_consult_planning.groupby(['method'])['planning_period'].mean()
302/722: works_consult_planning.groupby(['method'])['planning_period'].mean().reset_index()
302/723: works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index()
302/724: works_consult_planning.groupby(['method'])['implementation_period'].mean().sort_value('implementation_period', ascending=False).reset_index()
302/725: works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index().sort_index()
302/726: works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index().sort()
302/727: works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index().sort_values('implementation_period')
302/728:
works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index()
                            .sort_values('implementation_period', ascending=False)
302/729: works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index().sort_values('implementation_period', ascending=False)
302/730:

works_consult_planning.groupby(['method'])['planning_period'].mean().reset_index().sort_values('planning_period', ascending=False)
302/731:

works_consult_planning_grp = works_consult_planning.groupby(['method'])['planning_period'].mean().reset_index().sort_values('planning_period', ascending=False)

works_consult_planning_grp['years'] = works_consult_planning_grp['planning_period'] / 365
302/732:

works_consult_planning_grp = works_consult_planning.groupby(['method'])['planning_period'].mean().reset_index().sort_values('planning_period', ascending=False)

works_consult_planning_grp['years'] = works_consult_planning_grp['planning_period'] / 365

works_consult_planning_grp
302/733:

works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index().sort_values('implementation_period', ascending=False)

works_consult_planning_grpI['years'] = works_consult_planning_grp['implementation_period'] / 365

works_consult_planning_grpI
302/734:

works_consult_planning_grp = works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index().sort_values('implementation_period', ascending=False)

works_consult_planning_grpI['years'] = works_consult_planning_grp['implementation_period'] / 365

works_consult_planning_grpI
302/735:

works_consult_planning_grpI = works_consult_planning.groupby(['method'])['implementation_period'].mean().reset_index().sort_values('implementation_period', ascending=False)

works_consult_planning_grpI['years'] = works_consult_planning_grpI['implementation_period'] / 365

works_consult_planning_grpI
302/736:

works_consult_planning_grp = works_consult_planning.groupby(['method'])['planning_period', 'maximum_indicative_time'].mean().reset_index().sort_values('planning_period', ascending=False)

works_consult_planning_grp['years'] = works_consult_planning_grp['planning_period'] / 365

works_consult_planning_grp
302/737: works_consult_planning.shape
302/738: works_consult_planning.values
302/739: works_consult_planning.values()
302/740: works_consult_planning.info()
302/741: works_consult_planning['planned_contract_signature_date'].isna()
302/742: works_consult_planning['planned_contract_signature_date'].isna().value_counts()
302/743:

works_consult_planning_grpI = works_consult_planning.groupby(['method'])['implementation_period', 'maximum_indicative_time'].mean().reset_index().sort_values('implementation_period', ascending=False)

works_consult_planning_grpI['years'] = works_consult_planning_grpI['implementation_period'] / 365

works_consult_planning_grpI
302/744:

works_consult_planning_grp = works_consult_planning.groupby(['method'])['planning_period', 'maximum_indicative_time'].mean().reset_index().sort_values('planning_period', ascending=False)

works_consult_planning_grp['years'] = works_consult_planning_grp['planning_period'] / 365

works_consult_planning_grp
302/745:

works_consult_planning_grp.plot(x = "method", y="planning_period", kind="scatter")
302/746:

works_consult_planning_grp.plot(x = "method", y="planning_period", kind="scatter");
302/747:

works_consult_planning_grp.plot(kind="bar")
plt.show()
302/748:

works_consult_planning_grp.plot(kind="bar" x="method")
plt.show()
302/749:

works_consult_planning_grp.plot(kind="bar")
plt.show()
302/750:

works_consult_planning_grp.plot(kind="barh")
plt.show()
304/1:
# Read in dataset
import pandas as pd
apps_with_duplicates = pd.read_csv('datasets/apps.csv')

# Drop duplicates
apps = apps_with_duplicates.drop_duplicates()

# Print the total number of apps
print('Total number of apps in the dataset = ', apps.shape)

# Print a concise summary of apps dataframe
print(apps.info())

# Have a look at a random sample of n rows
n = 5
apps.sample(n)
304/2:
# List of characters to remove
chars_to_remove = ['+', ',', '$']
# List of column names to clean
cols_to_clean = ['Installs', 'Price']

# Loop for each column
for col in cols_to_clean:
    # Replace each character with an empty string
    for char in chars_to_remove:
        apps[col] = apps[col].astype(str).str.replace(chars_to_remove[char], '')
    # Convert col to numeric
    apps[col] = pd.to_numeric(apps[col])
305/1:
# Import pandas
import pandas as pd

# Load the CSV data into DataFrames
super_bowls = pd.read_csv('datasets/super_bowls.csv')
tv = pd.read_csv('datasets/tv.csv')
halftime_musicians = pd.read_csv('datasets/halftime_musicians.csv')

# Display the first five rows of each DataFrame
display(super_bowls.head())
display(tv.head())
display(halftime_musicians.head())
305/2:
# Summary of the TV data to inspect
tv.info()

print('\n')

# Summary of the halftime musician data to inspect
halftime_musicians.info()
305/3: super_bowls.info()
305/4:
# Import matplotlib and set plotting style
from matplotlib import pyplot as plt
%matplotlib inline
plt.style.use('seaborn')

# Plot a histogram of combined points
plt.hist(x=super_bowls['combined_pts'])
plt.xlabel('Combined Points')
plt.ylabel('Number of Super Bowls')
plt.show()

# Display the Super Bowls with the highest and lowest combined scores
display(super_bowls[super_bowls['combined_pts'] > 70])
display(super_bowls[super_bowls['combined_pts'] < 70])
305/5:
# Plot a histogram of point differences
plt.hist(super_bowls.difference_pts)
plt.xlabel('Point Difference')
plt.ylabel('Number of Super Bowls')
plt.show()

# Display the closest game(s) and biggest blowouts
display(super_bowls[super_bowls['difference_pts'] == 1])
display(super_bowls[super_bowls['difference_pts'] >= 35])
305/6:
# Join game and TV data, filtering out SB I because it was split over two networks
games_tv = pd.merge(tv[tv['super_bowl'] > 1], super_bowls, on='super_bowl')

# Import seaborn
import seaborn as sns

# Create a scatter plot with a linear regression model fit
sns.regplot(
    x=games_tv['difference_pts'],
    y=games_tv['share_household'],
    data=games_tv)
305/7: games_tv.info()
305/8:
# Create a figure with 3x1 subplot and activate the top subplot
plt.subplot(3, 1, 1)
plt.plot(games_tv['super_bowl'], games_tv['avg_us_viewers'], color='#648FFF');
plt.title('Average Number of US Viewers')


# Activate the middle subplot
plt.subplot(3, 1, 2)
plt.plot(games_tv['super_bowl'], games_tv['rating_household'], color='#DC267F');
plt.title('Household Rating')

# Activate the bottom subplot
plt.subplot(3, 1, 3)
plt.plot(games_tv['super_bowl'], games_tv['ad_cost'], color='#FFB000');
plt.title('Ad Cost')
plt.xlabel('SUPER BOWL')

# Improve the spacing between subplots
plt.tight_layout(2)
305/9:
halftime_musicians.info()

halftime_musicians.head(3)
305/10:
# Display all halftime musicians for Super Bowls up to and including Super Bowl XXVII
# ... YOUR CODE FOR TASK 7 ...
display(halftime_musicians[halftime_musicians['super_bowl'] >= 27].sort_values('super_bowl', ascending=True))
305/11:
# Count halftime show appearances for each musician and sort them from most to least
halftime_appearances = halftime_musicians.groupby('musician').count()['super_bowl'].reset_index()
halftime_appearances = halftime_appearances.sort_values('super_bowl', ascending=False)

# Display musicians with more than one halftime show appearance
# ... YOUR CODE FOR TASK 8 ...
halftime_appearances[halftime_appearances['super_bowl'] > 1]
305/12:
# Filter out most marching bands
no_bands = halftime_musicians[~halftime_musicians.musician.str.contains('Marching')]
no_bands = no_bands[~no_bands.musician.str.contains('Spirit')]

# Plot a histogram of number of songs per performance
most_songs = int(max(no_bands['num_songs'].values))
plt.hist(no_bands.num_songs.dropna(), bins=most_songs)
plt.xlabel('Number of Songs Per Halftime Show Performance')
plt.ylabel('Number of Musicians')
plt.show()

# Sort the non-band musicians by number of songs per appearance...
no_bands = no_bands.sort_values('num_songs', ascending=False)
# ...and display the top 15
display(no_bands.head(15))
305/13:
# 2018-2019 conference champions
patriots = 'New England Patriots'
rams = 'Los Angeles Rams'

# Who will win Super Bowl LIII?
super_bowl_LIII_winner = patriots
print('The winner of Super Bowl LIII will be the', super_bowl_LIII_winner)
302/751:

works_consult_planning_grp.plot(y=works_consult_planning_grp['method'], kind="barh")
plt.show()
302/752:

works_consult_planning_grp.plot(kind="barh")
plt.show()
306/1:

works_consult['difference_in_days'] = np.busday(works_consult['planning_period'], works_consult['implementation_period'])
306/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/3: framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/4: works_consult.head()
306/5:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/6:
works_consult = pd.read_csv('works_consult.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/7: works_consult.head()
306/8:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/9: works_consult['contract_value'].sum()
306/10:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/11: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/12: works_consult.sample(frac=0.4)
306/13:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/14:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/15:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/16:
works_consult['planning_period'] = np.busday(works_consult['initiation_date'], works_consult['planned_contract_signature_date'])

works_consult['implementation_period'] = np.busday(works_consult['initiation_date'], works_consult['actual_signed_date'])
306/17:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/18:
works_consult = pd.read_csv('works_consult.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/19: works_consult.head()
306/20:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/21: works_consult['contract_value'].sum()
306/22:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/23: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/24: works_consult.sample(frac=0.4)
306/25:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/26:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/27:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/28:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'], works_consult['planned_contract_signature_date'])

works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'], works_consult['actual_signed_date'])
306/29:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/30:
works_consult = pd.read_csv('works_consult.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/31: works_consult.head()
306/32:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/33: works_consult['contract_value'].sum()
306/34:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/35: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/36: works_consult.sample(frac=0.4)
306/37:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/38:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/39:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], format="%d/%m/%Y").date()
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], format="%d/%m/%Y").date()
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], format="%d/%m/%Y").date()
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], format="%d/%m/%Y").date()
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], format="%d/%m/%Y").date()
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], format="%d/%m/%Y").date()
306/40:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/41:
works_consult = pd.read_csv('works_consult.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/42: works_consult.head()
306/43:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/44: works_consult['contract_value'].sum()
306/45:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/46: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/47: works_consult.sample(frac=0.4)
306/48:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/49:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/50:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], format="%Y/%m/%d").date()
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], format="%Y/%m/%d").date()
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], format="%Y/%m/%d").date()
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], format="%Y/%m/%d").date()
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], format="%Y/%m/%d").date()
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], format="%Y/%m/%d").date()
306/51:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], format="%Y/%m/%d", errors="coerce").date()
306/52:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/53:
works_consult = pd.read_csv('works_consult.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/54: works_consult.head()
306/55:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/56: works_consult['contract_value'].sum()
306/57:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/58: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/59: works_consult.sample(frac=0.4)
306/60:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/61:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/62:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], format="%Y/%m/%d", errors="coerce").date()
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], format="%Y/%m/%d", errors="coerce").date()
306/63:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/64:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'], works_consult['planned_contract_signature_date'])

works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'], works_consult['actual_signed_date'])
306/65:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/66: works_consult.head(3)
306/67:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/68: works_consult.dtypes
306/69:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)
306/70:
works_consult['planning_period'] = pd.to_numeric(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))

works_consult['implementation_period'] = pd.to_numeric(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date))
306/71:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/72:
works_consult = pd.read_csv('works_consult.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/73: works_consult.head()
306/74:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/75: works_consult['contract_value'].sum()
306/76:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/77: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/78: works_consult.sample(frac=0.4)
306/79:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/80:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/81:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/82: works_consult.dtypes
306/83:
works_consult['planning_period'] = pd.to_numeric(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))

works_consult['implementation_period'] = pd.to_numeric(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date))
306/84: works_consult_planning.shape
306/85:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/86:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/87: works_consult
306/88: works_consult.dtypes
306/89: works_consult['initiation_date'].isna()
306/90: works_consult[works_consult['initiation_date'].isna()]
306/91: works_consult.dtypes
306/92:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce').date()
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/93:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/94: works_consult.dtypes
306/95:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)
306/96:
works_consult['planning_period'] = int(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))

works_consult['implementation_period'] = int(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date))
306/97:
# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

print(works_consult['initiation_date'].dt.date)
306/98:
# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

print(works_consult['initiation_date'])
306/99:
# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

print(works_consult['initiation_date'].dt.date)
306/100:
# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

print(works_consult['initiation_date'].dt.date.dtypes)
306/101:
# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

print(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)
306/102:
# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/103:
if ~works_consult['initiation_date'].isnan():
    works_consult['planning_period'] = 0
else:
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/104:
if ~works_consult['initiation_date'].isnan():
    works_consult['planning_period'] = 0
else:
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/105:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/106:
works_consult = pd.read_csv('works_consult.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/107: works_consult.head()
306/108:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/109: works_consult['contract_value'].sum()
306/110:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/111: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/112: works_consult.sample(frac=0.4)
306/113:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/114:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/115: works_consult.dtypes
306/116:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/117: works_consult.dtypes
306/118:
if ~works_consult['initiation_date'].isnan():
    works_consult['planning_period'] = 0
else:
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/119: works_consult[works_consult['initiation_date'].isnan()]
306/120: works_consult[works_consult['initiation_date'].isna()]
306/121:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/122:
works_consult = pd.read_csv('works_consult1.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/123: works_consult.head()
306/124:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/125: works_consult['contract_value'].sum()
306/126:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/127: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/128: works_consult.sample(frac=0.4)
306/129:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/130:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/131: works_consult.dtypes
306/132:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/133: # works_consult[works_consult['initiation_date'].isna()]
306/134:

works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/135:

works_consult['planning_period'] = try:
    np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/136:

works_consult['planning_period'] 
    = try: 
        np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)
    except:
# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/137:

try: 
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)
except:
# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/138:

try: 
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)
except:
    works_consult['planning_period'] = 0
# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))
306/139:

try: 
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)
except:
    works_consult['planning_period'] = 0
# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/140:
works_consult = works_consult[works_consult['initiation_date'].isnull()]

works_consult = works_consult[works_consult['planned_contract_signature_date'].isnull()]

works_consult = works_consult[works_consult['actual_signed_date'].isnull()]

print(works_consult.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult = works_consult[~works_consult['initiation_date'].isna()]

works_consult = works_consult[~works_consult['planned_contract_signature_date'].isna()]

works_consult = works_consult[~works_consult['actual_signed_date'].isna()]

print(works_consult.shape)
306/141:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/142:
works_consult = pd.read_csv('works_consult1.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/143: works_consult.head()
306/144:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/145: works_consult['contract_value'].sum()
306/146:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/147: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/148: works_consult.sample(frac=0.4)
306/149:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/150:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/151:
works_consult = works_consult[works_consult['initiation_date'].isnull()]

works_consult = works_consult[works_consult['planned_contract_signature_date'].isnull()]

works_consult = works_consult[works_consult['actual_signed_date'].isnull()]

print(works_consult.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult = works_consult[~works_consult['initiation_date'].isna()]

works_consult = works_consult[~works_consult['planned_contract_signature_date'].isna()]

works_consult = works_consult[~works_consult['actual_signed_date'].isna()]

print(works_consult.shape)
306/152: works_consult.dtypes
306/153:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/154: # works_consult[works_consult['initiation_date'].isna()]
306/155:

try: 
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)
except:
    works_consult['planning_period'] = 0
# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/156:
# works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

# works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')

print('panning period ', works_consult['planning_period'])

print('implementation period ', works_consult['implementation_period'])
306/157:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/158:
works_consult = pd.read_csv('works_consult1.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/159: works_consult.head()
306/160:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/161: works_consult['contract_value'].sum()
306/162:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/163: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/164: works_consult.sample(frac=0.4)
306/165:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/166:
works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/167:
# works_consult = works_consult[~works_consult['initiation_date'].isnull()]

# works_consult = works_consult[~works_consult['planned_contract_signature_date'].isnull()]

# works_consult = works_consult[~works_consult['actual_signed_date'].isnull()]

# print(works_consult.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult = works_consult[~works_consult['initiation_date'].isna()]

works_consult = works_consult[~works_consult['planned_contract_signature_date'].isna()]

works_consult = works_consult[~works_consult['actual_signed_date'].isna()]

print(works_consult.shape)
306/168: works_consult.dtypes
306/169:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/170: # works_consult[works_consult['initiation_date'].isna()]
306/171:

try: 
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)
except:
    works_consult['planning_period'] = 0
# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/172:
# works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

# works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')

print('panning period ', works_consult['planning_period'])

print('implementation period ', works_consult['implementation_period'])
306/173:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/174:
works_consult = pd.read_csv('works_consult1.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/175: works_consult.head()
306/176:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/177: works_consult['contract_value'].sum()
306/178:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/179: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/180: works_consult.sample(frac=0.4)
306/181:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/182:
# works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/183:
# works_consult = works_consult[~works_consult['initiation_date'].isnull()]

# works_consult = works_consult[~works_consult['planned_contract_signature_date'].isnull()]

# works_consult = works_consult[~works_consult['actual_signed_date'].isnull()]

# print(works_consult.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult = works_consult[~works_consult['initiation_date'].isna()]

works_consult = works_consult[~works_consult['planned_contract_signature_date'].isna()]

works_consult = works_consult[~works_consult['actual_signed_date'].isna()]

print(works_consult.shape)
306/184: works_consult.dtypes
306/185:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/186: # works_consult[works_consult['initiation_date'].isna()]
306/187:

try: 
    works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)
except:
    works_consult['planning_period'] = 0
# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/188:
# works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

# works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')

print('panning period ', works_consult['planning_period'])

print('implementation period ', works_consult['implementation_period'])
306/189:
# works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

# works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')

print('panning period ', works_consult['planning_period'].shape)

print('implementation period ', works_consult['implementation_period'].shape)
306/190:
# works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/191: works_consult.head(1)
306/192:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/193:
works_consult = pd.read_csv('works_consult1.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/194: works_consult.head()
306/195:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/196: works_consult['contract_value'].sum()
306/197:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/198: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/199: works_consult.sample(frac=0.4)
306/200:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/201:
# works_consult['initiation_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_contract_award_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_contract_signature_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['actual_signed_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['planned_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
# works_consult['actual_completion_date'].fillna(value='0000-00-00 00:00:00', inplace=True)
306/202: works_consult.head(1)
306/203:
# works_consult = works_consult[~works_consult['initiation_date'].isnull()]

# works_consult = works_consult[~works_consult['planned_contract_signature_date'].isnull()]

# works_consult = works_consult[~works_consult['actual_signed_date'].isnull()]

# print(works_consult.shape)

# filter out the works and consultancy contracts with initiation_date, planned_contract_signature_date
# and actual_signed_date
works_consult = works_consult[~works_consult['initiation_date'].isna()]

works_consult = works_consult[~works_consult['planned_contract_signature_date'].isna()]

works_consult = works_consult[~works_consult['actual_signed_date'].isna()]

print(works_consult.shape)
306/204: works_consult.dtypes
306/205:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/206: # works_consult[works_consult['initiation_date'].isna()]
306/207:
# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/208:
# works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

# works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')

print('panning period ', works_consult['planning_period'].shape)

print('implementation period ', works_consult['implementation_period'].shape)
306/209: works_consult.head(1)
306/210: works_consult.dtypes
306/211:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/212:
works_consult = pd.read_csv('works_consult1.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/213: works_consult.head()
306/214:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/215: works_consult['contract_value'].sum()
306/216:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/217: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/218: works_consult.sample(frac=0.4)
306/219:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/220:
works_consult['initiation_date'].fillna(value='0000-00-00', inplace=True)
works_consult['planned_contract_award_date'].fillna(value='0000-00-00', inplace=True)
works_consult['planned_contract_signature_date'].fillna(value='0000-00-00', inplace=True)
works_consult['actual_signed_date'].fillna(value='0000-00-00', inplace=True)
works_consult['planned_completion_date'].fillna(value='0000-00-00', inplace=True)
works_consult['actual_completion_date'].fillna(value='0000-00-00', inplace=True)
306/221: works_consult.head(1)
306/222: works_consult.dtypes
306/223: works_consult.dtypes
306/224:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce')
306/225: # works_consult[works_consult['initiation_date'].isna()]
306/226:
# works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/227:
# works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

# works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')

print('panning period ', works_consult['planning_period'].shape)

print('implementation period ', works_consult['implementation_period'].shape)
306/228:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/229: works_consult.dtypes
306/230:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/231: works_consult.dtypes
306/232:
works_consult['planning_period'] = int(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/233: works_consult[works_consult['initiation_date'].isnac]
306/234: works_consult[works_consult['initiation_date'].isnan]
306/235: works_consult[works_consult['initiation_date'].isna()]
306/236:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/237:
works_consult = pd.read_csv('works_consult1.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/238: works_consult.head()
306/239:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/240: works_consult['contract_value'].sum()
306/241:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/242: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/243: works_consult.sample(frac=0.4)
306/244:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/245:
works_consult['initiation_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
works_consult['actual_signed_date'].fillna(value=0, inplace=True)
works_consult['planned_completion_date'].fillna(value=0, inplace=True)
works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/246: works_consult.head(1)
306/247: works_consult.dtypes
306/248:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/249: works_consult.dtypes
306/250: works_consult[works_consult['initiation_date'].isna()]
306/251: # works_consult[works_consult['initiation_date'].isna()]
306/252:
works_consult['planning_period'] = int(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/253: works_consult[works_consult['initiation_date'].isna()]
306/254:
works_consult['planning_period'] = int(np.busday_count(works_consult['initiation_date'] == 0 ? 0 : works_consult['initiation_date'].dt.date,
                                                       works_consult['planned_contract_signature_date'] == 0 ? 0 : works_consult['planned_contract_signature_date'].dt.date))

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/255:
works_consult['planning_period'] = (np.busday_count(np.where(works_consult['initiation_date'] == 0, 0, works_consult['initiation_date'].dt.date), np.where(works_consult['planned_contract_signature_date'] == 0, 0, works_consult['planned_contract_signature_date'].dt.date)))

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/256:
# works_consult['planning_period'] = (np.busday_count(np.where(works_consult['initiation_date'] == 0, 0, works_consult['initiation_date'].dt.date), np.where(works_consult['planned_contract_signature_date'] == 0, 0, works_consult['planned_contract_signature_date'].dt.date)))

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
307/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
307/2: works_consult = pd.read_csv('works_consult1.csv')
307/3: works_consult.shape
307/4:
works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
307/5:
works_consult = works_consult[~works_consult['initiation_date'].isna()]

works_consult = works_consult[~works_consult['planned_contract_signature_date'].isna()]

works_consult = works_consult[~works_consult['actual_signed_date'].isna()]

works_consult.shape
307/6:
works_consult = works_consult[works_consult['initiation_date'].isna()]

works_consult = works_consult[works_consult['planned_contract_signature_date'].isna()]

works_consult = works_consult[works_consult['actual_signed_date'].isna()]

works_consult.shape
308/1: bids_works_consult.groupby['method']['contract_price'].mean()
308/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
308/3:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

print(bids_works_consult.shape)
308/4: bids_works_consult.iloc[bids_works_consult['']]
308/5:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
308/6:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

print(bids_works_consult.shape)
308/7: # bids_works_consult.iloc[bids_works_consult['']]
308/8:

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/9:

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]
308/10: bids_works_consult.groupby['method']['contract_price'].mean()
308/11: bids_works_consult.groupby(['method'])['contract_price'].mean()
308/12: bids_works_consult.groupby(['method'])['no_of_bids'].mean()
308/13:

bids_works_consult['no_of_bids'] = np.where(bids_works_consult['no_of_bids'] == 0, 1, bids_works_consult['no_of_bids'])
308/14: bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]
308/15:
bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/16: bids_works_consult.groupby(['method'])['no_of_bids'].mean()
308/17: bids_works_consult.groupby(['method'])['no_of_bids'].mean().reset_index().sort_values('no_of_bids', ascending=True)
308/18:

bids_works_consult.groupby(['method'])['no_of_bids'].mean().reset_index().sort_values('no_of_bids', ascending=False)
308/19: bids_works_consult.sort_values('no_of_bids', ascending=False)
308/20:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
308/21:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

print(bids_works_consult.shape)
308/22: # bids_works_consult.iloc[bids_works_consult['']]
308/23:

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/24:

bids_works_consult['no_of_bids'] = np.where(bids_works_consult['no_of_bids'] == 0, 1, bids_works_consult['no_of_bids'])
308/25:
bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/26:

bids_works_consult.groupby(['method'])['no_of_bids'].mean().reset_index().sort_values('no_of_bids', ascending=False)
308/27: bids_works_consult.sort_values('no_of_bids', ascending=False)
308/28: bids_works_consult.to_csv('bids_works_consult.csv')
306/257:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/258:
works_consult = pd.read_csv('works_consult2.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/259: works_consult.head()
306/260:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/261: works_consult['contract_value'].sum()
306/262:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/263: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/264: works_consult.sample(frac=0.4)
306/265:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/266:
works_consult['initiation_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
works_consult['actual_signed_date'].fillna(value=0, inplace=True)
works_consult['planned_completion_date'].fillna(value=0, inplace=True)
works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/267: works_consult.head(1)
306/268: works_consult.dtypes
306/269:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/270: works_consult.dtypes
306/271: works_consult[works_consult['initiation_date'].isna()]
306/272: # works_consult[works_consult['initiation_date'].isna()]
306/273:
# works_consult['planning_period'] = (np.busday_count(np.where(works_consult['initiation_date'] == 0, 0, works_consult['initiation_date'].dt.date), np.where(works_consult['planned_contract_signature_date'] == 0, 0, works_consult['planned_contract_signature_date'].dt.date)))

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/274:
# works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

# works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')

# print('panning period ', works_consult['planning_period'].shape)

# print('implementation period ', works_consult['implementation_period'].shape)
306/275:

works_consult['difference_in_days'] = np.busday_count(works_consult['planning_period'], works_consult['implementation_period'])
306/276:
works_consult['planning_period'] = pd.to_numeric(works_consult['planning_period'].dt.days, downcast='integer')

# works_consult['implementation_period'] = pd.to_numeric(works_consult['implementation_period'].dt.days, downcast='integer')

# print('panning period ', works_consult['planning_period'].shape)

# print('implementation period ', works_consult['implementation_period'].shape)
306/277:
works_consult['planning_period'] = (np.busday_count(np.where(works_consult['initiation_date'] == 0, 0, works_consult['initiation_date'].dt.date), np.where(works_consult['planned_contract_signature_date'] == 0, 0, works_consult['planned_contract_signature_date'].dt.date)))

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/278:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/279:
non_empty = works_consult[~works_consult['initiation_date'].isna()]

non_empty.shape()
306/280:
non_empty = works_consult[~works_consult['initiation_date'].isna()]

non_empty.shape
306/281:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/282:
works_consult = pd.read_csv('works_consult2.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/283: works_consult.head()
306/284:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/285: works_consult['contract_value'].sum()
306/286:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/287: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/288: works_consult.sample(frac=0.4)
306/289:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/290:
works_consult['initiation_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
works_consult['actual_signed_date'].fillna(value=0, inplace=True)
works_consult['planned_completion_date'].fillna(value=0, inplace=True)
works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/291: works_consult.head(1)
306/292: works_consult.dtypes
306/293:
# works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
# works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
# works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
# works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
# works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
# works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/294: works_consult.dtypes
306/295:
non_empty = works_consult[~works_consult['initiation_date'].isna()]

non_empty.shape
306/296: # works_consult[works_consult['initiation_date'].isna()]
306/297:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/298:
non_empty = works_consult[~works_consult['initiation_date'].isna()]
non_empty = works_consult[~works_consult['planned_contract_signature_date'].isna()]
non_empty = works_consult[~works_consult['actual_signed_date'].isna()]

non_empty.shape
306/299:
non_empty = works_consult[~works_consult['initiation_date'].isna()]
non_empty = works_consult[~works_consult['planned_contract_signature_date'].isna()]
non_empty = works_consult[~works_consult['actual_signed_date'].isna()]

non_empty.shape
306/300:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/301:
works_consult = pd.read_csv('works_consult2.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/302: works_consult.head()
306/303:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/304: works_consult['contract_value'].sum()
306/305:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/306: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/307: works_consult.sample(frac=0.4)
306/308:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/309:
# works_consult['initiation_date'].fillna(value=0, inplace=True)
# works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
# works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
# works_consult['actual_signed_date'].fillna(value=0, inplace=True)
# works_consult['planned_completion_date'].fillna(value=0, inplace=True)
# works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/310: works_consult.head(1)
306/311: works_consult.dtypes
306/312:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/313: works_consult.dtypes
306/314:
non_empty = works_consult[~works_consult['initiation_date'].isna()]
non_empty = works_consult[~works_consult['planned_contract_signature_date'].isna()]
non_empty = works_consult[~works_consult['actual_signed_date'].isna()]

non_empty.shape
306/315: # works_consult[works_consult['initiation_date'].isna()]
306/316:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/317:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/318:
works_consult = pd.read_csv('works_consult2.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/319: works_consult.head()
306/320:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/321: works_consult['contract_value'].sum()
306/322:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/323: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/324: works_consult.sample(frac=0.4)
306/325:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/326:
# works_consult['initiation_date'].fillna(value=0, inplace=True)
# works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
# works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
# works_consult['actual_signed_date'].fillna(value=0, inplace=True)
# works_consult['planned_completion_date'].fillna(value=0, inplace=True)
# works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/327: works_consult.head(1)
306/328: works_consult.dtypes
306/329:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/330: works_consult.dtypes
306/331:
non_empty = works_consult[works_consult['initiation_date'].isna()]
non_empty = works_consult[works_consult['planned_contract_signature_date'].isna()]
non_empty = works_consult[works_consult['actual_signed_date'].isna()]

non_empty.shape
306/332: # works_consult[works_consult['initiation_date'].isna()]
306/333:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/334:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/335:
works_consult = pd.read_csv('works_consult2.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/336: works_consult.head()
306/337:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/338: works_consult['contract_value'].sum()
306/339:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/340: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/341: works_consult.sample(frac=0.4)
306/342:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/343:
works_consult['initiation_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
works_consult['actual_signed_date'].fillna(value=0, inplace=True)
works_consult['planned_completion_date'].fillna(value=0, inplace=True)
works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/344: works_consult.head(1)
306/345: works_consult.dtypes
306/346:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/347: works_consult.dtypes
306/348:
non_empty = works_consult[works_consult['initiation_date'].isna()]
non_empty = works_consult[works_consult['planned_contract_signature_date'].isna()]
non_empty = works_consult[works_consult['actual_signed_date'].isna()]

non_empty.shape
306/349: # works_consult[works_consult['initiation_date'].isna()]
306/350:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/351:
non_empty = works_consult[~works_consult['initiation_date'].isna()]
non_empty = works_consult[~works_consult['planned_contract_signature_date'].isna()]
non_empty = works_consult[~works_consult['actual_signed_date'].isna()]

non_empty.shape
306/352: non_empty.head(3)
306/353: non_empty.sample(frac=0.8)
306/354:
non_empty = works_consult[works_consult['initiation_date'].isnull()]
# non_empty = works_consult[~works_consult['planned_contract_signature_date'].isna()]
# non_empty = works_consult[~works_consult['actual_signed_date'].isna()]

non_empty.shape
306/355:
non_empty = works_consult[works_consult['initiation_date'].isnull()]
non_empty = works_consult[~works_consult['planned_contract_signature_date'].isnull()]
non_empty = works_consult[~works_consult['actual_signed_date'].isnull()]

non_empty.shape
306/356:
non_empty = works_consult[works_consult['initiation_date'].isnull()]
# non_empty = non_empty[works_consult['planned_contract_signature_date'].isnull()]
# non_empty = non_empty[works_consult['actual_signed_date'].isnull()]

non_empty.shape
306/357:
non_empty = works_consult[works_consult['initiation_date'].isnull()]
non_empty = non_empty[works_consult['planned_contract_signature_date'].isnull()]
# non_empty = non_empty[works_consult['actual_signed_date'].isnull()]

non_empty.shape
306/358:
non_empty = works_consult[works_consult['initiation_date'].isnull()]
non_empty = non_empty[non_empty['planned_contract_signature_date'].isnull()]
# non_empty = non_empty[works_consult['actual_signed_date'].isnull()]

non_empty.shape
306/359:
# non_empty = works_consult[works_consult['initiation_date'].isnull()]
non_empty = non_empty[non_empty['planned_contract_signature_date'].isnull()]
# non_empty = non_empty[works_consult['actual_signed_date'].isnull()]

non_empty.shape
306/360:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/361:
works_consult = pd.read_csv('works_consult2.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/362: works_consult.head()
306/363:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/364: works_consult['contract_value'].sum()
306/365:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/366: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/367: works_consult.sample(frac=0.4)
306/368:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/369:
works_consult['initiation_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
works_consult['actual_signed_date'].fillna(value=0, inplace=True)
works_consult['planned_completion_date'].fillna(value=0, inplace=True)
works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/370: works_consult.head(1)
306/371: works_consult.dtypes
306/372:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/373: works_consult.dtypes
306/374:
non_empty = works_consult[works_consult['initiation_date'].isnull()]
non_empty = non_empty[non_empty['planned_contract_signature_date'].isnull()]
non_empty = non_empty[non_empty['actual_signed_date'].isnull()]

non_empty.shape
306/375: non_empty.sample(frac=0.8)
306/376:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/377:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/378:
works_consult = pd.read_csv('works_consult2.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/379: works_consult.head()
306/380:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/381: works_consult['contract_value'].sum()
306/382:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/383: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/384: works_consult.sample(frac=0.4)
306/385:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/386:
works_consult['initiation_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
works_consult['actual_signed_date'].fillna(value=0, inplace=True)
works_consult['planned_completion_date'].fillna(value=0, inplace=True)
works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/387: works_consult.head(1)
306/388: works_consult.dtypes
306/389:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/390: works_consult.dtypes
306/391:
non_empty = works_consult[~works_consult['initiation_date'].isnull()]
non_empty = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
non_empty = non_empty[~non_empty['actual_signed_date'].isnull()]

non_empty.shape
306/392: non_empty_works_consult.dtypes
306/393:
non_empty_works_consult = works_consult[~works_consult['initiation_date'].isnull()]
non_empty_works_consult = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
non_empty_works_consult = non_empty[~non_empty['actual_signed_date'].isnull()]

non_empty.shape
306/394: non_empty_works_consult.dtypes
306/395:
works_consult['planning_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/396:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


works_consult.head(3)
306/397:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date'].dt.date)

# works_consult['implementation_period'] = np.busday_count(works_consult['initiation_date'].dt.date, works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


non_empty_works_consult.head(3)
306/398:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date', weekmask=[1,1,1,1,1,0,0]].dt.date)

non_empty_works_consult['timeline_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


non_empty_works_consult.head(3)
306/399:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date'].dt.date, weekmask=[1,1,1,1,1,0,0])

non_empty_works_consult['timeline_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


non_empty_works_consult.head(3)
306/400:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date'].dt.date, weekmask='1,1,1,1,1,0,0')

non_empty_works_consult['timeline_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


non_empty_works_consult.head(3)
306/401:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty_works_consult['timeline_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date)

# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


non_empty_works_consult.head(3)
306/402:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty_works_consult['timeline_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date, weekmask='1111100')

non_empty_works_consult['implementation_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date, weekmask='1111100')


# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


non_empty_works_consult.head(3)
306/403:

non_empty_works_consult['difference_in_days'] = np.busday_count(non_empty_works_consult['planning_period'], non_empty_works_consult['implementation_period'])

non_empty_works_consult.head(3)
306/404:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty_works_consult['timeline_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date, weekmask='1111100')

non_empty_works_consult['implementation_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date, weekmask='1111100')


# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


non_empty_works_consult.head(3)
306/405:


non_empty_works_consult.to_csv('timeline_works_consult.csv')
306/406:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
306/407:
works_consult = pd.read_csv('works_consult2.csv')

framework_contracts = pd.read_csv('framework_contracts_works.csv')
306/408: works_consult.head()
306/409:

works_consult = works_consult[works_consult['financial_year'] != '2016-2017']

works_consult = works_consult[works_consult['financial_year'] != '2017-2018']

works_consult.shape
306/410: works_consult['contract_value'].sum()
306/411:

works_consult.groupby(['financial_year'])['contract_value'].sum().reset_index().to_csv('works_consult_value_by_fy.csv')
306/412: works_consult.groupby(['financial_year'])['Entity'].count().reset_index()
306/413: works_consult.sample(frac=0.4)
306/414:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=works_consult,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
306/415:
works_consult['initiation_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_award_date'].fillna(value=0, inplace=True)
works_consult['planned_contract_signature_date'].fillna(value=0, inplace=True)
works_consult['actual_signed_date'].fillna(value=0, inplace=True)
works_consult['planned_completion_date'].fillna(value=0, inplace=True)
works_consult['actual_completion_date'].fillna(value=0, inplace=True)
306/416: works_consult.head(1)
306/417: works_consult.dtypes
306/418:
works_consult['initiation_date'] = pd.to_datetime(works_consult['initiation_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_award_date'] = pd.to_datetime(works_consult['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_contract_signature_date'] = pd.to_datetime(works_consult['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_signed_date'] = pd.to_datetime(works_consult['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
works_consult['planned_completion_date'] = pd.to_datetime(works_consult['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
works_consult['actual_completion_date'] = pd.to_datetime(works_consult['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
306/419: works_consult.dtypes
306/420:
non_empty_works_consult = works_consult[~works_consult['initiation_date'].isnull()]
non_empty_works_consult = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
non_empty_works_consult = non_empty[~non_empty['actual_signed_date'].isnull()]

non_empty.shape
306/421: non_empty_works_consult.dtypes
306/422:
non_empty_works_consult['planning_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty_works_consult['timeline_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date, weekmask='1111100')

non_empty_works_consult['implementation_period'] = np.busday_count(non_empty_works_consult['initiation_date'].dt.date, non_empty_works_consult['actual_signed_date'].dt.date, weekmask='1111100')


# print(np.busday_count(works_consult['initiation_date'].dt.date, works_consult['planned_contract_signature_date'].dt.date))


non_empty_works_consult.head(3)
306/423:

non_empty_works_consult['difference_in_days'] = np.busday_count(non_empty_works_consult['planning_period'], non_empty_works_consult['implementation_period'])

non_empty_works_consult.head(3)
308/29:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
308/30:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

print(bids_works_consult.shape)
308/31: # bids_works_consult.iloc[bids_works_consult['']]
308/32:

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/33:

bids_works_consult['no_of_bids'] = np.where(bids_works_consult['no_of_bids'] == 0, 1, bids_works_consult['no_of_bids'])
308/34:
bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/35:

bids_works_consult.groupby(['method'])['no_of_bids'].mean().reset_index().sort_values('no_of_bids', ascending=False)
308/36: bids_works_consult.sort_values('no_of_bids', ascending=False)
308/37: bids_works_consult.to_csv('bids_works_consult.csv')
308/38:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
308/39:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

print(bids_works_consult.shape)
308/40: # bids_works_consult.iloc[bids_works_consult['']]
308/41:

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/42:

bids_works_consult['no_of_bids'] = np.where(bids_works_consult['no_of_bids'] == 0, 1, bids_works_consult['no_of_bids'])
308/43:
bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/44:

bids_works_consult.groupby(['method'])['no_of_bids'].mean().reset_index().sort_values('no_of_bids', ascending=False)
308/45: bids_works_consult.sort_values('no_of_bids', ascending=False)
308/46: bids_works_consult.to_csv('bids_works_consult.csv')
308/47:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
308/48:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

print(bids_works_consult.shape)
308/49: # bids_works_consult.iloc[bids_works_consult['']]
308/50:

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/51:

bids_works_consult['no_of_bids'] = np.where(bids_works_consult['no_of_bids'] == 0, 1, bids_works_consult['no_of_bids'])
308/52:
bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/53:

bids_works_consult.groupby(['method'])['no_of_bids'].mean().reset_index().sort_values('no_of_bids', ascending=False)
308/54: bids_works_consult.sort_values('no_of_bids', ascending=False)
308/55: bids_works_consult.to_csv('bids_works_consult.csv')
308/56:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
308/57:
bids_works_consult = pd.read_csv('bids_works_consult.csv')

print(bids_works_consult.shape)
308/58: # bids_works_consult.iloc[bids_works_consult['']]
308/59:

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/60:

bids_works_consult['no_of_bids'] = np.where(bids_works_consult['no_of_bids'] == 0, 1, bids_works_consult['no_of_bids'])

bids_works_consult['no_of_bids_through'] = np.where(bids_works_consult['no_of_bids_through'] == 0, 1, bids_works_consult['no_of_bids_through'])
308/61:
bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].min()]

bids_works_consult[bids_works_consult['no_of_bids'] == bids_works_consult['no_of_bids'].max()]
308/62:

bids_works_consult.groupby(['method'])['no_of_bids'].mean().reset_index().sort_values('no_of_bids', ascending=False)
308/63: bids_works_consult.sort_values('no_of_bids', ascending=False)
308/64: bids_works_consult.to_csv('bids_works_consult.csv')
310/1: ## Data
310/2: 2+2
310/3: 2+2
310/4: 2+2
312/1:
The Indicators to be looked at include:
    -- Disclosure Levels (GPP Reporting)
    -- Local Vs Foreign
    -- Distribution of Contracts
    -- Average Bids
    -- Market Price Assessment
    -- Competitiveness
312/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
312/3:
# Reading the csv file

agric_water = pd.read_csv('agric_water.csv')

agric_water.head()
312/4:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
312/5:
# Reading the csv file

agric_water = pd.read_csv('agric_water.csv')

agric_water.head()
315/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
315/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
315/3:

agric_water = pd.read_csv('agriculture_water.csv')


# framework_contracts = pd.read_csv('framework_contracts_works.csv')
315/4: agric_water.head()
315/5:

agric_water = pd.read_csv('agriculture_water.csv', index_col="")


# framework_contracts = pd.read_csv('framework_contracts_works.csv')
315/6:

agric_water = pd.read_csv('agriculture_water.csv')


# framework_contracts = pd.read_csv('framework_contracts_works.csv')
315/7: agric_water.head()
315/8:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=agric_water,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
315/9:
agric_water['initiation_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_award_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_signature_date'].fillna(value=0, inplace=True)
agric_water['actual_signed_date'].fillna(value=0, inplace=True)
agric_water['planned_completion_date'].fillna(value=0, inplace=True)
agric_water['actual_completion_date'].fillna(value=0, inplace=True)
315/10:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/11:

agric_water['market_price_status'] = np.where(agric_water['contract_value'] <= agric_water['estimated_amount'], 'within', 'above')
315/12:
works_consult['time_status'] = np.where(
    (works_consult['planned_contract_signature_date'] >= works_consult['actual_signed_date']) &
    (works_consult['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
315/13:
agric_water['time_status'] = np.where(
    (agric_water['planned_contract_signature_date'] >= agric_water['actual_signed_date']) &
    (agric_water['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
315/14:
agric_water['time_status'] = np.where(
    (agric_water['planned_contract_signature_date'] >= agric_water['actual_signed_date']) &
    (agric_water['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
315/15: agric_water.head(5)
315/16:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/17:
agric_water['planning_period'] = np.busday_count(agric_water['initiation_date'].dt.date, agric_water['planned_contract_signature_date'].dt.date, weekmask='1111100')

agric_water['timeline_period'] = np.busday_count(agric_water['initiation_date'].dt.date, agric_water['actual_signed_date'].dt.date, weekmask='1111100')

agric_water['implementation_period'] = np.busday_count(agric_water['initiation_date'].dt.date, agric_water['actual_signed_date'].dt.date, weekmask='1111100')

agric_water.head(3)
315/18:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/19:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/20:
non_empty = agric_water[~agric_water['initiation_date'].isnull()]
non_empty = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
non_empty = non_empty[~non_empty['actual_signed_date'].isnull()]

non_empty.shape
315/21:
non_empty = agric_water[~agric_water['initiation_date'].isnull()]
non_empty = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
non_empty = non_empty[~non_empty['actual_signed_date'].isnull()]

non_empty.shape
315/22: agric_water.shape
315/23: agric_water.shape
315/24: 3736 - 1301
315/25:
non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty.head(3)
315/26:
non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty.tail(3)
315/27: 3736 - 1301
315/28: non_empty.shape
315/29: non_empty.head()
315/30: non_empty.tail()
315/31:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
315/32:

agric_water = pd.read_csv('agriculture_water.csv')


# framework_contracts = pd.read_csv('framework_contracts_works.csv')
315/33: agric_water.head()
315/34:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=agric_water,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
315/35:
agric_water['initiation_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_award_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_signature_date'].fillna(value=0, inplace=True)
agric_water['actual_signed_date'].fillna(value=0, inplace=True)
agric_water['planned_completion_date'].fillna(value=0, inplace=True)
agric_water['actual_completion_date'].fillna(value=0, inplace=True)
315/36:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/37:
non_empty = agric_water[~agric_water['initiation_date'].isnull()]
non_empty = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
non_empty = non_empty[~non_empty['actual_signed_date'].isnull()]

non_empty.shape
315/38: agric_water.shape
315/39: 3736 - 1301
315/40: non_empty.tail()
315/41:
non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty.tail(3)
315/42:

agric_water['market_price_status'] = np.where(agric_water['contract_value'] <= agric_water['estimated_amount'], 'within', 'above')
315/43:
agric_water['time_status'] = np.where(
    (agric_water['planned_contract_signature_date'] >= agric_water['actual_signed_date']) &
    (agric_water['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
315/44: agric_water.head(5)
315/45:
non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty.tail()
315/46:
non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty.sample(frac=.7)
315/47:
non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty.sample(frac=.2)
315/48:
non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty.sample(frac=.3)
315/49:
non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

non_empty.sample(frac=.3)
315/50:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
315/51:

agric_water = pd.read_csv('agriculture_water.csv')


# framework_contracts = pd.read_csv('framework_contracts_works.csv')
315/52: agric_water.head()
315/53:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=agric_water,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
315/54:
agric_water['initiation_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_award_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_signature_date'].fillna(value=0, inplace=True)
agric_water['actual_signed_date'].fillna(value=0, inplace=True)
agric_water['planned_completion_date'].fillna(value=0, inplace=True)
agric_water['actual_completion_date'].fillna(value=0, inplace=True)
315/55:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/56:
# non_empty = agric_water[~agric_water['initiation_date'].isnull()]
# non_empty = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
# non_empty = non_empty[~non_empty['actual_signed_date'].isnull()]

# non_empty.shape
315/57: # agric_water.shape
315/58: # 3736 - 1301
315/59: # non_empty.tail()
315/60:
# non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

# non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

# non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

# non_empty.sample(frac=.3)
315/61:
agric_water['planning_period'] = agric_water['planned_contract_signature_date'] - agric_water['initiation_date']

agric_water['timeline_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']

agric_water['implementation_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']
315/62:

agric_water['market_price_status'] = np.where(agric_water['contract_value'] <= agric_water['estimated_amount'], 'within', 'above')
315/63:
agric_water['time_status'] = np.where(
    (agric_water['planned_contract_signature_date'] >= agric_water['actual_signed_date']) &
    (agric_water['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
315/64: agric_water.head(5)
315/65: agric_water.head(5)
315/66: agric_water.shape
315/67:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
315/68:

agric_water = pd.read_csv('agriculture_water.csv')


# framework_contracts = pd.read_csv('framework_contracts_works.csv')
315/69: agric_water.head()
315/70:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=agric_water,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
315/71:
agric_water['initiation_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_award_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_signature_date'].fillna(value=0, inplace=True)
agric_water['actual_signed_date'].fillna(value=0, inplace=True)
agric_water['planned_completion_date'].fillna(value=0, inplace=True)
agric_water['actual_completion_date'].fillna(value=0, inplace=True)
315/72:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/73:
# non_empty = agric_water[~agric_water['initiation_date'].isnull()]
# non_empty = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
# non_empty = non_empty[~non_empty['actual_signed_date'].isnull()]

# non_empty.shape
315/74: # agric_water.shape
315/75: # 3736 - 1301
315/76: # non_empty.tail()
315/77:
# non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

# non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

# non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

# non_empty.sample(frac=.3)
315/78:
agric_water['planning_period'] = agric_water['planned_contract_signature_date'] - agric_water['initiation_date']

agric_water['timeline_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']

agric_water['implementation_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']
315/79: agric_water.head()
315/80:

agric_water['market_price_status'] = np.where(agric_water['contract_value'] <= agric_water['estimated_amount'], 'within', 'above')
315/81:
agric_water['time_status'] = np.where(
    (agric_water['planned_contract_signature_date'] >= agric_water['actual_signed_date']) &
    (agric_water['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
315/82: agric_water.head(5)
315/83: agric_water.shape
315/84:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
315/85:

agric_water = pd.read_csv('agriculture_water.csv')


# framework_contracts = pd.read_csv('framework_contracts_works.csv')
315/86: agric_water.head()
315/87:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=agric_water,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
315/88:
agric_water['initiation_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_award_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_signature_date'].fillna(value=0, inplace=True)
agric_water['actual_signed_date'].fillna(value=0, inplace=True)
agric_water['planned_completion_date'].fillna(value=0, inplace=True)
agric_water['actual_completion_date'].fillna(value=0, inplace=True)
315/89:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/90:
# non_empty = agric_water[~agric_water['initiation_date'].isnull()]
# non_empty = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
# non_empty = non_empty[~non_empty['actual_signed_date'].isnull()]

# non_empty.shape
315/91: # agric_water.shape
315/92: # 3736 - 1301
315/93: # non_empty.tail()
315/94:
# non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

# non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

# non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

# non_empty.sample(frac=.3)
315/95:
agric_water['planning_period'] = agric_water['planned_contract_signature_date'] - agric_water['initiation_date']

agric_water['timeline_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']

agric_water['implementation_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']
315/96: agric_water.head()
315/97:

agric_water['market_price_status'] = np.where(agric_water['contract_value'] <= agric_water['estimated_amount'], 'within', 'above')
315/98:
agric_water['time_status'] = np.where(
    (agric_water['planned_contract_signature_date'] >= agric_water['actual_signed_date']) &
    (agric_water['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
315/99: agric_water.head(5)
315/100: agric_water.shape
315/101:
agric_water['planning_period'] = agric_water['planned_contract_signature_date'] - agric_water['initiation_date']

agric_water['timeline_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']

agric_water['implementation_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']
315/102:
agric_water['planning_period'] = agric_water['planned_contract_signature_date'] - agric_water['initiation_date']

agric_water['timeline_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']

agric_water['implementation_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']
315/103: agric_water['difference_in_days'] = agric_water['implementation_period'] - agric_water['planning_period']
315/104: agric_water.head()
315/105: agric_water.shape
315/106:
# Save to pdf

agric_water.to_csv('agric_water_disclosure.csv')
315/107:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
315/108:

agric_water = pd.read_csv('agriculture_water.csv')


# framework_contracts = pd.read_csv('framework_contracts_works.csv')
315/109: agric_water.head()
315/110:
year_order = [
    '2018-2019',
    '2019-2020',
    '2020-2021'
]

sns.catplot(x="financial_year",
            y="contract_value",
            hue="Entity", kind="bar",
            ci=None,
            data=agric_water,
            height=6.27,
            aspect=11.7/8.27,
            order=year_order
           )

plt.show()
315/111:
agric_water['initiation_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_award_date'].fillna(value=0, inplace=True)
agric_water['planned_contract_signature_date'].fillna(value=0, inplace=True)
agric_water['actual_signed_date'].fillna(value=0, inplace=True)
agric_water['planned_completion_date'].fillna(value=0, inplace=True)
agric_water['actual_completion_date'].fillna(value=0, inplace=True)
315/112:
agric_water['initiation_date'] = pd.to_datetime(agric_water['initiation_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_award_date'] = pd.to_datetime(agric_water['planned_contract_award_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_contract_signature_date'] = pd.to_datetime(agric_water['planned_contract_signature_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_signed_date'] = pd.to_datetime(agric_water['actual_signed_date'], errors='coerce', format='%Y-%m-%d')
agric_water['planned_completion_date'] = pd.to_datetime(agric_water['planned_completion_date'], errors='coerce', format='%Y-%m-%d')
agric_water['actual_completion_date'] = pd.to_datetime(agric_water['actual_completion_date'], errors='coerce', format='%Y-%m-%d')
315/113:
# non_empty = agric_water[~agric_water['initiation_date'].isnull()]
# non_empty = non_empty[~non_empty['planned_contract_signature_date'].isnull()]
# non_empty = non_empty[~non_empty['actual_signed_date'].isnull()]

# non_empty.shape
315/114: # agric_water.shape
315/115: # 3736 - 1301
315/116: # non_empty.tail()
315/117:
# non_empty['planning_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['planned_contract_signature_date'].dt.date, weekmask='1111100')

# non_empty['timeline_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

# non_empty['implementation_period'] = np.busday_count(non_empty['initiation_date'].dt.date, non_empty['actual_signed_date'].dt.date, weekmask='1111100')

# non_empty.sample(frac=.3)
315/118:
agric_water['planning_period'] = agric_water['planned_contract_signature_date'] - agric_water['initiation_date']

agric_water['timeline_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']

agric_water['implementation_period'] = agric_water['actual_signed_date'] - agric_water['initiation_date']
315/119: agric_water['difference_in_days'] = agric_water['implementation_period'] - agric_water['planning_period']
315/120: agric_water.head()
315/121:

agric_water['market_price_status'] = np.where(agric_water['contract_value'] <= agric_water['estimated_amount'], 'within', 'above')
315/122:
agric_water['time_status'] = np.where(
    (agric_water['planned_contract_signature_date'] >= agric_water['actual_signed_date']) &
    (agric_water['planned_contract_signature_date'] is not None),
    'In time', 'Delayed')
315/123: agric_water.head(5)
315/124: agric_water.shape
315/125:
# Save to pdf

agric_water.to_csv('agric_water_disclosure.csv')
317/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_colwidth', None)
317/2:
# Reading the csv file

agric_water = pd.read_csv('agric_water.csv')

agric_water.head()
319/1: 2+3
321/1:
import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle

df = pd.read_csv("FuelConsumption.csv")
#use required features
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]

#Training Data and Predictor Variable
# Use all data for training (tarin-test-split not used)
x = cdf.iloc[:, :3]
y = cdf.iloc[:, -1]
regressor = LinearRegression()

#Fitting model with trainig data
regressor.fit(x, y)

# Saving model to current directory
# Pickle serializes objects so they can be saved to a file, and loaded in a program again later on.
pickle.dump(regressor, open('model.pkl','wb'))

'''
#Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))
print(model.predict([[2.6, 8, 10.1]]))
'''
322/1:
import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle

df = pd.read_csv("FuelConsumption.csv")
#use required features
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]

#Training Data and Predictor Variable
# Use all data for training (tarin-test-split not used)
x = cdf.iloc[:, :3]
y = cdf.iloc[:, -1]
regressor = LinearRegression()

#Fitting model with trainig data
regressor.fit(x, y)

# Saving model to current directory
# Pickle serializes objects so they can be saved to a file, and loaded in a program again later on.
pickle.dump(regressor, open('model.pkl','wb'))

'''
#Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))
print(model.predict([[2.6, 8, 10.1]]))
'''
322/2:
import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle

df = pd.read_csv("FuelConsumption.csv")
#use required features
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]

#Training Data and Predictor Variable
# Use all data for training (tarin-test-split not used)
x = cdf.iloc[:, :3]
y = cdf.iloc[:, -1]
regressor = LinearRegression()

#Fitting model with trainig data
regressor.fit(x, y)

# Saving model to current directory
# Pickle serializes objects so they can be saved to a file, and loaded in a program again later on.
pickle.dump(regressor, open('model.pkl','wb'))

'''
#Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))
print(model.predict([[2.6, 8, 10.1]]))
'''

model = pickle.load(open('model.pkl','rb'))
print(model.predict([[2.6, 8, 10.1]]))
322/3: 2+3
322/4:
import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle

df = pd.read_csv("FuelConsumption.csv")
#use required features
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]

#Training Data and Predictor Variable
# Use all data for training (tarin-test-split not used)
x = cdf.iloc[:, :3]
y = cdf.iloc[:, -1]
regressor = LinearRegression()

#Fitting model with trainig data
regressor.fit(x, y)

# Saving model to current directory
# Pickle serializes objects so they can be saved to a file, and loaded in a program again later on.
pickle.dump(regressor, open('model.pkl','wb'))

'''
#Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))
print(model.predict([[2.6, 8, 10.1]]))
'''
322/5: import pandas as pd
322/6: import pandas as pd
324/1: import pandas as pd
324/2:
import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle

df = pd.read_csv("FuelConsumption.csv")
#use required features
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]

#Training Data and Predictor Variable
# Use all data for training (tarin-test-split not used)
x = cdf.iloc[:, :3]
y = cdf.iloc[:, -1]
regressor = LinearRegression()

#Fitting model with trainig data
regressor.fit(x, y)

# Saving model to current directory
# Pickle serializes objects so they can be saved to a file, and loaded in a program again later on.
pickle.dump(regressor, open('model.pkl','wb'))

'''
#Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))
print(model.predict([[2.6, 8, 10.1]]))
'''
324/3:

model = pickle.load(open('model.pkl','rb'))
print(model.predict([[2.6, 8, 10.1]]))
330/1: import numpy as np
330/2:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
330/3:
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')
330/4: from sklearn.datasets import load_boston
330/5: boston = load_boston()
330/6: print(boston.DESCR)
330/7:
plt.hist(boston.target, bins=50)

plt.xlabel('Prices in $1000s')
plt.ylabel('Number of houses')
330/8:
plt.scatter(boston.data[:,5], boston.target)

plt.ylabel('Price in $1000s')
plt.xlabel('Number of rooms')
330/9:
boston_df = DataFrame(boston.data)

boston_df.columns = boston.feature_names

boston_df.head()
330/10: boston_df['Price'] = boston.target
330/11: boston_df.head()
330/12: breakpoint()
330/13: sns.Implot('RM', 'Price', data=boston_df)
332/1: sns.Implot('RM', 'Price', data=boston_df)
332/2:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
332/3:
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')
332/4: from sklearn.datasets import load_boston
332/5: boston = load_boston()
332/6: print(boston.DESCR)
332/7:
plt.hist(boston.target, bins=50)

plt.xlabel('Prices in $1000s')
plt.ylabel('Number of houses')
332/8: sns.Implot('RM', 'Price', data=boston_df)
332/9: # sns.Implot('RM', 'Price', data=boston_df)
332/10:
plt.scatter(boston.data[:,5], boston.target)

plt.ylabel('Price in $1000s')
plt.xlabel('Number of rooms')
332/11:
boston_df = DataFrame(boston.data)

boston_df.columns = boston.feature_names

boston_df.head()
332/12:
boston_df = DataFrame(boston.data)

boston_df.columns = boston.feature_names

boston_df.head()
332/13: boston_df['Price'] = boston.target
332/14: boston_df['Price'] = boston.target
332/15: boston_df.head()
332/16:
plt.hist(boston.target, bins=50)

plt.xlabel('Prices in $1000s')
plt.ylabel('Number of houses')
332/17:
breakpoint()
plt.scatter(boston.data[:,5], boston.target)

plt.ylabel('Price in $1000s')
plt.xlabel('Number of rooms')
332/18:
breakpoint()
plt.scatter(boston.data[:,5], boston.target)

plt.ylabel('Price in $1000s')
plt.xlabel('Number of rooms')
332/19: # sns.Implot('RM', 'Price', data=boston_df)
332/20: X = boston_df.RM
332/21: X
332/22: X = np.vstack(boston_df.RM)
332/23: X.shape
332/24:
X = boston_df.RM
X.shape
332/25: Y = boston_df.Price
332/26: X
332/27: Y = boston_df.Price
332/28: X
332/29:
X = boston_df.RM
X.shape
332/30: X = np.vstack(boston_df.RM)
332/31: X.shape
332/32: Y = boston_df.Price
332/33: X
332/34:
# [X 1]
X = np.array( [ [Value,1] for value in X ] )
332/35: Y = boston_df.Price
332/36: X
332/37:
# [X 1]
X = np.array( [ [value,1] for value in X ] )
332/38: X
332/39: m, b = np.linalg.lstsq(X,Y)[0]
332/40: X
332/41: m, b = np.linalg.lstsq(X,Y)[0]
332/42: m, b = np.linalg.lstsq(X, Y, rcond=None)[0]
332/43:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
332/44:
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')
332/45: from sklearn.datasets import load_boston
332/46: boston = load_boston()
332/47: print(boston.DESCR)
332/48:
plt.hist(boston.target, bins=50)

plt.xlabel('Prices in $1000s')
plt.ylabel('Number of houses')
332/49:
plt.scatter(boston.data[:,5], boston.target)

plt.ylabel('Price in $1000s')
plt.xlabel('Number of rooms')
332/50:
boston_df = DataFrame(boston.data)

boston_df.columns = boston.feature_names

boston_df.head()
332/51: boston_df['Price'] = boston.target
332/52: boston_df.head()
332/53: # sns.Implot('RM', 'Price', data=boston_df)
332/54:
X = boston_df.RM
X.shape
332/55: X = np.vstack(boston_df.RM)
332/56: X.shape
332/57: Y = boston_df.Price
332/58: X
332/59:
# [X 1]
X = np.array( [ [value,1] for value in X ] )
332/60: X
332/61: m, b = np.linalg.lstsq(X, Y)[0]
332/62: # m, b = np.linalg.lstsq(X, Y)[0]
332/63:
plt.plot(boston_df.RM, boston_df.Price, 'o')

x = boston_df.RM

plt.plot(x, m*x + b, 'r', label='Best Fit Line')
332/64:
# plt.plot(boston_df.RM, boston_df.Price, 'o')

# x = boston_df.RM

# plt.plot(x, m*x + b, 'r', label='Best Fit Line')
332/65:
result = np.linalg.lstsq(X,Y)

error_total = result[1]

rmse = np.sqrt(error_total/len(X))

print('The root mean square error was %.2f' %rmse)
332/66:
# result = np.linalg.lstsq(X,Y)

# error_total = result[1]

# rmse = np.sqrt(error_total/len(X))

# print('The root mean square error was %.2f' %rmse)
332/67:
# Import for Linear Regression

import sklearn
from sklearn.linear_model import LinearRegression
332/68:
# Create a LinearRegression Object
lreg = LinearRegression()
332/69:
X_multi = boston_df.drop('Price', 1)

Y_target = boston_df.Price
332/70: lreg.fit(X_multi, Y_target)
332/71:
print('The estimated intercept coefficient is %.2f' % lreg.intercept_)

print('The number of coefficients usd was %d ' %len(lreg.coef_))
332/72:
coeff_df = DataFrame(boston_df.columns)
coeff_df.columns = ['Features']

coeff_df['Coefficient Estimate'] = pd.Series(lreg.coef_)

coeff_df
332/73: X_train,X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X, boston_df.Price)
332/74:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
332/75:
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')
332/76: from sklearn.datasets import load_boston
332/77: boston = load_boston()
332/78: print(boston.DESCR)
332/79:
plt.hist(boston.target, bins=50)

plt.xlabel('Prices in $1000s')
plt.ylabel('Number of houses')
332/80:
plt.scatter(boston.data[:,5], boston.target)

plt.ylabel('Price in $1000s')
plt.xlabel('Number of rooms')
332/81:
boston_df = DataFrame(boston.data)

boston_df.columns = boston.feature_names

boston_df.head()
332/82: boston_df['Price'] = boston.target
332/83: boston_df.head()
332/84: # sns.Implot('RM', 'Price', data=boston_df)
332/85:
X = boston_df.RM
X.shape
332/86: X = np.vstack(boston_df.RM)
332/87: X.shape
332/88: Y = boston_df.Price
332/89: X
332/90:
# [X 1]
X = np.array( [ [value,1] for value in X ] )
332/91: X
332/92: # m, b = np.linalg.lstsq(X, Y)[0]
332/93:
# plt.plot(boston_df.RM, boston_df.Price, 'o')

# x = boston_df.RM

# plt.plot(x, m*x + b, 'r', label='Best Fit Line')
332/94:
# result = np.linalg.lstsq(X,Y)

# error_total = result[1]

# rmse = np.sqrt(error_total/len(X))

# print('The root mean square error was %.2f' %rmse)
332/95:
# Import for Linear Regression

import sklearn
from sklearn.linear_model import LinearRegression
332/96:
# Create a LinearRegression Object
lreg = LinearRegression()
332/97:
X_multi = boston_df.drop('Price', 1)

Y_target = boston_df.Price
332/98: lreg.fit(X_multi, Y_target)
332/99:
print('The estimated intercept coefficient is %.2f' % lreg.intercept_)

print('The number of coefficients usd was %d ' %len(lreg.coef_))
332/100:
coeff_df = DataFrame(boston_df.columns)
coeff_df.columns = ['Features']

coeff_df['Coefficient Estimate'] = pd.Series(lreg.coef_)

coeff_df
332/101: X_train,X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X, boston_df.Price)
332/102:
import sklearn.cross_validation

X_train,X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X, boston_df.Price)
332/103:
import sklearn.model_selection

X_train,X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, boston_df.Price)
332/104: print(X_train.shape, X_test.shape, Y_trian.shape, Y_test.shape)
332/105: print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)
332/106:
lreg = LinearRegression()

lreg.fit(X_train, Y_train)
332/107:
pred_train = lreg.predict(X_train)
pred_test = lreg.predict(X_test)
332/108:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' ]% np.mean(Y_train-pred_train) ** 2)

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean(Y_test - pred_test) ** 2)
332/109:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean(Y_train-pred_train) ** 2)

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean(Y_test - pred_test) ** 2)
332/110:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean(Y_train - pred_train) ** 2)

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean(Y_test - pred_test) ** 2)
332/111:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean((Y_train - pred_train) ** 2)

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean((Y_test - pred_test) ** 2)
332/112:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean((Y_train - pred_train)**2)

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean((Y_test - pred_test)**2)
332/113:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
332/114:
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')
332/115: from sklearn.datasets import load_boston
332/116: boston = load_boston()
332/117: print(boston.DESCR)
332/118:
plt.hist(boston.target, bins=50)

plt.xlabel('Prices in $1000s')
plt.ylabel('Number of houses')
332/119:
plt.scatter(boston.data[:,5], boston.target)

plt.ylabel('Price in $1000s')
plt.xlabel('Number of rooms')
332/120:
boston_df = DataFrame(boston.data)

boston_df.columns = boston.feature_names

boston_df.head()
332/121: boston_df['Price'] = boston.target
332/122: boston_df.head()
332/123: # sns.Implot('RM', 'Price', data=boston_df)
332/124:
X = boston_df.RM
X.shape
332/125: X = np.vstack(boston_df.RM)
332/126: X.shape
332/127: Y = boston_df.Price
332/128: X
332/129:
# [X 1]
X = np.array( [ [value,1] for value in X ] )
332/130: X
332/131: # m, b = np.linalg.lstsq(X, Y)[0]
332/132:
# plt.plot(boston_df.RM, boston_df.Price, 'o')

# x = boston_df.RM

# plt.plot(x, m*x + b, 'r', label='Best Fit Line')
332/133:
# result = np.linalg.lstsq(X,Y)

# error_total = result[1]

# rmse = np.sqrt(error_total/len(X))

# print('The root mean square error was %.2f' %rmse)
332/134:
# Import for Linear Regression

import sklearn
from sklearn.linear_model import LinearRegression
332/135:
# Create a LinearRegression Object
lreg = LinearRegression()
332/136:
X_multi = boston_df.drop('Price', 1)

Y_target = boston_df.Price
332/137: lreg.fit(X_multi, Y_target)
332/138:
print('The estimated intercept coefficient is %.2f' % lreg.intercept_)

print('The number of coefficients usd was %d ' %len(lreg.coef_))
332/139:
coeff_df = DataFrame(boston_df.columns)
coeff_df.columns = ['Features']

coeff_df['Coefficient Estimate'] = pd.Series(lreg.coef_)

coeff_df
332/140:
import sklearn.model_selection

X_train,X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, boston_df.Price)
332/141: print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)
332/142:
lreg = LinearRegression()

lreg.fit(X_train, Y_train)
332/143:
pred_train = lreg.predict(X_train)
pred_test = lreg.predict(X_test)
332/144:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean((Y_train - pred_train)**2)

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean((Y_test - pred_test)**2)
332/145:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean((Y_train - pred_train)**2))

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean((Y_test - pred_test)**2))
332/146:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean((Y_train - pred_train)**2))

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean((Y_test - pred_test)**2))
332/147:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean((Y_train - pred_train)**2))

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean((Y_test - pred_test)**2))
332/148:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean((Y_train - pred_train)**2))

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean((Y_test - pred_test)**2))
332/149:
print('Fit a model X_train, and calculate the MSE with Y_train: %.2f' % np.mean((Y_train - pred_train)**2))

print('Fit a model X_train, and calculate MSE with X-test and Y_test: %.2f ' % np.mean((Y_test - pred_test)**2))
332/150:
train = plt.scatter(pred_train, (pred_train - Y_train), c='b', alpha=0.5)

test = plt.scatter(pred_test, (pred_test - Y_test), c='r', alpha=0.5)

plt.hlines(y=0,xmin=10,xmax=10)

plt.legend((train, test), ('Training', 'Test'), loc='lower left')

plt.title('Residual Plots')
332/151:
train = plt.scatter(pred_train, (pred_train - Y_train), c='b', alpha=0.5)

test = plt.scatter(pred_test, (pred_test - Y_test), c='r', alpha=0.5)

plt.hlines(y=0,xmin=10,xmax=40)

plt.legend((train, test), ('Training', 'Test'), loc='lower left')

plt.title('Residual Plots')
334/1:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import math

import matplotlit.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

from sklean.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn import metrics

import statsmodels.api as sm
334/2:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import math

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

from sklean.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn import metrics

import statsmodels.api as sm
334/3:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import math

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn import metrics

import statsmodels.api as sm
334/4:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import math

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn import metrics

import statsmodels.api as sm
334/5:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import math

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn import metrics

import statsmodels.api as sm
334/6:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import math

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn import metrics

import statsmodels.api as sm
334/7:
# Logistic Function

def logistic(t):
    return 1.0 / (1 + math.exp((-1.0)*t))

# Set t from -6 to 6 ( 500 elements, linearly spaced)
t = np.linspace(-6,6,500)

# Set up y values (using list comprehension)
y = np.array([logistic(ele) for ele in t])

# Plot
plt.plot(t, y)
plt.title('Logistic Function')
334/8: df = sm.datasetes.fair.load_pandas().data
334/9: df = sm.datasets.fair.load_pandas().data
334/10: df.head()
334/11:
def affair_ceck(x):
    if x != 0:
        return 1
    else:
        return 0
334/12: df['Had_Affair'] = df['affairs'].apply(affair_check)
334/13:
def affair_ceck(x):
    if x != 0:
        return 1
    else:
        return 0
334/14: df['Had_Affair'] = df['affairs'].apply(affair_check)
334/15:
def affair_check(x):
    if x != 0:
        return 1
    else:
        return 0
334/16: df['Had_Affair'] = df['affairs'].apply(affair_check)
334/17: df
334/18: df.groupby('Had_Affair').mean()
334/19: sns.factorplot('age', data=df, hue='Had_Affair', palette='coolwarm')
334/20: df
334/21: sns.factorplot(df['age'], data=df, hue=df['Had_Affair'], palette='coolwarm')
334/22: sns.factorplot('age',data=df,hue='Had_Affair',palette='coolwarm')
334/23:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import math

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn import metrics

import statsmodels.api as sm
334/24:
# Logistic Function

def logistic(t):
    return 1.0 / (1 + math.exp((-1.0)*t))

# Set t from -6 to 6 ( 500 elements, linearly spaced)
t = np.linspace(-6,6,500)

# Set up y values (using list comprehension)
y = np.array([logistic(ele) for ele in t])

# Plot
plt.plot(t, y)
plt.title('Logistic Function')
334/25: df = sm.datasets.fair.load_pandas().data
334/26: df.head()
334/27:
def affair_check(x):
    if x != 0:
        return 1
    else:
        return 0
334/28: df['Had_Affair'] = df['affairs'].apply(affair_check)
334/29: df
334/30: df.groupby('Had_Affair').mean()
334/31: df
334/32: sns.factorplot('age',data=df,hue='Had_Affair',palette='coolwarm')
334/33: # sns.factorplot('age',data=df,hue='Had_Affair',palette='coolwarm')
334/34: sns.factorplot('yrs_married',data=df,hue='Had_Affair',palette='coolwarm')
334/35: # sns.factorplot('yrs_married',data=df,hue='Had_Affair',palette='coolwarm')
334/36: sns.factorplot(x='age',data=df,hue='Had_Affair',palette='coolwarm')
334/37: df.value_counts()
334/38: count(df)
334/39: df
334/40: sns.factorplot(x='age', y='children', data=df,hue='Had_Affair',palette='coolwarm')
334/41: sns.factorplot(x='age', y='count', data=df,hue='Had_Affair',palette='coolwarm')
334/42: sns.factorplot(x='age', y=len(df.index), data=df,hue='Had_Affair',palette='coolwarm')
334/43: sns.factorplot(x='age', y='', data=df,hue='Had_Affair',palette='coolwarm')
334/44: sns.factorplot(x='age', data=df,hue='Had_Affair',palette='coolwarm')
334/45: sns.factorplot(x='age',data=df,hue='Had_Affair',palette='coolwarm')
334/46: # sns.factorplot(x='age',data=df,hue='Had_Affair',palette='coolwarm')
334/47: sns.factorplot(x='age', y=None data=df,hue='Had_Affair',palette='coolwarm')
334/48: sns.factorplot(x='age', y=None, data=df,hue='Had_Affair',palette='coolwarm')
334/49: sns.factorplot(x='age', data=df,hue='Had_Affair',palette='coolwarm')
334/50: # sns.factorplot(x='age', data=df,hue='Had_Affair',palette='coolwarm')
334/51: occ_dummies = pd.get_dummies(df['occupation'])
334/52: hus_occ_dummies = pd.get_dummies(df['occupation_husb'])
334/53: occ_dummies.head()
334/54: occ_dummies.columns = ['occ1', 'occ2', 'occ3', 'occ4', 'occ5', 'occ6']
334/55: hus_occ_dummies.columns = ['hocc1', 'hocc2', 'hocc3', 'hocc4', 'hocc5', 'hocc6']
334/56: X = df.drop(['occupation', 'occupation_husb', 'Had_Affair'], axis=1)
334/57: dummies = pd.concat([occ_dummies, hus_occ_dummies], axis=1)
334/58: X.head()
334/59: dummies.head()
334/60: X = pd.concat([X, dummies], axis=1)
334/61: X.head()
334/62:
Y = df.Had_Affair

Y.head()
334/63: X = X.drop('hocc1', axis=1)
334/64: X
334/65: X.head()
334/66: X = X.drop('affairs', axis=1)
334/67: X.head()
334/68: Y.head()
334/69: Y = np.ravel(Y)
334/70: Y
334/71:
log_model = LogidticRegression()

log_model.fit(X, Y)

log_model.score(X, Y)
334/72:
log_model = LogisticRegression()

log_model.fit(X, Y)

log_model.score(X, Y)
334/73: Y.mean()
334/74: coeff_df = DataFrame(zip(X.columns, np.transpose(log_model.coef_)))
334/75: coeff_df
334/76: X_train, X_test, Y_train, Y_test = train_test_split(X,Y)
334/77:
log_model2 = LogisticRegression()

log_model2.fit(X_train, Y_train)
334/78: class_predict = log_model2.predict(X_test)
334/79: print(metrics.accuracy_score(Y_test, class_predict))
336/1:
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import math

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

from sklearn import linear_model
from sklearn.datasets import load_iris
336/2: iris = load_iris()
336/3:
X = iris.data

Y = iris.target
336/4: print(iris.DESCR)
336/5: iris_data = DataFrame(X, columns=['Seal Length', 'Sepal Width', 'Petal Length', 'Petal Width'])
336/6: iris_target = DataFrame(Y, columns=['Species'])
336/7: iris_target.head()
336/8: iris_target.tail()
336/9:
def flower(num):
    if num == 0:
        return 'Setosa'
    elif num == 1:
        return 'Versicolour'
    else:
        return 'Virginica'
336/10: iris_target['Species'] = iris_target['Species'].apply(flower)
336/11: iris_target.tail()
336/12: iris = pd.concat([iris_data, iris_target], axis=1)
336/13: iris.head()
336/14: sns.pairplot(iris, hue='Species', size=2)
336/15: sns.pairplot(iris, hue='Species', size=2);
336/16: sns.factorplot('Petal Length', data=iris, hue='Species', size=10)
336/17: # sns.factorplot('Petal Length', data=iris, hue='Species', size=10)
336/18:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
336/19:
logreg = LogisticRegression()

X_train, Xtest, Y_train, Y_test = train_test_split(X, Y, test_size = 0.4, random_state=3)
336/20: logreg.fit(X_train, Y_train)
336/21: from sklearn import metrics
336/22: Y_pred = logreg.predict(X_test)
336/23:
logreg = LogisticRegression()

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.4, random_state=3)
336/24: logreg.fit(X_train, Y_train)
336/25: from sklearn import metrics
336/26: Y_pred = logreg.predict(X_test)
336/27: print(metrics.accuracy_score(Y_test, Y_pred))
336/28:
logreg = LogisticRegression()

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.5, random_state=3)
336/29: logreg.fit(X_train, Y_train)
336/30: from sklearn import metrics
336/31: Y_pred = logreg.predict(X_test)
336/32: print(metrics.accuracy_score(Y_test, Y_pred))
336/33:
logreg = LogisticRegression()

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=3)
336/34: logreg.fit(X_train, Y_train)
336/35: from sklearn import metrics
336/36: Y_pred = logreg.predict(X_test)
336/37: print(metrics.accuracy_score(Y_test, Y_pred))
336/38:
logreg = LogisticRegression()

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.4, random_state=3)
336/39: logreg.fit(X_train, Y_train)
336/40: from sklearn import metrics
336/41: Y_pred = logreg.predict(X_test)
336/42: print(metrics.accuracy_score(Y_test, Y_pred))
336/43: from sklearn.neightbors import KNeighborsClassifier
336/44: from sklearn.neighbors import KNeighborsClassifier
336/45: knn = KNeighborsClassifier(n_neighbors = 6)
336/46: knn.fit(X_train, Y_train)
336/47: Y_pred = knn.predict(X_test)
336/48: print(metrics.accuracy_score(Y_test, Y_pred))
336/49: knn = KNeighborsClassifier(n_neighbors = 3)
336/50: knn.fit(X_train, Y_train)
336/51: Y_pred = knn.predict(X_test)
336/52: print(metrics.accuracy_score(Y_test, Y_pred))
336/53: knn = KNeighborsClassifier(n_neighbors = 1)
336/54: knn.fit(X_train, Y_train)
336/55: Y_pred = knn.predict(X_test)
336/56: print(metrics.accuracy_score(Y_test, Y_pred))
336/57: knn = KNeighborsClassifier(n_neighbors = 6)
336/58: knn.fit(X_train, Y_train)
336/59: Y_pred = knn.predict(X_test)
336/60: print(metrics.accuracy_score(Y_test, Y_pred))
336/61: knn = KNeighborsClassifier(n_neighbors = 1)
336/62: knn.fit(X_trian, Y_train)
336/63: knn.fit(X_trian, Y_train)
336/64: knn.fit(X_train, Y_train)
336/65: Y_pred = knn.predict(X_test)
336/66: print(metrics.accuracy_score(Y_test, Y_pred))
336/67:
k_range = range(1, 21)

accuracy = []
336/68:
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, Y_train)
    Y_pred = knn.predict(X_test)

    accuracy.append(metrics.accuracy_score(Y_test, Y_pred))
336/69:
plt.plot(k_range, accuracy)
plt.xlabel('K value')
plt.ylabel('Testing Accuracy')
338/1:
import numpy as np
import matplotlib.pyplot as pt
338/2: from sklearn import datasets
338/3: iris = datasets.load_iris()
338/4:
iris = datasets.load_iris()

X = iris.ddata

Y = iris.target
338/5:
iris = datasets.load_iris()

X = iris.data

Y = iris.target
338/6: print(iris.DESCR)
338/7: from sklearn.svm import SVC
338/8: model = SVC
338/9: from sklearn.model_selection import train_test_split
338/10: X_train, X_test, Y_train, Y_test = train_test_split(X, Y)
338/11: model.fit(X_train, Y_train)
338/12: X_train, X_test, Y_train, Y_test = train_test_split(X,Y)
338/13: model.fit(X_train, Y_train)
338/14: from sklearn.model_selection import train_test_split
338/15: X_train, X_test, Y_train, Y_test = train_test_split(X,Y)
338/16: model.fit(X_train, Y_train)
338/17: from sklearn.svm import SVC
338/18: model = SVC()
338/19: from sklearn.model_selection import train_test_split
338/20: X_train, X_test, Y_train, Y_test = train_test_split(X,Y)
338/21: model.fit(X_train, Y_train)
338/22: from sklearn import metrics
338/23:
predicted = model.predict(X_test)

expected = Y_test
338/24: print(metrics.accuracy_score(expected, predicted))
338/25: X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.4, random_state=3)
338/26: model.fit(X_train, Y_train)
338/27: from sklearn import metrics
338/28:
predicted = model.predict(X_test)

expected = Y_test
338/29: print(metrics.accuracy_score(expected, predicted))
338/30: X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3, random_state=3)
338/31: model.fit(X_train, Y_train)
338/32: from sklearn import metrics
338/33:
predicted = model.predict(X_test)

expected = Y_test
338/34: print(metrics.accuracy_score(expected, predicted))
338/35: from sklearn import svm
338/36:
X = iris.data[:,:2]

Y = iris.target
338/37: C = 1.0
338/38: svc = svm.SVC(kernel='linear', C=C).fit(X,Y)
338/39: rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
338/40: poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
338/41: lin_svc = svm.LinearSVC(C=C).fit(X, Y)
338/42:
h = 0.02

x_min = X[:,0].min() - 1
x_max = X[:,0].max() + 1
338/43:
y_min = X[:,1].min() - 1
y_max = X[:,1].max() + 1
338/44: xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
338/45: titles = ['SVC with linear kernel', 'LinearSVC (linear kernel)', 'SVC with RBF kernel', 'SVC with polynomial (degree 3) kernel']
338/46:
for i,clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):

    plt.figure(figsize=(15,15))

    plt.subplot(2,2,i+1)

    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, cmap=plt.cm.terrain, alpha=0.5, linewidths=0)

    plt.scatter(X[:,0],X[:,1],c=Y,cmap=plt.cm.Dark2)

    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.xticks(())
    plt.yticks(())
    plt.title(title[i])
338/47:
for i,clf,plt in enumerate((svc, lin_svc, rbf_svc, poly_svc)):

    plt.figure(figsize=(15,15))

    plt.subplot(2,2,i+1)

    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, cmap=plt.cm.terrain, alpha=0.5, linewidths=0)

    plt.scatter(X[:,0],X[:,1],c=Y,cmap=plt.cm.Dark2)

    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.xticks(())
    plt.yticks(())
    plt.title(title[i])
338/48:
for i,clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):

    plt.figure(figsize=(15,15))

    plt.subplot(2,2,i+1)

    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, cmap=plt.cm.terrain, alpha=0.5, linewidths=0)

    plt.scatter(X[:,0],X[:,1],c=Y,cmap=plt.cm.Dark2)

    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.xticks(())
    plt.yticks(())
    plt.title(titles[i])
338/49:
import numpy as np
import matplotlib.pyplot as pt
338/50: from sklearn import datasets
338/51:
iris = datasets.load_iris()

X = iris.data

Y = iris.target
338/52: print(iris.DESCR)
338/53: from sklearn.svm import SVC
338/54: model = SVC()
338/55: from sklearn.model_selection import train_test_split
338/56: X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3, random_state=3)
338/57: model.fit(X_train, Y_train)
338/58: from sklearn import metrics
338/59:
predicted = model.predict(X_test)

expected = Y_test
338/60: print(metrics.accuracy_score(expected, predicted))
338/61: from sklearn import svm
338/62:
X = iris.data[:,:2]

Y = iris.target
338/63: C = 1.0
338/64: svc = svm.SVC(kernel='linear', C=C).fit(X,Y)
338/65: rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
338/66: poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
338/67: lin_svc = svm.LinearSVC(C=C).fit(X, Y)
338/68:
h = 0.02

x_min = X[:,0].min() - 1
x_max = X[:,0].max() + 1
338/69:
y_min = X[:,1].min() - 1
y_max = X[:,1].max() + 1
338/70: xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
338/71: titles = ['SVC with linear kernel', 'LinearSVC (linear kernel)', 'SVC with RBF kernel', 'SVC with polynomial (degree 3) kernel']
338/72:
for i,clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):

    plt.figure(figsize=(15,15))

    plt.subplot(2,2,i+1)

    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, cmap=plt.cm.terrain, alpha=0.5, linewidths=0)

    plt.scatter(X[:,0],X[:,1],c=Y,cmap=plt.cm.Dark2)

    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.xticks(())
    plt.yticks(())
    plt.title(titles[i])
338/73:
import numpy as np
import matplotlib.pyplot as plt
338/74: from sklearn import datasets
338/75:
iris = datasets.load_iris()

X = iris.data

Y = iris.target
338/76: print(iris.DESCR)
338/77: from sklearn.svm import SVC
338/78: model = SVC()
338/79: from sklearn.model_selection import train_test_split
338/80: X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3, random_state=3)
338/81: model.fit(X_train, Y_train)
338/82: from sklearn import metrics
338/83:
predicted = model.predict(X_test)

expected = Y_test
338/84: print(metrics.accuracy_score(expected, predicted))
338/85: from sklearn import svm
338/86:
X = iris.data[:,:2]

Y = iris.target
338/87: C = 1.0
338/88: svc = svm.SVC(kernel='linear', C=C).fit(X,Y)
338/89: rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
338/90: poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
338/91: lin_svc = svm.LinearSVC(C=C).fit(X, Y)
338/92:
h = 0.02

x_min = X[:,0].min() - 1
x_max = X[:,0].max() + 1
338/93:
y_min = X[:,1].min() - 1
y_max = X[:,1].max() + 1
338/94: xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
338/95: titles = ['SVC with linear kernel', 'LinearSVC (linear kernel)', 'SVC with RBF kernel', 'SVC with polynomial (degree 3) kernel']
338/96:
for i,clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):

    plt.figure(figsize=(15,15))

    plt.subplot(2,2,i+1)

    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, cmap=plt.cm.terrain, alpha=0.5, linewidths=0)

    plt.scatter(X[:,0],X[:,1],c=Y,cmap=plt.cm.Dark2)

    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.xticks(())
    plt.yticks(())
    plt.title(titles[i])
340/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
340/2:
from sklearn.datasets import make_blobs

X,y = make_blobs(n_samples=500, centers=4, random_state=8, cluster_std=2.4)

plt.figure(figsize=(6,6))
plt.scatter(X[:,0], X[:,1],c=y,s=5-,cmap='jet')
340/3:
from sklearn.datasets import make_blobs

X,y = make_blobs(n_samples=500, centers=4, random_state=8, cluster_std=2.4)

plt.figure(figsize=(6,6))
plt.scatter(X[:,0], X[:,1],c=y,s=50,cmap='jet')
340/4: from sklearn.tree import DecisionTreeClassifier
342/1:
0 . An end-to-tnd Scikit-Learn workflow
1. Getting tht data ready
2. Choose the right estimator/algorithm for our problems
3. Fit the model/algorithm and use it to make predictions on our data
4. Evaluating a model
5. Improve a model
6. Save and load a trained model
7. Putting it all together!
344/1: # Streamlit
344/2: import pandas as pd
344/3: df = sm.datasets.fair.load_pandas().data
344/4:
import pandas as pd

import statsmodels.api as sm
344/5: df = sm.datasets.fair.load_pandas().data
344/6: df.head()
348/1: st.markdown('### Markdown header')
352/1: st.exception('Variable not defined')
356/1: my_bar.progress(p + 1)
358/1:
import numpy as np
import pandas as pd
358/2: df = pd.read_excel('hospitals.xlsx')
358/3: df = pd.read_excel('hospitals')
358/4: df = pd.read_excel('hospitals.xlsx')
358/5: df = pd.read_excel('hospitals.xlsx')
358/6:
import numpy as np
import pandas as pd
358/7: df = pd.read_excel('hospitals.xlsx')
358/8: df = pd.read_excel('hospitals.xlsx')
358/9: df.head()
358/10: df.tail()
358/11: df.head()
358/12:
import numpy as np
import pandas as pd
358/13:
df = pd.read_excel('hospitals.xlsx')

# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]
358/14: df.head()
358/15:
import numpy as np
import pandas as pd
358/16:
df = pd.read_excel('hospitals.xlsx')

df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

df.head()
358/17:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
pd.to_excel('hospitals.xlsx')

df.head()
358/18:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
pd.to_xlsx('hospitals.xlsx')

df.head()
358/19:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
df.to_excel('hospitals.xlsx')

df.head()
358/20:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx')

df.head()
358/21:
df = pd.read_excel('hospitals.xlsx', index=None)

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx')

df.head()
358/22:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
df.to_excel('hospitals.xlsx', index=None)

df.head()
358/23:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head()
358/24:

districts = df['Districts']
358/25:

districts = df['District']
358/26: districts
358/27: len(districts)
358/28: len(districts)
358/29: len(df)
358/30:
# Looping through the df

for item in range(len(df)):
    print(item)
358/31:
# Looping through the df

for item in range(len(df)):
    print(df['District'])
358/32:
# Looping through the df

for item in range(len(df)):
    print(item)
358/33:
from mimesis import Datetime
import random
358/34:
from mimesis import Datetime
import random
358/35:
from faker import Faker
import random
358/36:
from faker import Faker
import random
358/37: faker = Faker()
358/38: faker = Faker()
358/39:
# Looping through the df

# for item in range(len(df)):
#     print(item)
358/40:
for i in df:
    print(df[i])
358/41:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.tail()
358/42:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
df.to_excel('hospitals.xlsx', index=None)

df.tail()
358/43:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
df.to_excel('hospitals.xlsx', index=None)

df.tail(50)
358/44:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.tail(50)
358/45:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(50)
358/46:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(50)
358/47:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.tail(50)
358/48:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.tail(50)
358/49:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.tail(50)
358/50:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(50)
358/51:
for i in df:
    print(df[i])
358/52:
for i in df:
    print(df[i] == 'District')
358/53:
for i in df:
    print(df[i])
358/54:
for name, values in df.iteritems():
    print('{name}: {value}'.format(name=name, value=values[0]))
358/55:
for name, values enumerate df.iteritems():
    print('{name}: {value}'.format(name=name, value=values[0]))
358/56:
for name, values enumerate(df.iteritems()):
    print('{name}: {value}'.format(name=name, value=values[0]))
358/57:
for name, values in df.iteritems():
    print('{name}: {value}'.format(name=name, value=values[0]))
358/58:
for name, values in df.iteritems():
    for item in range(len(df)):
        print('{name}: {value}'.format(name=name, value=values[item]))
358/59:
for name, values in df.iteritems():
    for item in range(len(df) / 100):
        print('{name}: {value}'.format(name=name, value=values[item]))
358/60: len(df)
358/61: len(df)/100
358/62:
for name, values in df.iteritems():
    for item in range(100):
        print('{name}: {value}'.format(name=name, value=values[item]))
358/63:
for name, values in df.iteritems():
    for item in range(5):
        print('{name}: {value}'.format(name=name, value=values[item]))
358/64:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/65: faker = Faker()
358/66:
for item in range(len(df)):
    df['blood_delivery_date'] = fake.date_time()
358/67: faker = Faker()
358/68:
for item in range(len(df)):
    df['blood_delivery_date'] = fake.date_time()
358/69:
for item in range(len(df)):
    df['blood_delivery_date'] = fakee.date_time()
358/70:
for item in range(len(df)):
    df['blood_delivery_date'] = faker.date_time()
358/71: df
358/72:
for item in range(len(df)):
    df['blood_delivery_date'] = faker.date_time('2010')
358/73:
for item in range(len(df)):
    df['blood_delivery_date'] = faker.date_time('2010-01-01')
358/74:
for item in range(len(df)):
    df['blood_delivery_date'] = faker.date_time(5)
358/75:
for item in range(len(df)):
    df['blood_delivery_date'] = faker.date_time()
358/76: df
358/77: df.tail()
358/78: df.head()
358/79:
faker = Faker()
datetime = Datetime()
358/80:
def create_fake_rows(num=1):
    out_put = [
        {
            'blood_delivery_date': faker.date_time()
        }
    for x in range(num)]

    return out_put
358/81:
def create_fake_rows(num=1):
    out_put = [
        {
            'blood_delivery_date': faker.date_time()
        }
    for x in range(num)]

    return out_put
358/82: create_fake_rows(5)
358/83: create_fake_rows(5)
358/84: len(df)/100
358/85: create_fake_rows(5)
358/86:
def create_fake_rows(num=len(df)):
    out_put = [
        {
            'blood_delivery_date': faker.date_time()
        }
    for x in range(num)]

    return out_put
358/87: create_fake_rows(5)
358/88:
def create_fake_rows(num=1):
    out_put = [
        {
            'blood_delivery_date': faker.date_time()
        }
    for x in range(num)]

    return out_put
358/89: create_fake_rows(len(df))
358/90: create_fake_rows(5)
358/91: faker = Faker()
358/92:
# for item in range(len(df)):
df['blood_delivery_date'] = faker.date_time() for item in range(len(df))
358/93:
for item in range(len(df)):
    df['blood_delivery_date'] = faker.date_time()
358/94: df.head()
358/95:
for name, values in df.iteritems():
    for item in range(5):
        df['blood_delivery_date'] = faker.date_time()
        print('{name}: {value}'.format(name=name, value=values[item]))
358/96: df.head()
358/97:
import numpy as np
import pandas as pd
358/98:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/99:

districts = df['District']
358/100:
from faker import Faker
import random
358/101: faker = Faker()
358/102:
# for item in range(len(df)):
#     df['blood_delivery_date'] = faker.date_time()
358/103:
def create_fake_rows(num=1):
    out_put = [
        {
            'blood_delivery_date': faker.date_time()
        }
    for x in range(num)]

    return out_put
358/104: # create_fake_rows(5)
358/105: len(df)/100
358/106: df.head()
358/107:
for name, values in df.iteritems():
    for item in range(len(df)):
        df['blood_delivery_date'] = faker.date_time()
        print('{name}: {value}'.format(name=name, value=values[item]))
358/108: df.head()
358/109:
for name, values in df.iteritems():
    for item in range(len(df)):
        df['blood_delivery_date'] = faker.date_time()
        # print('{name}: {value}'.format(name=name, value=values[item]))
358/110: df.head()
358/111:
for name, values in df.iteritems():
    fake_date = faker.date_time()
    for item in range(len(df)):
        df['blood_delivery_date'] = fake_date
        # print('{name}: {value}'.format(name=name, value=values[item]))
358/112: df.head()
358/113:
for name, values in df.iteritems():
    for item in range(len(df)):
        fake_date = faker.date_time()
        df['blood_delivery_date'] = fake_date
        fake_date = faker.date_time()
        # print('{name}: {value}'.format(name=name, value=values[item]))
358/114: df.head()
358/115: Faker.seed(0)
358/116:
for name, values in df.iteritems():
    for item in range(len(df)):
        fake_date = faker.date()
        df['blood_delivery_date'] = fake_date
        # fake_date = faker.date_time()
        # print('{name}: {value}'.format(name=name, value=values[item]))
358/117: df.head()
358/118:
for name, values in df.iteritems():
    for item in range(len(df)):
        fake_date = fake.date()
        df['blood_delivery_date'] = fake_date
        # fake_date = faker.date_time()
        # print('{name}: {value}'.format(name=name, value=values[item]))
358/119: Faker.seed(0)
358/120:
for name, values in df.iteritems():
    for item in range(len(df)):
        fake_date = fake.date()
        df['blood_delivery_date'] = fake_date
        # fake_date = faker.date_time()
        # print('{name}: {value}'.format(name=name, value=values[item]))
358/121:
def random_dates(start, end, n=10):

    start_u = start.value//10**9
    end_u = end.value//10**9

    return pd.to_datetime(np.random.randint(start_u, end_u, n), unit='s')
358/122:
start = pd.to_datetime('2015-01-01')
end = pd.to_datetime('2018-01-01')
random_dates(start, end)
358/123:
def random_dates(start, end, n=10):

    start_u = start.value//10**9
    end_u = end.value//10**9

    return np.random.randint(start_u, end_u, n)
358/124:
start = pd.to_datetime('2015-01-01')
end = pd.to_datetime('2018-01-01')
random_dates(start, end)
358/125:
for name, values in df.iteritems():
    for item in range(len(df)):
        df['blood_delivery_date'] = np.random.choice(pd.date_range('1980-01-01', '2000-01-01'), len(x))
        # print('{name}: {value}'.format(name=name, value=values[item]))
358/126:
for name, values in df.iteritems():
    df['blood_delivery_date'] = np.random.choice(pd.date_range('1980-01-01', '2000-01-01'), len(df))
        # print('{name}: {value}'.format(name=name, value=values[item]))
358/127: df.head()
358/128:
for name, values in df.iteritems():
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
358/129: df.head()
358/130:
for name, values in df.iteritems():
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.randint(50, 1000)
358/131: df.head()
358/132:
for name, values in df.iteritems():
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(50, 1000), len(df))
358/133: df.head()
358/134: df.tail()
358/135:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/136:
 # Repeat the dataframe
 
 df.append([df]*5, ignore_index=True)
358/137: len(df)
358/138:
 # Repeat the dataframe

 df = df.append([df]*5, ignore_index=True)
358/139: len(df)
358/140:
 # Repeat the dataframe

 df = df.append([df]*50, ignore_index=True)
358/141: len(df)
358/142: import random
358/143: df.head()
358/144:
for name, values in df.iteritems():
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(50, 1000), len(df))
358/145: df.tail()
358/146: len(df)
358/147: df.tail(20)
358/148: df.head(20)
358/149:
import numpy as np
import pandas as pd
358/150:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
# df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/151:
 # Repeat the dataframe

 df = df.append([df]*50, ignore_index=True)
358/152: len(df)
358/153: import random
358/154: df.head()
358/155:
for name, values in df.iteritems():
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2019-12-01'))
    df['blood_volume_delivered'] = np.random.choice(range(50, 1000), len(df))
358/156: df.head(20)
358/157: len(df)
358/158: len(df)
358/159: df.tail(20)
358/160:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/161:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.tail(20)
358/162:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
df.to_excel('hospitals.xlsx', index=None)

df.tail(20)
358/163:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/164:
 # Repeat the dataframe

 df = df.append([df]*50, ignore_index=True)
358/165: len(df)
358/166: import random
358/167: df.head()
358/168:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/169:
 # Repeat the dataframe

 df = df.append([df]*1000, ignore_index=True)
358/170: len(df)
358/171: import random
358/172: df.head()
358/173:
for name, values in df.iteritems():
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2019-12-01'))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/174: df.tail(20)
358/175: len(df)
358/176:
import numpy as np
import pandas as pd
358/177:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/178:
 # Repeat the dataframe

 df = df.append([df]*10000, ignore_index=True)
358/179: len(df)
358/180: import random
358/181: df.head()
358/182:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2019-12-01'))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/183: df.tail(20)
358/184: len(df)
358/185: df.to_excel('hospitals_1.xlsx', index=None)
358/186:
import numpy as np
import pandas as pd
358/187:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/188:
 # Repeat the dataframe

 df = df.append([df]*10000, ignore_index=True)
358/189: len(df)
358/190: import random
358/191: df.head()
358/192:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2019-12-01'))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/193: df.tail(20)
358/194: len(df)
358/195: df.to_csv('hospitals_1.csv', index=None)
358/196:
import numpy as np
import pandas as pd
358/197:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/198:
 # Repeat the dataframe

 df = df.append([df]*100, ignore_index=True)
358/199: len(df)
358/200: import random
358/201: df.head()
358/202:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2019-12-01'))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/203: df.tail(20)
358/204: len(df)
358/205: df.to_csv('hospitals_1.csv', index=None)
358/206:
import numpy as np
import pandas as pd
358/207:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/208:
 # Repeat the dataframe

 df = df.append([df]*100, ignore_index=True)
358/209: len(df)
358/210: import random
358/211: df.head()
358/212:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2019-12-01'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/213: df.tail(20)
358/214: len(df)
358/215: df.to_csv('hospitals_1.csv', index=None)
358/216:
import numpy as np
import pandas as pd
358/217:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/218:
import numpy as np
import pandas as pd
358/219:
 # Repeat the dataframe

 df = df.append([df]*100, ignore_index=True)
358/220:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/221: len(df)
358/222:
 # Repeat the dataframe

 df = df.append([df]*100, ignore_index=True)
358/223: import random
358/224: len(df)
358/225: df.head()
358/226: import random
358/227:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2017-12-30'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2018-02-30'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/228: df.head()
358/229: df.tail(20)
358/230:
import numpy as np
import pandas as pd
358/231: len(df)
358/232:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/233: df.to_csv('hospitals_1.csv', index=None)
358/234:
 # Repeat the dataframe

 df = df.append([df]*100, ignore_index=True)
358/235: len(df)
358/236: import random
358/237: df.head()
358/238:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2017-12-30'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2018-02-30'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/239:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2017-12-30'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2018-02-30'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/240:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-11-15', '2017-12-30'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2018-02-30'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/241:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2019-12-01'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/242:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2017-12-15'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2020-01-01'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/243:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2017-12-15'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2018-01-20'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/244:
import numpy as np
import pandas as pd
358/245:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/246:
 # Repeat the dataframe

 df = df.append([df]*100, ignore_index=True)
358/247: len(df)
358/248: import random
358/249: df.head()
358/250:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.date_range('2017-12-01', '2017-12-15'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.date_range('2018-01-01', '2018-01-20'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/251: df.tail(20)
358/252: len(df)
358/253: df.to_csv('hospitals_1.csv', index=None)
358/254:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.bdate_range('2017-12-01', '2017-12-15'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.bdate_range('2018-01-01', '2018-01-20'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/255:
import numpy as np
import pandas as pd
358/256:
df = pd.read_excel('hospitals.xlsx')

# Remove Unamed columns
df =  df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Save File after removing unamed columns
# df.to_excel('hospitals.xlsx', index=None)

df.head(5)
358/257:
 # Repeat the dataframe

 df = df.append([df]*100, ignore_index=True)
358/258: len(df)
358/259: import random
358/260: df.head()
358/261:
for name, values in df.iteritems():
    df['blood_order_date'] = np.random.choice(pd.bdate_range('2017-12-01', '2017-12-15'), len(df))
    df['blood_delivery_date'] = np.random.choice(pd.bdate_range('2018-01-01', '2018-01-20'), len(df))
    df['blood_volume_delivered'] = np.random.choice(range(300, 1000), len(df))
358/262: df.tail(20)
358/263: len(df)
358/264: df.to_csv('hospitals_1.csv', index=None)
360/1: st.markdown(['Boadzie Daniel']('http://boadzie.surge.sh/'))
380/1:
# 1. Get the data ready
import pandas as pd

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/2:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
Y = heart_disease['target']
380/3:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We'll keep the default hyperparameters
clf.get_params_
380/4:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We'll keep the default hyperparameters
clf.get_params_
380/5:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We'll keep the default hyperparameters
clf.get_params()
380/6:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/7:
# 1. Get the data ready
import pandas as pd

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/8:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/9:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We'll keep the default hyperparameters
clf.get_params()
380/10:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/11: clf.fit(X_train, y_train)
380/12:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/13:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/14: clf.fit(X_train, y_train)
380/15:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/16:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/17:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/18:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/19: clf.fit(X_train, y_train)
380/20: # Make a prediction
380/21: # Make a prediction
380/22:
y_preds = clf.predict(X_test)
y_preds
380/23: y_test
380/24:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/25: clf.score(X_test, y_test)
380/26:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/27: confusion_matrix(y_test, y_preds)
380/28: accuracy_score(y_test, y_preds)
380/29: print(confusion_matrix(y_test, y_preds))
380/30: confusion_matrix(y_test, y_preds)
380/31:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/32:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/33:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/34:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/35:
loaded_model = pickle.load(open('random_forest_model_1', 'rb'))
loaded_model.score(X_test, y_test)
380/36:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/37:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/38: X
380/39: X
380/40: y
380/41: # Make a prediction
380/42:
y_preds = clf.predict(X_test)
y_preds
380/43:
y_preds = clf.predict(X_test)
y_preds
380/44:
y_preds = clf.predict(X_test)
y_preds
380/45:
y_preds = clf.predict(X_test)
y_preds
380/46:
y_preds = clf.predict(X_test)
y_preds
380/47:
y_preds = clf.predict(X_test)
y_preds
380/48: y_test
380/49: clf.score(X_test, y_test)
380/50:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/51: confusion_matrix(y_test, y_preds)
380/52: accuracy_score(y_test, y_preds)
380/53:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/54:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/55:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/56:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/57:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/58: X
380/59: y
380/60:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/61:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/62: clf.fit(X_train, y_train)
380/63: # Make a prediction
380/64:
y_preds = clf.predict(X_test)
y_preds
380/65: y_test
380/66:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/67: clf.score(X_test, y_test)
380/68:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/69: confusion_matrix(y_test, y_preds)
380/70: accuracy_score(y_test, y_preds)
380/71:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/72:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/73:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/74:
## 1. Getting the data ready to be used by machine learning

Three main things we have to do:
    1.Split the data into features and labels (usually `X` and `y`)
    2.Filling (also called imputing) or disregarding mising values
    3.Converting non-numeric values to numeric values (also called feature encoding)
380/75: heart_disease.head()
380/76: X = heart_disease.drop('target', axis=1)
380/77: X.head()
380/78: y = heart_disease['target']
380/79: y
380/80: y.head()
380/81:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, testsize=0.2)
380/82:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/83: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/84:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
380/85: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/86: X.shape
380/87: len(heart_disease)
380/88:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/89:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/90: X
380/91: y
380/92:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/93:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/94: clf.fit(X_train, y_train)
380/95: # Make a prediction
380/96:
y_preds = clf.predict(X_test)
y_preds
380/97: y_test
380/98:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/99: clf.score(X_test, y_test)
380/100:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/101: confusion_matrix(y_test, y_preds)
380/102: accuracy_score(y_test, y_preds)
380/103:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/104:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/105:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/106: heart_disease.head()
380/107: X = heart_disease.drop('target', axis=1)
380/108: X.head()
380/109: y = heart_disease['target']
380/110: y.head()
380/111:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
380/112: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/113: X.shape
380/114: len(heart_disease)
380/115: ## 1.1 Make sure it's numerical
380/116:
car_sales = pd.read_csv('data/car-sales-extended.csv')

car_sales.head()
380/117: len(car_sales)
380/118: car_sales.dtypes
380/119:
# Split into X/y

X = car_sales.drop('Price', axis=1)

y = car_sales['Price']

# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/120:
# Build Machine Learning model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score()
380/121:
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncorder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncorder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(X)
transformed_X
380/122:
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncorder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncorder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(X)
transformed_X
380/123:
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncorder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncorder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(X)
transformed_X
380/124:
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncorder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(X)
transformed_X
380/125:
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(X)
transformed_X
380/126:  pd.DataFrame(transformed_X)
380/127:

dummies = pd.get_dummies(car_sales[['Make', 'Colour', 'Doors']])

dummies
380/128:
# Lets fit the model

np.random.seed(42)
X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)

model.fit(X_train, y_train)
380/129: model.score(X_test, y_test)
380/130:
### 1.2 What if there ere missing values?

1. Fill them with some value (also known as as imputation).
2. Remove the samples with missing data altogether.
380/131:
# Import car sales missing data

car_sales_missing = pd.read_csv('data/car-sales-extended-missing-data.csv')

car_sales_missing.head()
380/132: car_sales_missing.isna().sum()
380/133: car_sales_missing.isna().sum()
380/134:
# Create X & y
X = car_sales_missing.drop('Price', axis=1)
y = car_sales_missing['Price']
380/135:
# Let's try to convert our data to numbers
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(X)
transformed_X
380/136: car_sales_missing
380/137:
# Fill the "Make" column
car_sales_missing['Make'].fillna('missing', inplace=True)

# Fill the "Colour" column
car_sales_missing['Colur'].fillna('missing', inplace=True)

# Fill the "Odometer (KM)" column
car_sales_missing['Odometer (KM)'].fillna(car_sales_missing['Odometer (KM)'].mean(), inplace=True)

# Fill the "Doors" column
car_sales_missing['Doors'].fillna(4, inplace=True)
380/138:
# Fill the "Make" column
car_sales_missing['Make'].fillna('missing', inplace=True)

# Fill the "Colour" column
car_sales_missing['Colour'].fillna('missing', inplace=True)

# Fill the "Odometer (KM)" column
car_sales_missing['Odometer (KM)'].fillna(car_sales_missing['Odometer (KM)'].mean(), inplace=True)

# Fill the "Doors" column
car_sales_missing['Doors'].fillna(4, inplace=True)
380/139:
# Checkout our dataframe again
car_sales_missing.isna.sum()
380/140:
# Checkout our dataframe again
car_sales_missing.isna().sum()
380/141:
# Remove rows with missing Price values
car_sales_missing.dropna()
380/142: car_sales_missing.isna().sum()
380/143:
# Remove rows with missing Price values
car_sales_missing.dropna(inplace=True)
380/144: car_sales_missing.isna().sum()
380/145: len(car_sales_missing)
380/146:
X = car_sales_missing.drop('Price', axis=1)
y = car_sales_missing['Price']
380/147:
# Let's try to convert our data to numbers
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(X)
transformed_X
380/148:
# Let's try to convert our data to numbers
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(X)
transformed_X
380/149: ### Option 2: Fill Missing values with Scikit-Learn
380/150:
car_sales_missing = pd.read_csv('data/car-sales-extended-missing-data.csv')

car_sales_missing.head()
380/151: car_sales_missing.isna().sum()
380/152:
# Drop the rows with no labels
car_sales_missing.dropna(subset=['Price'], inplace=True)
car_sales_missing.isna().sum()
380/153:
# Split into X & y
X = car_sales_missing.drop('Price', axis=1)
y = car_sales_missing['Price']
380/154:
# Split into X & y
X = car_sales_missing.drop('Price', axis=1)
y = car_sales_missing['Price']
380/155: X.isna().sum()
380/156:
# Fill missing values with Scikit-Learn
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

# Fill categorical values with 'missing' & numerical values with mean
cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')
door_imputer = SimpleImputer(strategy='constant', fill_value=4)
num_imputer = SimpleImputer(strategy='mean')

# Define columns
cat_features = ['Make', 'Colour']
door_feature = ['Doors']
num_feature = ['Odometer (KM)']

# Create an imputer (something that fills missing data)
imputer = ColumnTransformer([
    ('cat_imputer', cat_imputer, cat_features),
    ('door_imputer', door_imputer, door_feature),
    ('num_imputer', num_imputer, num_feature)
])

# Transform the data
filled_X = imputer.fit_transform(X)
filled_X
380/157:
car_sales_filled = pd.DataFrame(filled_X, columns=['Make', 'Colours', 'Doors', 'Odometer (KM)'])

car_sales_filled.head()
380/158: car_sales_filled.isna().sum()
380/159: car_sales_filled.isna().sum()
380/160: car_sales_filled.head()
380/161:
# Let's try to convert our data to numbers
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
380/162:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/163:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/164: X
380/165: y
380/166:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/167:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/168: clf.fit(X_train, y_train)
380/169: # Make a prediction
380/170:
y_preds = clf.predict(X_test)
y_preds
380/171: y_test
380/172:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/173: clf.score(X_test, y_test)
380/174:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/175: confusion_matrix(y_test, y_preds)
380/176: accuracy_score(y_test, y_preds)
380/177:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/178:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/179:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/180: heart_disease.head()
380/181: X = heart_disease.drop('target', axis=1)
380/182: X.head()
380/183: y = heart_disease['target']
380/184: y.head()
380/185:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
380/186: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/187: X.shape
380/188: len(heart_disease)
380/189:
car_sales = pd.read_csv('data/car-sales-extended.csv')

car_sales.head()
380/190: len(car_sales)
380/191: car_sales.dtypes
380/192:
# Split into X/y

X = car_sales.drop('Price', axis=1)

y = car_sales['Price']

# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/193:
# Build Machine Learning model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score()
380/194:
# Let's try to convert our data to numbers
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors', 'Odometer (KM)']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
380/195: car_sales_filled.isna().sum()
380/196: car_sales_filled.isna().sum()
380/197: car_sales_filled
380/198: car_sales_filled.drop('Odometer (KM)')
380/199: car_sales_filled.drop('Odometer (KM)', axis=1)
380/200: car_sales_filled.drop('Odometer (KM)', axis=1)
380/201: car_sales_filled
380/202: car_sales_filled.drop('Odometer (KM)', axis=1, inplace=True)
380/203: car_sales_filled
380/204:
# Let's try to convert our data to numbers
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colour', 'Doors']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
380/205:
# Let's try to convert our data to numbers
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colours', 'Doors']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
380/206:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/207:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/208: X
380/209: y
380/210:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/211:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/212: clf.fit(X_train, y_train)
380/213: # Make a prediction
380/214:
y_preds = clf.predict(X_test)
y_preds
380/215: y_test
380/216:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/217: clf.score(X_test, y_test)
380/218:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/219: confusion_matrix(y_test, y_preds)
380/220: accuracy_score(y_test, y_preds)
380/221:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/222:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/223:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/224: heart_disease.head()
380/225: X = heart_disease.drop('target', axis=1)
380/226: X.head()
380/227: y = heart_disease['target']
380/228: y.head()
380/229:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
380/230: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/231: X.shape
380/232: len(heart_disease)
380/233:
car_sales = pd.read_csv('data/car-sales-extended.csv')

car_sales.head()
380/234: len(car_sales)
380/235: car_sales.dtypes
380/236:
# Split into X/y

X = car_sales.drop('Price', axis=1)

y = car_sales['Price']

# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/237:
# Build Machine Learning model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score()
380/238:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/239:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/240:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/241: X
380/242: y
380/243:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/244:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/245: clf.fit(X_train, y_train)
380/246: # Make a prediction
380/247:
y_preds = clf.predict(X_test)
y_preds
380/248: y_test
380/249:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/250: clf.score(X_test, y_test)
380/251:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/252: confusion_matrix(y_test, y_preds)
380/253: accuracy_score(y_test, y_preds)
380/254:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/255:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/256:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/257: heart_disease.head()
380/258: X = heart_disease.drop('target', axis=1)
380/259: X.head()
380/260: y = heart_disease['target']
380/261: y.head()
380/262:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
380/263: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/264: X.shape
380/265: len(heart_disease)
380/266:
car_sales = pd.read_csv('data/car-sales-extended.csv')

car_sales.head()
380/267: len(car_sales)
380/268: car_sales.dtypes
380/269:
# Split into X/y

X = car_sales.drop('Price', axis=1)

y = car_sales['Price']

# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/270:
# Build Machine Learning model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score()
380/271:
# Let's try to convert our data to numbers
# Turn the categories into numbers

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Make', 'Colours', 'Doors']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X
380/272: X_train
380/273: car_sales_filled
380/274:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/275:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/276: X
380/277: y
380/278:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/279:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/280: clf.fit(X_train, y_train)
380/281: # Make a prediction
380/282:
y_preds = clf.predict(X_test)
y_preds
380/283: y_test
380/284:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/285: clf.score(X_test, y_test)
380/286:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/287: confusion_matrix(y_test, y_preds)
380/288: accuracy_score(y_test, y_preds)
380/289:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/290:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/291:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/292: heart_disease.head()
380/293: X = heart_disease.drop('target', axis=1)
380/294: X.head()
380/295: y = heart_disease['target']
380/296: y.head()
380/297:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
380/298: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/299: X.shape
380/300: len(heart_disease)
380/301:
car_sales = pd.read_csv('data/car-sales-extended.csv')

car_sales.head()
380/302: len(car_sales)
380/303: car_sales.dtypes
380/304:
# Split into X/y

X = car_sales.drop('Price', axis=1)

y = car_sales['Price']

# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/305:
# Build Machine Learning model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score()
380/306:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/307:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/308: X
380/309: y
380/310:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/311:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/312: clf.fit(X_train, y_train)
380/313: # Make a prediction
380/314:
y_preds = clf.predict(X_test)
y_preds
380/315: y_test
380/316:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/317: clf.score(X_test, y_test)
380/318:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/319: confusion_matrix(y_test, y_preds)
380/320: accuracy_score(y_test, y_preds)
380/321:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/322:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/323:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/324: heart_disease.head()
380/325: X = heart_disease.drop('target', axis=1)
380/326: X.head()
380/327: y = heart_disease['target']
380/328: y.head()
380/329:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
380/330: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/331: X.shape
380/332: len(heart_disease)
380/333:
car_sales = pd.read_csv('data/car-sales-extended.csv')

car_sales.head()
380/334: len(car_sales)
380/335: car_sales.dtypes
380/336:
# Split into X/y

X = car_sales.drop('Price', axis=1)

y = car_sales['Price']

# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/337:
# Build Machine Learning model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score()
380/338: X.shape, y.shape
380/339: filled_X.shape
380/340: filled_X
380/341: y.shape
380/342: y.head()
380/343: y.isna().sum()
380/344:
y = y[:950]

y.shape
380/345:
# 1. Get the data ready
import pandas as pd
import numpy as np

heart_disease = pd.read_csv('data/heart_disease.csv')
heart_disease.head()
380/346:
# Create X (feature matrix)
X = heart_disease.drop("target", axis=1)

# Create Y (labels)
y = heart_disease['target']
380/347: X
380/348: y
380/349:
# 2. Choose the right model and hyperarameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# We'll keep the default hyperparameters
clf.get_params()
380/350:
# 3 Fit the model to the training data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/351: clf.fit(X_train, y_train)
380/352: # Make a prediction
380/353:
y_preds = clf.predict(X_test)
y_preds
380/354: y_test
380/355:
# 4. Evaluate the model on the training data and test data
clf.score(X_train, y_train)
380/356: clf.score(X_test, y_test)
380/357:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(classification_report(y_test, y_preds))
380/358: confusion_matrix(y_test, y_preds)
380/359: accuracy_score(y_test, y_preds)
380/360:
# 5. Improve a model
# Try different amount of n_estimators

np.random.seed(42)
for i in range(10, 100, 10):
    print(f'Trying model with {i} estimators...')
    clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
    print(f'Model acuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%')
    print('')
380/361:
# 6. Save a model and load it
import pickle

pickle.dump(clf, open('random_forest_model_1.pkl', 'wb'))
380/362:
loaded_model = pickle.load(open('random_forest_model_1.pkl', 'rb'))
loaded_model.score(X_test, y_test)
380/363: heart_disease.head()
380/364: X = heart_disease.drop('target', axis=1)
380/365: X.head()
380/366: y = heart_disease['target']
380/367: y.head()
380/368:
# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
380/369: X_train.shape, X_test.shape, y_train.shape, y_test.shape
380/370: X.shape
380/371: len(heart_disease)
380/372:
car_sales = pd.read_csv('data/car-sales-extended.csv')

car_sales.head()
380/373: len(car_sales)
380/374: car_sales.dtypes
380/375:
# Split into X/y

X = car_sales.drop('Price', axis=1)

y = car_sales['Price']

# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
380/376:
# Build Machine Learning model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score()
380/377: transformed_X
380/378: transformed_X.shape
380/379: transformed_X.shape, y.shape
380/380: y = y[:950]
380/381: transformed_X.shape, y.shape
380/382:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/383:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)

model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/384:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)

model = RandomForestRegressor(n_estimators=50)
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/385:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)

model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/386:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.3)

model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/387:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.5)

model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/388:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.8)

model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/389:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.3)

model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/390: what were covering
380/391:
# Now we've got our data as numbers and filled (no missing values)
# Let's fit a model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.3)

model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)
model.score(X_test, y_test)
380/392:
## 2. Choosing the right estimator/algorithm for our problem
Scikit-Learn uses estimator as another term for machine learning model or algorithm
* Classification - predicting whether a sample is one thing or another
* Regression - predicting a number
380/393:
# import boston housing dataset
from sklearn.datasets import load_boston
boston = load_boston()

boston
380/394:
# import boston housing dataset
from sklearn.datasets import load_boston
boston = load_boston()

boston.head()
380/395:
# import boston housing dataset
from sklearn.datasets import load_boston
boston = load_boston()

boston
380/396: boston_df = pd.DataFrame(boston)
380/397:
# import boston housing dataset
from sklearn.datasets import load_boston
boston = load_boston()

boston.data
380/398: boston_df = pd.DataFrame(boston.data)
380/399: boston_df
380/400: boston_df = pd.DataFrame(boston['data'], columns=boston['feature_names'])
380/401: boston_df
380/402: boston_df.head()
380/403:
boston_df = pd.DataFrame(boston['data'], columns=boston['feature_names'])
boston_df['target'] = pd.Series(boston['target'])
380/404: boston_df.head()
380/405:
#How Many Samples?
len(boston_df)
380/406:
# Lets try the Ridge regression model
from sklearn.linear_model import Ridge

# Setup random seed
np.random.seed(42)

# Create the data
X = boston_df.drop('target', axis=1)
y = boston_df['target']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Ridge model
model = Ridge()
model.fit(X_train, y_train)

# Check the score of the Ridge model on test data
model.score(X_test, y_test)
380/407:
# Let's try the Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

# Setup random seed
np.random.seed(42)

# Create the data
X = boston_df.drop('target', axis=1)
y = boston_df['target']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Random Forest Regressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)
rf.score(X_test, y_test)
380/408:
# Let's try the Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

# Setup random seed
np.random.seed(42)

# Create the data
X = boston_df.drop('target', axis=1)
y = boston_df['target']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Random Forest Regressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)

# Evaluate the Random Forest Regressor
rf.score(X_test, y_test)
380/409:
# Let's try the Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

# Setup random seed
np.random.seed(42)

# Create the data
X = boston_df.drop('target', axis=1)
y = boston_df['target']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train, y_train)

# Evaluate the Random Forest Regressor
rf.score(X_test, y_test)
380/410:
# Let's try the Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

# Setup random seed
np.random.seed(42)

# Create the data
X = boston_df.drop('target', axis=1)
y = boston_df['target']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Random Forest Regressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)

# Evaluate the Random Forest Regressor
rf.score(X_test, y_test)
380/411:
heart_disease = pd.read_csv('data/heart-disease.csv')
heart_disease.head()
380/412: len(heart_disease)
380/413:
# import the LinearSVC estimator class
from sklearn.svm import LinearSVC

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop('target', axis=1)
y = heart_disease['target']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate LinearSVC
clf = LinearSVC()
clf.fit(X_train, y_train)

# Evaluate the LinearSVC model
clf.score(X_test, y_test)
380/414:
# import the LinearSVC estimator class
from sklearn.svm import LinearSVC

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop('target', axis=1)
y = heart_disease['target']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate LinearSVC
clf = LinearSVC()
clf.fit(X_train, y_train)

# Evaluate the LinearSVC model
clf.score(X_test, y_test)
380/415: heart_disease['target'].value_counts()
380/416:
# import the Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

# Setup random seed
np.random.seed(42)

# Make the data
X = heart_disease.drop('target', axis=1)
y = heart_disease['target']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Instantiate Random Forest Classifier
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Evaluate the Random Forest Classifier model
clf.score(X_test, y_test)
380/417:
Tid bit:
1. If you have structured data, use ensemble methods
2. If you have unstructured data, use deep learning or transfer learning
387/1:
import geopandas
import pandas as pd

data = geopandas.read_file("crops.geojson")
data.head() # show the first 5 rows of the data.
387/2:
import geopandas
import pandas as pd

data = geopandas.read_file("crops.geojson")
data.head() # show the first 5 rows of the data.
387/3:
import geopandas
import pandas as pd

data = geopandas.read_file("crops.geojson")
data.head() # show the first 5 rows of the data.
388/1:
import geopandas
import pandas as pd

data = geopandas.read_file("crops.geojson")
data.head() # show the first 5 rows of the data.
389/1:
import geopandas
import pandas as pd

data = geopandas.read_file("crops.geojson")
data.head() # show the first 5 rows of the data.
392/1:
import requests
import json
import re
import pandas as pd
392/2:
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'html.parser')
392/3:
import requests
import json
import re
import pandas as pd
392/4:
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'html.parser')
392/5: from bs4 import BeautifulSoup
392/6:
url = 'http://nocopo.bpp.gov.ng/ContractReportCovid19.aspx'
res = req.get(url)

print(res)
392/7:
import requests
import json
import re
import pandas as pd
392/8: from bs4 import BeautifulSoup
392/9:
url = 'http://nocopo.bpp.gov.ng/ContractReportCovid19.aspx'
res = requests.get(url)

print(res)
392/10:
url = 'http://nocopo.bpp.gov.ng/ContractReportCovid19.aspx'
res = requests.get(url)

print(res)
print(res.text)
392/11:
soup = BeautifulSoup(res.text, 'html.parser')

print(soup.prettify())
392/12:
print(soup.find('table', id="RepMinistry_RepLession_0_DGMda_0"))

print(soup.find('table', id="RepMinistry_RepLession_1_DGMda_0"))

print(soup.find('table', id="RepMinistry_RepLession_1_DGMda_1"))

print(soup.find('table', id="RepMinistry_RepLession_1_DGMda_2"))

print(soup.find('table', id="RepMinistry_RepLession_1_DGMda_3"))

print(soup.find('table', id="RepMinistry_RepLession_1_DGMda_4"))

print(soup.find('table', id="RepMinistry_RepLession_1_DGMda_5"))

print(soup.find('table', id="RepMinistry_RepLession_1_DGMda_6"))

print(soup.find('table', id="RepMinistry_RepLession_1_DGMda_7"))

print(soup.find('table', id="RepMinistry_RepLession_2_DGMda_0"))

print(soup.find('table', id="RepMinistry_RepLession_3_DGMda_0"))

print(soup.find('table', id="RepMinistry_RepLession_4_DGMda_0"))

print(soup.find('table', id="RepMinistry_RepLession_5_DGMda_0"))
396/1:
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
396/2:
if __name__ == '__main__':
    br = '\n'
    iris = datasets.load_iris()
    keys = iris.keys()
    print(keys, br)
    X = iris.data
    y = iris.target
    print('features shape:', X.shape)
    print('target shape:', y.shape, br)
    features = iris.feature_names
    targets = iris.target_names
    print('feature set:')
    print(features, br)
    print('targets: ')
    print(targets, br)
    print(iris.DESCR[525:900], br)
    rnd_clf = RandomForestClassifier(random_state=0, n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances
    importance = sorted(zip(feature_importances, features), reverse=True)
    print('most important features ' + ' (' + rnd_name + '):')
    [print(row) for i, row in enumerate(importance)]
396/3:
if __name__ == '__main__':
    br = '\n'
    iris = datasets.load_iris()
    keys = iris.keys()
    print(keys, br)
    X = iris.data
    y = iris.target
    print('features shape:', X.shape)
    print('target shape:', y.shape, br)
    features = iris.feature_names
    targets = iris.target_names
    print('feature set:')
    print(features, br)
    print('targets: ')
    print(targets, br)
    print(iris.DESCR[525:900], br)
    rnd_clf = RandomForestClassifier(random_state=0, n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features), reverse=True)
    print('most important features ' + ' (' + rnd_name + '):')
    [print(row) for i, row in enumerate(importance)]
399/1:
if __name__ == '__main__':
    br = '\n'
    iris = datasets.load_iris()
    keys = iris.keys()
    print(keys, br)
    X = iris.data
    y = iris.target
    print('features shape:', X.shape)
    print('target shape:', y.shape, br)
    features = iris.feature_names
    targets = iris.target_names
    print('feature set:')
    print(features, br)
    print('targets: ')
    print(targets, br)
    print(iris.DESCR[525:900], br)
    rnd_clf = RandomForestClassifier(random_state=0, n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features), reverse=True)
    print('most important features ' + ' (' + rnd_name + '):')
    [print(row) for i, row in enumerate(importance)]
399/2:
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
399/3:
if __name__ == '__main__':
    br = '\n'
    iris = datasets.load_iris()
    keys = iris.keys()
    print(keys, br)
    X = iris.data
    y = iris.target
    print('features shape:', X.shape)
    print('target shape:', y.shape, br)
    features = iris.feature_names
    targets = iris.target_names
    print('feature set:')
    print(features, br)
    print('targets: ')
    print(targets, br)
    print(iris.DESCR[525:900], br)
    rnd_clf = RandomForestClassifier(random_state=0, n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features), reverse=True)
    print('most important features ' + ' (' + rnd_name + '):')
    [print(row) for i, row in enumerate(importance)]
399/4:
from sklearn.datasets import load_wine
from sklearn.ensemble import RandomForestClassifier
399/5:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    keys = data.keys
    print(keys, br)
    X, y = data.data, data.target
    print('features: ', X.shape)
    print('targets', y.shape, br)
    print(X[0], br)
    features = data.feature_names
    targets = data.target_names
    print('features set:')
    print(features, br)
    print('targets:')
    print(targets, br)
    rnd_clf = RandomForestClassifier(random_state=0,
                                     n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features),
                        reverse=True)
    n= 6
    print (n, 'most important features' + ' (' + rnd_name + '):') [print (row) for i, row in enumerate(importance) if i < n]
399/6:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    keys = data.keys
    print(keys, br)
    X, y = data.data, data.target
    print('features: ', X.shape)
    print('targets', y.shape, br)
    print(X[0], br)
    features = data.feature_names
    targets = data.target_names
    print('features set:')
    print(features, br)
    print('targets:')
    print(targets, br)
    rnd_clf = RandomForestClassifier(random_state=0,
                                     n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features),
                        reverse=True)
    n= 6
    print (n, 'most important features' + ' (' + rnd_name + '):') 
    [print (row) for i, row in enumerate(importance) if i < n]
399/7:
import numpy as np
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt
399/8:
if __name__ == "__main__":
    br = '\n'
    digits = load_digits()
    print (digits.keys(), br)
    print ('2D shape of digits data:', digits.images.shape, br)
    X = digits.data
    y = digits.target
    print ('X shape (8x8 flattened to 64 pixels):', end=' ')
    print (X.shape)
    print ('y shape:', end=' ')
    print (y.shape, br)
    i = 500
    print ('vector (flattened matrix) of "feature" image:')
    print (X[i], br)
    print ('matrix (transformed vector) of a "feature" image:')
    X_i = np.array(X[i]).reshape(8, 8)
    print (X_i, br)
    print ('target:', y[i], br)
    print ('original "digits" image matrix:')
    print (digits.images[i])
    plt.figure(1, figsize=(3, 3))
    plt.title('reshaped flattened vector')
    plt.imshow(X_i, cmap='gray', interpolation='gaussian')
    plt.figure(2, figsize=(3, 3))
    plt.title('original images dataset')
    plt.imshow(digits.images[i], cmap='gray',
               interpolation='gaussian')
    plt.show()
399/9:
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
399/10:
if __name__ == '__main__':
    br = '\n'
    iris = datasets.load_iris()
    keys = iris.keys()
    print(keys, br)
    X = iris.data
    y = iris.target
    print('features shape:', X.shape)
    print('target shape:', y.shape, br)
    features = iris.feature_names
    targets = iris.target_names
    print('feature set:')
    print(features, br)
    print('targets: ')
    print(targets, br)
    print(iris.DESCR[525:900], br)
    rnd_clf = RandomForestClassifier(random_state=0, n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features), reverse=True)
    print('most important features ' + ' (' + rnd_name + '):')
    [print(row) for i, row in enumerate(importance)]
399/11:
from sklearn.datasets import load_wine
from sklearn.ensemble import RandomForestClassifier
399/12:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    keys = data.keys
    print(keys, br)
    X, y = data.data, data.target
    print('features: ', X.shape)
    print('targets', y.shape, br)
    print(X[0], br)
    features = data.feature_names
    targets = data.target_names
    print('features set:')
    print(features, br)
    print('targets:')
    print(targets, br)
    rnd_clf = RandomForestClassifier(random_state=0,
                                     n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features),
                        reverse=True)
    n= 6
    print (n, 'most important features' + ' (' + rnd_name + '):') 
    [print (row) for i, row in enumerate(importance) if i < n]
399/13:
import numpy as np
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt
399/14:
if __name__ == "__main__":
    br = '\n'
    digits = load_digits()
    print (digits.keys(), br)
    print ('2D shape of digits data:', digits.images.shape, br)
    X = digits.data
    y = digits.target
    print ('X shape (8x8 flattened to 64 pixels):', end=' ')
    print (X.shape)
    print ('y shape:', end=' ')
    print (y.shape, br)
    i = 500
    print ('vector (flattened matrix) of "feature" image:')
    print (X[i], br)
    print ('matrix (transformed vector) of a "feature" image:')
    X_i = np.array(X[i]).reshape(8, 8)
    print (X_i, br)
    print ('target:', y[i], br)
    print ('original "digits" image matrix:')
    print (digits.images[i])
    plt.figure(1, figsize=(3, 3))
    plt.title('reshaped flattened vector')
    plt.imshow(X_i, cmap='gray', interpolation='gaussian')
    plt.figure(2, figsize=(3, 3))
    plt.title('original images dataset')
    plt.imshow(digits.images[i], cmap='gray',
               interpolation='gaussian')
    plt.show()
399/15: ### News Group data
399/16:
from sklearn.datasets import fetch_20newsgroups

if __name__ == "__main__":
    br = '\n'
    train = fetch_20newsgroups(subset='train')
    test = fetch_20newsgroups(subset='test')
    print ('data:')
    print (train.target.shape, 'shape of train data')
    print (test.target.shape, 'shape of test data', br)
    targets = test.target_namesprint (targets, br)
    categories = ['rec.autos', 'rec.motorcycles', 'sci.space','sci.med']
    train = fetch_20newsgroups(subset='train',categories=categories)
    test = fetch_20newsgroups(subset='test',categories=categories)
    print ('data subset:')
    print (train.target.shape, 'shape of train data')
    print (test.target.shape, 'shape of test data', br)
    targets = train.target_names
    print (targets)
399/17:
from sklearn.datasets import fetch_20newsgroups

if __name__ == "__main__":
    br = '\n'
    train = fetch_20newsgroups(subset='train')
    test = fetch_20newsgroups(subset='test')
    print ('data:')
    print (train.target.shape, 'shape of train data')
    print (test.target.shape, 'shape of test data', br)
    targets = test.target_names
    print (targets, br)
    categories = ['rec.autos', 'rec.motorcycles', 'sci.space','sci.med']
    train = fetch_20newsgroups(subset='train',categories=categories)
    test = fetch_20newsgroups(subset='test',categories=categories)
    print ('data subset:')
    print (train.target.shape, 'shape of train data')
    print (test.target.shape, 'shape of test data', br)
    targets = train.target_names
    print (targets)
399/18:
import numpy as np
from random import randint
import matplotlib.pyplot as plt

def find_image(data, labels, d):
    for i, row in enumerate(labels):
        if d == row:
            target = row
            X_pixels = np.array(data[i])
            return (target, X_pixels)


if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_mnist.npy')
    y = np.load('data/y_mnist.npy')
    target = np.load('data/mnist_targets.npy')
    print ('labels (targets):')
    print (target, br)
    print ('feature set shape:')
    print (X.shape, br)
    print ('target set shape:')
    print (y.shape, br)
    indx = randint(0, y.shape[0]-1)
    target = y[indx]
    X_pixels = np.array(X[indx])
    print ('the feature image consists of', len(X_pixels),'pixels')
    X_image = X_pixels.reshape(28, 28)
    plt.figure(1, figsize=(3, 3))
    title = 'image @ indx ' + str(indx) + ' is digit ' \+ str(int(target))
    plt.title(title)
    plt.imshow(X_image, cmap='gray')
    digit = 7
    target, X_pixels = find_image(X, y, digit)
    X_image = X_pixels.reshape(28, 28)
    plt.figure(2, figsize=(3, 3))
    title = 'find first ' + str(int(target)) + ' in dataset'
    plt.title(title)
    plt.imshow(X_image, cmap='gray')
    plt.show()
399/19:
import numpy as np
from random import randint
import matplotlib.pyplot as plt

def find_image(data, labels, d):
    for i, row in enumerate(labels):
        if d == row:
            target = row
            X_pixels = np.array(data[i])
            return (target, X_pixels)


if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_mnist.npy')
    y = np.load('data/y_mnist.npy')
    target = np.load('data/mnist_targets.npy')
    print ('labels (targets):')
    print (target, br)
    print ('feature set shape:')
    print (X.shape, br)
    print ('target set shape:')
    print (y.shape, br)
    indx = randint(0, y.shape[0]-1)
    target = y[indx]
    X_pixels = np.array(X[indx])
    print ('the feature image consists of', len(X_pixels),'pixels')
    X_image = X_pixels.reshape(28, 28)
    plt.figure(1, figsize=(3, 3))
    title = 'image @ indx ' + str(indx) + ' is digit ' + str(int(target))
    plt.title(title)
    plt.imshow(X_image, cmap='gray')
    digit = 7
    target, X_pixels = find_image(X, y, digit)
    X_image = X_pixels.reshape(28, 28)
    plt.figure(2, figsize=(3, 3))
    title = 'find first ' + str(int(target)) + ' in dataset'
    plt.title(title)
    plt.imshow(X_image, cmap='gray')
    plt.show()
399/20:
import numpy as np
import matplotlib as pyplot
402/1: import numpy as np, pandas as pd, seaborn as sns
402/2:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
402/3: import numpy as np, pandas as pd, seaborn as sns
402/4:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
402/5:
if __name__ == '__main__':
    br = '\n'
    sns.set_color_codes
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
402/6:
if __name__ == '__main__':
    br = '\n'
    sns.set_color_codes()
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
402/7:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
    X = tips.drop(['tip'], axis=1).values
    y = tips['tips'].values
    print(X.shape, y.shape)
402/8: import numpy as np, pandas as pd, seaborn as sns
402/9:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
    X = tips.drop(['tip'], axis=1).values
    y = tips['tips'].values
    print(X.shape, y.shape)
402/10:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
    X = tips.drop(['tip'], axis=1).values
    y = tips['tip'].values
    print(X.shape, y.shape)
402/11:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
    X = tips.drop(['tip'], axis=1).values
    y = tips['tip'].values
    print(X.shape, y.shape)
    print(X.head())
    print(y.head())
402/12:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
    X = tips.drop(['tip'], axis=1).values
    y = tips['tip'].values
    print(X.shape, y.shape)
    print(X.head())
    print(y[0:5])
402/13:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
    X = tips.drop(['tip'], axis=1).values
    y = tips['tip'].values
    print(X.shape, y.shape)
    print(X[0:5])
    print(y[0:5])
402/14:
if __name__ == '__main__':
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print(tips.head(), br)
    X = tips.drop(['tip'], axis=1).values
    y = tips['tip'].values
    print(X.shape, y.shape)
    print(X[0:5])
    print('-----------')
    print(y[0:5])
402/15:
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
402/16:
if __name__ == '__main__':
    br = '\n'
    f = 'data/redwine.csv'
    redwine = pd.read_csv(f)

    X = redwine.drop(['quality'], axis=1)
    y = redwine['quality']

    print(X.shape)
    print(y.shape, br)

    features = list(X)
    rfr = RandomForestRegressor(random_state=0, n_estimators=100)
    rfr_name = rfr.__class__.__name__
    rfr.fit(X, y)

    feature_importances = rfr.feature_importances_
    importance = sorted(zip(feature_importances, features), reverse=True)
    
    n = 3
    print(n, ' most importance features ' + '( ' + rfr_name + '):')
    [print (row) for i, row in enumerate(importance) if i < n]

    for row importance:
        print(row)
    
    print()

    print(redwine[['alcohol', 'sulphates', 'volatile acidity', 'total sulfur dioxide', 'quality']].head())
402/17:
if __name__ == '__main__':
    br = '\n'
    f = 'data/redwine.csv'
    redwine = pd.read_csv(f)

    X = redwine.drop(['quality'], axis=1)
    y = redwine['quality']

    print(X.shape)
    print(y.shape, br)

    features = list(X)
    rfr = RandomForestRegressor(random_state=0, n_estimators=100)
    rfr_name = rfr.__class__.__name__
    rfr.fit(X, y)

    feature_importances = rfr.feature_importances_
    importance = sorted(zip(feature_importances, features), reverse=True)
    
    n = 3
    print(n, ' most importance features ' + '( ' + rfr_name + '):')
    [print (row) for i, row in enumerate(importance) if i < n]

    for row in importance:
        print(row)
    
    print()

    print(redwine[['alcohol', 'sulphates', 'volatile acidity', 'total sulfur dioxide', 'quality']].head())
402/18:
import numpy as np, pandas as pd
from sklearn.ensemble import RandomForestRegressor
402/19:
if __name__ == '__main__':
    br = '\n'
    f = 'data/whitewine.csv'
    white_wine = pd.read_csv(f)

    X = white_wine.drop(['quality'], axis=1)
    y = white_wine['quality']

    print(X.shape)
    print(y.shape, br)
402/20:
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
402/21:
if __name__ == '__main__':
    br = '\n'
    boston = load_boston()
    print(boston.keys, br)
402/22:
if __name__ == '__main__':
    br = '\n'
    boston = load_boston()
    print(boston.keys(), br)
402/23:
if __name__ == '__main__':
    br = '\n'
    boston = load_boston()
    print(boston.keys(), br)
    print(boston.feature_names)
402/24:
if __name__ == '__main__':
    br = '\n'
    boston = load_boston()
    print(boston.keys(), br)
    print(boston.feature_names, br)
402/25:
if __name__ == '__main__':
    br = '\n'
    boston = load_boston()

    print(boston.keys(), br)
    print(boston.feature_names, br)

    X = boston.data
    y = boston.target

    print('feature shape ', X.shape)
    print('target shape ', y.shape, br)

    keys = boston.keys()
    rfr = RandomForestRegressor(random_state=0, n_estimators=100)
    rfr.fit(X, y)

    features = boston.feature_names
    feature_importance = rfr.feature_importances_
    importance = sorted(zip(feature_importances, features), reverse=True)

    [print(row) for i, row in enumerate(importance) if i < 3]
402/26:
if __name__ == '__main__':
    br = '\n'
    boston = load_boston()

    print(boston.keys(), br)
    print(boston.feature_names, br)

    X = boston.data
    y = boston.target

    print('feature shape ', X.shape)
    print('target shape ', y.shape, br)

    keys = boston.keys()
    rfr = RandomForestRegressor(random_state=0, n_estimators=100)
    rfr.fit(X, y)

    features = boston.feature_names
    feature_importances = rfr.feature_importances_
    importance = sorted(zip(feature_importances, features), reverse=True)

    [print(row) for i, row in enumerate(importance) if i < 3]
404/1:
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
404/2:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<<' + sgd_name + '>>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:')
    scaler = StandardScaler.fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)

    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_test_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/3:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<<' + sgd_name + '>>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:')
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)

    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_test_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/4:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<<' + sgd_name + '>>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:')
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)

    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_test_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/5:
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
404/6:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<<' + sgd_name + '>>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:')
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)

    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_test_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/7:
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
404/8:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<<' + sgd_name + '>>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:')
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)

    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_test_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/9:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<<' + sgd_name + '>>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:')
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)

    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_train_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/10:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<<' + sgd_name + '>>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:', accuracy)
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)

    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_train_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/11:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<< ' + sgd_name + ' >>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:', accuracy)

    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)
    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_train_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/12:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()

    X = digits.data
    y = digits.target

    X_train, X_test, y_tain, y_test = train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__

    print('<< ' + sgd_name + ' >>', br)

    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Unscalled \'test\' accuracy:', accuracy)

    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train), scaler.transform(X_test)
    sgd_std = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_std.fit(X_train_std, y_tain)

    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print('scaled \'test\' accuracy:', np.round(accuracy, 4))
404/13:
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

import seaborn asn sns, matplotlib.pyplot as plt

if __name__ == '__main__':
    br = '\n'
    iris = load_iris()
    X = iris.data
    y = iris.target
    pca = PCA(n_components=0.95)
    X_reduced = pca.fit_transform(X)
    components = pca.n_components_
    model = PCA(n_components=components)
    model.fit(X)
    X_2D = model.transform(X)
    iris_df = sns.load_dataset('iris')
    iris_df['PCA1'] = X_2D[:, 0]
    iris_df['PCA2'] = X_2D[:, 1]
    
    print(iris_df[['PCA1', 'PCA2']].head(3), br)
    sns.set(color_codes=True)
    sns.Implot('PCA1', 'PCA2', hue='species', data=iris_df, fit_reg=False)
    plt.subtitle('PCA reduction')

    lda = LinearDiscriminantAnalysis(n_components=2)
    transform_lda = lda.fit_transform(X, y)
    iris_df['LDA1'] = transform_lda[:, 0]
    iris_df['LDA2'] = transform_lda[:, 1]

    print(iris_df[['LDA1', 'LDA2']].head(3))
    sns.Implot('LDA1', 'LDA2', hue='species', data=iris_df, fit_reg=False)
    plt.subtitle('LDA Reduction')
    plt.show()
404/14:
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

import seaborn as sns, matplotlib.pyplot as plt

if __name__ == '__main__':
    br = '\n'
    iris = load_iris()
    X = iris.data
    y = iris.target
    pca = PCA(n_components=0.95)
    X_reduced = pca.fit_transform(X)
    components = pca.n_components_
    model = PCA(n_components=components)
    model.fit(X)
    X_2D = model.transform(X)
    iris_df = sns.load_dataset('iris')
    iris_df['PCA1'] = X_2D[:, 0]
    iris_df['PCA2'] = X_2D[:, 1]
    
    print(iris_df[['PCA1', 'PCA2']].head(3), br)
    sns.set(color_codes=True)
    sns.Implot('PCA1', 'PCA2', hue='species', data=iris_df, fit_reg=False)
    plt.subtitle('PCA reduction')

    lda = LinearDiscriminantAnalysis(n_components=2)
    transform_lda = lda.fit_transform(X, y)
    iris_df['LDA1'] = transform_lda[:, 0]
    iris_df['LDA2'] = transform_lda[:, 1]

    print(iris_df[['LDA1', 'LDA2']].head(3))
    sns.Implot('LDA1', 'LDA2', hue='species', data=iris_df, fit_reg=False)
    plt.subtitle('LDA Reduction')
    plt.show()
404/15:
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

import seaborn as sns, matplotlib.pyplot as plt

if __name__ == '__main__':
    br = '\n'
    iris = load_iris()
    X = iris.data
    y = iris.target
    pca = PCA(n_components=0.95)
    X_reduced = pca.fit_transform(X)
    components = pca.n_components_
    model = PCA(n_components=components)
    model.fit(X)
    X_2D = model.transform(X)
    iris_df = sns.load_dataset('iris')
    iris_df['PCA1'] = X_2D[:, 0]
    iris_df['PCA2'] = X_2D[:, 1]
    
    print(iris_df[['PCA1', 'PCA2']].head(3), br)
    sns.set(color_codes=True)
    sns.lmplot('PCA1', 'PCA2', hue='species', data=iris_df, fit_reg=False)
    plt.subtitle('PCA reduction')

    lda = LinearDiscriminantAnalysis(n_components=2)
    transform_lda = lda.fit_transform(X, y)
    iris_df['LDA1'] = transform_lda[:, 0]
    iris_df['LDA2'] = transform_lda[:, 1]

    print(iris_df[['LDA1', 'LDA2']].head(3))
    sns.lmplot('LDA1', 'LDA2', hue='species', data=iris_df, fit_reg=False)
    plt.subtitle('LDA Reduction')
    plt.show()
404/16:
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

import seaborn as sns, matplotlib.pyplot as plt

if __name__ == '__main__':
    br = '\n'
    iris = load_iris()
    X = iris.data
    y = iris.target
    pca = PCA(n_components=0.95)
    X_reduced = pca.fit_transform(X)
    components = pca.n_components_
    model = PCA(n_components=components)
    model.fit(X)
    X_2D = model.transform(X)
    iris_df = sns.load_dataset('iris')
    iris_df['PCA1'] = X_2D[:, 0]
    iris_df['PCA2'] = X_2D[:, 1]
    
    print(iris_df[['PCA1', 'PCA2']].head(3), br)
    sns.set(color_codes=True)
    sns.lmplot('PCA1', 'PCA2', hue='species', data=iris_df, fit_reg=False)
    plt.subtitle('PCA reduction')

    lda = LinearDiscriminantAnalysis(n_components=2)
    transform_lda = lda.fit_transform(X, y)
    iris_df['LDA1'] = transform_lda[:, 0]
    iris_df['LDA2'] = transform_lda[:, 1]

    print(iris_df[['LDA1', 'LDA2']].head(3))
    sns.lmplot('LDA1', 'LDA2', hue='species', data=iris_df, fit_reg=False)
    plt.suptitle('LDA Reduction')
    plt.show()
404/17:
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

import seaborn as sns, matplotlib.pyplot as plt

if __name__ == '__main__':
    br = '\n'
    iris = load_iris()
    X = iris.data
    y = iris.target
    pca = PCA(n_components=0.95)
    X_reduced = pca.fit_transform(X)
    components = pca.n_components_
    model = PCA(n_components=components)
    model.fit(X)
    X_2D = model.transform(X)
    iris_df = sns.load_dataset('iris')
    iris_df['PCA1'] = X_2D[:, 0]
    iris_df['PCA2'] = X_2D[:, 1]
    
    print(iris_df[['PCA1', 'PCA2']].head(3), br)
    sns.set(color_codes=True)
    sns.lmplot('PCA1', 'PCA2', hue='species', data=iris_df, fit_reg=False)
    plt.suptitle('PCA reduction')

    lda = LinearDiscriminantAnalysis(n_components=2)
    transform_lda = lda.fit_transform(X, y)
    iris_df['LDA1'] = transform_lda[:, 0]
    iris_df['LDA2'] = transform_lda[:, 1]

    print(iris_df[['LDA1', 'LDA2']].head(3))
    sns.lmplot('LDA1', 'LDA2', hue='species', data=iris_df, fit_reg=False)
    plt.suptitle('LDA Reduction')
    plt.show()
404/18:
from sklearn.datasets import load_digits
from sklearn.manifold import Isomap
import matplotlib.pyplot as plt
404/19:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits
    X = digits.data
    y = digits.target

    print('feature data shape: ', X.shape)
404/20:
from sklearn.datasets import load_digits
from sklearn.manifold import Isomap
import matplotlib.pyplot as plt
404/21:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits
    X = digits.data
    y = digits.target

    print('feature data shape: ', X.shape)
404/22:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target

    print('feature data shape: ', X.shape)
404/23:
if __name__ == '__main__':
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target

    print('feature data shape: ', X.shape)


    iso = Isomap(n_components=2)
    iso_name = iso.__class__.__name__
    iso.fit(digits.data)
    data_projected = iso.transform(X)

    print('project data to 2D:', data_projected.shape)

    project_1, project_2 = data_projected[:, 0], data_projected[:, 1]

    plt.figure(iso_name)
    plt.scatter(project_1, project_2, c=y, edgecolors='none', alpha=0.5, cmap='jet')
    plt.colorbar(label='digit label', ticks=range(10))
    plt.clim(-0.5, 9.5)
    plt.show()
404/24:
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

from random import *
404/25:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA.fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2f%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2f%}'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
404/26:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2f%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2f%}'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
404/27:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd = sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2f%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2f%}'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
404/28:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd.fit(X_train, y_tain)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2f%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2f%}'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
404/29:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd.fit(X_train, y_train)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2f%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2f%}'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
404/30:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd.fit(X_train, y_train)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2%}\n'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
404/31:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd.fit(X_train, y_train)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2%}\n'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
404/32:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd.fit(X_train, y_train)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2%}\n'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
404/33: print(lda)
404/34:
if __name__ == '__main__':
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print(lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)

    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'

    print(lda_name + ':')
    print('train: ', accuracy)

    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'

    print('test ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)

    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)

    sgd = SGDClassifier(max_iter=5, random_state=0)
    print(sgd, br)
    sgd.fit(X_train, y_train)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)

    print(sgd_name + ':')
    print('train: {:.2%}'.format(metrics.accuracy_score(y_train, y_pred)))
    print('test: {:.2%}\n'.format(metrics.accuracy_score(y_test, y_pred_test)))

    print('Confusion Matrix ', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)

    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print('MCS (true test accuracy): ', avg)
406/1:
import requests
import json
import re
import pandas as pd
406/2:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

print(res)
406/3:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

print(res.data)
406/4:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

print(res.data())
406/5:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

print(res.text)
406/6:
from bs4 import BeautifulSoup

import pandas as pd
import requests
import json
import re
406/7:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

print(soup.prettify())
406/8:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

print(soup.find_all('h5'))
406/9:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

print(soup.find_all('h5>b'))
406/10:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

print(soup.find_all('b'))
406/11:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

print(soup.find_all('h5'))
406/12:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    header_list.append(item)

# print(soup.find_all('h5'))
print(header_list)
406/13:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(item)

# print(soup.find_all('h5'))
# print(header_list)
406/14:
from bs4 import BeautifulSoup

import pandas as pd
import requests
import json
import re
406/15:
def striphtml(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/16:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/17:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(remove_html_tags(item))

# print(soup.find_all('h5'))
print(header_list)
406/18:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/19:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(remove_html_tags(item))

# print(soup.find_all('h5'))
print(header_list)
406/20:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/21: remove_html_tags(<h5><b>Name: </b>JASCINTA AACA</h5>)
406/22: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/23: remove_html_tags(str(<h5><b>Name: </b>JASCINTA AACA</h5>))
406/24: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/25:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(remove_html_tags('{}'.format(item)))

# print(soup.find_all('h5'))
print(header_list)
406/26:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(remove_html_tags('"{}"'.format(item)))

# print(soup.find_all('h5'))
print(header_list)
406/27:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(remove_html_tags(f"{item}")

# print(soup.find_all('h5'))
print(header_list)
406/28:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(all_headers.get_text())

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/29:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(all_headers)

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/30:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(all_headers[0])

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/31:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(all_headers[2])

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/32:
def remove_html_tags(data):
    p = re.compile(r<.*?>)
    return p.sub('', data)
406/33:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/34: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/35:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(remove_html_tags(all_headers[0]))

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/36:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(str(remove_html_tags(all_headers[0])))

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/37:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(f"'{remove_html_tags(all_headers[0])}'")

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/38:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub(' ', data)
406/39: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/40: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/41:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/42: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/43:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(BeautifulSoup(all_headers[0], 'lxml').text)

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/44:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(BeautifulSoup(all_headers[0], 'lxml').text)

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/45:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/46:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(all_headers[0])

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/47:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(type all_headers[0])

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/48:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(type(all_headers[0]))

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/49:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5').text

print(type(all_headers[0]))

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/50:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(all_headers[0])

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/51:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(all_headers[0].string)

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/52:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/53: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/54:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(all_headers[0])

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/55:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(f'"{all_headers[0]}"')

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/56:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(f"'{all_headers[0]}'")

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/57:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

# for item in all_headers:
#     print(item, '\n')
#     header_list.append(remove_html_tags(f"{item}")

# # print(soup.find_all('h5'))
# print(header_list)
406/58:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(remove_html_tags(f"'{all_headers[0]}'"))

# # print(soup.find_all('h5'))
# print(header_list)
406/59:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(remove_html_tags(f"'{all_headers[item]}'"))

# # print(soup.find_all('h5'))
# print(header_list)
406/60:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    print(item, '\n')
    header_list.append(remove_html_tags(f"'{item}'"))

# # print(soup.find_all('h5'))
# print(header_list)
406/61:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"'{item}'")
    header_list.append(item)
    print(item, '\n')

# # print(soup.find_all('h5'))
# print(header_list)
406/62:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"'{item}'")
    header_list.append(item)
    # print(item, '\n')

# # print(soup.find_all('h5'))
print(all_headers)
406/63:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"'{item}'")
    header_list.append(item)
    # print(item, '\n')

print(header_list)
406/64:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/65:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"'{item}'")
    header_list.append(eval(item))
    # print(item, '\n')

print(header_list)
406/66:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"'{item}'")
    header_list.append(item.replace('"', ''))
    # print(item, '\n')

print(header_list)
406/67:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"'{item}'")
    header_list.append(item.replace("'", ''))
    # print(item, '\n')

print(header_list)
406/68:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"'{item}'")
    header_list.append(item.replace('"', ''))
    # print(item, '\n')

print(header_list)
406/69:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)
    # print(item, '\n')

print(header_list)
406/70: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/71:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)
    # print(item, '\n')

print(header_list)
406/72:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];
406/73:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)
    # print(item, '\n')

print(header_list)
406/74:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

# header_list = [];

for item in all_headers:
    # item = remove_html_tags(f"{item}")
    header_list.append(item)
    # print(item, '\n')

print(header_list)
406/75:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

header_list = [];

for item in all_headers:
    # item = remove_html_tags(f"{item}")
    header_list.append(item)
    # print(item, '\n')

print(header_list)
406/76:
url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

# print(soup.prettify())

all_headers = soup.find_all('h5')

# print(remove_html_tags(f"'{all_headers[0]}'"))

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)
    # print(item, '\n')

print(header_list)
406/77:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=2'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(header_list)
406/78:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(header_list)
406/79:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(header_list)
406/80:
health_p_df = pd.DataFrame({'col': header_list})

health_p_df.head()
406/81:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(header_list)
406/82: print(header_list[0])
406/83: print(header_list[0:5])
406/84: print(header_list[0][0])
406/85: print(header_list[0][0:5])
406/86: print(header_list[0][0:5])
406/87:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list})

health_p_df.head()
406/88: print(header_list[1][0:5])
406/89: print(header_list[1][0:6])
406/90: print(header_list[1][0:8])
406/91:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list[0]})

health_p_df.head()
406/92:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list})

health_p_df.head()
406/93:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list[0][0]})

health_p_df.head()
406/94:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list})

health_p_df.head()
406/95:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list})

health_p_df.head()
406/96: print(header_list[0])
406/97: print(header_list[0][:])
406/98: print(header_list[0])
406/99: print(header_list[1])
406/100: print(header_list[2])
406/101:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2]: header_list})

health_p_df.head()
406/102:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2]: header_list, header_list[3][0:8]: header_list})

health_p_df.head()
406/103:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2]: header_list, header_list[3]: header_list, header_list[4]: header_list})

health_p_df.head()
406/104:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3]: header_list, header_list[4]: header_list})

health_p_df.head()
406/105:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:15]: header_list, header_list[4]: header_list})

health_p_df.head()
406/106:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:17]: header_list, header_list[4]: header_list})

health_p_df.head()
406/107:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:18]: header_list, header_list[4]: header_list})

health_p_df.head()
406/108:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:19]: header_list, header_list[4]: header_list})

health_p_df.head()
406/109:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:19]: header_list, header_list[4][0:3]: header_list})

health_p_df.head()
406/110:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:19]: header_list, header_list[4][0:13]: header_list})

health_p_df.head()
406/111:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:19]: header_list, header_list[4][0:14]: header_list})

health_p_df.head()
406/112:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:19]: header_list, header_list[4][0:15]: header_list})

health_p_df.head()
406/113: print(header_list)
406/114:
print(header_list)

print(len(header_list))
406/115:
print(header_list[0])

print(len(header_list))
406/116:
print(header_list[5])

print(len(header_list))
406/117:
print(header_list[5])

print(len(header_list))
406/118:

names = []

for i in len(120):
    print(header_list[0][i])
406/119:

names = []

for i in len(header_list):
    print(header_list[0][i])
406/120:

names = []

for i in range(len(header_list)):
    print(header_list[0][i])
406/121:

names = []

for i in range(len(header_list)):
    print(header_list[i])
406/122:

names = []

for i in range(len(header_list)):
    print(header_list[i])
406/123:
health_p_df = pd.DataFrame({
    'health_info': header_list
})
406/124:
health_p_df = pd.DataFrame({
    'health_info': header_list
})
406/125: health_p_df.head(5)
406/126: health_p_df.head(5)
406/127: health_p_df.T
406/128:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    counter++;
    if(counter == 5):
        print('============')
406/129:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    counter += 1;
    if(counter == 5):
        print('============')
406/130:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0
406/131:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    print('name ', header_list[i][6:])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0
406/132:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    # print('name ', header_list[i][6:])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0
406/133:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    # print('name ', header_list[i][6:])
    print('council ', header_list[i][6:])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0
406/134:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    # print('name ', header_list[i][6:])
    print('council ', header_list[i][1][6:])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0
406/135:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    # print('name ', header_list[i][6:])
    print('council ', header_list[i][2][6:])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0
406/136:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    # print('name ', header_list[i][6:])
    # print('council ', header_list[i][2][6:])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0
406/137:

names = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    names.append(header_list[i])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names)
406/138:

names = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    names.append(header_list[i])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[0])
406/139:
health_p_df = pd.DataFrame({
    'health_info': header_list
})

health_p_df.to_csv('health_p_df.csv')
406/140:
health_p_df = pd.DataFrame({
    'health_info': header_list
})

health_p_df.T.to_csv('health_p_df.csv')
406/141:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(header_list)

print(json.dumps(header_list))
406/142:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(header_list)

print(json.dumps(header_list))
406/143:
print(header_list[5])

print(len(header_list))
406/144:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(header_list)

print(json.dumps(header_list))
406/145:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(header_list)

print(json.dumps(header_list))

df1 = pd.DataFrame({'col': json.dumps(header_list)})

df1
406/146:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(header_list)

# print(json.dumps(header_list))
406/147:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

print(all_headers[0])

# header_list = [];

# for item in all_headers:
#     item = remove_html_tags(f"{item}")
#     header_list.append(item)

# print(header_list)

# print(json.dumps(header_list))
406/148:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

print(all_headers[5])

# header_list = [];

# for item in all_headers:
#     item = remove_html_tags(f"{item}")
#     header_list.append(item)

# print(header_list)

# print(json.dumps(header_list))
406/149:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

print(all_headers[6])

# header_list = [];

# for item in all_headers:
#     item = remove_html_tags(f"{item}")
#     header_list.append(item)

# print(header_list)

# print(json.dumps(header_list))
406/150:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

print(all_headers[6])

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(header_list)

# print(json.dumps(header_list))
406/151:

names = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[0])
406/152:

names = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i])
    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[2])
406/153:

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[2])
print(councils[2])
print(registration_no[2])
print(registration_date[2])
print(license_no[2])
406/154:

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[2])
print(councils[2])
print(registration_no[2])
print(registration_date[2])
print(license_no[2])
406/155:

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[2])
print(councils[2])
print(registration_no[2])
print(registration_date[2])
print(license_no[2])
406/156:

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i][19:]:
        registration_date.append(header_list[i][15:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[2])
print(councils[2])
print(registration_no[2])
print(registration_date[2])
print(license_no[2])
406/157:

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][19:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][15:])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[2])
print(councils[2])
print(registration_no[2])
print(registration_date[2])
print(license_no[2])
406/158:

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

print(names[2])
print(councils[2])
print(registration_no[2])
print(registration_date[2])
print(license_no[2])
406/159:


health_prof = {}

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

df = pd.DataFrame()

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(names[2])
print(councils[2])
print(registration_no[2])
print(registration_date[2])
print(license_no[2])
print(health_prof)
406/160:


health_prof = {}

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(names[2])
print(councils[2])
print(registration_no[2])
print(registration_date[2])
print(license_no[2])
print(health_prof)

# df = pd.DataFrame()
406/161: health_prof['name']
406/162: health_prof.name
406/163: health_prof
406/164: pd.DataFrame(health_prof)
406/165:


# health_prof = {
#     'name': '',
#     'councils': '',
#     'registration_no': '',
#     'registration_date': '',
#     'license_no': ''
# }

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof)

# df = pd.DataFrame()
406/166:


# health_prof = {
#     'name': '',
#     'councils': '',
#     'registration_no': '',
#     'registration_date': '',
#     'license_no': ''
# }

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof)

# df = pd.DataFrame()
406/167:


# health_prof = {
#     'name': '',
#     'councils': '',
#     'registration_no': '',
#     'registration_date': '',
#     'license_no': ''
# }

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof)

# df = pd.DataFrame()
406/168:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    
    health_prof['name'] = if 'Name:' in header_list[i]: names.append(header_list[i][6:]),
    health_prof['councils'] = if 'Council:' in header_list[i]: councils.append(header_list[i][9:]),
    health_prof['registration_no'] = if 'Registration No :' in header_list[i]: registration_no.append(header_list[i][18:]),
    health_prof['registration_date'] = if 'Registration Date :' in header_list[i]: registration_date.append(header_list[i][20:]),
    health_prof['license_no'] = if 'License Number:' in header_list[i]: license_no.append(header_list[i][16:])

    counter += 1;
    
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof)

# df = pd.DataFrame()
406/169:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    
    health_prof['name'] = (if 'Name:' in header_list[i]: names.append(header_list[i][6:])),
    health_prof['councils'] = if 'Council:' in header_list[i]: councils.append(header_list[i][9:]),
    health_prof['registration_no'] = if 'Registration No :' in header_list[i]: registration_no.append(header_list[i][18:]),
    health_prof['registration_date'] = if 'Registration Date :' in header_list[i]: registration_date.append(header_list[i][20:]),
    health_prof['license_no'] = if 'License Number:' in header_list[i]: license_no.append(header_list[i][16:])

    counter += 1;

    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof)

# df = pd.DataFrame()
406/170:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof)

# df = pd.DataFrame()
406/171:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof['name'])

# df = pd.DataFrame()
406/172:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof['name'])

# df = pd.DataFrame()
406/173:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof['councils'])

# df = pd.DataFrame()
406/174:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(health_prof['registration_no'])

# df = pd.DataFrame()
406/175:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

print(json.dumps(health_prof))

# df = pd.DataFrame()
406/176:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# print(json.dumps(health_prof))

print(health_prof['name'])

# df = pd.DataFrame()
406/177:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# print(json.dumps(health_prof))

df = pd.DataFrame.from_dict(health_prof)

df.head()
406/178:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# print(json.dumps(health_prof))

print(ps.json_normalize(health_prof))

# df = pd.DataFrame()
406/179:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# print(json.dumps(health_prof))

print(pd.json_normalize(health_prof))

# df = pd.DataFrame()
406/180:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# print(json.dumps(health_prof))

print(pd.json_normalize(health_prof['name']))

# df = pd.DataFrame()
406/181:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# print(json.dumps(health_prof))

print(pd.json_normalize(health_prof))

# df = pd.DataFrame()
406/182:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

heath_info_json = json.dumps(health_prof);

# print(json.dumps(health_prof))
with open('health_info.json', 'w', encoding='utf-8') as f:
    json.dump(heath_info_json, f, ensure_ascii=False, indent=4)


# df = pd.DataFrame()
406/183: pd.DataFrame(health_prof)
406/184:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

health_prof



# df = pd.DataFrame()
406/185:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof



df = pd.DataFrame(health_prof)

df.head()
406/186:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df = pd.DataFrame.from_dict(health_prof, orient='columns')

df.head()
406/187:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df = pd.DataFrame.from_dict(names, orient='columns')

df.head()
406/188:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')

df_names.head()
406/189:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
license_no = pd.DataFrame.from_dict(license_no, orient='columns')

df_names.head()
df_councils.head()
df_registration_no.head()
df_registration_date.head()
license_no.head()
406/190:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')

df_names.head()
df_councils.head()
df_registration_no.head()
df_registration_date.head()
df_license_no.head()
406/191:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')


df_license_no.head()
406/192:
health_p_df = pd.DataFrame({
    'health_info': header_list
})

health_p_df.T.to_csv('health_p_df.csv')
406/193: health_p_df.head(5)
406/194: health_p_df.T
406/195:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:19]: header_list, header_list[4][0:15]: header_list})

health_p_df.head()
406/196:
from bs4 import BeautifulSoup

import pandas as pd
import requests
import json
import re
406/197:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/198: remove_html_tags('<h5><b>Name: </b>JASCINTA AACA</h5>')
406/199:


url = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page=1'

res = requests.get(url)

soup = BeautifulSoup(res.text, 'html.parser')

all_headers = soup.find_all('h5')

print(all_headers[6])

# header_list = [];

for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(header_list)

# print(json.dumps(header_list))
406/200:
print(header_list[5])

print(len(header_list))
406/201:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')


df_license_no.head()
406/202:
health_p_df = pd.DataFrame({
    'health_info': header_list
})

health_p_df.T.to_csv('health_p_df.csv')
406/203: health_p_df.head(5)
406/204: health_p_df.T
406/205:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:19]: header_list, header_list[4][0:15]: header_list})

health_p_df.head()
406/206:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')


df_councils.head()
406/207:


health_prof = {} 

names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    health_prof['name'] = names,
    health_prof['councils'] = councils,
    health_prof['registration_no'] = registration_no,
    health_prof['registration_date'] = registration_date,
    health_prof['license_no'] = license_no

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
406/208:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
406/209:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
406/210:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
406/211: df_merged = df_names.merge(df_councils)
406/212:
health_p_df = pd.DataFrame({
    'health_info': header_list
})

health_p_df.T.to_csv('health_p_df.csv')
406/213: df_merged = df_names.merge(df_councils)
406/214: df_merged
406/215:
df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged.head()
406/216:
df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_2, df_merged_3, left_index=True, right_index=True)

df_final_merge.head()
406/217:
df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_2, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/218:
df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_2, df_merged_3, left_index=True, right_index=True)

df_final_merge.sample(frac=0.5)
406/219:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/220:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/221:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/222:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/223:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Endpoint Reached ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/224:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
406/225:


df_final_merge.T.to_csv('health_p_df.csv')
406/226:


df_final_merge.to_csv('health_p_df.csv')
406/227:
df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

df_final_merge.sample(frac=0.5)
406/228:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/229:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/230:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Endpoint Reached ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/231:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
406/232:
df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

df_final_merge.sample(frac=0.5)
406/233:


df_final_merge.to_csv('health_p_df.csv')
406/234: health_p_df.head(5)
406/235: health_p_df.T
406/236:
health_p_df = pd.DataFrame({header_list[0][0:5]: header_list, header_list[1][0:8]: header_list, header_list[2][0:17]: header_list, header_list[3][0:19]: header_list, header_list[4][0:15]: header_list})

health_p_df.head()
406/237:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/238:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/239:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Endpoint Reached ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/240:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
406/241:
df_merged = pd.merge(df_names, df_councils, suffixes=('', '__2'), left_index=True, right_index=True)

# df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

# df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

# df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

df_final_merge.sample(frac=0.5)
406/242:


# df_final_merge.to_csv('health_p_df.csv')
406/243:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/244:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/245:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Endpoint Reached ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/246:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
406/247:
df_merged = pd.merge(df_names, df_councils, suffixes=('', '__2'), left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

df_final_merge.sample(frac=0.5)
406/248:

df_final_merge.to_csv('health_p_df.csv')
406/249:
df_merged = pd.merge(df_names, df_councils, suffixes=('', '__2'), left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

df_final_merge.head()
406/250:
df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

df_final_merge.head()
406/251:
df_merged = pd.merge(df_names, df_councils, df_registration_no, df_registration_date, df_license_no, on='index')

df_merged.head()

# df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

# df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

# df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

# df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

# df_final_merge.head()
406/252:
df_merged = pd.merge(df_names, df_councils, df_registration_no, df_registration_date, df_license_no)

df_merged.head()

# df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

# df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

# df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

# df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

# df_final_merge.head()
406/253:
df_merged = pd.concat([df_names, df_councils, df_registration_no, df_registration_date, df_license_no])

df_merged.head()

# df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

# df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

# df_merged_3 = pd.merge(df_merged, df_license_no, left_index=True, right_index=True)

# df_final_merge = pd.merge(df_merged_3, df_merged_2, left_index=True, right_index=True)

# df_final_merge.head()
406/254:

df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged_3, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged, df_merged_2, left_index=True, right_index=True)

df_final_merge.head()
406/255:

df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged_2, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged, df_merged_2, left_index=True, right_index=True)

df_final_merge.head()
406/256:

df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged_2, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged, df_merged_3, left_index=True, right_index=True)

df_final_merge.head()
406/257:

df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged_2, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/258:

df_final_merge.to_csv('health_p_df.csv')
406/259:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][16:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')

print(license_expiry_date[1])
406/260:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][12:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')

print(license_expiry_date[1])
406/261:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][18:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')

print(license_expiry_date[1])
406/262:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')

print(license_expiry_date[1])
406/263:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_daye = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/264:

df_merged = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_merged_2, df_license_no, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/265:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/266:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/267:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/268:

df_final_merge.to_csv('health_p_df.csv')
406/269:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/270:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/271:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Endpoint Reached ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/272:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/273:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/274:

df_final_merge.to_csv('health_p_df.csv')
406/275:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/276:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    sleep(0.5)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/277:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    sleep(0.05)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/278:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/279:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/280:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    sleep(0.05)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/281:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    sleep(3000)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/282:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    sleep(2000)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/283:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    sleep(2000)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/284:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    # sleep(2000)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/285:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/286:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/287:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    # sleep(2000)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/288:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/289:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/290:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    # sleep(2000)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/291:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/292:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/293:

df_final_merge.to_csv('health_p_df.csv')
406/294:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/295:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/296:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    sleep(2000)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/297:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/298:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/299:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/300:
pagination_max_value = 10


for pagination_value in range(1, pagination_max_value):
    sleep(5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/301:
pagination_max_value = 4500


for pagination_value in range(4440, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/302: 4500-10
406/303:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/304:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/305:
pagination_max_value = 4500


for pagination_value in range(4490, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/306:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/307:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/308:

df_final_merge.to_csv('health_p_df.csv')
406/309: 4500-10
406/310:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/311:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/312:
pagination_max_value = 4501


for pagination_value in range(4495, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/313:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        license_expiry_date.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        license_expiry_date.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/314:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/315:

df_final_merge.to_csv('health_p_df.csv')
406/316: 4500-10
406/317:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        license_expiry_date.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        license_expiry_date.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/318:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/319:

df_final_merge.to_csv('health_p_df.csv')
406/320:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/321:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/322:
pagination_max_value = 4501


for pagination_value in range(4495, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/323:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        license_expiry_date.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        license_expiry_date.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/324:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/325:

df_final_merge.to_csv('health_p_df.csv')
406/326:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/327:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/328:
pagination_max_value = 4501


for pagination_value in range(4495, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/329:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_address = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/330:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_qualification, df_address, left_index=True, right_index=True)

df_merged_5 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_merged_6 = pd.merge(df_merged_3, df_merged_4, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_5, df_merged_6, left_index=True, right_index=True)

df_final_merge.tail()
406/331:

df_final_merge.to_csv('health_p_df.csv')
406/332:

df_final_merge.to_csv('health_p_df.csv')
406/333:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/334:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/335:
pagination_max_value = 4501


for pagination_value in range(4495, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/336:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_address = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
406/337:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_qualification, df_address, left_index=True, right_index=True)

df_merged_5 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_merged_6 = pd.merge(df_merged_3, df_merged_4, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_5, df_merged_6, left_index=True, right_index=True)

df_final_merge.tail()
406/338:

df_final_merge.to_csv('health_p_df.csv')
406/339:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/340:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/341:
pagination_max_value = 4501


for pagination_value in range(4495, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/342:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/343:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/344:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/345:
pagination_max_value = 4501


for pagination_value in range(4495, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/346:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/347:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_qualification, df_address, left_index=True, right_index=True)

df_merged_5 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_merged_6 = pd.merge(df_merged_3, df_merged_4, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_5, df_merged_6, left_index=True, right_index=True)

df_final_merge.tail()
406/348:

df_final_merge.to_csv('health_p_df.csv')
406/349:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/350:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/351:
pagination_max_value = 4501


for pagination_value in range(4495, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/352:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/353:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_qualification, df_address, left_index=True, right_index=True)

df_merged_5 = pd.merge(df_merged_1, df_merged_2)

df_merged_6 = pd.merge(df_merged_3, df_merged_4)

df_final_merge = pd.merge(df_merged_5, df_merged_6)

df_final_merge.tail()
406/354:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/355:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/356:
pagination_max_value = 4501


for pagination_value in range(4495, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/357:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/358:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/359:

df_final_merge.to_csv('health_p_df.csv')
406/360:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/361:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/362:
pagination_max_value = 4501


for pagination_value in range(1, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/363:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/364:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/365:
pagination_max_value = 4501


for pagination_value in range(1, pagination_max_value):
    sleep(3)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/366:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/367:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
406/368:

df_final_merge.to_csv('health_p_df.csv')
406/369:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/370:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/371:
pagination_max_value = 4181


for pagination_value in range(1, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/372:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/373:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/374:
pagination_max_value = 2536


for pagination_value in range(0, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
404/35:
from sklearn.datasets 
import load_digits
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

import matplotlib.pyplot as plt
import seaborn as sns
404/36:
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

import matplotlib.pyplot as plt
import seaborn as sns
406/375:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/376:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge.tail()
406/377:

df_final_merge.to_csv('nurses.csv')
406/378:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/379:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/380:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/381:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/382:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge.tail()
406/383:

df_final_merge.to_csv('nurses.csv')
406/384:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/385:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/386:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/387:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/388:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/389:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/390:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/391:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge.tail()
406/392:

df_final_merge.to_csv('nurses.csv')
406/393:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = [df_names, df_councils]

df_final_merge.tail()
406/394:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(df_names, df_councils, on='index')

df_final_merge.tail()
406/395:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(df_names, df_councils, left_index=True)

df_final_merge.tail()
406/396:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/397:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/398:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/399:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/400:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(df_names, df_councils, left_index=True)

df_final_merge.tail()
406/401:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_final_merge.tail()
406/402:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True)

df_final_merge.tail()
406/403: 50701 - 50714
406/404:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True)

df_final_merge.tail()

df_final_merge.shape
406/405:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/406:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/407:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/408:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/409:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_final_merge.tail()
406/410:

df_final_merge.to_csv('nurses.csv')
406/411: 50701 - 50714
406/412:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_final_merge.tail()
406/413:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/414:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/415:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/416:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/417:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/418:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/419:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_final_merge.tail()
406/420:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/421:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/422:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_final_merge.tail()
406/423:

df_final_merge.to_csv('nurses.csv')
406/424: 50701 - 50714
406/425:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/426:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/427:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/428:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_address.head()
406/429:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_final_merge.tail()
406/430:

df_final_merge.to_csv('nurses.csv')
406/431: 50701 - 50714
406/432:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/433:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/434:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/435:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_address.head()
406/436:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=False)

df_final_merge.tail()
406/437:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/438:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/439:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/440:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_license_expiry_date.head()
406/441:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=False)

df_final_merge.tail()
406/442:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/443:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/444:
pagination_max_value = 2536


for pagination_value in range(2534, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/445: print(header_list)
406/446:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_license_expiry_date.head()
406/447:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge = pd.merge(df_final_merge, df_address, left_index=True, right_index=True)

df_final_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=False)

df_final_merge.tail()
406/448: print(header_list[-1])
406/449: print(header_list)
406/450:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_merge.tail()
406/451:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/452:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/453:
pagination_max_value = 2536


for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/454: print(header_list)
406/455:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_license_expiry_date.head()
406/456:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_merge.tail()
406/457:

df_final_merge.to_csv('nurses.csv')
406/458: 50701 - 50714
406/459:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/460:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/461:
pagination_max_value = 2537


for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/462: print(header_list)
406/463:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_license_expiry_date.head()
406/464:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_merge.tail()
406/465:

df_final_merge.to_csv('nurses.csv')
406/466: 50701 - 50714
406/467:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/468:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/469:
pagination_max_value = 2537


for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page=2536'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/470: print(header_list)
406/471:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_license_expiry_date.head()
406/472:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_merge.tail()
406/473:

df_final_merge.to_csv('nurses.csv')
406/474:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/475:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/476:
pagination_max_value = 2537
header_list = []


for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page=2536'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/477: print(header_list)
406/478:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_license_expiry_date.head()
406/479:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_merge.tail()
406/480:

df_final_merge.to_csv('nurses.csv')
406/481:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/482:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/483:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/484: print(header_list)
406/485:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_license_expiry_date.head()
406/486:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_merge.tail()
406/487:

df_final_merge.to_csv('nurses.csv')
406/488:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/489:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/490:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/491: print(header_list)
406/492:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_license_expiry_date.head()
406/493:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_merge.tail()
406/494:

df_final_merge.to_csv('nurses.csv')
406/495:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(all_headers)

# print(json.dumps(header_list))
406/496:
pagination_max_value = 2537
header_list = []

# for pagination_value in range(2535, pagination_max_value):
#     sleep(1.5)

end_point = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page=2536'

res = requests.get(url)
print('Data Retrieved from page ', pagination_value)

soup = BeautifulSoup(res.text, 'html.parser')
all_headers = soup.find_all('h5')
    


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(all_headers)

# print(json.dumps(header_list))
406/497:
pagination_max_value = 2537
header_list = []

# for pagination_value in range(2535, pagination_max_value):
#     sleep(1.5)

end_point = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page=2536'

res = requests.get(url)
print('Data Retrieved from page ', pagination_value)

soup = BeautifulSoup(res.text, 'html.parser')
all_headers = soup.find_all('h5')

print(soup.prettify())
    


# for item in all_headers:
#     item = remove_html_tags(f"{item}")
#     header_list.append(item)

# print(all_headers)

# print(json.dumps(header_list))
406/498:
pagination_max_value = 2537
header_list = []

# for pagination_value in range(2535, pagination_max_value):
#     sleep(1.5)

end_point = 'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page=2535'

res = requests.get(url)
print('Data Retrieved from page ', pagination_value)

soup = BeautifulSoup(res.text, 'html.parser')
all_headers = soup.find_all('h5')

print(soup.prettify())
    


# for item in all_headers:
#     item = remove_html_tags(f"{item}")
#     header_list.append(item)

# print(all_headers)

# print(json.dumps(header_list))
406/499:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/500: print(header_list)
406/501:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][7:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_address.head()
406/502:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][5:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_address.head()
406/503:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_address.head()
406/504:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_qualification.head()
406/505:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/506: print(header_list)
406/507:
pagination_max_value = 1
header_list = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/508:
pagination_max_value = 2
header_list = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/509: print(header_list)
406/510:
pagination_max_value = 2
header_list = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/511: print(header_list)
406/512:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_qualification.head()
406/513:
pagination_max_value = 1000
header_list = []

for pagination_value in range(998, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/514: print(header_list)
406/515:
pagination_max_value = 12
header_list = []

for pagination_value in range(8, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/516:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_qualification.head()
406/517:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_qualification.head()
406/518:
pagination_max_value = 12
header_list = []

for pagination_value in range(8, pagination_max_value):
    sleep(1.5)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)
    
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
406/519:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(json.dumps(header_list))
406/520:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/521:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/522:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/523:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/524:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(json.dumps(header_list))
406/525:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/526:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/527:

df_final_merge.to_csv('nurses.csv')
406/528:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/529:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/530:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5BissuerId%5D=2&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(json.dumps(header_list))
406/531:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/532:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/533:

df_merge.to_csv('nurses.csv')
406/534:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/535:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/536:
pagination_max_value = 2537
header_list = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(json.dumps(header_list))
406/537:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/538:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/539:

df_merge.to_csv('nurses.csv')
406/540:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/541:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/542:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers.append(soup.find_all('h5'))


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(json.dumps(header_list))
406/543:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/544:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/545:

df_merge.to_csv('nurses.csv')
406/546:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/547:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/548:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
406/549:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/550:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/551:

df_merge.to_csv('nurses.csv')
406/552:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/553:
df_merge.rename({'0_x': 'Name', '0_y': 'Council', '0_x': 'Registration Number', '0_y': 'Registration Date', '0_x': 'License Number', '0_y': 'License Expiry Date', '0': 'Address'})

df_merge.head()
406/554:
df_merge.rename({'0_x': 'Name', '0_y': 'Council', '0_x': 'Registration Number', '0_y': 'Registration Date', '0_x': 'License Number', '0_y': 'License Expiry Date', '0': 'Address'})

df_merge.head()
406/555:
df_merge.rename(columns={'0_x': 'Name', '0_y': 'Council', '0_x': 'Registration Number', '0_y': 'Registration Date', '0_x': 'License Number', '0_y': 'License Expiry Date', '0': 'Address'})

df_merge.head()
406/556:
df_merge.rename(columns={"0_x": "Name", '0_y': 'Council', '0_x': 'Registration Number', '0_y': 'Registration Date', '0_x': 'License Number', '0_y': 'License Expiry Date', '0': 'Address'})

df_merge.head()
406/557:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"})

df_merge.head()
406/558:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/559:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/560:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
406/561:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/562:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/563:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"})

df_merge.head()
406/564:

df_merge.to_csv('nurses.csv')
406/565:
df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"])

df_merge.head()
406/566:
df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]

df_merge.head()
406/567:

df_merge.to_csv('nurses.csv')
406/568:
df_merge = df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]

df_merge.head()
406/569:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/570:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/571:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
406/572:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/573:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/574:
df_merge = df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]

df_merge.head()
406/575:
df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]

df_merge.head()
406/576:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/577:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/578:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
406/579:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/580:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/581:
df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]

df_merge.head()
406/582:

df_merge.to_csv('nurses.csv')
406/583:
df_merge = df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"})

df_merge.head()
406/584:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/585:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/586:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
406/587:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/588:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/589:
df_merge = df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"})

df_merge.head()
406/590: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
406/591:

df_merge.to_csv('nurses.csv')
406/592:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/593:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/594:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
406/595:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/596:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/597:
df_merge = df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"})

df_merge.head()
406/598: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
406/599:

df_merge.to_csv('nurses.csv')
406/600:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/601:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/602:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(2535, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
406/603:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/604:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/605:
df_merge = df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"})

df_merge.head()
406/606: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
406/607:

df_merge.to_csv('nurses.csv')
406/608:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
406/609: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
406/610:

df_merge.to_csv('nurses.csv')
406/611:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
406/612:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
406/613:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
406/614:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
406/615:
pagination_max_value = 2537
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    print(end_point)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
406/616:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
406/617:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
406/618:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
406/619: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
406/620:

df_merge.to_csv('data/nurses.csv')
407/1:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/2:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/3:
pagination_max_value = 11
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/4:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_names.shape
407/5:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/6:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/7:
pagination_max_value = 11
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/8:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    # health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_names.shape
407/9:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
407/10:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
407/11: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
407/12:

df_merge.to_csv('data/nurses.csv')
407/13:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/14:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/15:
pagination_max_value = 11
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/16:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    # health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
407/17:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
407/18:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
407/19: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
407/20:

df_merge.to_csv('data/nurses.csv')
407/21:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/22:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/23:
pagination_max_value = 11
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/24:

health_prof = []

names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
407/25:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/26:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/27:
pagination_max_value = 11
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/28:

health_prof = []

names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    # health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')
407/29:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.tail()
407/30:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
407/31: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
407/32:

df_merge.to_csv('data/nurses.csv')
407/33:

health_prof = []

names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    # health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_names[:5]

print('\n')

df_councils[:5]

print('\n'

df_registration_no[:5]
407/34:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/35:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/36:
pagination_max_value = 11
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/37:

health_prof = []

names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    # health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_names.head()
407/38:

df_councils.head()
407/39:

df_registration_no.head()
407/40:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.shape

df_merge.tail()
407/41:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
407/42: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
407/43:

df_merge.to_csv('data/nurses.csv')
407/44:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/45:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/46:
pagination_max_value = 11
header_list = []
all_headers = []

for pagination_value in range(10, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/47:

health_prof = []

names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    # health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_names.head()
407/48:

df_councils.head()
407/49:

df_registration_no.head()
407/50: df_registration_date.head()
407/51: df_license_no.head()
407/52: df_license_expiry_date.head()
407/53: df_qualification.head()
407/54: df_address.head()
407/55:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True), df_address, left_index=True, right_index=True)

df_merge.shape

df_merge.tail()
407/56:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
407/57: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
407/58:

df_merge.to_csv('data/nurses.csv')
407/59:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/60:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/61:
pagination_max_value = 11
header_list = []
all_headers = []

for pagination_value in range(10, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/62:

health_prof = []

names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []
qualification = []
address = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])
    if 'Qualifications:' in header_list[i]:
        qualification.append(header_list[i][14:])
    if 'Address:' in header_list[i]:
        address.append(header_list[i][9:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    # health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
df_address = pd.DataFrame.from_dict(address, orient='columns')

df_names.head()
407/63:

df_councils.head()
407/64:

df_registration_no.head()
407/65: df_registration_date.head()
407/66: df_license_no.head()
407/67: df_license_expiry_date.head()
407/68: df_qualification.head()
407/69: df_address.head()
407/70:

df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

df_merge.shape

df_merge.tail()
407/71:
df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

df_merge.head()
407/72: # df_merge.columns= ["Name", "Council", "Registration Number", "Registration Date", "License Number", "License Expiry Date", "Address"]
407/73:

df_merge.to_csv('data/nurses.csv')
407/74:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/75:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/76:
pagination_max_value = 1000
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(1.2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
407/77:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
407/78:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
407/79:
pagination_max_value = 1000
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

# print(json.dumps(header_list))
404/37:
def find_misses(test, pred):
    return [i for i, row in enumerate(test) if row != pred[i]]

if __name__ == '__main__':
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    gnb = GuassianNB().fit(X_train, y_train)
    gnb_name = gnb.__class__.__name__
    y_pred = gnb.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(gnb_name + '\'test\' accuracy: ', accuracy)

    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)

    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_name = sgd.__class__.__name__
    sgd.fit(X_test_std, y_tain)
    y_pred = sgd.predict(X_test_std)

    accuracy = accuracy_score(y_test, y_pred)
    print(sgd_name + '\'test\' accuracy: ', accuracy)

    svm = SVC(gamma='auto').fit(X_train_std, y_tain)
    svm_name = svm.__class__.__name__
    y_pred = svm.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print (svm_name + ' \'test\' accuracy:', accuracy, br)
    indx = find_misses(y_test, y_pred)

    print ('total misclassifications (' + str(svm_name) +\ '):', len(indx), br)
    print ('pred', 'actual')    
    misses = [(y_pred[row], y_test[row], i)for i, row in enumerate(indx)]    
    [print (row[0], '  ', row[1]) for row in misses]    
    img_indx = misses[0][2]    
    img_pred = misses[0][0]    
    img_act = misses[0][1]    
    text = str(img_pred)    
    print(classification_report(y_test, y_pred))    
    cm = confusion_matrix(y_test, y_pred)    
    plt.figure(1)    
    ax = plt.axes()    
    sns.heatmap(cm.T, annot=True, fmt="d",cmap='gist_ncar_r', ax=ax)    
    title = svm_name + ' confusion matrix'    
    ax.set_title(title)    
    plt.xlabel('true value')    
    plt.ylabel('predicted value')    
    test_images = X_test.reshape(-1, 8, 8)    
    plt.figure(2)
    plt.title('1st misclassifcation')
    plt.imshow(test_images[img_indx], cmap='gray', interpolation='gaussian')
    plt.text(0, 0.05, text, color='r', bbox=dict(facecolor='white'))
    plt.show()
404/38:
def find_misses(test, pred):
    return [i for i, row in enumerate(test) if row != pred[i]]

if __name__ == '__main__':
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    gnb = GuassianNB().fit(X_train, y_train)
    gnb_name = gnb.__class__.__name__
    y_pred = gnb.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(gnb_name + '\'test\' accuracy: ', accuracy)

    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)

    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_name = sgd.__class__.__name__
    sgd.fit(X_test_std, y_tain)
    y_pred = sgd.predict(X_test_std)

    accuracy = accuracy_score(y_test, y_pred)
    print(sgd_name + '\'test\' accuracy: ', accuracy)

    svm = SVC(gamma='auto').fit(X_train_std, y_tain)
    svm_name = svm.__class__.__name__
    y_pred = svm.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print (svm_name + ' \'test\' accuracy:', accuracy, br)
    indx = find_misses(y_test, y_pred)

    print ('total misclassifications (' + str(svm_name) + '\'):', len(indx), br)
    print ('pred', 'actual')    
    misses = [(y_pred[row], y_test[row], i)for i, row in enumerate(indx)]    
    [print (row[0], '  ', row[1]) for row in misses]    
    img_indx = misses[0][2]    
    img_pred = misses[0][0]    
    img_act = misses[0][1]    
    text = str(img_pred)    
    print(classification_report(y_test, y_pred))    
    cm = confusion_matrix(y_test, y_pred)    
    plt.figure(1)    
    ax = plt.axes()    
    sns.heatmap(cm.T, annot=True, fmt="d",cmap='gist_ncar_r', ax=ax)    
    title = svm_name + ' confusion matrix'    
    ax.set_title(title)    
    plt.xlabel('true value')    
    plt.ylabel('predicted value')    
    test_images = X_test.reshape(-1, 8, 8)    
    plt.figure(2)
    plt.title('1st misclassifcation')
    plt.imshow(test_images[img_indx], cmap='gray', interpolation='gaussian')
    plt.text(0, 0.05, text, color='r', bbox=dict(facecolor='white'))
    plt.show()
404/39:
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

import matplotlib.pyplot as plt
import seaborn as sns
404/40:
def find_misses(test, pred):
    return [i for i, row in enumerate(test) if row != pred[i]]

if __name__ == '__main__':
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    gnb = GuassianNB().fit(X_train, y_train)
    gnb_name = gnb.__class__.__name__
    y_pred = gnb.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(gnb_name + '\'test\' accuracy: ', accuracy)

    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)

    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_name = sgd.__class__.__name__
    sgd.fit(X_test_std, y_tain)
    y_pred = sgd.predict(X_test_std)

    accuracy = accuracy_score(y_test, y_pred)
    print(sgd_name + '\'test\' accuracy: ', accuracy)

    svm = SVC(gamma='auto').fit(X_train_std, y_tain)
    svm_name = svm.__class__.__name__
    y_pred = svm.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print (svm_name + ' \'test\' accuracy:', accuracy, br)
    indx = find_misses(y_test, y_pred)

    print ('total misclassifications (' + str(svm_name) + '\'):', len(indx), br)
    print ('pred', 'actual')    
    misses = [(y_pred[row], y_test[row], i)for i, row in enumerate(indx)]    
    [print (row[0], '  ', row[1]) for row in misses]    
    img_indx = misses[0][2]    
    img_pred = misses[0][0]    
    img_act = misses[0][1]    
    text = str(img_pred)    
    print(classification_report(y_test, y_pred))    
    cm = confusion_matrix(y_test, y_pred)    
    plt.figure(1)    
    ax = plt.axes()    
    sns.heatmap(cm.T, annot=True, fmt="d",cmap='gist_ncar_r', ax=ax)    
    title = svm_name + ' confusion matrix'    
    ax.set_title(title)    
    plt.xlabel('true value')    
    plt.ylabel('predicted value')    
    test_images = X_test.reshape(-1, 8, 8)    
    plt.figure(2)
    plt.title('1st misclassifcation')
    plt.imshow(test_images[img_indx], cmap='gray', interpolation='gaussian')
    plt.text(0, 0.05, text, color='r', bbox=dict(facecolor='white'))
    plt.show()
404/41:
def find_misses(test, pred):
    return [i for i, row in enumerate(test) if row != pred[i]]

if __name__ == '__main__':
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    gnb = GaussianNB().fit(X_train, y_train)
    gnb_name = gnb.__class__.__name__
    y_pred = gnb.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(gnb_name + '\'test\' accuracy: ', accuracy)

    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)

    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_name = sgd.__class__.__name__
    sgd.fit(X_test_std, y_tain)
    y_pred = sgd.predict(X_test_std)

    accuracy = accuracy_score(y_test, y_pred)
    print(sgd_name + '\'test\' accuracy: ', accuracy)

    svm = SVC(gamma='auto').fit(X_train_std, y_tain)
    svm_name = svm.__class__.__name__
    y_pred = svm.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print (svm_name + ' \'test\' accuracy:', accuracy, br)
    indx = find_misses(y_test, y_pred)

    print ('total misclassifications (' + str(svm_name) + '\'):', len(indx), br)
    print ('pred', 'actual')    
    misses = [(y_pred[row], y_test[row], i)for i, row in enumerate(indx)]    
    [print (row[0], '  ', row[1]) for row in misses]    
    img_indx = misses[0][2]    
    img_pred = misses[0][0]    
    img_act = misses[0][1]    
    text = str(img_pred)    
    print(classification_report(y_test, y_pred))    
    cm = confusion_matrix(y_test, y_pred)    
    plt.figure(1)    
    ax = plt.axes()    
    sns.heatmap(cm.T, annot=True, fmt="d",cmap='gist_ncar_r', ax=ax)    
    title = svm_name + ' confusion matrix'    
    ax.set_title(title)    
    plt.xlabel('true value')    
    plt.ylabel('predicted value')    
    test_images = X_test.reshape(-1, 8, 8)    
    plt.figure(2)
    plt.title('1st misclassifcation')
    plt.imshow(test_images[img_indx], cmap='gray', interpolation='gaussian')
    plt.text(0, 0.05, text, color='r', bbox=dict(facecolor='white'))
    plt.show()
404/42:
def find_misses(test, pred):
    return [i for i, row in enumerate(test) if row != pred[i]]

if __name__ == '__main__':
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    gnb = GaussianNB().fit(X_train, y_train)
    gnb_name = gnb.__class__.__name__
    y_pred = gnb.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(gnb_name + '\'test\' accuracy: ', accuracy)

    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)

    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_name = sgd.__class__.__name__
    sgd.fit(X_train_std, y_train)
    y_pred = sgd.predict(X_test_std)

    accuracy = accuracy_score(y_test, y_pred)
    print(sgd_name + '\'test\' accuracy: ', accuracy)

    svm = SVC(gamma='auto').fit(X_train_std, y_tain)
    svm_name = svm.__class__.__name__
    y_pred = svm.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print (svm_name + ' \'test\' accuracy:', accuracy, br)
    indx = find_misses(y_test, y_pred)

    print ('total misclassifications (' + str(svm_name) + '\'):', len(indx), br)
    print ('pred', 'actual')    
    misses = [(y_pred[row], y_test[row], i)for i, row in enumerate(indx)]    
    [print (row[0], '  ', row[1]) for row in misses]    
    img_indx = misses[0][2]    
    img_pred = misses[0][0]    
    img_act = misses[0][1]    
    text = str(img_pred)    
    print(classification_report(y_test, y_pred))    
    cm = confusion_matrix(y_test, y_pred)    
    plt.figure(1)    
    ax = plt.axes()    
    sns.heatmap(cm.T, annot=True, fmt="d",cmap='gist_ncar_r', ax=ax)    
    title = svm_name + ' confusion matrix'    
    ax.set_title(title)    
    plt.xlabel('true value')    
    plt.ylabel('predicted value')    
    test_images = X_test.reshape(-1, 8, 8)    
    plt.figure(2)
    plt.title('1st misclassifcation')
    plt.imshow(test_images[img_indx], cmap='gray', interpolation='gaussian')
    plt.text(0, 0.05, text, color='r', bbox=dict(facecolor='white'))
    plt.show()
404/43:
import humanfriendly as hf
import timefrom sklearn.datasets 
import load_digits
from sklearn.model_selection 
import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_scorefrom sklearn.metrics import f1_score
404/44:
import humanfriendly as hf
import time
from sklearn.datasets 
import load_digits
from sklearn.model_selection 
import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_scorefrom sklearn.metrics import f1_score
404/45:
import humanfriendly as hf
import time
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_scorefrom sklearn.metrics import f1_score
404/46:
import humanfriendly as hf
import time
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
404/47:
import humanfriendly as hf
import time
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
407/80: df_merge
407/81: df_merge.shape
407/82:
pagination_max_value = 1000
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

    
    health_prof = []

    names = []
    councils = []
    registration_no = []
    registration_date = []
    license_no = []
    license_expiry_date = []
    qualification = []
    address = []

    counter = 0

    for i in range(len(header_list)):
        # print(header_list[i])
        if 'Name:' in header_list[i]:
            names.append(header_list[i][6:])
        if 'Council:' in header_list[i]:
            councils.append(header_list[i][9:])
        if 'Registration No :' in header_list[i]:
            registration_no.append(header_list[i][18:])
        if 'Registration Date :' in header_list[i]:
            registration_date.append(header_list[i][20:])
        if 'License Number:' in header_list[i]:
            license_no.append(header_list[i][16:])
        if 'License Expiry Date:' in header_list[i]:
            license_expiry_date.append(header_list[i][21:])
        if 'Qualifications:' in header_list[i]:
            qualification.append(header_list[i][14:])
        if 'Address:' in header_list[i]:
            address.append(header_list[i][9:])

        counter += 1;
        if(counter == 5):
            # print('============')
            counter = 0

        # health_prof[i] = [names, councils, registration_no, license_no]

    # health_prof

    df_names = pd.DataFrame.from_dict(names, orient='columns')
    df_councils = pd.DataFrame.from_dict(councils, orient='columns')
    df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
    df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
    df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
    df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
    df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
    df_address = pd.DataFrame.from_dict(address, orient='columns')

    df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

    df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

    df_merge.to_csv(f'/data/nurses_{pagination_value}.csv')


# print(json.dumps(header_list))
407/83:
pagination_max_value = 1000
header_list = []
all_headers = []

for pagination_value in range(0, pagination_max_value):
    sleep(2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=3&LicenseSearch%5Bq%5D=&page={pagination_value}'

    res = requests.get(end_point)

    print('Data Retrieved from page ', pagination_value)

    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


    for item in all_headers:
        item = remove_html_tags(f"{item}")
        header_list.append(item)

    
    health_prof = []

    names = []
    councils = []
    registration_no = []
    registration_date = []
    license_no = []
    license_expiry_date = []
    qualification = []
    address = []

    counter = 0

    for i in range(len(header_list)):
        # print(header_list[i])
        if 'Name:' in header_list[i]:
            names.append(header_list[i][6:])
        if 'Council:' in header_list[i]:
            councils.append(header_list[i][9:])
        if 'Registration No :' in header_list[i]:
            registration_no.append(header_list[i][18:])
        if 'Registration Date :' in header_list[i]:
            registration_date.append(header_list[i][20:])
        if 'License Number:' in header_list[i]:
            license_no.append(header_list[i][16:])
        if 'License Expiry Date:' in header_list[i]:
            license_expiry_date.append(header_list[i][21:])
        if 'Qualifications:' in header_list[i]:
            qualification.append(header_list[i][14:])
        if 'Address:' in header_list[i]:
            address.append(header_list[i][9:])

        counter += 1;
        if(counter == 5):
            # print('============')
            counter = 0

        # health_prof[i] = [names, councils, registration_no, license_no]

    # health_prof

    df_names = pd.DataFrame.from_dict(names, orient='columns')
    df_councils = pd.DataFrame.from_dict(councils, orient='columns')
    df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
    df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
    df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
    df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
    df_qualification = pd.DataFrame.from_dict(qualification, orient='columns')
    df_address = pd.DataFrame.from_dict(address, orient='columns')

    df_merge = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_names, df_councils, left_index=True, right_index=True), df_registration_no, left_index=True, right_index=True), df_registration_date, left_index=True, right_index=True), df_license_no, left_index=True, right_index=True), df_license_expiry_date, left_index=True, right_index=True)

    df_merge.rename(columns={"0_x": "Name", "0_y": "Council", "0_x": "Registration Number", "0_y": "Registration Date", "0_x": "License Number", "0_y": "License Expiry Date", "0": "Address"}, inplace=True)

    df_merge.to_csv(f'data/nurses_{pagination_value}.csv')


# print(json.dumps(header_list))
409/1:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
409/2:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
409/3:
pagination_max_value = 4500


for pagination_value in range(0, pagination_max_value):
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')
    # sleep(2000)


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
409/4:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
409/5:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
409/6:
pagination_max_value = 4500


for pagination_value in range(0, pagination_max_value):
    sleep(2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
409/7:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
409/8:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
409/9:
pagination_max_value = 4


for pagination_value in range(0, pagination_max_value):
    sleep(2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

print(len(header_list))

# print(json.dumps(header_list))
409/10:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
409/11:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
409/12:
pagination_max_value = 4
header_list = []

for pagination_value in range(0, pagination_max_value):
    sleep(2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(json.dumps(header_list))
409/13:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

    health_prof[i] = [names, councils, registration_no, license_no]

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
409/14:
from bs4 import BeautifulSoup
from time import sleep

import pandas as pd
import requests
import json
import re
409/15:
def remove_html_tags(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)
409/16:
pagination_max_value = 4
header_list = []

for pagination_value in range(0, pagination_max_value):
    sleep(2)
    end_point = f'https://www.ehealthlicense.go.ug/index.php/search/cadre?LicenseSearch%5BissuerId%5D=&LicenseSearch%5Bq%5D=&page={pagination_value}'

    print('Data Retrieved from page ', pagination_value)

    res = requests.get(end_point)
    soup = BeautifulSoup(res.text, 'html.parser')
    all_headers = soup.find_all('h5')


for item in all_headers:
    item = remove_html_tags(f"{item}")
    header_list.append(item)

# print(json.dumps(header_list))
409/17:


names = []
councils = []
registration_no = []
registration_date = []
license_no = []
license_expiry_date = []

counter = 0

for i in range(len(header_list)):
    # print(header_list[i])
    if 'Name:' in header_list[i]:
        names.append(header_list[i][6:])
    if 'Council:' in header_list[i]:
        councils.append(header_list[i][9:])
    if 'Registration No :' in header_list[i]:
        registration_no.append(header_list[i][18:])
    if 'Registration Date :' in header_list[i]:
        registration_date.append(header_list[i][20:])
    if 'License Number:' in header_list[i]:
        license_no.append(header_list[i][16:])
    if 'License Expiry Date:' in header_list[i]:
        license_expiry_date.append(header_list[i][21:])

    counter += 1;
    if(counter == 5):
        # print('============')
        counter = 0

# health_prof

df_names = pd.DataFrame.from_dict(names, orient='columns')
df_councils = pd.DataFrame.from_dict(councils, orient='columns')
df_registration_no = pd.DataFrame.from_dict(registration_no, orient='columns')
df_registration_date = pd.DataFrame.from_dict(registration_date, orient='columns')
df_license_no = pd.DataFrame.from_dict(license_no, orient='columns')
df_license_expiry_date = pd.DataFrame.from_dict(license_expiry_date, orient='columns')
409/18:

df_merged_1 = pd.merge(df_names, df_councils, left_index=True, right_index=True)

df_merged_2 = pd.merge(df_registration_no, df_registration_date, left_index=True, right_index=True)

df_merged_3 = pd.merge(df_license_no, df_license_expiry_date, left_index=True, right_index=True)

df_merged_4 = pd.merge(df_merged_1, df_merged_2, left_index=True, right_index=True)

df_final_merge = pd.merge(df_merged_4, df_merged_3, left_index=True, right_index=True)

df_final_merge.tail()
409/19:

df_final_merge.to_csv('health_p_df.csv')
   1: %history -g
   2: %history -g -f Feature Scalling.ipynb
